[
    {
        "ID": "1",
        "Question": "A coworker of mine believes that any use of in-code comments (ie, not javadoc style method or class comments) is a code smell.  What do you think? ",
        "Best answer": "Only if the comment describes what the code is doing. If I wanted to know what was happening in a method or block, I would read the code. I would hope, anyway, that any developers working on a given project were at least familiar enough with the development language to read what is written and understand what it is doing. In some cases of extreme optimization, you might be using techniques that makes it difficult for someone to follow what your code is doing. In these cases, comments can and should be used to not only explain why you have such optimizations, but what the code is doing. A good rule of thumb would be to have someone else (or multiple other people) familiar with the implementation language and project look at your code - if they can't understand both the why and the how, then you should comment both the why and the how. However, what's not clear in the code is why you have done something. If you take an approach that might not be obvious to others, you should have a comment that explains why you made the decisions that you did. I would suspect that you might not even realize that a comment is needed until after something like a code review, where people want to know why you did X instead of Y - you can capture your answer in the code for everyone else who looks at it in the future. The most important thing, though, is to change your comments when you change your code. If you change an algorithm, be sure to update the comments with why you went with algorithm X over Y. Stale comments are an even bigger code smell. "
    },
    {
        "ID": "4",
        "Question": "When starting a project for a company that's not primarily a programming company, one of the expectations is that there's a finished product at the end free of all bugs and does everything needed right away. However, that's rarely the case.  What are some ways to manage expectations and explain to non-programmers how software development differs from other types of product development? ",
        "Best answer": "Pretty much everybody with a computer has encountered the concept of \"bugs\" these days, so you might start there.  \"What's the most annoying way an application has ever failed on you?  Multiply that by ten, and you'll have our users' experience if we don't devote enough resources to testing and maintenance.\" And don't underestimate the value of establishing a good working relationship with the non-programmers.  If you can establish that your judgment may be trusted, they'll take you seriously when you sound the alarm that X is going to fail spectacularly if you don't do Y pronto, even if they don't completely understand your reasoning. "
    },
    {
        "ID": "9",
        "Question": "Sometimes, the things I have to do for my job are interesting and engaging.  Sometimes, they aren't.  Occasionally, they really aren't. Do you have any particular strategies or tricks for dealing with those dull stretches when your brain would rather be doing anything other than what you're supposed to be doing, particularly when the Internet and its boundless opportunities for interesting Slack beckon? ",
        "Best answer": "Increments. I will pick something that I can do, even if it's a small task (i.e. modify comments in code). Pretty soon, the perfectionist in me wants to do more and more stuff and I'm back rolling. "
    },
    {
        "ID": "18",
        "Question": "At first glance, I'd say a good programming team needs to understand the following matters:  The details of the core language / framework HTML, (asynchronous) javascript SQL / data model Usability / UI design Security Project management.  If this team is not complete: what is needed? ",
        "Best answer": "Customer (or an advocate of some sort).   Basically, someone that can say, \"Yes, you're building what I want and it's going to solve my problem.\" or that the team can go to when they have questions about how something is supposed to work.  That may or may not be the same person that is doing Project Management. Also, you're missing out on a QA person there.  You can't really depend on the developers to adequately test - they're usually too close to the code and the normal response for developers is to test happy path stuff, not doing weird crap like copy pasting War & Peace into a textbox and seeing what the application does. "
    },
    {
        "ID": "38",
        "Question": "Where waterfall commonly includes:  Requirements Functional design Technical design Build Test Review (Deploy)  And iterations across this list. If I forgot a step, let me know. ",
        "Best answer": "It depends on many factors, including:  The domain (eg, I'd spend more time on a defense contract's requirements than I would on a Twitter clone) The developers (if they have experience building similar systems with similar technology, for example, I might spend less time on the design). The customers (How likely the requirements are to change affects how long that phase will take) The criticality of the system (I'll spend more time testing the space shuttle's launch system than I will the StackExchange rep system).  I'd give you a rough breakdown of what percent of the total effort I'd give to each section, but it'd have a margin of error +/- 10% on each based on the above factors so it'd be kinda meaningless. "
    },
    {
        "ID": "39",
        "Question": "What's your favourite quote about programming? One quote per answer, and please check for duplicates before posting! ",
        "Best answer": " Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.    — Brian W. Kernighan "
    },
    {
        "ID": "42",
        "Question": "We've all had them, managers who have either come from sales or last looked at code 10 or more years ago but think they know how to write code. What can I do to give the impression that I'm grateful for their intervention, but keep it as short as possible so I can get on with my work? Or, should I be engaging more with the manager to educate them with modern coding techniques and practices? After all, a manager who understands these will be able to talk sensibly to clients and more senior management when discussing the project and its timescales. ",
        "Best answer": "I say, go ahead and try to engage and educate. If they're honestly trying to help you, the chance to learn something could be valuable to them.  If they're just shoving their nose in for ego or political reasons (\"See, I'm helping, I'm helping!\"), they'll likely get the notion you'll embarrass them if they keep this nonsense up -- or bore them to death with a wall of jargon they're only pretending to understand. And if you've got the dreaded egomaniac who truly thinks they're an expert at your job no matter what evidence you can bring to the contrary, then smile, nod, and make whatever trivial cosmetic changes will make them go the hell away.  And update your resume. "
    },
    {
        "ID": "44",
        "Question": "I am finishing my college degree in programming soon and I'm exploring the next steps to take to further my career. One option I've been considering is getting a certification or a series of certifications in the area of development I want to work in. Are these certifications worth the time and money? Do employers place a lot of value in them? ",
        "Best answer": "The main purpose of certifications is to make money for the certifying body. Having said that, I think certifications are more important the earlier on in your career you are. As a hiring manager, I never use certifications or the lack thereof to filter potential employees, but I do think some companies may look for these as proof that you know what you are doing. Personally, I want the job candidate to show me they can do something (which is a whole other question, I realize!) The more experience you have, the more you can prove by examples that you know what you are doing and the less important certifications become. "
    },
    {
        "ID": "57",
        "Question": "The coding standards for the code hosted in drupal.org suggest to use two spaces to indent the code; other sites suggest to use tabs to indent the code. What is the proper indentation character for everything, and in every situation? Please explain the answer you give. ",
        "Best answer": "Tabs Now, of course, consistency matters more than either one, and a good IDE makes the differences negligible.  That said, the point of this thread is to be a holy war, so: I prefer tabs:  They're a character specifically meant for indentation They allow developers with different preferences in indentation size to change how it's the code looks without changing the code (separation of data and presentation for the proverbial win!) It's impossible to half-indent something with tabs.  So when you copy code from some website that used 3 spaces into your 4-space indented file, you don't have to deal with misalignment.  "
    },
    {
        "ID": "73",
        "Question": "We often receive last minute requests from the business asking for an extra feature to be implemented.  The project manager is usually responsible for filtering out these requests as \"must haves\" or \"nice to have\", but there are cases where the business wants to squeeze all these features into a release.   Is there a good way to say NO to the business?  What steps can be taken to stop or minimize scope creep? ",
        "Best answer": "Ask them what they want you to drop so you'll have the time to squeeze this latest request in. I've not had to do this for a while, and when I did, I used it sparingly otherwise it loses it's potency. I found it most effective towards the end of the phase when you were tidying stuff up or doing the little improvements and tweaks you'd agreed on in the planning stage. "
    },
    {
        "ID": "94",
        "Question": "I realize there have been lots of discussions about this type of thing and they often devolve into dogma around whether you ask the \"100 logical pirates\" type of questions or whether you get them to write \"fizz buzz\".  I'm interested in what techniques and questions have been effective for you when interviewing potential developers for jobs.  One technique per answer so we can vote on them, please. ",
        "Best answer": "Besides real technical questions, and typically at the end of the interview I try to get a grasp of their level of interest in the industry and it's culture with questions like:  Have you seen anything recently programming-related that you found interesting and would like to recommend to other fellow programmers? A new language, tool, platform, technique, website? Can you name any well known person in our industry whose work you like or find inspiring and why? (developer, web site founder, author, speaker, etc) What are you reading now or what was the last software related book you read? What programming related sites do you frequent?  Although failing to answer these questions at all (sadly it happens very frequently) does not mean a 'no-hire' to me, they say a lot about the way a person approaches the software development profession. "
    },
    {
        "ID": "104",
        "Question": "We've often run across scenarios where the business will promise a client a new feature.  The business will promise that the feature be implemented in a specific way.  These technical details promised by the business are usually poor.  Unfortunately, client is now set and want this feature to be implemented in the way described by the business. In the end, the business just wants this feature to be completed without regard to quality and maintainability.  Is there a good way to push back?  How can we explain to the business that providing technical details before the requirements have been gathered is a bad idea? ",
        "Best answer": "That's an organizational issue.  If the higher-ups don't understand this, there's not much you can do.  Try to explain the issue to your non-technical bosses, but don't be surprised when you get nowhere. It's is a common problem for developers working in non-development companies that, for whatever reason, sell software. It's not a pleasant tactic, but you can just bludgeon them with evidence.  At the start of a project, write down exactly why it's going to fail (because technical details were poor) and email it to relevant people.  Keep emailing them throughout, and when the project eventually ends up a disaster with pissed off customers, cite those emails you sent at every opportunity.  It may generate some ill will, but there's really no good way to try to fix a systemic issue like that. "
    },
    {
        "ID": "134",
        "Question": "How do you bill your programming projects? Do you do it per hour? Per job?  Please include what kind of project you are doing in the answer. (Mobile, Web, Desktop, etc... You can be more specific if you want.) BONUS: If you'd like to give specific amounts in your answer, you may. ;-) ",
        "Best answer": "There's always going to be a struggle between you and the client over costs: you want to charge as much as you can from a client, and a client is going to want to get as much work for as little cost as possible. So, when you charge hourly, it leaves open to negotiation how many hours a project should take to complete. You may think it'll take 10 hours, but your client thinks you should do it in 5. By charging by project, there's very little open to negotiation: it costs $X, and the client can take it or leave it. "
    },
    {
        "ID": "135",
        "Question": "Who in the software engineering and software development fields uses Twitter to tweet about relevant happenings in the field? ",
        "Best answer": "I'll probably get flamed for this but... 140 characters is hardly the format to get any real pearls of programming wisdom.  Most (but not all) programming concepts/thoughts/ideas require more space to be articulated. I would follow the blogs of the list of programmers that everyone is suggesting. "
    },
    {
        "ID": "163",
        "Question": "Are there any great programming or software development books that are language agnostic?  Why should I read it? ",
        "Best answer": "The Pragmatic Programmer: From Journeyman to Master - Andrew Hunt and David Thomas This book is all about how to write code that works and is maintainable. A key concept is being practical. Everything in the book is boiled down to 70 tips that are easy to remember that help you deliver better, higher-quality software. "
    },
    {
        "ID": "177",
        "Question": "The Actor Model which is used by Erlang seems to be a very different way to do concurrent programming. What are your thoughts about the Actor Model? Will it be a popular solution for concurrency? ",
        "Best answer": "I presume you knows the concept. Well, I don't know very well yet. The short answer is that I believe in popularity of this model to concurrency in short and medium term. I always loved lambda and closures and I see actor like closures specially handled to power concurrency. Thus multi core processors bring to us a new paradigm to programming, concurrency is obligatory to achieve high performance. Actor model seems the ease path to exploit modern hardware. Sooner or later (i hope sooner) almost every mainstream language will support a better model to concurrency. IMHO Actor model so well implemented in Erlang should be the best candidate. One obstacle to Actor model gain popularity is the little impedance with OOP, the dominant paradigm now. But the first step is being given, the main languages are implementing some functional features which will clean the path to Actor model. I am reading a lot about this issue, I think it is very important to every programmer advance in his/her skills. Late Edit: Now I understand better the paradigma. So I can say it's better to a programming languague allow allow the problem be handled by libraries or eventually has a feature to facilitate the adoption of one model. Tie a language with just one model doesn't see a good ideia. "
    },
    {
        "ID": "192",
        "Question": "If I have some code that has 80% test coverage (all tests pass), is it fair to say that it's of higher quality than code with no test coverage?   Or is it fair to say it's more maintainable? ",
        "Best answer": "In a strict sense, it is not fair to make any claims until the quality of the test suite is established. Passing 100% of the tests isn't meaningful if most of the tests are trivial or repetitive with each other. The question is: In the history of the project, did any of those tests uncover bugs? The goal of a test is to find bugs. And if they didn't, they failed as tests. Instead of improving code quality, they might only be giving you a false sense of security. To improve you test designs, you can use (1) whitebox techniques, (2) blackbox techniques, and (3) mutation testing. (1) Here are some good whitebox techniques to apply to your test designs. A whitebox test is constructed with specific source code in mind. One important aspect of whitebox testing is code coverage:  Is every function called? [Functional coverage] Is every statement executed? [Statement coverage-- Both functional coverage and statement coverage are very basic, but better than nothing] For every decision (like if or while), do you have a test that forces it to be true, and other that forces it to be false? [Decision coverage] For every condition that is a conjunction (uses &&) or disjunction (uses ||), does each subexpression have a test where it is true/false? [Condition coverage] Loop coverage: Do you have a test that forces 0 iterations, 1 iteration, 2 iterations? Is each break from a loop covered?  (2) Blackbox techniques are used when the requirements are available, but the code itself is not. These can lead to high-quality tests:  Do your blackbox tests cover multiple testing goals? You'll want your tests to be \"fat\": Not only do they test feature X, but they also test Y and Z. The interaction of different features is a great way to find bugs. The only case you don't want \"fat\" tests is when you are testing an error condition. For example, testing for invalid user input. If you tried to achieve multiple invalid input testing goals (for example, an invalid zip code and an invalid street address) it's likely that one case is masking the other. Consider the input types and form an \"equivalence class\" for the types of inputs. For example, if your code tests to see if a triangle is equilateral, the test that uses a triangle with sides (1, 1, 1) will probably find the same kinds of errors that the test data (2, 2, 2) and (3, 3, 3) will find. It's better to spend your time thinking of other classes of input. For example, if your program handles taxes, you'll want a test for each tax bracket. [This is called equivalence partitioning.] Special cases are often associated with defects. Your test data should also have boundary values, such as those on, above, or below the edges of an equivalence task. For example, in testing a sorting algorithm, you'll want to test with an empty array, a single element array, an array with two elements, and then a very large array. You should consider boundary cases not just for input, but for output as well. [This is call boundary-value analysis.] Another technique is \"Error guessing.\" Do you have the feeling if you try some special combination that you can get your program to break? Then just try it! Remember: Your goal is to find bugs, not to confirm that the program is valid. Some people have the knack for error guessing.  (3) Finally, suppose you already have lots of nice tests for whitebox coverage, and applied blackbox techniques. What else can you do? It's time to Test your Tests. One technique you can use is Mutation Testing. Under mutation testing, you make a modification to (a copy of) your program, in the hopes of creating a bug. A mutation might be:  Change a reference of one variable to another variable;   Insert the abs() function;   Change less-than to greater-than;   Delete a statement;   Replace a variable with a constant;   Delete an overriding method;   Delete a reference to a super method;   Change argument order  Create several dozen mutants, in various places in your program [the program will still need to compile in order to test]. If your tests do not find these bugs, then you now need to write a test that can find the bug in the mutated version of your program. Once a test finds the bug, you have killed the mutant and can try another.  Addendum: I forgot to mention this effect: Bugs tend to cluster. What that means is that the more bugs you find in one module, the higher the probability that you'll find more bugs. So, if you have a test that fails (which is to say, the test is successful, since the goal is to find bugs), not only should you fix the bug, but you should also write more tests for the module, using the techniques above. So long as you are finding bugs at a steady rate, testing efforts must continue. Only when there is a decline in the rate of new bugs found should you have confidence that you've made good testing efforts for that phase of development. "
    },
    {
        "ID": "206",
        "Question": "Test driven development. I get it, like it. But writing tests does require overhead. So should TDD be used universally throughout the code base, or are there areas where TDD provides a high ROI and other areas where the ROI is so low that it is not worth following. ",
        "Best answer": "I'd say avoid TDD in places where the code is likely to change structurally a lot.  Ie, it's great to have a pile of tests for a method whose signature changes rarely but gets refactored internally more frequently, but it sucks to have to fix your tests every time a highly volatile interface changes dramatically. The apps I've been working on recently have been data-driven webapps built on a Gui->Presenter->BusinessLogic->Data Access Layer-based architecture.  My data access layer is tested like nobody's business.  The business logic layer is pretty well tested.  The Presenters are only tested in the more stable areas, and the GUI, which is changing hourly, has almost no tests. "
    },
    {
        "ID": "215",
        "Question": "Having taken a course in Operating Systems I'm interested in expanding my knowledge of the Linux kernel and practice working with a big system. What are some interesting but not overly difficult projects I can try my hand at?  EDIT: a bit of background on what I did do with the kernel.   Implemented a new scheduling policy Implemented User Threads and a corresponding mutex library (not really kernel hacking but contained kernel related theory)  ",
        "Best answer": "I've not done much work on kernels before (they scare me), but I've heard that http://kernelnewbies.org/ is a fantastic resource if you want to get involved in kernel work. As already mentioned SourceForge is a great place to look if you want to get involved in an open-source kernel-related project. Also, a similar question has been asked on Stack Overflow before, so you may want to look here. What are some interesting, small Linux kernel projects to help learn the source? "
    },
    {
        "ID": "220",
        "Question": "How would someone implement Agile process concepts as a solo developer?  Agile seems useful for getting applications developed at a faster pace, but it also seems very team oriented... ",
        "Best answer": " By doing test-driven development By developing in small sprints By having a lot of contact with the customer  I remember reading a thesis about Cowboy Development, that is essentially Agile for solo developers. The thesis can be read here: Cowboy: An Agile Programming Methodology For a Solo Programmer (PDF) "
    },
    {
        "ID": "221",
        "Question": " Possible Duplicate: Using “Foo” and “Bar” in examples   I know AT&T labs used them in their Unix days, but do they have even deeper histories? ",
        "Best answer": "From the Jargon file:  When ‘foo’ is used in connection with ‘bar’ it has generally traced to the WWII-era Army slang acronym FUBAR (‘Fucked Up Beyond All Repair’ or ‘Fucked Up Beyond All Recognition’), later modified to foobar. Early versions of the Jargon File interpreted this change as a post-war bowdlerization, but it it now seems more likely that FUBAR was itself a derivative of ‘foo’ perhaps influenced by German furchtbar (terrible) — ‘foobar’ may actually have been the original form. For, it seems, the word ‘foo’ itself had an immediate prewar history in comic strips and cartoons. The earliest documented uses were in the Smokey Stover comic strip published from about 1930 to about 1952. Bill Holman, the author of the strip, filled it with odd jokes and personal contrivances, including other nonsense phrases such as “Notary Sojac” and “1506 nix nix”. The word “foo” frequently appeared on license plates of cars, in nonsense sayings in the background of some frames (such as “He who foos last foos best” or “Many smoke but foo men chew”), and Holman had Smokey say “Where there's foo, there's fire”.  "
    },
    {
        "ID": "247",
        "Question": "Does learning COBOL still make sense? ",
        "Best answer": "Nooo, of course not. COBOL is a dead language, after all. Or is it? The problem with that view is that programmers at sites like this one usually work with high tech, fast-running (and equally fast burning-out) companies. For them COBOL is a dead language - it is nowhere to be seen. Has not been for some time now, 'tis true. But COBOL was not meant for them. There is more to the software industry than this. Computers were not invented for people with some irrational need for upgrading and replacing old with new all the time. They were made for business purposes. You want to see COBOL? Go to a company that processes payroll, or handles trucking of goods, or shipping (as in ships), or handles your bank account. There is a huge invisible system of code out there that's practically invisible to the users, and most of them never think about it although they encounter it in one way or another everyday (ATMs?) No, it is not dead. But it is \"legacy\" for sure... or is it? Again, depends how you look at it. Nowadays, a lot of people will use Java, C, or anything else instead of COBOL, rewriting from scratch... introducing new bugs as they go along, naturally. That is not saying COBOL doesn't have bugs, and quirks. It does, as much as the next language. Of course it does. But in \"COBOL times\", companies which took bugs more seriously than usual (insurance, banks) tended to produce higher quality code with special quality service groups; today, there are deadlines where time and budget always wins over quality. Also, these systems were originally developed for longer periods back then compared to the equivalent now. If some software has been working for 30+ years, where is the incentive to switch? Whole companies went out of business because they ignored the old adage of \"if it ain't broke, don't fix it.\" Many tried to rewrite the thing... then the first rewrite cost a lot, then the second one cost even more... and none of those new & improved managed to replace it. As I said, this industry is fast-burning, and it also tends to forget fast.  In the 70s COBOL was dead or dying soon, C/C++ were going to rule. Then again in the early 80s Pascal was taking over. Then in the 90s it was Java as THE Language...  Think of Unisys Mapper, dBase, Clipper, Cold fusion... do people even remember those? Each one of them was going to be the gravedigger for COBOL. Taking that into account, and the fact that it is great for processing high volumes of transactions, batch processing or record/transaction-oriented processing, and that one can compile (without errors) a subroutine written 30 years old as managed COBOL code and call it from a managed COBOL.NET should one wish to go Windows and .NET, I'm having trouble finding a suitable replacement for it. (I'm also having trouble finding a Microsoft technology that lasted more then a decade.) Yes, new COBOL code is being written today. One just has to know where to look. For those laughing at COBOL, IMHO, it is like laughing at the Egyptian Pyramids, they are there from 5000 years and they still will be there in next 5000 years, while today's \"hello world\" housing needing 24 controls to work will be deleted, replaced, forgotten next month.  So where are all those COBOL programmers?  Ah, for here lies the rub. The thing is that a lot of them don't have any computing science background. A lot of them are not professional programmers (as in university graduates from a CS/SE program). For the most part, they are people in their late 30's-50's, from all areas of expertise, trained entirely by the company specifically for that job. So they aren't \"COBOL programmers\" - the training they got is specific to the company which so heavily promotes from within. And that makes them pretty much invisible. "
    },
    {
        "ID": "252",
        "Question": "There is a widely accepted opinion that Singleton is an anti-pattern. As usual, there are always exceptions to the rule. Can you explain why Singleton is a bad choice in general and give an example of some valid use cases for it? ",
        "Best answer": "I agree that it is an anti-pattern. Why? Because it allows your code to lie about its dependencies, and you can't trust other programmers to not introduce mutable state in your previously immutable singletons. A class might have a constructor that only takes a string, so you think it is instantiated in isolation and does not have side effects. However, silently, it is communicating with some sort of public, globabally available singleton object, so that whenever you instantiate the class, it contains different data. This is a big problem, not only for users of your API, but also for the testability of the code. To properly unit-test the code, you need to micro-manage and be aware of the global state in the singleton, to get consistent test results. "
    },
    {
        "ID": "262",
        "Question": "Will Java have the same importance it had in the past, or it will be less relevant than nowadays? ",
        "Best answer": "Java is relevant and will continue to be relevant for many years in the Enterprise computing world.   Whether it continues to be relevant in other areas depends a lot on what Oracle does.  If they inject some life (and resources) into ME, desktop applications and other areas, and if they press on with the evolution of the Java language, then Java will do well.   But if Oracle cuts back on R&D and/or tries to stomp other players in the Java space, there's a good chance that someone / some company will develop a better (and more open) Java-like language.  If Oracle win their lawsuit against Google, I predict that the next generation of the Android platform will have a new language, just like happened with C#.  If Google get the openness right ... then, the game is on! "
    },
    {
        "ID": "294",
        "Question": "I just started working a year ago, and I want to join an open source project for the same reasons as anyone else: help create something useful and develop my skills further. My problem is, I don't know how to find a project where I'll fit in. How can I find a beginner-friendly project?  What attributes should I be searching for?  What are warning signs that a project might not be the right fit?  Are there any tools out there to help match people with open source projects? There's a similar question here, but that question has to do with employment and is limited to PHP/Drupal. ",
        "Best answer": "My first open source contribution was for a library that I had previously used (and would've suffered greatly without) on a previous paid project. During my initial use I had spotted a bug in the code so I created a patch, joined the project, and submitted it for review. About 8 months later when I had some free time I decided that I would give back (and work on my development skills) by contributing more to the project. So I cloned the repository and started getting familiar with the codebase. After a few weeks of submitting minor patch fixes to the codebase and monitoring the feature requests, I picked up a feature request to add a pretty substantial module to the project. Since generating many individual patch fixes is pretty tedious for any significant development I cloned the repository to a branch on git hub and started punching away code. A few weeks and several thousand lines of code later the project leader and me worked through integrating and testing my fixes into the library in a way that worked consistently with the rest of the codebase. It was an invaluable process that I learned a lot from:   When I started I didn't know how to use Git, by the end I could proficiently create remote tracking branches and merge or rebase them into the master branch without breaking a sweat.  I started in VS 2008 and ended up migrating to Linux and Monodevelop to work on writing code (because VS is unicode retarded and line endings are such a pain in git). It turns out that theres not much you can't do in *nix that you can do in *dows. I had never really done any unit testing before, Nunit is a piece of cake to use and writing unit tests is pretty elementary stuff. I had to learn to swallow my tongue and listen as well as practice patience. There's no point in standing a firm ground on your position on an open source project because everybody involved is knowledgeable (probably more so than yourself) and capable of accepting/rejecting your ideas based on substance not delivery. It's extremely humbling and rewarding at the same time. Just having one other skilled developer's eyes on a large base of my code pointed out flaws in my style that I had never considered before (as well as I pointed out flaws in his code). For me, I learned that it's easier/better to define constants than it is to use a bunch of magical numbers with detailed commenting.  That particular project was based around generating and decoding networking packets on all levels of networking protocols. I have a personal interest in lower level networking so it was great to have discussions with another developer with shared interest and knowledge in the domain. If you want to just get your feet wet: find a project that you already use; clone the repository; and start seeing if you can fix some bugs and/or add some unit tests. It seems intimidating to look at someone else's codebase with fresh eyes but it's an extremely valuable skill to learn. Submit some patches. You can expect your code to be closely scrutinized at first. Don't worry about it, it's a normal part of the process to gain the trust of the project admin(s). After establishing a base of merit with the projects admin(s) start seeking more responsibilities such as, proposing new features, or asking to be assigned to implementing feature requests.  If you can't find an already existing project on one of the main open source repository networks (github, sourceforge, google code) think of an app that you'd really like to use that doesn't exist yet and start your own. Be prepared to be humbled and expect work to be rejected in favor of further revisions. The myth that anybody can add code to an open source project is completely false. There's always a gatekeeper between you and push access. The better your code, the less it will be scrutinized in the long run as you gain trust of the project admin(s). If it's your project, you'll be that gatekeeper. Update: I just thought about it and realized that I didn't bother to mention which project that a lot of my answer is referencing. For those who want to know, it's SharpPcap. The lead developer Chris Morgan is very professional and on point. He does a hell of a job managing the project and taught me a lot about what it takes to mature a OSS project. Due to personal time constraints I haven't been able to contribute code in over a year but I still try to give back by lurking on Stack Overflow and answering questions about SharpPcap occasionally. "
    },
    {
        "ID": "348",
        "Question": "Elite developers can be 10x more productive than an average developer.  Clearly it's easier to find an elite developer around the whole world than in a company's backyard.  If a company is not located in a programming hot spot, should they consider hiring people who work from home? ",
        "Best answer": "I have worked as, and managed staff in both situations, and combinations of both.  I've made the following observations:  Junior staff do not work remotely.  They require a good and personal working relationship with a mentor.  I find my junior staff would rather wait for me to be available than to ask the rather senior (and good) remote developer anything. Ensure anyone you consider for working remotely is effective when self-guided and doesn't go off on tangents. Remote staff can get isolated really easily and not feel part of a team unless special effort is made to be inclusive of them.  This isolation can lead to a misunderstanding of the specific business driver for a project, or to misinterpret events in a negative manner. Never get a contractor working remotely, unless they have the right incentive to perform. When working with a remote team member, make sure they get equitable access to resources, including source control, reference material, etc.  Don't make them jump through hoops to get work done. Arrange those face to face meetings as often as practical.  This encourages far better team collaboration as people are more comfortable with those they have met.  "
    },
    {
        "ID": "368",
        "Question": "For a long time in SO and in other places Java has the reputation of being slow. From jokes to many comments in questions and answers, people still believe Java is slow based solely on experience with it in the 90s. This is my issue: we have disproved (most) of the reasons that people believe Java is slow. Outside of small things, Java is pretty fast. So why is it that people still refuse to believe Java is fast now? Is it part of their mindset that anything thats not C/C++ is slow? Is it because people don't check over time? Is it because people are just biased? ",
        "Best answer": "It's the applications. As you note, we have proved, time and time again, that in contrived scenarios Java code can meet or even beat the performance of so-called \"performant\" languages like C, C++, Lisp, VB6, or JavaScript. And when presented with such evidence, most sane, open-minded opponents will hang their heads in shame and promise never again to spread such slander. ...but then, they fire up Eclipse, or NetBeans, or Guiffy, or enable the Java support in their browser, or try to run an app on their favorite feature phone. And they wait for it to become responsive... ...and wait...   ...and wait...     ...and wait...         ...and wait...             ...and...      ...what did I promise never to do again? Sorry, must have dozed off... "
    },
    {
        "ID": "404",
        "Question": "Joel Spolsky wrote a famous blog post \"Human Task Switches considered harmful\". While I agree with the premise and it seems like common sense, I'm wondering if there are any studies or white papers on this to calculate the overhead on task switches, or is the evidence merely anecdotal?  ",
        "Best answer": "The abstract of a study that says 'maybe' Another study [PDF] that says interruptions make things seem like they took longer. A study[PDF] that says interruptions increase resumption lag time, but that cues seen in the task before the interruption can speed recovery time. Task switching[PDF] takes a significant portion of our work week. More reading on the psychology of interruptions than you can shake a stick at. "
    },
    {
        "ID": "487",
        "Question": "If you were to design a programming language, how would you do it? What features would you put in? What would you leave out? Statically or dynamically typed? Strongly or weakly typed? Compiled or interpreted? Justify your answers. ",
        "Best answer": " I definitely think that functional programming languages will catch on, so my language will be functional. See Taming Effects with Functional Programming I think the CPUs soon will have hundreads of cores, and threads will he a hell to manage. So the Actor Model is a must instead of threads. See Erlang - software for a concurrent world I also think that OOP has failed, the communication between objects was assumed to be asynchronous. So I think we need message passing, with immutable messages. Send and Forget. As in the Actor model. See Object Oriented Programming: The Wrong Path? I think that it would be good to have static typing, so errors are catched earlier in the development cycle. But I would use type inference as in Haskell, so that the developer don't need to write the type everywhere in the code as in C, C# and Java. See Learn You A Haskell for Great Good I would also design a great UI library, with declarative layout, as in WPF and Android. But I would like to have it as in Functional Reactive Programming.  So my language would be like the concurrency in Erlang but with the typing as in Haskell and a GUI framework as in WPF.NET. "
    },
    {
        "ID": "492",
        "Question": "Did you learn to touch-type when you were already working as a programmer?  If so how did it affect your productivity?  Or are you still unable to touch type and do you think it holds you back? According to Steve Yegge it is essential, Personally I did not notice much difference, possibly because I was spending less than 25% of my work time actually typing (I was working on a large legacy project at the time and I was spending more time on reading and debugging existing code.) ",
        "Best answer": "Well, I said my piece on this here:  When you're a fast, efficient typist, you spend less time between thinking that thought and expressing it in code. Which means, if you're me at least, that you might actually get some of your ideas committed to screen before you completely lose your train of thought. Again.  Personally, I can't take slow typists seriously as programmers. When was the last time you saw  a hunt-and-peck pianist? "
    },
    {
        "ID": "500",
        "Question": "Rather than slavishly pair program all the time, we use pair programming selectively on our team. I think it works best in the following circumstances:  Ramping up brand new team members on a project (instead of letting them wade through documentation or code on their own). Having junior and senior people work together (helps to show some of the skills and tricks of the more experienced developers, plus it allows the old dogs to learn new tricks sometimes). When someone is trying to track down a defect, it often helps to pair with a fresh set of eyes.  When to use pair program and why? When to avoid pair programming? Why? ",
        "Best answer": "Research compiled by Laurie Williams indicates that pair programming works best on industrial teams when  Pairs work on specification, design, and complex programming tasks - experiments indicate that no quality improvement is shown when working on simple tasks in a pair but there may be speed improvements.  Also note that pair \"programming\" often includes activities other than writing code. Each individual in a pairing has about the same level of expertise - while pair programming is great for training, pairs are most engaged when they are about on the same level. Roles rotate regularly - rotating regularly helps keep the current copilot engaged as individuals tend to contribute most when they drive or sense they are about to drive. Pairs rotate regularly - teams have expressed comfort in knowing about different parts of the system they are building.  Pair rotation helps with knowledge transfer which reduces certain risks in the project.  In an academic setting pairs are often assigned, however industry they are generally self-assigned often during stand-ups.  In both cases, the pair is most effective when both individuals are willing participants who see value in the pairing activity.  In my personal experience I've found that my XP team spends about 60% of our development time pair programming on average.  The remainder of the time is spent doing individual development.  It is not uncommon to pair up to create an initial design, work alone on the design for a few hours, then come back together to finish tricky or difficult parts of the code. I've also found that pair programming is most effective in approximately 1.5 to 2.5 hour blocks.  Anything much less tends to require too much overhead to setup while much more and the pairs tend to get cranky and tired.  Cranky and tired means you're not communicating well and might be letting defects slip into the system. "
    },
    {
        "ID": "501",
        "Question": "No matter how much you love a programming language, there are always a few details in it that aren’t quite as nice as they could be. In this question, I would like to specifically focus on syntax elements. In a programming language that you use frequently (perhaps your favourite programming language, or perhaps the one you are forced to use at work), which syntax element do you find most unreadable, unclear, inconvenient or unpleasant? ",
        "Best answer": "Semicolon insertion in JavaScript. I haven't really been bitten by it often, but it's just such a phenomenally bad idea it makes my head spin.  Here's the rules (from ECMA-262 Section 7.9)  When the program contains a token that is not allowed by the formal grammar, then a semicolon is inserted if (a) there is a line break at that point, or (b) the unexpected token was a closing brace. When the end of a file is reached, if the program cannot be parsed otherwise, then a semicolon is inserted. When a \"restricted production\" is encountered and contains a line terminator in a place where the grammar contains the annotation \"[no LineTerminator here]\", then a semicolon is inserted.    Example: return 1; // returns 1  return 1; // returns undefined  "
    },
    {
        "ID": "502",
        "Question": "I think everyone has their own program or set of features beyond \"Hello World!\", that they use when trying out a new language. Mine is a guessing game: I'm thinking of a number 1-10, guess what it is! Guess: 3 Nope, too low! Guess: 7 Nope, too high! Guess: 5 Yes, You win! Play again (Y/N)? N  What do you write? ",
        "Best answer": "It usually goes like this:  Hello World Hello [user inputted name] A few problems from Project Euler A linked list A simple blog engine (either terminal or web-based, depending on what language) And from there I dive into a project that I want to work on (but don't care if the design gets mangled as I learn my way through a new language).  "
    },
    {
        "ID": "507",
        "Question": "I have seen this on the SO on many times. Whenever a question is vague and the question is asking some magical answer somebody or the other leaves a comment saying answer is 42. Even a book I am reading right now uses '42' as the number whenever it wants demonstrate some basic concept using an integer. So is there any history behind it or it is just a coincidence? ",
        "Best answer": "It's the answer to Life, The Universe, and Everything from Douglas Adams' Hitchhiker's Guide to the Galaxy. "
    },
    {
        "ID": "558",
        "Question": "I am a moderately capable web developer. I can put stuff where I want it to go and put some JQuery stuff in there if I need to. However, if I am making my own website (which I am starting to do) I have no idea how to design it. If someone was to sit next to me a point to the screen and say \"put this picture there, text there\" I can do that quite easily. But designing my own site with my choice of colours and text will look like a toddler has invented it. Does anyone know any websites/books I can look at or has anyone got any tips on the basics of non-toddler web design? ",
        "Best answer": "You have a few things to do. Tips: Learn to use Photoshop. (In particular, layer styles are excellent. Just note that they can be difficult effects to reimplement in CSS2) It goes a long way towards making good mockups.  Look at professionally designed sites. What sites have you been to that look nice to you?   Find sites that bother you and consider what could make it better. Look at product advertisements as well. Food packaging. Newspaper ads. You name it.  Also, once you start getting the hang of it, practice, practice, practice. Graphics takes time to develop as a skill, especially from a programmer who has coding to consider. (Gradients vs \"tileability\")  Tools: (Photoshop is a personal favorite. Paint.NET is a good Windows alternative, but is not quite as powerful.) Nathan Smith's 960 grid system. It has templates for many of the mainstream graphics programs. Check it out. References: Look at some of these sites: (I've seen more, I'll try to add as I come across them)  A List Apart John McCain BarackObama.com  Hayon2010.com (Okay, I made this one, but I think that it's nice. edit: I'd appreciate comments on it too. ;-D)  "
    },
    {
        "ID": "566",
        "Question": "goto is almost universally discouraged. Is using this statement ever worthwhile? ",
        "Best answer": "This has been discussed several times on Stack Overflow, and Chris Gillum summarized the possible uses of goto:  Cleanly exiting a function Often in a function, you may allocate resources and need to exit in multiple places.  Programmers can simplify their code by putting the resource cleanup code at the end of the function all all \"exit points\" of the function would goto the cleanup label.  This way, you don't have to write cleanup code at every \"exit point\" of the function. Exiting nested loops If you're in a nested loop and need to break out of all loops, a goto can make this much cleaner and simpler than break statements and if-checks. Low-level performance improvements This is only valid in perf-critical code, but goto statements execute very quickly and can give you a boost when moving through a function.  This is a double-edged sword, however, because a compiler typically cannot optimize code that contains gotos.  I'd argue, as many others would argue, that in all of these cases, the usage of goto is used as a means to get out of a corner one coded oneself into, and is generally a symptom of code that could be refactored. "
    },
    {
        "ID": "570",
        "Question": "All but the most trivial programs are filled with bugs and so anything that promises to remove them is extremely alluring. At the moment, correctness proofs are code are extremely esoteric, mainly because of the difficultly of learning this and the extra effort it takes to prove a program correct. Do you think that code proving will ever take off? ",
        "Best answer": "Not really in that sense, but pure functional programming is good in this domain. If you use Haskell, it's likely that your program is correct if the code compiles. Except from IO, a good type system is a good help. Also programming to contract can be helpful. See Microsoft Code Contracts "
    },
    {
        "ID": "604",
        "Question": "on a widescreen monitor one can easily see more than 80 characters at a time, without scrollbars. even linus torvalds sees the 80 character limit as outdated. so, is the 80 character limit still relevant in times of widescreen monitors? ",
        "Best answer": "If I keep my lines to less than about 100 characters, I can have two editor windows side-by-side on a widescreen monitor.  It's very useful to have both the class header file and implementation both visible at the same time, or have code on one side that calls into the code on the other.  And, if I keep the lines short, I don't need a horizontal scrollbar on my editor windows, which gives me more vertical space. 80 characters may be outdated, but there's some merit in keeping things within reason. "
    },
    {
        "ID": "616",
        "Question": "When I am in a code or design rut, I tend to find a non-dev coworker to discuss the problem with. It forces me to explain the problem in great detail and I'll usually find something I missed in the process. What are your \"unsticking\" methods? ",
        "Best answer": "Some of my tactics:  Explain the problem to someone, or even no one.  My girlfriend used to explain problems to a potato she kept. Work on something else for a bit (if opportunity allows)- some other functionality or even another project.  Get your ming off the current project.  A lot of times problems that seem impossible at 4:30pm seem trivial at 9:30 am the next day. Go to the pub (if possible).  Same principle as above. Beat your head against it.  This isn't often that productive for solving the problem, but at least for me, I tend to learn a lot.  If my gridview isn't auto-sorting, I'll try and read everything I can about the problem.  It'll still take me 3 hours to solve a stupid error on my part, but by the end, I'll have learned everything there is to know about gridviews and how they bind to data- I'll be able to solve any number of similar problems in the future. Get another input- preferably someone who knows at least something about the context of the project.  Most of my errors are stupid ones that only require a few minutes from a second set of eyes to solve where it would take me hours. Isolate the problem.  I keep a folder labeled \"proof of bugs\" where I keep a pile of project that each reproduce a specific issue outside the overall context of the large, complex project.  This can be a little time consuming, but it allows you to narrow down the cause of the issue independent of the bazillion interfering factors of a large project.  "
    },
    {
        "ID": "678",
        "Question": "I know some people are massive proponents of test driven development. I have used unit tests in the past, but only to test operations that can be tested easily or which I believe will quite possibly be correct. Complete or near complete code coverage sounds like it would take a lot of time.  What projects do you use test-driven development for? Do you only use it for projects above a certain size? Should I be using it or not? Convince me!  ",
        "Best answer": "Ok, some advantages to TDD:  It means you end up with more tests.  Everyone likes having tests, but few people like writing them.  Building test-writing into your development flow means you end up with more tests. Writing to a test forces you to think about the testability of your design, and testable design is almost always better design.  It's not entirely clear to me why this happens to be the case, but my experience and that of most TDD evangelists seems to bear it out. Here's a study saying that although TDD takes a bit longer to write, there's a good return on investment because you get higher quality code, and therefore fewer bugs to fix. It gives you confidence in refactoring.  It's a great feeling to be able to change one system without worrying about breaking everything else because it's pretty well covered by unit tests. You almost never get a repeat bug, since every one you find should get a test before it gets a fix.  You asked to be convinced, so these were benefits.  See this question for a more balanced view. "
    },
    {
        "ID": "729",
        "Question": "As programmers we have a lot of inputs:  Ebooks  Code snippets  Interesting emails  Documents Web articles  Blog posts StackOverflow questions Podcasts ...  Which tools do you use to store, organize, search and consult all of this stuff? Is there a silver bullet solution to handle this huge amount of data?  ",
        "Best answer": "I've started blogging about things I've learned.  Just a simple free blog, I keep it private, if it's worth sharing I'll spend some time to transform it into a post that's intelligible enough someone else can read it and walk away with something. You can tag ideas and search by grouping later too. Also helpful for creating an online identity for employment sake. "
    },
    {
        "ID": "739",
        "Question": "What is the recommended  User Account Control (UAC) setting when developing on Windows? Even on Win7 I find it annoying enough to turn it off (because it makes me more productive with it off) but sometimes I feel bad because I know I'll find more problems in my code if I leave it on. ",
        "Best answer": "The recommendation (even from Microsoft) is to leave it ON, and also to run your IDE unelevated whenever possible. First of all, it forces the programmer to live with the same \"annoyances\" a real world user will have (if you don't know it, would you program correctly around it?). Then, disabling UAC and working as an administrator is as bad as in Unix to work as root (decades of common wisdom will tell you why that's bad). "
    },
    {
        "ID": "745",
        "Question": "What methods do you use to stay awake and alert while working? Personally I drink coffee non stop throughout the day.  But I've also heard of this thing called exercise that should help too.  Does anyone else have tips and tricks to stay more awake and alert while working?  Redbull? Maybe a magic pill that won't require me to sleep? ",
        "Best answer": "Caffeine is a major cause of the problem, not the solution.  It might seem to work in the short-term but it makes things worse overall by interfering with your sleep. If you don't sleep properly you will be tired and unfocused.  If you try to solve that with a high caffeine intake you won't sleep properly. Exercise, eat well, try to restrict the coffee to a couple of cups a day and don't work stupid long hours. "
    },
    {
        "ID": "750",
        "Question": "I'm freshly out of college, and starting university somewhere next week. We've seen unit tests, but we kinda not used them much; and everyone talks about them, so I figured maybe I should do some. The problem is, I don't know what to test. Should I test the common case? The edge case? How do I know that a function is adequately covered? I always have the terrible feeling that while a test will prove that a function works for a certain case, it's utterly useless to prove that the function works, period. ",
        "Best answer": "My personal philosophy has thusfar been:  Test the common case of everything you can.  This will tell you when that code breaks after you make some change (which is, in my opinion, the single greatest benefit of automated unit testing). Test the edge cases of a few unusually complex code that you think will probably have errors. Whenever you find a bug, write a test case to cover it before fixing it Add edge-case tests to less critical code whenever someone has time to kill.  "
    },
    {
        "ID": "756",
        "Question": "I'm trying to find places where I can hone my craft outside the context of school or work. Are there places online, or books available, where I can access lists of programming puzzles or challenges? ",
        "Best answer": "Moderator note: this is intended to be a canonical list; please check to see if your suggestion has already been added to the answer. If it hasn't, edit the answer to add yours, preferably with an explanation or reason why you're suggesting it. On Stack Exchange Pick a tag, follow the new questions posted, and try to solve them. If you find a good one, bookmark it for later use:  Stack Overflow Code Review Community Challenges Programming Puzzles and Code Golf Solve algorithmic and datatypes problems  Books  Algorithms for Interviews by Adnan Aziz Cracking the Coding Interview (6th Edition) by Gayle Laakmann Programming Challenges by Steven S. Skiena The Art of Computer Programming by Donald E. Knuth  Communities and Blogs  Algorithm Geeks Google Group CodeKata LessThanDot's Programmer Puzzles forum The Daily WTF's Bring Your Own Code series /r/dailyprogrammer  Game sites and ongoing contests  Codingame - fun games (solo and multiplayer) to practice your coding skills. Supports 25+ programming languages. CodeChef Code Combat - Javascript and Python solo and multiplayer games in the style of a strategy game. Hacker.org Challenge — \"The hacker.org challenges are a series of puzzles, tricks, tests, and brainteasers designed to probe the depths your hacking skills. To master this series you will need to crack cryptography, write clever code, and dissect the impenetrable; and in the process you will enrich your understanding of the world of hacking.\" Pex for fun — game from Microsoft research where you duel against other programmers Rankk — \"You start with the easy levels and progress to the intermediate and hard levels by solving the minimum number of required challenges at each level. The journey to the top is an arduous yet rewarding one. You need to be sufficiently determined and persevering to go far. Only a few are expected to reach the apex and attain Geb.\" TopCoder Google Code Jam—algorithmic puzzles  Language specific  4Clojure (Clojure) — \"4Clojure is a resource to help fledgling clojurians learn the language through interactive problems. The first few problems are easy enough that even someone with no prior experience should find the learning curve forgiving. See 'Help' for more information.\"  Prolog Problems (Prolog) — \"The purpose of this problem collection is to give you the opportunity to practice your skills in logic programming. Your goal should be to find the most elegant solution of the given problems. Efficiency is important, but logical clarity is even more crucial. Some of the (easy) problems can be trivially solved using built-in predicates. However, in these cases, you learn more if you try to find your own solution.\"  Python Challenge (Python) — \"Python Challenge is a game in which each level can be solved by a bit of (Python) programming.\"  Ruby Quiz (Ruby) - \"Ruby Quiz is a weekly programming challenge for Ruby programmers in the spirit of the Perl Quiz of the Week. A new Ruby Quiz is sent to the Ruby Talk mailing list each Friday.\"  IOCCC (C) - \"A contest to write the most obscure/obfuscated C program. (Fun to try to understand the previous year's entries, or to submit a new one.)\"  Underhanded C Contest (C) - \"contest to turn out code that is malicious, but passes a rigorous inspection, and looks like an honest mistake. (Try to understand previous year's entries, and learn to find similar mistakes in other people's code)\"  CheckiO - Python programming challenges. Custom \"Missions\" can be created by members.  109 Python Problems for CCPS 109 Python problems of various levels of difficulty, with an automated pseudorandom fuzz tester to verify that the functions are correct.   Online judges / automatic assessment  Codingbat has lots of coding challenges ranging from warm-ups to Harder recursion problems. It is available in Java and Python. Cyber-dojo has a nice variety of katas and supports a good selection of languages. It is intended to support doing deliberate practice of TDD, but could be used for personal development too. LeetCode Peking University JudgeOnline for ACIP/ICPC Sphere Online Judge University of Valladolid Online Judge Codewars — Training with code challenges. Rosalind algorithms and bioinformatics Quizful - interactive programming quizzes in \"Duolingo style\". This site looks fun and has a good set of questions, at least in Java. Plus, as they say, it has adaptive learning algorithm, that makes learning more effective. exercism - Challenges in more than 30 languages that will be evaluated automatically.  Problem lists and contest archives  ACM/ICPC Problem Index @ HIT — List of problems from the ACM International Collegiate Programming Contest Algorithmist — Includes lists of algorithms and other puzzle sites Career Cup — Collects community-subumitted interview questions from various tech companies Educational Computing Organization of Ontairo's past computer programming puzzles Engineering Puzzles at Facebook — Puzzles provided for the purposes of evaluating potential hires Google Code Jam contest archives Ninth Annual ICFP Programming Contest Task archive Ponder this at IBM Research — Puzzles provided for the purposes of evaluating potential hires Programming Praxis Project Euler Rosetta Code TopCoder Match List International Olympiad in Informatics - yearly contents for teams of students. (previous year's problem sets)  Security oriented  Smashthestack gera's insecure programming challenges  "
    },
    {
        "ID": "778",
        "Question": "What question have you found especially valuable in interviewing software developers?  What is it about the question that has made it particularly useful? I'm looking for a particular question you like to ask, not just an interviewing approach like \"make them write code\". ",
        "Best answer": "Take a look at this sample code and tell me how you'd improve it. "
    },
    {
        "ID": "779",
        "Question": "It doesn't have to be programming or software development related, but just asked during an interview for an IT related job. I know some \"left field\" questions are meant to see how the candidate copes with unexpected and novel situations, but here I'm looking for a question that appeared to be completely unrelated to the job they were interviewing you for, or something that made you think \"what useful information could they possibly get from my answer to that question?\". ",
        "Best answer": " Where do you see yourself in 5 years?  Do they really think people are dumb enough to say that they want to do something completely different? Or don't want to work for them? I guess it can be useful as an indicator of who not to hire but it's so stupid easy to fake that you can't use it as an indicator of who to hire in any way if they answer correctly. "
    },
    {
        "ID": "811",
        "Question": "Literate programming has good ideals. Why do you think that this isn't mainstream? It is because it has failed to deliver? ",
        "Best answer": "I first saw it in a book of Knuth's writings, and thought it looked neat.  Then I tried to use the literary programming display to comprehend what was going on in the program, and found it harder than it looked.  It may have been that I was too used to going through program listings, but it seemed confusing. Then I looked at the source code, and that turned me off then and there.  I'd have to learn to write programs in an entirely new way, with less correspondence between the program text and what the compiler saw, and saw no corresponding benefit. In addition, people can write long and convincing arguments that the code is doing X when it's actually doing Y, and I've run into my share of misleading comments.  I developed a fondness for reading the code to see what it's doing fairly early.  Literate programming is the antithesis of that. "
    },
    {
        "ID": "843",
        "Question": "I was instructed by my project manager that frequent deployment in PROD or to test server should be avoided. But I don't understand why? We roll our testing copy to PROD on every sprint end but suddenly client would ask a simple change to the existing application which would require a re-deployment. When every thing was well tested and QA approved. why should we avoid frequent deployment? How it was done universally? ",
        "Best answer": "If you are talking about a hosted web application, the users don't get a say-so in when they get upgrades. That is, they are forced to upgrade each time you do a push to production. If your changes dramatically change the system rules or UI, you should definitely consider bundling your releases and doing it less frequently. It is very frustrating to users to have to continually re-learn how to use the tools they rely on and violates the UI principle of making them feel they are in control of their computer/software. "
    },
    {
        "ID": "866",
        "Question": "I know this is obviously not true for some games, minesweeper, card games, etc.  But what about any game involving simulating the real world environment.  Obviously these games have some form of a physics engineer powering the graphics.  For someone like me who personally did not enjoy physics but loves video games, do I have a chance at ever getting involved in the development cycle of a video game such as this while disliking physics?  ",
        "Best answer": "There are many aspects to game programing and you don't need to be an expert at all of them.  If you're on a bigger team you may not even be doing the core game programming but just network communication for example.  For core game programming I would think that above a sound physics understanding you would need a strong mathematical background.  Having a strong mathematical background will allow you to do things like modelling transformations, clippings, keyframe animation, ray tracing, image processing, texture mapping, etc. Having a strong physics background may help you with coming up with new computer graphics methods, but it is not required if you want to become a game programmer.  "
    },
    {
        "ID": "870",
        "Question": "One per answer please. I'll add my favourite as an answer. ",
        "Best answer": " Code Complete by Steve McConnell. I don't even think it needs explanation. It's the definitive book on software construction. Incredibly well written and covers all aspects of the practical (programming) side of creating software.  "
    },
    {
        "ID": "937",
        "Question": "I know that they are implemented extremely unsafely in C/C++. Can't they be implemented in a safer way? Are the disadvantages of macros really bad enough to outweigh the massive power they provide? ",
        "Best answer": "I think the main reason is that macros are lexical. This has several consequences:  The compiler has no way of checking that a macro is semantically closed, i.e. that it represents a “unit of meaning” like a function does. (Consider #define TWO 1+1 — what does TWO*TWO equal? 3.) Macros are not typed like functions are. The compiler cannot check that the parameters and return type make sense. It can only check the expanded expression that uses the macro. If the code doesn’t compile, the compiler has no way of knowing whether the error is in the macro itself or the place where the macro is used. The compiler will either report the wrong place half of the time, or it has to report both even though one of them is probably fine. (Consider #define min(x,y) (((x)<(y))?(x):(y)): What should the compiler do if the types of x and y don’t match or don’t implement operator<?) Automated tools cannot work with them in semantically useful ways. In particular, you can’t have things like IntelliSense for macros that work like functions but expand to an expression. (Again, the min example.) The side-effects of a macro are not as explicit as they are with functions, causing potential confusion for the programmer. (Consider again the min example: in a function call, you know that the expression for x is evaluated only once, but here you can’t know without looking at the macro.)  Like I said, these are all consequences of the fact that macros are lexical. When you try to turn them into something more proper, you end up with functions and constants. "
    },
    {
        "ID": "940",
        "Question": "This is more a discussion question than an actual attempt to determine the \"best\", since that clearly varies by the needs of the organization. I'm more curious about the arguments in favor of different systems across categories (centralized vs distributed, open vs proprietary, etc). So, what do you think is the best version control system? ",
        "Best answer": "Mercurial Because of it's sophisticated ability to branch and merge code, it is the best I've used. The whole DVCS paradigm just makes so much sense. I've not used Git, but I suppose that it qualifies as well.  "
    },
    {
        "ID": "966",
        "Question": "There are a lot of questions about what programming books should be on the programmer's bookshelf. How about non-programming related books that can help you become a better programmer or developer? It would also be interesting to know why they would help. My first choice would be Sun Tzu's \"Art of War\" (however cliché), because it made it obvious that the success of any project depends on the strength of its weakest link (and warfare is a big project). ",
        "Best answer": "The Design of Everyday Things by Donald Norman  "
    },
    {
        "ID": "991",
        "Question": "What are the first 5 things you do before starting a new project? Do you always spend a day researching new frameworks? Or, using similar or competing products? ",
        "Best answer": "This is pretty project-dependent.  Is this a project I'm starting with the intent of selling, or a project for a specific customer?  Also, what constitutes \"starting\"? Is that before or after requirements gathering? A rough list, though:  Get a context for the project.  That is, figure out what your customer or potential user is trying to accomplish and why.  If you're building a hotel registration system, what's wrong with OTS options, for example. Gather requirements.  Meet with stakeholders.  Meet with users.  Meet with anyone who has a say in the project, if you can.  Look at existing solutions that this project will replace, either that the customer is using or that exist in the market place.  From there, write it all down in a non-technical language as you can- a good reqs doc should describe what's to be done but not how to do it.  Then discuss this doc with the customer and iterate until they agree with it.  This step can be less formal for smaller projects (possibly even entirely verbal). Start making technical decisions.  Pick languages, frameworks, ORMs, databases, etc that best solve the problem, whether this means sticking with something you know or learning something new. Analyze the risks for this project.  If this is a government contract, you probably want a 100 page leather-bound risk report.  If it's a 3-man 4-month project, you might be fine with some notes in a text file or a spreadsheet.  Either way, you want to figure out what can go wrong with the project, how likely it is to happen, how much it'll hurt, and what you are going to do to prepare for it, handle it, and/or mitigate it's effects after the fact.  A common one, for example, is \"One of the devs gets hit by a bus, quits, gets sick, etc.\"  So you might mitigate that by pair programming to share knowledge, using good source control practices to keep code centralized, etc.  Overall, the process of sitting and thinking about what could go wrong and being prepared for the possibilities is more important than actually writing out all the contingency plans. Set up the technology.  It's the sort of thing that no one wants to do once you're in the thick of actually coding, so set up your repo, your build server, your build system, your wikis, your bug tracker, or whatever you intend to use for your project.  "
    },
    {
        "ID": "1007",
        "Question": "Tester and blogger Lanette Creamer recently posted this question on Twitter:  If you are a professional software developer who works with testers, think of the best testers you know. What traits do they have in common?  I thought it would make an excellent question for here. My thoughts are:  They want to remove ambiguity from requirements even if it means asking awkward questions. They create new features by seeing the way software \"should\" work, rather than just how it's documented. They demonstrate honesty and integrity and encourage but not demand it from those around them. In other words, they model behavior.  What are the traits of the best testers you've worked with? ",
        "Best answer": "Some of the best testers I've worked with really understand how the users are going to use the software.  They understand what business function the software is supposed to play and how that software will effect the user's role/job/function.  It makes for a successful project when the tester has as much knowledge of the business as the developer and the business owner. "
    },
    {
        "ID": "1009",
        "Question": "I'd like to sell my software on the 'net but am not sure how to do the whole Merchant setup.  I have access to Commerce Server 2009, and I want to seem professional so a plain old PayPal account is out. What do I need to know/do to sell a few things using ASP.NET, accept credit cards, and what not?  ",
        "Best answer": "Definately use a 3rd party vendor at first.  There's a lot of shareware and software sales services that will handle the whole process of purchase and download.  Then once you get sales going and have an idea of what type of revenue you are generating, you might look at implementing a store of your own on your site.  What you dont want to do is bite off too much to chew at one time.  Releasing a new product is tough enough, you dont want to compound that by having to learn all about credit card processing & sales/vat taxes, and maintaining your own store.  Nor do you want to invest a large amount of time up front doing all that if it turns out no one is buying your software. "
    },
    {
        "ID": "1025",
        "Question": "I'm considering learning iPhone development and Objective C but don't want to avoid developing something for the most saturated markets and app categories. What categories should I avoid?  Are there too many dating applications, or should I just stick to coming up with a creative game or two? ",
        "Best answer": "If you want to know what's on the App Store, look at the App Store. I don't think you are going to produce good work if you start by asking yourself, \"What should I not do?\" Come up with a few ideas for apps, then search the App Store for them. If you find a dozen apps already there, you can avoid that category. (Unless you think you can do something much better than what's already there.) Then you can make your plans based on the search results. "
    },
    {
        "ID": "1058",
        "Question": "I'm not exactly sure when to use Enterprise Library, and when not to... and that is making me not learn it at all.  I feel that I have enough of a reason to start learning then perhaps one day I'll use it. Are there times when I should use EntLib? When shouldn't I use it?   ",
        "Best answer": "I've used EntLib for many years (since they were indiviual App Blocks).  I've found that at times it can be pretty heavy as far as the size of the components, especially if you only need one block and it needs to be downloaded.  Often I'll use the Data and Logging components together and that feels like enough functionality to justify the size.  If your app is strictly on the server side then this really isn't too much of an issue.  One of the things that is nice about it is that if you need more than one block you don't have to go to multiple implementations from multiple vendors that are configured in different ways.  They also provide a tool to help with the configuration (that's a plus and a minus, a plus that they make it easy, a minus that they NEED a tool to help configure it). I've had the pleasure of being invinted to a couple of Patterns and Practices workshops where I was working side by side with the team members who wrote EntLib.  The intent in creating EntLib was to implement Microsoft's Best Practices in common components that everyone needs that are not part of the base Framework.  They are very stable, provide very good performance and very good flexibility. I would start by using some of the easier blocks, like Data and Logging.  They're not too hard to configure and get started with.  Then once you understand those it will be a bit easier to move on to some of the other blocks.  I have not found a situation where you shouldn't use them, other than when you don't need them. "
    },
    {
        "ID": "1059",
        "Question": "I have often heard it said that objects have not delivered in terms of code reuse. Do you agree? If you believe that they haven't, why not? ",
        "Best answer": "No, not necessarily. Objects deliver better semantics, organization of code/functionality and, possibly, ease-of-use. Well designed libraries deliver on the promise of code reuse, not objects per se. "
    },
    {
        "ID": "1060",
        "Question": "Aspect oriented programming promises to deal with cross cutting concerns, but I'm not completely sold on it yet. Have there been any other attempts to deal with this problem? ",
        "Best answer": "When possible, you can encapsulate cross-cutting concerns into separate modules that are then used throughout the app via dependency injection.  This allows you to somewhat decouple the cross-cutting concern implementation from it's use throughout the code. This doesn't always work elegantly, though.  That's the reason people are trying to address the issue with things like AOP. "
    },
    {
        "ID": "1063",
        "Question": "I have often wondered if it would be possible to write a programming language that would work as a conversation. I would tell the computer that I want to store coordinates and it would ask what representation I would use. I would then ask to find the shortest path between points and it would ask if I want to use Djkstra's, brute force or use Floyd's? What research has been done towards producing a system like this? ",
        "Best answer": "I think such a language would need an artificial intelligence in place, or at least a system that can learn. The problem is that humans don't know what they want. Also, even writing in classical imperative language we still make logical errors. Imagine trying telling a non-intelligent software what he has to do. "
    },
    {
        "ID": "1065",
        "Question": "I'm comparing different STS's for claims based authentication and am trying when it's appropriate to use each one.   It appears that ADFSv2 should be used in all Windows Environments.  Ping and Siteminder are more for the Unix side... although a sales rep said Ping was better for Windows, I didn't really understand \"why\" Any comparative information is much appreciated! ",
        "Best answer": "We are in the middle of doing SSO where I'm at right now.  We couldn't get Site Minder to let us pay for a developer license; they literally didn't return the numerous calls we made begging to use their software (one of our clients was using it and we figured it would be the easiest path). We went with Ping Identity's Pingfederate product (or is it the other way around)? The documentation isn't amazing, but it's working for us.  When you can get past first-level support, they have been very helpful and getting us going.  Your application will be dependent upon their APIs, but your application will also support numerous open standards for SSO.  They also have consulting available to do the heavy lifting if you have the budget. "
    },
    {
        "ID": "1090",
        "Question": "Language shortcuts can often be used to make code more concise. For example, ternary and null coalescing operators can reduce the amount of code, but arguably to the detriment of readability: In C#: Person newGuy = new Person(); if (boss == null) {     newGuy.Boss = GetDefaultBoss(); } else {     newGuy.Boss = boss; }  is functionally equivalent to: Person newGuy = new Person(); newGuy.Boss = boss ?? GetDefaultBoss();  but obviously a lot more verbose. Where do you draw the line when it comes to conciseness vs readability? ",
        "Best answer": "Both. Your first example is certainly more verbose, and arguably more explicit... but it also requires me to scan five lines instead of one. Worse, it deemphasizes its purpose - assigning a value to newGuy.Boss. Your second example may cost me a second if I'm unfamiliar with the null coalescing operator, but there can be no doubt as to its purpose, and if I'm scanning through a larger routine looking for the source of a value, it will be much easier for me to pick this one out. Now, contrast this: if (boss == null) {     newGuy.Boss = GetDefaultBoss();     newGuy.IsTemp = true;     newGuy.AddTask(\"orientation\"); } else {     newGuy.Boss = boss;     newGuy.IsTemp = false; }  ...with: newGuy.Boss = boss ?? GetDefaultBoss(); newGuy.IsTemp = boss == null; if ( boss == null ) newGuy.AddTask(\"orientation\");  The latter example is again much shorter, but now it obscures its purpose by making tasks triggered by the same test appear to be distinct. Here, I feel the verbosity of the former is justified. "
    },
    {
        "ID": "1095",
        "Question": "I used ad-hoc MUML (made-up modeling language) to design and explain system fairly frequently.  It looks similar to UML and tends to be pretty well understood. However, I've had a professor or two that harped on the use of strict, formal UML, as close to the spec as possible.  I always suspected that strict UML wasn't really as common as they claimed.  So, how 'bout it- how often do you actually draw out complete diagrams that use all the proper line endings, multiplicity, member type symbols, etc? ",
        "Best answer": "Never. Heck, it's been years since I last created any UML. Line diagrams on whiteboards and scraps of paper don't count. In fact, we just removed the sole UML question from the guide we use during interviews, because none of us really cared about the answers. "
    },
    {
        "ID": "1180",
        "Question": "I've been in workplaces where, at the start of a project, the \"Should we use VB.Net or C#\" question has been raised. Granted, it's probably less common to have to make that decision now than it was in the early days of .Net, particularly given the trend towards language convergence, but it can still be a heated debate. So, between VB.Net and C#, Which language do you prefer and why? ",
        "Best answer": "I prefer C# over VB.NET because  it's easier to find programmers/jobs:    it's easier to find help:   (from stackoverflow) "
    },
    {
        "ID": "1189",
        "Question": "By now I work with asp.net and C#. I have done a decent work in Java as well. I am planning my career in such a way I should be language-agnostic someday. What are the things that I need to learn?  First would OOP paradigms as its speaks about the Class design. Are there any others? ",
        "Best answer": "To be language agnostic you need to have experience in all of the common styles and types of languages.  An imperative language (You tell it what to do, step by step. Eg - C) A declarative language (You tell it your goal, it figures out what to do. Eg - SQL/HTML/Prolog)  Also:  A functional language (Functions are key, avoiding state and side effects are the goals. Eg - Haskell/OCaml/Lisp/F#) An object oriented language (Architecture where objects encapsulate related data and the methods that act on them). Eg - Java/C#)  Some typing styles:  A statically typed language (Data types are defined and checked at compile time. Eg - C#) A dynamically typed language (Data types are checked at runtime. Eg - Python/Javascript) Experience of strong vs. weak typing is also useful.  Some different runtime styles:  Something compiled (Eg - C++) Something interpreted (Eg - PHP) Something Managed (Eg - C#/Java)  Lower level stuff:  Something fairly low level (Eg - C) Some dialect of assembly (Eg - NASM)  On top of that I would say you need experience of some concurrent programming and something event driven. You should probably also make sure you know something about the various domains such as web programming (client & server), rich client development/desktop, games. You might also want to learn about embedded programming, or dedicated hardware (like games consoles), and mobile development is becoming an increasingly relevant domain. Others have also mentioned that it's worth getting some experience of Generic programming and Meta programming approaches. When you learn these paradigms avoid just learning the syntax and writing in your old style. I've seen many C# devs write JavaScript as if it's statically typed. Don't do this, try to learn the language paradigms and embrace them. If you've done all of this, the differences between languages will become largely syntactical so switching will become a fairly simple exercise of learning some new syntax. Don't forget though that modern programming is almost always dependant on a framework, so familiarising yourself with the common and popular frameworks for each language you learn is also critical. Knowing C# is irrelevant without .net.  "
    },
    {
        "ID": "1200",
        "Question": "Coming from a procedural/OO programming background, I tend to write Scheme programs in a procedural fashion. I would be intersted in learning Scheme or Lisp in a functional way from the ground up, to kind of reset my programmer's mind. Is there a tutorial or book out there that's the de-facto standard for describing best practices, design methodologies, and other helpful information on functional programming concepts? What about that book makes it special? ",
        "Best answer": "Use it. If you do functional programming daily, maybe smaller applications or exercises from books, then you will be better on it. I have used it since the first programming lecture in university. At the beginning it was very hard, because it is so different, but now I prefer it to imperative programming. If you are looking for a good book, I would recommend Real World Functional Programming: With Examples in F# and C# by Tomas Petricek and Jon Skeet "
    },
    {
        "ID": "1217",
        "Question": "I think we’ve all seen this. Beginners ask questions on Stack Overflow that follow the basic outline...  I’m trying to do (very vague description of the goal) but it doesn’t work/I get an error/exception. Please help!  Isn’t it bizarre that so many of them seem to consider it unnecessary to paste the error message? I wonder what the psychology of this is. What is it about error messages that makes people initially assume that they are useless and not worth paying any attention to? The answer I’m looking for is not “they don’t understand the error message”. That doesn’t explain why they wouldn’t consider telling anyone else who might understand it. ",
        "Best answer": "I think the real reason is that ordinary computer users, even if they should go on to become programmers, are conditioned to believe they can't do anything about errors. Think about it. What do non-programmer types do when they encounter a cryptic error message*? They might read it, but nine times out of ten they'll simply dismiss it and try again. Only if it consistently fails will they look it up. Therefore, when beginning to learn how to program, people don't immediately realise that the error they're getting contains useful information on how to fix it; and yea, though compiler errors can be nigh unreadable even to the trained professional (I'm looking at you, C++ template metaprogramming), at least they provide a general starting point, and once you've seen the same error a couple of times, you'll always know what you've done to cause it. *Honestly, though, most error messages look to Joe Average like \"Error X2412: Unable to establish frobnicatory interplatforming dongledash: please verify bandersnatch settings or contact your system administrator.\" "
    },
    {
        "ID": "1224",
        "Question": "I've never found the ideal way to perform code reviews and yet often my customers require them. Each customer seems to do them in a different way and I've never felt satisfied in any of them.  What has been the most effective way for you to perform code reviews? For example:  Is one person regarded as the gatekeeper for quality and reviews the code, or do the team own the standard?   Do you do review code as a team exercise using a projector? Is it done in person, via email or using a tool?  Do you eschew reviews and use things like pair programming and collective code ownership to ensure code quality?  ",
        "Best answer": "At my work we have a very simple rule: changes must be reviewed by at least one other developer before a merge or a commit to the trunk.  In our case this means the other person physically sits with you at your computer and goes through the change list.  This is not a perfect system, but it has noticeably improved code quality nonetheless. If you know that your code is going to be reviewed that forces you to look it over first.  Many problems become apparent then.  Under our system, you have to explain what you did to the reviewer, which again causes you to notice problems you may have missed before.  Also, if something in your code is not immediately clear to the reviewer, that is a good indication that a better name, a comment, or a refactoring is required.  And, of course, the reviewer may find problems too.  Furthermore, in addition to looking at the change, the reviewer may also notice problems in the nearby code. There are two main drawbacks to this system.  When the change is trivial, it makes little sense to have it reviewed.  However, we absolutely have to stick to the rules, to avoid the slippery slope of declaring changes to be \"trivial\" when they are not.  On the other hand, this is not a very good way to review significant changes to the system or addition of large new components. We have tried more formal reviews before, when one developer would email code to be reviewed to the rest of the team, and then the whole team would get together and discuss it.  This took a lot of everyone's time, and as a result these reviews were few and far between, and only a small percentage of the code base got reviewed.  The \"one other person reviews changes before commit\" has worked much better for us.   "
    },
    {
        "ID": "1262",
        "Question": "I'm considering whether I should start using VIM again instead of an IDE. What are the most useful features of VIM that aren't standard in an IDE? ",
        "Best answer": "I don't think its necessarily the advanced features of VIM that make it so powerful. Its the fact that you never have to take your hands off the keyboard to do anything. Finding something in a huge file is as simple as a couple of keystrokes. Opening and closing multiple files in the same window is incredibly fast as well. While it may not seem intuitive at first, its well worth your time. Even if you don't use it as your standard IDE (I generally use Visual Studio or Eclipse, for example), you'll find your self using VIM to quickly open and edit files because it becomes way faster than waiting for the IDE to load. Invest the time to learn how to use VIM well and you'll never regret it. I'd say its comparable to learning to touch-type. "
    },
    {
        "ID": "1280",
        "Question": "What best practices should be undertaken for a website that needs to \"scale out\" to handle capacity?  This is especially relevant now that people are considering the cloud, but may be missing out on the fundamentals. I'm interested in hearing about anything you consider a best practice from development-level tasks, to infrastructure, to management. ",
        "Best answer": "Design for Concurrency That is, as you're coding, plan around having multiple threads going.  Plan the shared state (often just the db).  Plan for multiple processes. Plan for physical distribution. This allows you to distribute your system across multiple machines, and across multiple processes with load balancing.  It allows you to have redundant processes running in case of failure, and in case you need to modify the system in-place, you don't have to kill all service to do so. "
    },
    {
        "ID": "1323",
        "Question": "I recently saw that Microsoft released a coding standards document (All-In-One Code Framework Coding Standards) and it got me thinking...  The company that I work for has no formal coding standards at all.  There are only a few developers and we have been together long enough to have evolved into similar styles and its never been an issue. Does the company you work for have a documented coding standards?  If no, why not?  Does having a standard make a difference?  Is it worth writing a standard from scratch or should you adopt another standard as your own (ie. make Microsoft's standards yours)? ",
        "Best answer": "It's important for a team to have a single coding standard for each language to avoid several problems:  A lack of standards can make your code unreadable. Disagreement over standards can cause check-in wars between developers. Seeing different standards in the same class can be extremely irritating.  I'm a big fan of what Uncle Bob has to say about standards:   Let them evolve during the first few iterations. Let them be team specific instead of company specific. Don't write them down if you can avoid it. Rather, let the code be   the way the standards are captured. Don't legislate good design. (e.g. don't tell people not to use goto) Make sure everyone knows that the standard is about communication, and   nothing else. After the first few iterations, get the team together to decide.   "
    },
    {
        "ID": "1338",
        "Question": "Have you ever had to work to coding standards that:  Greatly decreased your productivity? Were originally included for good reasons but were kept long after the original concern became irrelevant? Were in a list so long that it was impossible to remember them all? Made you think the author was just trying to leave their mark rather than encouraging good coding practice? You had no idea why they were included?  If so, what is your least favorite rule and why?  Some examples here ",
        "Best answer": "Had a professor once who demanded we have at least one comment for each line of code. //Set x to 3 var x = 3;  //if x is greater than 2 if(x>2){      //Print x     Print(x); }  It was pretty ridiculous. "
    },
    {
        "ID": "1376",
        "Question": "I have a tester that while testing will have an error occur (ok so far), but then he frequently reports it right away.  We (the developers) then later find that the tester has not tried to reproduce the issue and (when asked) cannot find a way to make it happen again. Now these are still bugs, I don't want to ignore them.  But without repro steps I am kind of stuck.  Sometimes there is a stack trace (though frequently it is not useful because this is compact framework and there are no line numbers).  But when there is one I can take the stack trace and crack open the code and start guessing, but that does not lead to testable \"fixes\". What do you do in scenarios like this? ",
        "Best answer": "A bug without context is not a bug, it's a fluke. The problem could be your code, it could be a third party library, it could be the hardware, or it could be solar radiation causing a single bit to flip on it's own. If you can't reproduce it with at least some regularity (even if only \"it happens once every 10 or 20 times I do X\"), it's not much better than your tester telling you \"Something somewhere went wrong somehow - fix it\". You may have to explain to your tester that his job is not to just generate input until something breaks. If it were, you could replace him with a random number generator. Part of his job is to identify bugs, which entails identifying how to produce them. "
    },
    {
        "ID": "1380",
        "Question": "We are starting a push for code coverage here at my work, and it has got me to thinking.... How much code coverage is enough? When do you get to the point of diminishing returns on code coverage?  What is the sweet spot between good coverage and not enough?  Does it vary by the type of project your are making (ie WPF, WCF, Mobile, ASP.NET)  (These are C# classes we are writing.) ",
        "Best answer": "I'm of the opinion that code coverage alone is a poor metric.  It's easy to produce tons of useless tests that cover the code, but don't adequately check the output, or don't test edge cases, for example.  Covering code just means it doesn't throw an exception, not that it's right.  You need quality tests- the quantity isn't that important. "
    },
    {
        "ID": "1474",
        "Question": "I've read Peopleware in 2009. It was one of the best book I ever read. But this book is a little old. I'd like to know, in your opinion, what is and what is not relevant in this book? ",
        "Best answer": "It's been a while since I read it, but I don't remember anything in the book that wasn't relevant to someone. What stood out the most was the discussion of process improvement using CMM and CMMI, and no mention of agile processes (although the second edition was printed in 1999, which is a few years before the Manifesto for Agile Software Development and agile development went mainstream). But the book is about people, and people haven't changed that much since the first printing of the book in 1987. "
    },
    {
        "ID": "1483",
        "Question": "I've heard it said (by coworkers) that everyone \"codes in English\" regardless of where they're from. I find that difficult to believe, however I wouldn't be surprised if, for most programming languages, the supported character set is relatively narrow. Have you ever worked in a country where English is not the primary language? If so, what did their code look like? ",
        "Best answer": "I'm from Canada, but live in the States now. It took me a while to get used to writing boolean variables with an \"Is\" prefix, instead of the \"Eh\" suffix that Canadians use when programming. For example: MyObj.IsVisible  MyObj.VisibleEh  "
    },
    {
        "ID": "1533",
        "Question": "If you're developer (Senior or Lead Developer) and you'd rather stay with code/design than pursue a management career, what are the available career paths at your company, or any you've heard of? How far can you go? Is it possible to continue being a geek until you bite the dust or is that too naive? Are people like Uncle Bob for example still considered developers, as they claim? ",
        "Best answer": "Depends on the sort of company you work for. Many companies don't value talented, experienced developers as highly as managers, and will never reward them to the same level <- This is not the sort of company people like you should be working for. Other (usually more tech-focused) companies will value their developers more, and staying in technology should not limit your career in terms of reward and status <- you probably want to work for this sort of company. If you have a bit of entrepreneurial spirit you could also start your own company - then you'd be the boss. "
    },
    {
        "ID": "1588",
        "Question": "Use of desktops are decreasing day by day in daily life but for coding purpose are there any reasons for using desktop over laptop?  ",
        "Best answer": "Assuming you have an external monitor and keyboard to connect to your laptop the difference is small.  It is always better to work in a desktop like setting (bigger screen realestate, more ergonomic environment), but you can't take your work with you without a laptop. So if portability is important, get a laptop and a good external screen and keyboard to connect it to. Otherwise you might as well stay with a desktop.  "
    },
    {
        "ID": "1620",
        "Question": "I am excited about the changes in PHP 6 previewed in PHP 5.3+. However, I wonder why it takes so long to release PHP 6? Books about it have been published since 2008, and announcements on it since 2007, but I am yet to hear about an alpha or a beta. Why does it take so long to release it? Or is that the way it goes with all languages when they transition to a major release where I guess it takes around 4-5 years to release? ",
        "Best answer": "The release timetable is not unusual for languages, and it's not even that unusual for PHP: 5.0 was released in 2004, but 4.0 was released in 2000.  Compare this to the last stable releases for C (2000), Fortran (2003), or C++ (2003). One other thing to keep in mind is that 5.3 was a major release in all but name. It adds a lot of stuff that was originally destined for PHP 6. Due to development problems with unicode support (a major part of PHP 6), it was decided to release what was stable at the time as a 5.x branch. "
    },
    {
        "ID": "1701",
        "Question": "I am currently reading the recently published Being Geek by Michael \"Rands\" Lopp and I can't get enough of it.  Is there any other career guidance books aimed directly or indirectly at programmers that are worth reading? ",
        "Best answer": " Code complete  The Pragmatic Programmer   "
    },
    {
        "ID": "1745",
        "Question": "To put it another way... What is the most commonly held and frustrating misunderstanding about programming, you have encountered? Which widespread and longstanding myths/misconceptions do you find hard for programmers to dispel/correct. Please, explain why this is a myth. ",
        "Best answer": "That because you're a programmer, you know how to fix [person]'s virus ridden machine. "
    },
    {
        "ID": "1752",
        "Question": "In fact this question is about cautions to be taken to enhance quality user experience and reduce avoidable support calls. ",
        "Best answer": "A lack of proper input validation is one of those things which tends to lead quite quickly to users doing \"bad\" things with your application, when it should really be handled by the programmer. I've seen legacy apps where users have been trained to:  not enter apostrophes in names not enter any symbol other than a-z0-9, ensure there are no spaces before or after the text they've entered check that a correctly formatted email address is being entered in to the email field, otherwise subsequent mailings to that user will use whatever's in the field and will fail make sure \"http://\" is put before web addresses  etc etc All of the above issues are ones which should be handled by an application developer. When your input validation is essentially \"make sure the user knows what format this field should be in and trust what they've entered is right\", then unexpected things are bound to find their way in to the app. Aside from the obvious security implications, users make mistakes. As programmers we often produce our best products by bending over backwards to make sure that the user can't get it wrong, no matter how hard they try! "
    },
    {
        "ID": "1785",
        "Question": "Please, stay on technical issues, avoid behavior, cultural, career or political issues. ",
        "Best answer": " The bug is in your code, not the compiler or the runtime libraries. If you see a bug that cannot possibly happen, check that you have correctly built and deployed your program.  (Especially if you are using a complicated IDE or build framework that tries to hide the messy details from you ... or if your build involves lots of manual steps.) Concurrent / multi-threaded programs are hard to write and harder to properly test.  It is best to delegate as much as you can to concurrency libraries and frameworks. Writing the documentation is part of your job as a programmer.  Don't leave it for \"someone else\" to do.  EDIT Yes, my point #1 is overstated.  Even the best engineered application platforms do have their share of bugs, and some of the less well engineered ones are rife with them.  But even so, you should always suspect your code first, and only start blaming compiler / library bugs when you have clear evidence that your code is not at fault.   Back in the days when I did C / C++ development, I remember cases where supposed optimizer \"bugs\" turned out to be a due to me / some other programmer having done things that the language spec says have undefined results.  This applies even for supposedly safe languages like Java; e.g. take a long hard look at the Java memory model (JLS chapter 17). "
    },
    {
        "ID": "1849",
        "Question": "If you've always loved unit testing, good for you! But for the unfortunate ones who weren't born with a liking for it, how have you managed to make this task more enjoyable ?  This is not a \"what is the right way to unit test\" question. I simply want to know little personal tricks that reduce the boredom (dare I say) of writing unit tests. ",
        "Best answer": "Firstly, I agree with you - if you are writing your unit tests on already completed code, or you are manually unit testing your code, I find that extremely boring too. I find there are two ways of unit testing for me that really make it enjoyable:  By using Test Driven Development (TDD) - writing the tests first allows me to think about the next piece of functionality or behaviour that I need in my code. I find driving towards my end goal in tiny steps and seeing tangible progress towards that goal every few minutes extremely rewarding and enjoyable. When there are bugs, rather than going straight to the debugger, it's a fun challenge to figure out a way to write a failing unit test that reproduces the bug. It's extremely satisfying to finally figure out the circumstances that make your code fail, then fix it and watch the bar turn green for the new failing test (and stay green for all of your existing tests).  "
    },
    {
        "ID": "1877",
        "Question": "As an example, say there's an interface that contains a table/grid of information that is periodically updated.  The table is meant to represent an event that has happened, perhaps the date and time of a stock price change. The actual frequency of these events could be dozens of events per second.  This is obviously too much information for a user to process/understand, so I'm trying to find out how much information a user COULD process in a given amount of time so that we can throttle the data and come up with an alternate display. I know some studies have been done on this, but I can't seem to find an authoritative source. ",
        "Best answer": "There is research into this topic but it will give you a complex answer.  You can increase how much a person can take in from a UI if you use different sensory modalities rather than just one.  For example using sights and sounds you may be able to pump more information into a user than using just sight or just sound.  There are also findings that suggest that if your user has to really process or think about the inputs there are more significant bottlenecks that are more difficult to avoid even if you cross sensory modalities.  Training helps.  Expert users can process more but in the typical cases you will run into limits. But to get down to your question of how fast you can change the display in particular table:  You can look into the Psychology literature on the topic of \"Attentional Blink\"  and \"Psychological Refractory Period (PRP)\"  but the general advice that I can give you from that is don't push faster than changes every 500ms for a single watched location.  Typical users can need that much time to process even simple single location changing input.  If you're doing it continuously 500ms is a speedy but perhaps roughly workable rate.  You may be able to push down to 250ms but this will depend on what percentage of your users you're willing to put off.  Also if your users are having to scan multiple locations for possible changes you may have to slow down even from a 500ms change rate.  This doesn't necessarily mean 1000ms if you have two locations.  It's not a linear relationship but the answer for that is going to be more complex and depend a lot more on what your UI looks like exactly. Wikipedia: Attentional Blink "
    },
    {
        "ID": "1885",
        "Question": "As programmers I think we are the most inclined to use new technologies and things that are not so mainstream. I absolutely love OpenId's ability to log in with credentials you already have, but I always use a provider that I already use and that is the fastest to log in, like google. God knows I'd never used it if i had to type in the whole \"https://www.google.com/accounts/o8/id\" I like to click on the google button and then just say yes (the first time, then its just clicking the button). Does any of you ever uses the Generic \"Enter your own provider\" OpenId? Do you think its worth to even offer it to more casual end users? ",
        "Best answer": "Yes. I set up delegation on my own site, so I just use one of my own URLs. Easy to remember, and doesn't tie me to a provider. "
    },
    {
        "ID": "1890",
        "Question": "What is the best practice, most commonly accepted naming conventions for private variables in C#?  private int myInteger; private int MyInteger; private int mMyInteger; private int _myInteger; private int _MyInteger; Mysterious other option  Which do you use and why?  (My company is fairly new to C# and I would like to pick the most \"industry accepted\" method to try and get into our coding standard.) ",
        "Best answer": "The MSDN class design guidlines http://msdn.microsoft.com/en-us/library/ta31s3bc.aspx recommends option 1 - myInteger.  I have always used this style. I have a personal dislike for the _ character. "
    },
    {
        "ID": "1947",
        "Question": " Possible Duplicate: Really “wow” them in the interview   Let's say I appear for an interview. What questions could I expect and how do I prepare? ",
        "Best answer": "Programming Interviews Exposed is also helpful.  This classic book uncovers what   interviews are really like at   America's top software and computer   companies and provides you with the   tools to succeed in any situation. The   authors take you step-by-step through   new problems and complex brainteasers   they were asked during recent   technical interviews. 50 interview scenarios are presented   along with in-depth analysis of the   possible solutions. The   problem-solving process is clearly   illustrated so you'll be able to   easily apply what you've learned   during crunch time. You'll also find   expert tips on what questions to ask,   how to approach a problem, and how to   recover if you become stuck.  I've used it in preparing for my last round of interviews and while I didn't end up needing it, reading through it certainly made me feel more confident and prepared. The book also has a section on non-programming questions such as salary negotiation, which I found very helpful. "
    },
    {
        "ID": "1997",
        "Question": "People make mistakes, even in the real life... Which should we, geeky programmers, avoid? ",
        "Best answer": "Learn that what constitutes \"An acceptable degree of precision\" to you is \"Annoying goddamn nitpicking\" to most of the world. "
    },
    {
        "ID": "2051",
        "Question": "See title, but I am asking from a technical perspective, not   Take my 40 year old virgin niece on a date or you're fired.  ",
        "Best answer": "To market Neal Stephenson's sci-fi thriller Snow Crash, I was asked to write a \"benign\" computer virus. It would \"benignly\" pretend to take over the user's computer and replace the screen with snow, a.k.a., a \"snow crash.\" After a minute or so of snow, the snow would fade out and be replaced by an advertisement for the book. This would be \"benign,\" you see. The virus would spread through normal means, but nobody would mind because after taking over their computer \"you'd just get a fun ad and then be relieved that nothing bad happened to your computer.\" I was actually told to do this at a major worldwide corporation. I had to write a memo explaining all the laws this would break and all 17 bad things that could happen if they really made me implement this. "
    },
    {
        "ID": "2086",
        "Question": "When writing or using an algorithm, should the Big Oh complexity be mentioned? ",
        "Best answer": "If you can back it up with real analysis, then yes, absolutely. @Casebash test != analysis.  If it should always be documented then just start throwing  // this algorithm is O(n!) on every function.  I've worked with people who would say things like 'This function is O(1) because there are no loops', and then I would point to the call $(someHugeList).each(function(//... "
    },
    {
        "ID": "2185",
        "Question": "When should a project be released to alpha, beta and to the public? Is it a good idea to extend the alpha and beta phases when it is needed? When in a later phase (eg. beta), is it wise to go back to an early phase (eg. alpha) if it didn't work out? ",
        "Best answer": "The right time is when the product is ready for each stage. It's up to you and the customer to define \"ready\". This might be when a certain number of bugs have been found and fixed or how much documentation has been completed. It all depends on the application and what the customer is expecting. I'll ignore the alpha phase for now. A beta phase is usually when you think you're feature complete for this release and require feedback on the fine details of your application. This is often seen as public testing, but that depends on your application. A small scale application will be beta tested by a few select users, a large application (like Visual Studio, or Stack Overflow) will be beta tested by anybody and everybody willing to help. The official release is when you (and your customers/users) are confident that your product can do the job it's designed for. It might not have all it's features, but those you implemented for this release will be. Alpha testing is more nebulous. It means different things to different people. You might release an early version that not feature complete because you need more user feedback on one particular aspect. You might also need to get something \"out there\" to meet a specific demand and can't afford to wait. "
    },
    {
        "ID": "2192",
        "Question": "What things tend to slow a developer down? Please try to refrain from posting answers that:  are slow now but useful in the feature. (TDD, Refactoring, ...) list a distraction.  ",
        "Best answer": "Oh this ones easy:  Meetings More Meetings Meetings about the last meeting Meetings to prepare for the upcoming meeting Developing a power point presentation for a meeting Developing a power point presentation for a meeting discussing features that haven't been implemented, shouldn't be implemented, and for whatever reason that guy from sales will jump all over.  I can't predict what document you want displayed in the app based upon your current location without an internet connection or access to your hard-drive.  No really, just give up asking for it too.  "
    },
    {
        "ID": "2204",
        "Question": "What steps and measures can I take to prevent deep indentations in my code? ",
        "Best answer": "The best thing you can do is extract methods: int Step1(int state) {     if (state == 100)     {         return Step2(state);     }     else     {         return Step3(state);     } }  int Step2(int state) {     if (state != 100)     {         throw new InvalidStateException(2, state);     }      // .... }  "
    },
    {
        "ID": "2226",
        "Question": "In an earlier question, I asked for career advice for new software engineers who did well before and during college.  But what about people who weren't fortunate enough to go to MIT or Yale, for whatever reason?  What if you went to what Joel Spolsky calls a JavaSchool? What can/should JavaSchool alumni do to develop their skills and make up for the things they missed in college?  (Or, was Joel wrong about those schools being disadvantageous?) ",
        "Best answer": "Despite the claims made by Joel in that article- and he concedes the point himself- a lot of the subject areas that may be missed by a \"JavaSchool\" are not necessary of many jobs. I attended something that I suppose resembles a JavaSchool in that we spend most of our time focusing on high level languages like C# and Java, but that doesn't change the fact that \"Algorithms & Data Structures\" is still part of the required class list- not to mention all of the other theory-oriented classes. Granted not all \"JavaSchools\" are the same, but that isn't the point. In my opinion, more important than an understanding of some of the grittier development topics is being able to problem solve effectively when unique challenges arise. As software engineers we do the vast majority of our learning on the job and as such, two of the biggest aspects of our job description are being able to problem solve and being able to pick up unfamiliar concepts. If, during an interview, one is unable to make a discernible and logical attempt at solving a problem which is new to them, then their incompatibility for a given position will likely reveal itself. Obviously, when hiring someone for a position that requires constant exposure to and use of some intricate topic that may be missed by a JavaSchool, it is often the logical choice to go with someone who has a prior understanding, but lack of experience shouldn't always preclude job eligibility.  More than likely, the 50 year old Java guy at your company that has been there for as long as anyone can remember did not have any understanding of Java until his job (current or previous) asked him to learn it- and he did so. Strictly speaking, it's bad practice to fire \"the old guy\" so that a younger and more \"up-to-date\" candidate can take his place; that being said, if the job description for any employee young or old changes, it is the responsibility of that employee to get caught up or find a new job. Just because an individual (especially a programmer with past experience) doesn't understand some concept, doesn't mean they are unwilling or incapable of learning it. In fact, if they are unwilling to learn then they probably do not belong at any job- much less yours. It's fair to say that some \"JavaSchools\" are better than others, and that fact should certainly be considered when selecting a candidate for a position, but there are a lot more important personal traits than just where someone went to school. It is our aptitude to tackle a problem and find a solution that defines us as engineers, most everything else is secondary. "
    },
    {
        "ID": "2247",
        "Question": "How can I track that I'm developing software more or less productive than the previous days?  ",
        "Best answer": "There's a simple answer: you can't. And moreover, you shouldn't. You want to measure your own productivity, but you can generalize: how can you measure productivity of programmers? First of all you have to define what you mean for \"productivity\": amount of code produced? Amount of design (or specification) implemented? Number of issues fixed? Quality of produced code? (Yes, quality is a productivity counter, you can produce a lot of bad code or few good code, what has been more productive?). All these values can hardly be mapped to a daily base, and any attempt to track daily productivity is dangerous for the project, for the company, and for the programmer. My advice is to clearly define what you mean as \"productivity\", then define a measure unit, and apply it on a weekly and monthly base. "
    },
    {
        "ID": "2259",
        "Question": "Sometimes, one creates a exploratory prototype and forgets about structure in the directories... What are good tips on dividing the programming files over (several levels of) directories? ",
        "Best answer": "If your language is OOP and package based (Eg Java), then you should probably keep each package in its own folder (eg my/package/name) to keep with the convention. If your language isn't package based (Eg PHP), then organize by what each file does. Here's an example  Does this do utility functions? Goes in /util Is this a 3rd party plugin? Goes in /plugin Is this part of the admin panel? Goes in /admin, along with ALL supporting files Is this Javascript? Goes in /javascript Is this CSS? Goes in /css Is this a template? Goes in /templates/templateName etc  Language agnostic, Most people have a /src directory for all source files, a /lib directory for libraries, and a /bin or /dist directory for builds.  "
    },
    {
        "ID": "2329",
        "Question": "Google sometimes come up with irrelevant links, not everything is available on SO, there are cases where the local documentation is also annoying to look through... Are there other efficient ways you use to search? ",
        "Best answer": "Practise your Google-fu. Google is pretty awesome, but it's not magic. Sometimes you'll need to use search operators to get better answers, especially on some code phrases that can be difficult to search. Check out the Google Guide, for example. If I know where the answer is, I might use the site: operator, or if I need something citable I often use inurl:edu. Google Code is handy for finding examples, and I use it to search for APIs sometimes. "
    },
    {
        "ID": "2331",
        "Question": "Please, explain why and list which languages have the (mis)feature implemented As far you know. Post what you consider a harmful feature, not what you dislike. ",
        "Best answer": "Register Globals in PHP Information : http://php.net/manual/en/security.globals.php This is by far the worst feature to be ever implemented for readability reasons and security reasons. Basicly all the GET parameter received are transformed into variables. For example with this URL : /index.php?value=foobar You can do the following : <?php echo $value; // return foobar ?>  When you are reading code, it is very confusing to know where the variable comes from. Also if the feature is misused, it can lead to security hole. Here's a code example from php.net that shows how it can be misused : <?php // define $authorized = true only if user is authenticated if (authenticated_user()) {     $authorized = true; }  // Because we didn't first initialize $authorized as false, this might be // defined through register_globals, like from GET auth.php?authorized=1 // So, anyone can be seen as authenticated! if ($authorized) {     include \"/highly/sensitive/data.php\"; } ?>  "
    },
    {
        "ID": "2497",
        "Question": "How useful are infix operators in a programming language? Are they worth the extra complexity they provide? Can you provide any examples where infix operators are better suited to the problem that can't be handled by just overloading the normal operators? ",
        "Best answer": "I think infix operators stem from mathematics. This: 2 + 3 * 4  is more readable to most people, than (+ 2 (* 3 4))  because most people are familiar with mathematics. Interesting enough in Haskell you can hop between infix and prefix. This is using the same funtion \"(+)\": (+) 1 2 1 + 2  and this is using the same function \"elem\": elem 42 [1,2,42] 42 `elem` [1,2,42]  "
    },
    {
        "ID": "2654",
        "Question": "In response to This Question, I ask:   What are the best parts of your coding standard? What are the best practices that help with code quality, reliability, maintainability, readability, etc. Please include the language, the item from the standard, and the reason it improves your code. ",
        "Best answer": " All Languages: Write readable code instead of comments   A comment followed by a block of code can be replaced by a method which states the intent just as well as the comment, and makes the code more modular and reusable as well.  It makes refactoring happen more often.  It helps us write simple, readable clean code. Readable code is a joy to work with.  It tends to make methods short and sweet.  It avoids comments getting out of sync with the code  It challenges you to rewrite commented code that is hard to understand.  Compare this: public void update() {   // Fetch the data from somewhere   lots of lines of;      code;      for;        fetching;      data;   from somewhere;   // Sort the data   more lines of;       code;          which sorts;          stuff;       around;   a bit and then;   // Update the database   lines of code;       which uses;          some lib;          to update;             using iteration;             and logic;        the database;   done; }  With this version where comments are replaced with function calls: public void update() {     data = fetchData();     sorted = sortResults(data);     updateDatabase(sorted); }  "
    },
    {
        "ID": "2700",
        "Question": "I have had a few that I am proud of and some of them were written by myself few years ago. It doesn't have to be necessarily buggy, just bad code.  ",
        "Best answer": "I don't know about being proud of the fix because it was so obvious, but the most horrible code I remember fixing was this. if (userName==\"John O'Reily\") { userName= \"John O''Reily\";} if (userName==\"Stacy O'Neil\") { userName= \"Stacy O''Neil\";} if (userName==\"Finnegan O'Connor\") { userName= \"Finnegan O''Connor\";} ... someSQL = \"SELECT * from Users where UserName='\" + userName + \"'\";  Apparently the previous developer just kept adding new lines every time a new (usually Irish) user started getting errors in the application. I'll leave it as an exercise for the class as to how it was fixed. "
    },
    {
        "ID": "2715",
        "Question": "Should curly braces be on their own line or not? What do you think about it? if (you.hasAnswer()) {     you.postAnswer(); } else {     you.doSomething(); }  or should it be if (you.hasAnswer()) {     you.postAnswer(); } else {     you.doSomething(); }  or even if (you.hasAnswer())     you.postAnswer(); else     you.doSomething();  Please be constructive! Explain why, share experiences, back it up with facts and references. ",
        "Best answer": "You should never do the 3rd method.  Skimping on braces might save you a few keystrokes the first time, but the next coder who comes along, adds something to your else clause without noticing the block is missing braces is going to be in for a lot of pain.  Write your code for other people.  "
    },
    {
        "ID": "2756",
        "Question": "Coding standards are common in any software development organization, but how important are they to follow?  I can understand the need for some consistency, but when dealing with simple things like the position of braces, line length, etc., I'm not sure excessively strict standards contribute much to software development. Isn't it more important that your code is readable, not that it conforms to a predefined standard?  It seems they're more like... guidelines anyway. ",
        "Best answer": "Asking everyone to 100% adhere to the same standard code formatting guideline is like asking everyone to collaborate separately on writing a 100 page paper with the same writing style.   Hopefully everyone will write the paper in English (or same language), but different styles will be apparent.  Some will write it well, others not.  Some will use contractions, some will spell the words out fully (example: it's verus it is).  Etc. I think you touched on the most important points:   It's a guideline Readability  If you want the code to adhere to the same formatting, like a paper to be in the same writing style, it'll need editing and revising.  The code will need to be cleaned up, reviewed, re-factored, etc. I've never been in a shop where I was completely happy with another developer's coding style or formatting (at minimal because it's not exactly like mine).  But I'll be content if I can read/understand it and if it's consistent.  Everything else is the sugar on the syntactic sugar. So to answer your question: somewhat important, but it's certainly not the end of the world if they don't. "
    },
    {
        "ID": "2776",
        "Question": "The Joel Test is a well known test for determining how good your team is. What do you think about the points? Do you disagree with any of them? Is there anything that you would add? ",
        "Best answer": "Jeff Atwood has The Programmer's Bill of Rights. From the post:   Every programmer shall have two monitors Every programmer shall have a fast PC Every programmer shall have their choice of mouse and keyboard Every programmer shall have a comfortable chair Every programmer shall have a fast internet connection Every programmer shall have quiet working conditions   This seems to have some items that I'd like to see on Joel's list.  Specifically in the area of hardware (dual monitor, fast PC, mouse/keyboard, comfortable chair, fast connection).   The only thing not mentioned is having a comfortable and adjustable desk. This could all be added by changing: Current #9: Do you use the best tools money can buy? to  Improved #9: Do you use the best tools and equipment money can buy? "
    },
    {
        "ID": "2777",
        "Question": "I have heard a lot of people mention Code Complete as a book worthwhile reading. Unfortunately, I am so busy that I don't have time to read it, so can anyone tell me what the key points of the book are? ",
        "Best answer": "Code Complete is about software craftsmanship; it is an advanced-beginner/intermediate-level book, written for the working programmer, but it would still be very useful to someone who's been programming for at least a year.   Thus the key points of Code Complete (2nd ed.) are nicely summarized in its Chapter 34, Themes in Software Craftsmanship.  As paraphrased from my notes:  Conquer Complexity: reduce the cognitive load on your mind via discipline, conventions, and abstraction. Pick Your Process: be conscious of quality from start (requirements) to finish (deployment) and beyond (maintenance). Write Programs for People First, Computers Second: code readability is hugely important for comprehensibility, review-ability, error-rate, error-correction, modifiability, and the consequent development time and quality. Program into Your Language, Not in it: think of the What? and Why? before the How? Focus Your Attention with the Help of Conventions: conventions manage complexity by providing structure where it's needed, so that the ultimate resource - your attention - can be effectively used. Program in Terms of the Problem Domain: work at the highest level of abstraction possible; top-level code should describe the problem being solved.  Distinguish OS level, programming language level, low-level implementation structures, low-level problem domain terms, and finally, high-level problem-domain terms that would make total sense to the (non-coder) user. Watch for Falling Rocks: as programming merges art and science, good judgement is vital, including heeding warning signs. Iterate, Repeatedly, Again and Again: iterate requirements, design, estimates, code, code tuning.  Thou Shalt Render Software and Religion Asunder: be eclectic and willing to experiment.  Don't be an inflexible zealot, it precludes curiosity and learning.  Go beyond having just a hammer in your toolbox.  But the most important take-aways are in Chapter 33, Personal Character: once you consciously seek to improve as a coder, you can and will.  The fastest way to do so is to take on the the attitudes of master coders (humility, curiosity, intellectual honesty, discipline, creativity), while also practicing their habits (many good habits are listed in the book, e.g. choosing good variable/value names).   Also, the book makes clear that the gap between average and excellent in software is immense; that fact alone should drive the conscientious coder to better himself. That's the short of it; the long version is in the book. :)  I can also send you my not-so-long, not-so-short notes if you want more details.  But the book is certainly money and time well-spent, even if the writing style is tiresome at times.   Beyond Code Complete, I'd highly recommend The Pragmatic Programmer.  It's for intermediate-level programmers, nicely-written and a great mix of high, medium, and low-level advice. "
    },
    {
        "ID": "2829",
        "Question": "I originally thought of creative commons when while reading a book about wordpress (professional wordpress), I learned that I should also specify that the product is provided   ... WITHOUT ANY WARRANTY; without even the   implied warranty of MERCHANTABILITY or   FITNESS FOR A PARTICULAR PURPOSE  and they recommend GNU GPL.  How do I write a license or select 1? btw, what does MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE mean actually? Isn't without warranty enough?  ",
        "Best answer": "For small bits of code, I generally release them under the X11 licence. The problem with the GPL is that it's far too complicated for code that you don't really care enough about to protect. If you really don't want people using your code in commercial products, you would need to monitor for infringement and fight it out in court, which isn't really worth the time or the effort for small, free, open-source projects.  Copyright (c)   Permission is hereby granted, free of charge, to any person obtaining a copy    of this software and associated documentation files (the \"Software\"), to deal    in the Software without restriction, including without limitation the rights    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell    copies of the Software, and to permit persons to whom the Software is    furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in    all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN    THE SOFTWARE.   EDIT: If the body of code is more substantial, and you feel that you've invested enough time in it that you would be willing to protect it, by all means use the GPL to protect it. "
    },
    {
        "ID": "2932",
        "Question": "When I say Free Software I mean it in the FSF terms. Free as in Free Speech, not as in Free Beer. Why is it a good idea for programmers to use and write Free Software? ",
        "Best answer": "There are literally scores of different reasons why someone might choose to distribute Free software: that's why there are scores of different F/OSS licenses. My favorite reason for going Free is from Linus Torvalds on why he chose and sticks with GPLv2:  Me, I just don't care about proprietary software. It's not \"evil\" or \"immoral,\" it just doesn't matter. I think that Open Source can do better, and I'm willing to put my money where my mouth is by working on Open Source, but it's not a crusade -- it's just a superior way of working together and generating code. It's superior because it's a lot more fun and because it makes cooperation much easier (no silly NDA's or artificial barriers to innovation like in a proprietary setting), and I think Open Source is the right thing to do the same way I believe science is better than alchemy. Like science, Open Source allows people to build on a solid base of previous knowledge, without some silly hiding. But I don't think you need to think that alchemy is \"evil.\" It's just pointless because you can obviously never do as well in a closed environment as you can with open scientific methods.  This goes to Eric S. Raymond's Linus's Law:  Given a large enough beta-tester and co-developer base, almost every problem will be characterized quickly and the fix obvious to someone. Or, less formally, \"Given enough eyeballs, all bugs are shallow.\"  "
    },
    {
        "ID": "2948",
        "Question": "How valuable (or not) do you think daily stand-up meetings are? If you're not familiar with it, this refers to a daily meeting that is part of Scrum adherents (and some other agile methodologies).  The idea is that you hold a daily meeting, timeboxed to 15 minutes, and in which everyone must stand (to encourage people to be to-the-point). In the meeting, you go around the room and each say: - What you did yesterday - What you plan to do today - Any blockers or impediments to your progress. Do you think this practice has value?  Has anyone worked at a place that's done it, and what did you think? ",
        "Best answer": "We had daily standups at my first job. Well, with all the co-ops/interns/temps, it was actually on the long side - usually around 30 minutes. But the idea of a short, timeboxed, daily meeting helped a lot just to know what other people were stuck on - and if it was something I was working on, I could reprioritize my tasks to finish what they needed to continue sooner. It also gave everyone a chance to know what everyone was working on so if someone had an emergency, everyone was at least aware of what was going on - reducing a truck factor is always a good thing. Honestly, every day might be a little extreme in some cases. But the idea of short, regular meetings for everyone to stay on the same page is a valuable addition to any process. "
    },
    {
        "ID": "2959",
        "Question": "One of the criteria of the Joel Test is daily builds. The idea is that if the build is broken, whoever broke it is around to fix it up. If the build cannot be fixed, everyone will have to check out an old version and work on that. I can understand how this can be pretty bad on centralised version control where it is important to avoid merging and branching as much as possible, but this only sounds like a minor nuisance for distributed version control. Do you agree with this? Are there other reasons why daily builds are important? ",
        "Best answer": "I think what's important to note here is that regular builds help catch errors sooner rather than later. It doesn't have to be daily, but often enough. Ideally, it can also run your unit tests. The goal is to find out when a build breaks before the final testing phase, to find them as soon as possible. Just set it up to build your main development branch(es). We use it at work (although we build hourly), and often when we forget to set-it up we find about problems just hours before releasing. "
    },
    {
        "ID": "3049",
        "Question": "I would like to do some web programming using functional programming. What decent web-frameworks exists for functional programming languages? ",
        "Best answer": "Compojure is an open source web framework for the Clojure programming language. http://en.wikibooks.org/wiki/Compojure "
    },
    {
        "ID": "3069",
        "Question": "If you browse the code golf questions on StackExchange, you notice a ton of non-standard but real world (Eg not brain-f*ck) languages like F#, Scala, R, J and Clojure. Visiting their websites, some of the languages look interesting solving problems in whatever language your already using. However there is coding in spare time, and coding for money. The closest thing we have to the truth on who uses a language is the TIOBE Index, which lists none of the above languages in the top 10-15. Which makes me wonder if I'll ever use them. Should I even bother learning some of the small niche languages? It doesn't seem I would be making money, and some will probably fail anyway. ",
        "Best answer": "The Sapir-Whorf Hypothesis.  It states, more or less, that what you can think of is limited by what you have the language to describe.  The consequence of this is that multi-lingual people are able to think in ways that single language speakers may not be able to. Many people (myself included) think this holds true to programming as well.  Learning Lisp, for example, teaches you a whole new way of thinking about problems that you can apply to C# or Java or Python.  The more language paradigms you've learnt, the more tools will be in your mental toolbox, regardless of what language you're actually using. "
    },
    {
        "ID": "3199",
        "Question": "Are different version naming conventions suited to different projects? What do you use and why? Personally, I prefer a build number in hexadecimal (e.g 11BCF), this should be incremented very regularly. And then for customers a simple 3 digit version number, i.e. 1.1.3. 1.2.3 (11BCF) <- Build number, should correspond with a revision in source control ^ ^ ^ | | | | | +--- Minor bugs, spelling mistakes, etc. | +----- Minor features, major bug fixes, etc. +------- Major version, UX changes, file format changes, etc.  ",
        "Best answer": "I tend to follow Jeff Atwood's opinion of the .NET convention of version numbering.  (Major version).(Minor version).(Revision number).(Build number)  More often than not, for personal projects, I find this to be overkill. The few times where I have worked on substantial projects like search engines in C# I've stuck to this convention and have been able to use it as an internal tracker effectively. "
    },
    {
        "ID": "3272",
        "Question": "How would you, as someone involved in the hiring process (manager,interviewer, etc) feel about a candidate that has changed jobs every 1-2 years? updateThanks for all the input everybody, some really great responses, and good info in every post.  I asked it because I'm currently at my 3 job in the last 5 years and I'm feeling like my position is going nowhere (like the position should have been contract in the first place, not full-time).   My only options here seem like transition to a different team doing something I'm not really interested in or look for new work, but I'm a little afraid my recent job history is all short stints. ",
        "Best answer": "It depends on the context:  In a startup culture (like Silicon Valley), one to two years is the lifetime of many companies, and it's expected you'd be switching your place of employment that often. If you're a contract worker, a contract may only be a short, set timespan. Everywhere else, one to two years is an unusually short stay at a company.  In any context, employers are generally looking for a person who's going to be in it for the long haul, whatever the long haul is for the company:  Startups are looking for someone who will last until the exit: acquisition, IPO, shuttering, etc. Contract hires should be able to successfully complete their contracts to term. Other companies are looking for an employee who will last long enough to make a return on the investment of hiring them: this can take several years.  It's a red-flag to potential employers if you're constantly leaving your job for personal reasons, even if you have perfectly valid reasons. I'd also note that having experience in one context isn't necessarily going to translate to another.  For example, if you're a life-long contract worker, it can look just as unappealing to a company looking to hire full-time employees as someone who went from regular job to regular job. Similarly, a person who stayed at a job for 10 years might be unappealing to a startup that wants people who are constantly looking for the next big thing. "
    },
    {
        "ID": "3277",
        "Question": "Today I found a GPLed project on SourceForge whose executables are spreading a virus. This fact has been pointed out several times in reviews of the project and the infected executable is still available for download. Apparently, older executables are not infected, so the project itself does not seem to be made with malicious purpose in mind.\r \r There is no preferred way to contact developers and forums for the project are dead.\r \r What should I do?",
        "Best answer": "If you can't get in touch with the developers, then contact SourceForge.  Report the problem, give them detailed information they can use to verify the issue, and they'll (probably) take it down.  They're a reputable site and I imagine they wouldn't want to be associated with malware. "
    },
    {
        "ID": "3317",
        "Question": "What's the difference in this terminology? Is one considered more professional than the other? ",
        "Best answer": "While the terms can be and often are interchangeable, I view a developer as someone who's involved in the whole process from requirements gathering, through specification and coding to testing and, yes, support. They might not be fully involved in all stages all of the time. A programmer is someone who just concentrates on the coding and has little involvement in the rest of the process. This may be their choice of course. As @sunpech points out in his comment most people writing software these days are (or should be) developers. You have to know much more than just how to code to write good software. "
    },
    {
        "ID": "3383",
        "Question": "Years ago, in my first real programming job, my boss encouraged me to keep a journal of my daily activities.  I still do so, although no longer a paper and hand-written one. Do you keep a journal, if so, what do you write in it, and how does it help you in your job?  Or, does it just take time that is not ever recovered? ",
        "Best answer": "I find an activity log helpful for several reasons:  I fully agree with Jon Sagara in that it helps answering the question \"what have you done past week (apart from sleeping)?\".  Additionally, it helps to keep track of the million interruptions, which are often forgotten but combined they take a lot of time. I also find it a great help for learning to estimate, as it gives you hard figures on how long things take (often longer than you'd think).  "
    },
    {
        "ID": "3425",
        "Question": "And what do you think about operator precedence? Would be harder programming in a language where the operations are executed in sequential order? Ex.: 2 + 3 * 4 == 20 2 + (3 * 4) == 14 OK, the Lisp family doesn't have precedence by definition. Let's gonna talk about procedural and object-oriented languages using this \"feature\". ",
        "Best answer": "Smalltalk. Everything's done with message sending, so 1 + 2 * 3 means \"send * with parameter 3 to the object returned by sending the message + with parameter 2 to the object 1\". That throws people (it threw me) because of how we usually write maths, but since I can never remember C's operator precedence I cope in the same manner in both languages - I use ()s to group terms: 1 + (2 * 3). "
    },
    {
        "ID": "3438",
        "Question": "It seems like in language holy wars, people constantly denigrate any feature they don't find particularly useful as being \"just syntactic sugar\".  The line between \"real features\" and \"syntactic sugar\" tends to get blurred in these debates.  What do you believe is a reasonable and unambiguous definition of syntactic sugar that avoids it being defined as any feature the speaker/writer doesn't find useful? ",
        "Best answer": "How about this: \"syntactic sugar is a convenience shorthand for some functionality that does not introduce any meaningful layer of abstraction.\" Take a->b, which, as you point out, is equivalent to (*a).b.  Does this notation allow you to consider the code it's in any useful, otherwise hidden manner?  No, so it's syntactic sugar. Now consider a[i] == *(a + i).  Think about any C program that uses arrays in any substantive way.  Can you imagine trying to comprehend it without the [] notation?  With multidimensional arrays?  It is meaningful to consider arrays as whole units, not as a reference to the start of a contiguous block of memory.  While it does help to know how arrays work in C if you're planning on doing complicated things with them, it is unproductive to always have to think \"I need to store the two bits of memory 2*i bytes to the right of the memory location referenced by a.\"  The whole point of an array is the ability to abstract away the process of storing a sequence as a coherent unit.  The [] notation facilitates this abstraction.  It's not syntactic sugar. This is not to imply that syntactic sugar is always bad thing.  Like many alliterations, it has become an epithet and pitted against \"real features.\"  But LISP and Scheme, for example, would be unreadable if not for the let shorthand (and others). The ternary operator, <pred> ? <cnsq> : <alt>, is another example.  Syntactic sugar can help to organize programs and remove redundant code, which may save in maintenance down the line.  Syntactic sugar may sometimes be preferable to piling on \"real features\" if it helps to remove syntactic barriers to programming. To quote R^5RS, \"Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary.\"  IMHO, syntax can qualify as a weakness and restriction and so letting programmers get away from syntax can increase a language's expressivity. "
    },
    {
        "ID": "3519",
        "Question": "I am C++ developer with some good experience on it. When I try to learn a new language ( have tried Java, C#, python, perl till now) I usually pickup a book and try to read it. But the problem with this is that these books typically start with some very basic programming concepts such as loops, operators etc and it starts to get very boring soon. Also, I feel I would get only theoeritcal knowledge without any practical knowledge on writing the code. So my question is how do you tacke these situations? do you just skip the chapters if its explaining something basic? also, do you have some standard set of programs that you will try to write in every new programming language you try to learn? ",
        "Best answer": "Basically by writing code in that language. You need to have a good example application to study/modify otherwise you're starting off on the wrong foot and you might never recover. Years ago the company I worked for at the time decided to use Ada for their next product, but as all the developers used FORTRAN in the previous product we ended up creating FORTRAN constructs in Ada. We never really recovered from that. Having access to the documentation and Stack Overflow is essential otherwise you'll potentially miss the important features of the language. On that score find out who are the Gurus in the language and read their blogs, these will often discuss the new features of a language/framework and also the obscurer areas you'll never find by yourself. If you can't find out who they are ask here! In an ideal world I'd like to learn by myself for a while and then be evaluated, but I've never managed that yet. "
    },
    {
        "ID": "3558",
        "Question": "At some point in time, I just stopped coding for fun.  I used to go to work, finish my assignments and then upon arriving home I'd go and write stuff on the side for fun.  However, I now just go home and try to avoid the computer.  I'd rather read the paper, watch TV, go out to the bar, etc. Is this a bad sign?  I mean I still try to keep up on the latest trends, hit up the developer forums/blogs/etc but I haven't said, \"I want to learn language X - I wonder if I could write app Y in it\" Has this happened to anyone else? ",
        "Best answer": "This is a very common issue called burn-out. It happens to everyone that takes their work seriously. My advice is to take a few weeks off from coding and plan a long term project for fun. Then set aside at least 15 minutes each night to complete a part of the project. As long as you take it slow you'll be back in the game in no time. "
    },
    {
        "ID": "3645",
        "Question": "I am a computer science student and learning Java now a days.  I want to be a good developer/programmer.  I like reading books. I search on the internet for the related topics and study them. I refer to StackOverflow and other good programming websites daily but I code rarely. Is this a bad sign? If yes then what should I do to overcome this problem? ",
        "Best answer": "Experience trumps all, if you aren't getting experience then yes you definitely have a problem if you want to be a great programmer. Start on a new project or join another person's open source project.   Get some experience.  Write some code. "
    },
    {
        "ID": "3678",
        "Question": "I work with C# professionally and I write code like this all the time. private IEnumerable<Something> GetAlotOfSomething() {     if (somethingA.IsReady)         yield return somethingA;      if (somethingB.IsReady)         yield return somethingB;      if (somethingC.IsReady)         yield return somethingC;       // ... More complex logic }  var specialSomethings =      GetAlotOfSomething()     .Where(s => s.IsSpecial);   Then one day I have to write a bit of VB6 or JScript and I end up writing so much boilerplate just to get things done. Anyone thoughts? ",
        "Best answer": "Iterators (generators etc) are certainly great features that I use a lot.  I don't qualify them as necessary, but I will certainly choose languages that have them when I get a choice. "
    },
    {
        "ID": "3730",
        "Question": "What tools do they use? What processes? What rules do they have regarding code? How do they test their code? ",
        "Best answer": "I was searching a couple of weeks ago for some info about google development methodologies and found the following which I posted on my blog  Steve Y - Good Agile vs Bad Agile A summary of google methodologies from Steve Y's post Google Product Development/Management Process  I can't post more than one link at the moment, though, (stackexchange spam prevention apparently), so follow the link to my blog or google the above strings. Chris. "
    },
    {
        "ID": "3747",
        "Question": "We have a legacy classic ASP application that's been around since 2001.  It badly needs to be re-written, but it's working fine from an end user perspective. The reason I feel like a rewrite is necessary is that when we need to update it (which is admittedly not that often) then it takes forever to go through all the spaghetti code and fix problems.  Also, adding new features is also a pain since it was architect-ed and coded badly. I've run cost analysis for them on maintenance but they are willing to spend more for the small maintenance jobs than a rewrite.  Any suggestions on convincing them otherwise? ",
        "Best answer": "I believe there's two factors you should consider that you at least didn't cover in your Q. Let me define these as I use them, then I'll get onto the business of answering your Q.  Risk Opportunity cost  Risk is probably obvious: The chance that they pile a mountain of money into something that goes nowhere. Risk is compounded by what Brooks called \"Second System Effect\" and the rest of us call \"Gold Plating\". Every rebuild I've seen carries risk from people who add every feature they didn't add the first time around. Opportunity Cost in this context is the cost associated with you rewriting functionality that from the business perspective was working fine. It is Opportunity Cost because it means you don't have the opportunity to add features.  To sell something that is purely a refactor is hard because Risk and Opportunity Cost both have money attached to them from a decision making perspective. What I generally recommend is that instead of selling a rewrite of the system, you sell an \"improve as you go\" at a component level. It costs more because you have to build adapters/facades/proxies, but it's less risky and easier to sell. I've been there on the \"we need to rebuild it all\" and it just doesn't go well.  And here's the rub: Over time, all systems turn into garbage unless you are disciplined enough to keep them from doing so.  Which leaves me with this question back to you: If you can't sell them, or even your team, on doing the right thing day to day, what makes you think you can actually see a rewrite through? It really does take some serious introspection to answer that question honestly. Sometimes you've been handed a system from someone who had no clue. Sometimes you've been handed a system by someone who started with the best of intentions and on the right foot but got compromised by a poor corporate culture along the way. If you can't tell which it is, you need to find out soon! "
    },
    {
        "ID": "3766",
        "Question": "I mean seriously, how do you tackle a guy who even changes our variable names (even though they are reasonable) and sends back the code (after review) like 4 times? I know for sure I'm not that bad a developer! So many times, he enforces his ideals, which are not even best practices in the industry! I point out to him whatever link I can find on the internet trying to prove my point, but in the end he uses his authority to shut us out. Sick and tired. Frustrated. Do I have any way out other than quitting the job? ",
        "Best answer": "Quit and find another job. Doing something about your boss is a lost cause so you might as well just find a new job and hope that your new boss isn't a douche like your previous one. Also, judging by his character, I'm pretty sure that even his boss won't be able to do anything about his attitude problems. "
    },
    {
        "ID": "3851",
        "Question": "How would you consider that a programmer is bad at what he or she is doing? If possible... How should he/she improve? ",
        "Best answer": "When they fail to learn from their mistakes and from peer reviews. We are all green at some point; however, if you're not getting better or attempting to get better then you're a bad programmer. "
    },
    {
        "ID": "3884",
        "Question": "I learned about them in a Structured Programming course, but never saw them used thereafter either at the analysis phase or for documentation purposes. Not even for highly structured languages like Pascal (Delphi). Does any of you actually use Nassi-Shneiderman diagrams? If yes, what tools do you use to create/maintain them? edit: Or have you never heard of them? ",
        "Best answer": "Heard of Nassi-Shneiderman diagrams, although I don't use them myself.  I can't help posting a link to the rejection letter that Nassi and Shneiderman received from Communications of the ACM when they first proposed the diagram:  http://www.cs.umd.edu/hcil/members/bshneiderman/nsd/rejection_letter.html "
    },
    {
        "ID": "3918",
        "Question": "What should you do, if a co-worker is editing your code?   Without the purpose of adding functionality or fixing bugs, just to change how it looks... ",
        "Best answer": "Talk to them about it.  Go into the conversation with the attitude of \"They're not doing this to annoy me or because they have some form of obsessive-compulsive disorder; they're trying to make my code better.\" Because you could be wrong.  That could be a subtle bug fix and you just didn't spot it. Or, it could be that there's a coding standard you don't know about that you're violating, and they're just correcting it. Or, it could be that they're trying to annoy you, or they have some form of obsessive-compulsive disorder.  If that's the case, ask them nicely to stop, and if that doesn't work, take it up with your boss. But you'll never know unless you ask. "
    },
    {
        "ID": "3921",
        "Question": "For which issues is it right to edit the code written by a co-worker? For which issues is it wrong? ",
        "Best answer": "When it's right When the edit improves the functionality of the program or makes the code itself more readable / maintainable. When it's wrong When the edit harms the functionality of the program or serves no purpose apart from providing the editor with busywork. "
    },
    {
        "ID": "3956",
        "Question": "In Windows the default way is registry. This allow you to differentiate system-wide and per-user settings. In Unix you should use text files in the /etc folder for system-wide settings (what's the convention for per-user settings?). Many new programs (and especially those designed for being portable) use XML files.  What's the best way (and location) to store non-BLOB settings? Should we follow each system default or have a unified solution? And what's the best portable way?  ",
        "Best answer": " What's the best way (and location) to store non-BLOB settings?  On Windows, it seems acceptable to use the registry. In my opinion, the registry was a poorly-devised system, and instead a simple text file in the Users\\Username\\AppData directory should be preferred. This is easier to back up, less dangerous for users to modify, and easier to clean up. On Linux and most Unixes, The preferred location is /home/user/.config/appname for user-specific settings and /etc/ for global (system-wide) settings. The less-preferred (but acceptable) location for user settings is ~/.appname, but this is generally falling out of favor. These files should be user-editable, so a human-readable format is always preferred. I disagree with most people that XML is an acceptable format for storing non-blob data. It is, in my opinion, an overwrought and excessively complex format for what usually ends up being very small pieces of structured data. I prefer to see files in YAML, JSON, ASN.1, name=value pairs, or similar formats. Having too much syntax makes it too easy for a user to mess up and leave the file in an invalid format.  Should we follow each system default or have a unified solution?  That is entirely up to you, but keep some things in mind:  Platforms like *nix have strict limitations on which locations are writable. More strict than Windows. So:  The only place you should write to anything is in the user's home directory. Unless your application is a system service; in which case, all mutable data files should be written in /var/. Nonmutable data files should be kept in your app directory in /usr/share/ or /usr/local/share/ or /opt/ Configuration files in /etc/ should never be written to by the application when it is running, even if it has write access to them. /etc/ should be the repository for default behaviors and nothing else. Plan for your application to be installed in one of three places: /usr/local/, /opt/appname, or /home/username/appname. Blobs should be stored alongside other configuration files if they are to be changed. It is generally preferable to use a user-editable format, so something like SQLite or Berkeley DB is preferred (since there are command-line tools for each), but not required.  On Windows, your applications should only ever write in the User directory. The standardized location for data files is Users\\User\\AppData. Nowhere else seems acceptable. On Mac OS X, your application settings should be stored in ~/Library/Preferences along with all of the other applications' plist files. plist seems to be the preferred format, but you'll want to double-check with the Apple guidelines.   And what's the best portable way?  There is no \"best,\" to be honest. There are only platform-specific limitations and expectations. My recommendation is to stick with platform-specific means, even if it means writing more code. "
    },
    {
        "ID": "3967",
        "Question": "The jRails project is a drop in replacement for the Prototype/scriptalicious helpers already a part of the Rails framework. Are your experiences with this project positive? Does it do what it says on the tin?  Is it still being maintained or is this a bad choice if I want to do jQuery with RoR? ",
        "Best answer": " Is it still being maintained or is this a bad choice if I want to do jQuery with RoR?  Their website doesn't exist anymore, their Google Group has some spam and their code isn't updated. I guess this isn't maintained anymore and might be a bad choice for continuous development... "
    },
    {
        "ID": "4028",
        "Question": "As programmers, we often take incredible pride in our skills and hold very strong opinions about what is 'good' code and 'bad' code. At any given point in our careers, we've probably had some legacy system dropped in our laps, and thought 'My god, this code sucks!' because it didn't fit into our notion of what good code should be, despite the fact that it may have well been perfectly functional, maintainable code. How do you prepare yourself mentally when trying to get your head around another programmer's work?  ",
        "Best answer": "For any legacy code base, the correct way to prepare yourself mentally for dealing with it is to start by writing unit tests for it. Whether it sucks or not, you need to first have the confidence to be able to change it without breaking stuff! "
    },
    {
        "ID": "4142",
        "Question": "Often when stating a new project I'll require a \"quick 'n' dirty\" content management solution. Ideally something that can read my database schema and generated HTML forms. Previously I've used; phpMyEdit and phpMyAdmin but they are lacking is key areas. My wish list woulds be:  Database independent Foreign key aware Handles views as-well-as tables Generates modern HTML and CSS AJAX interface.  What's your swiss army knife when it comes to CMS on a project? ",
        "Best answer": "I think you're looking for \"scaffolding\", where the software generates views that allow users to maintain the data without you having to do much or any work.  If you must stick with PHP, then look at CakePHP. http://book.cakephp.org/view/105/Scaffolding But two quick suggestions for you. Look at this site: http://www.phpscaffold.com/ Second suggestion: Consider switching to Python/Django or Ruby on Rails.  Both of those are better than what PHP has to offer in terms of scaffolding.  There may be something in PHP somewhere that's as good, but I have not seen it.  CakePHP is the closest I know of. "
    },
    {
        "ID": "4180",
        "Question": " Possible Duplicate: Will high reputation in Stack Overflow help to get a good job?   Just curious, what Web2.0 websites do employers use (if any) to pre-screen potential employees? Does any employer actually refer to a user's online \"reputation\" to get a job? ",
        "Best answer": "I can tell you that there are certain employers who do care about your stack overflow reputation score, and will factor it into their hiring. How do I know? Because those employers made me implement -- and I really didn't want to -- a reputation sort on http://careers.stackoverflow.com. It is not the default sort, though, because I insisted that it not be. Anyway, we always tell employers the same thing, that they should look at the content and evaluate someone's merit based on more than a number; the number is just shorthand for a bunch of other factors. "
    },
    {
        "ID": "4200",
        "Question": "Why would you hire in-house over outsourcing in developing a product for your company? I can only think of a few but I'm not entirely sure if they're good enough reason. This is actually for a debate that I'm going to have in class. I'm more inclined on the outsourcing part but unfortunately, I was asked to switch to the in-house side of the debate. Any ideas? ",
        "Best answer": " An in-house team will be more responsive to your needs, since they're actually part of your company, so they have a better idea of what you want. An in-house team is easier to communicate with- nothing beats regular face-to-face contact. Your in-house team will have more domain-specific knowledge that an external team would have to learn. You're investing not just in the software, but in the expertise solving the types of software problems your company has.  Using your own developers builds up a stock of programmers who've dealt with those specific problems before.  (For counter-arguments, see Joel's take on it.) "
    },
    {
        "ID": "4272",
        "Question": "From time to time I have tried some monitors. My main work is coding (work, phd, etc). At work I have an LG Flatron L246WH which I highly recommend. However at home I have an LG W2363V with which I feel pretty uncomfortable when coding. Fonts, subpixels or whatever mess with my minds when using smooth fonts. Currently, what are the best monitors out there, to best fit our needs? ",
        "Best answer": "The main thing you want to know is the type of panel -- is it TN, VA, or IPS? http://www.codinghorror.com/blog/2007/11/not-all-lcd-panels-are-created-equal.html They all have strengths and weaknesses, but the TN has a lot of weaknesses and only one primary strength -- it's cheap. Apple, for example, has NEVER to my knowledge ever shipped a TN LCD. I strongly advise avoiding TN panels if you want to invest in an LCD you won't mind keeping for a few years. "
    },
    {
        "ID": "4296",
        "Question": "Has anybody's organization started the migration from Java to Scala? If yes, how do you do that? What can I do to encourage my colleagues to do the same? ",
        "Best answer": "Probably the easiest way is to first use Scala only for testing. In this case, you might even not have to tell your boss :-) If he asks, tell him \"that's just my private test case, it's so much easier and faster to use Scala for it\". Once you (and your organization) has enough experience with Scala you can start using it for the 'real' code. "
    },
    {
        "ID": "4325",
        "Question": "I have never had the opportunity to work from home on certain days, but I would definitely like to try it if I can.  What are the pros and cons? I'll list a few that I can think of. Pros:  You don't need to do any work.  (That's a JOKE) You can be a lot more productive.  No commute, relaxed, no meetings, no interruptions  Cons:  Less of a team effort. Other team members can get held up due to having to wait for information for an off-site member  Apologies if this has been asked before - I did a search but couldn't find a pros and cons discussion. Edit: It appears The Oatmeal has already covered this! :-) ",
        "Best answer": "Pro:  No commute. Unless you have annoying neighbours, your environment's as quiet as you like. If you have kids, you have the option of seeing them during the day. You decide when best to work: maybe you're a night owl. Maybe you want to time-shift your work day into the evening so you can spend more time with your children.  Con:  If you're the only person not colocated, you're left out. (\"Why's Foo not answering my mail?\" \"Dude, he resigned a WEEK ago. Didn't you hear? Oh. Noone thought to mail you!\") You have to bring your own discipline to the party. It's tough to explain to your children just why you aren't available to play Lego. Cabin fever, if you're prone to it. (I'm not.) Some people just need to get out their domestic environments. Unless you're disciplined with your time, you can easily start working outside your required hours.  "
    },
    {
        "ID": "4391",
        "Question": "I've programmed a bit of Haskell and Prolog as part of a couple of uni courses, but that's about it. And I've never seen it been used in industry (not that I've had much of working experience to begin with but I've never seen an ad where you are required to know them). So should we be using functional and/or logic programming languages more often? Are there any advantages or disadvantages for using or not using them? ",
        "Best answer": "I believe in using the right tool for the job. Both imperative and functional languages have their place and there's no need to push for using one kind more than the other. For the advantages/disadvantages, I don't think I could beat Eric Lippert's answer to the \"Why hasn't functional programming taken over yet?\" SO question. "
    },
    {
        "ID": "4442",
        "Question": "I have had the question posed at my work that, since we are planning to eventually move to Sharepoint 2010 for most of our Development, and since Sharepoint 2010 supports asp.net web parts, should we start moving all of our new development to be exclusively asp.net web parts? It was also asked how prism factors into all of this.  (not sure what that is) We are now a mostly client/server based location but are moving to an SOA framework (slowly though). Is this a good idea?  Is it better to have some apps in WPF and Winforms hitting the services or should we just go to web parts solely?  What are we going to miss out on if we make this move? ",
        "Best answer": "If you write the web parts and your service-architecture well, you'll end up with a very good solution that will have many of the upsides of a desktop application with all the benefits of integrating with your SharePoint.  Instead of using the traditional MVC I'd recommend a service-oriented approach with most of the work being done in Services and front end jQuery (or extjs if that's your thing). The problem is going to be integrating with your existing client/server architecture as a stopgap during SOA coding and deployment.  You lose very little, especially because your Winforms applications are going to require access to the service layer anyway to be functional (I'm making an assumption here, but from your description it sounds like it), so you can guarantee access to SharePoint (again, making some assumptions about your network configuration). All in all, having a one-stop shop in Sharepoint with all your functionality seamlessly integrated and in one place seems like a best-case scenario to the end users.  It is, admittedly, a little more difficult to code your front end in JS, especially if you're new to this kind of development.  The effort to learn and do it right pays off in spades, at least IME. Disclaimer:  That's all based on a lot of assumptions about your app and your environment based on your post and my knowledge of people in similar situations.  Your situation may be radically different in ways I just don't know about.  Good luck! "
    },
    {
        "ID": "4507",
        "Question": "Considering the fact that you don't have to get involved in setting up/buying a server or even buying a domain, do you think that fact alone is enough to choose one over the other? I don't necessarily want to work on Google App Engine, I just find it convenient when it comes to hosting/environment/etc. and wondering if that's a good enough reason to learn python. In any case, I'm not looking for a debate between python and ruby but more on Google App Engine and whether its value is enough to dictate the language you should learn. ",
        "Best answer": "No. Google App Engine does provide a free/cheap infrastructure for hosting Python applications, but Ruby has virtually the same thing with Heroku (and Heroku has a lot less restrictions that GAE). Before using GAE, make sure to read very closely about the restrictions that Google puts on the platform, many of which can be significant barriers to the goal you're trying to achieve. Carlos Ble's blog post goes into much more depth about the many restrictions he's run into, but I'll give you a quick overview:  Python 2.7 only with no compiled extensions (major performance hit) 30 second request timeout (so much for easy large file uploads) and 10 second outside request timeout (so you can't query slow API's from your app) BigTable is stone age; no \"LIKE\" operators in your query and no single query can return more than 1,000 records Memcache has a 1 MB max value size Both BigTable and Memcache tend to die at a significant enough rate that it's productive to put code in your application to work around their failures  For a very small project, GAE is just fine; but if you're build something at a medium or large size, just be aware of the restrictions that may hamper you progress. For more information about what sort of quotas and limitations you may have to deal with, see the GAE docs:  http://code.google.com/appengine/docs/quotas.html http://code.google.com/appengine/docs/billing.html  Also see this response on Stack Overflow: https://stackoverflow.com/a/3068371/189079 "
    },
    {
        "ID": "4522",
        "Question": "I see a few developers that like to use virtual machines for web development.  Are there others that do this? If there are, why do you do it?  Are there any pros / cons to developing on a VM rather than in a non virtualised environment? I would think things are slower in a VM. ",
        "Best answer": "I use VMs for IE testing. I do have a dedicated Windows machine, but I lean towards using VMs for a few reasons:  It's a hassle to switch computers, even if it's right next to you It's extremely easy to rollback a VM to have a clean testing environment I'd rather use an environment users are actually going to use rather than kludges like IETester, and you generally can't run multiple versions of IE at the same time. It's cheaper to run multiple VM instances than it is to buy multiple testing computers VMs, at least the ones for Mac OS X, have gotten so good in the past couple of years that the \"slow\" stigma given to VMs is unwarranted.  "
    },
    {
        "ID": "4596",
        "Question": "We have an offshore development crew who has a bad habit of installing nonsense software on corporate desktops (which has nothing to do with their job function) and so are considering removing their local administrator rights. Is Local Administrator, or local Power User a requirement with VS2010?  How do you run without elevated rights?  What issues will you run into? ",
        "Best answer": "A programmer should work as a limited user with admin access. That is, the programmer should be the admin of the machine, but while working, he should always use a limited user account. If you need elevated rights to work, for anything but installing software, you're doing something very wrong. Worse, if you work as a power user or disable UAC prompts or the like, you're ignoring issues that will affect end-users of your software, forcing them to run with the same privileges you did. This is wrong. This is true, irrespective of the operating system you're on. Though Windows seems to be the only one where where it comes up. To clarify: When I say the developer should be a limited user, I mean that they should have full admin rights to the machine, but when they test their code, it should be done in a limited-user environment. For example, the developer could be operating the machine as an Admin-capable user, but runs all tests in a virtual machine or in a limited user account. On Linux, this means simply that the dev has sudo access; on Windows, this may mean an Administrator-level account with UAC and other security features fully enabled. "
    },
    {
        "ID": "4647",
        "Question": "There is a school of thought in linguistics that problem solving is very much tied to the syntax, semantics, grammar, and flexibility of one's own native spoken language. Working with various international development teams, I can clearly see a mental culture (if you will) in the codebase.  Programming language aside, the German coding is quite different from my colleagues in India.  As well, code is distinctly different in Middle America as it is in Coastal America (actually, IBM noticed this years ago). Do you notice with your international colleagues (from ANY country) that coding style and problem solving are in-line with native tongues? ",
        "Best answer": "Till now with my experience I have noticed that my native internationl fellow did the same job compared to the non-native. The issue arises when they tried to explain the concept or the requirement. Else I suppose the syntax name doesn't play much role until you read what excatly they do. Once a programmer acquires the knowledge of the syntax then it doesn't count what is the actual meaning of the word used for syntax. "
    },
    {
        "ID": "4662",
        "Question": "I want to know about Which language is best for long term career and How? Which language should I choose among Java and .NET Platform or Should I choose Oracle like DBMS Language (SQL/PLSQL)? I am confused? Detailed answer would be appreciated. ",
        "Best answer": "All of them. Both are solid technologies and they will stay in mainstream for long long time. Anyway the most characteristic of our career is change (evolution, new technologies introduction). You need learn new things forever. Technologies knowledge are not important to stay relevant on career, fundamentals, hard work, motivation and evolution is the key. "
    },
    {
        "ID": "4765",
        "Question": "Have you ever reached a point at your job when you just know it's time to move on?  When do you move to the point that you're willing to let go of the demons you know for the ones you don't know?  What was your deciding factor final straw so to speak when you finally faced the decision to find a new job?  ",
        "Best answer": "I had one job where I work up every morning wishing I was sick enough to go to the hospital so I wouldn't have to go to work.  At another job, I was working so many hours I was having trouble actually driving home at 2 or 3 am when I went home. Only job I ever quit without having another job, just physically couldn't take one more day and the final straw was when they asked me to do something unethical and illegal. Thanks to my exhaustion, I had a car accident in the parking lot the day I quit. Other signs it's time to move on:  You aren't sure if your paycheck will bounce or not You are part of a Death March The work is boring beyond belief You think someone is sabotaging you in terms of office politics  - you start getting fewer responsibilities and less interesting assignments and Joe is getting the credit for the things you did and you are starting to see emails blaming you for things that someone else did. You simply can't live with the corporate culture  "
    },
    {
        "ID": "4879",
        "Question": "Code needs to be written to file, on way or another. While all programmers should strive to write no more code than necessary, this \"small\" portion needs to be written nevertheless. What tips do you have for improving the code writing effort? Usage of IDEs? Different keyboards or character layouts? Minimal usage of mouse? Code generation tools? What else can you think of? ",
        "Best answer": "For me, an IDE with autocomplete is important. A programming language that requires less keystrokes would be nice (type less, read less) but keeping it understandable (unlike J). Keyboard layout: I don't think it's a problem. I switched the layout a few times (US/CH, PC/Mac), and after some time the brain adjusted. Code generation: I avoid them, except to generate getters, setter, and implement an interface. "
    },
    {
        "ID": "4889",
        "Question": "why not combine the best features of the all existent programming languages and fit it in a universal programming language? ",
        "Best answer": "For the same reason you don't use a Swiss army knife to carve a chicken...   The Swiss Army knife generally has a blade, as well as various tools, such as screwdrivers and can openers and many others. These attachments are stowed inside the handle of the knife through a pivot point mechanism... The design of the knife and its flexibility have both led to worldwide recognition...  "
    },
    {
        "ID": "4951",
        "Question": "What are the key differences between software engineers and programmers? ",
        "Best answer": "It's up to the company really, as I don't think there's a legal framework to enforce a denomination or another, or at least not that I am aware of and this might vary from country to country (for instance, the use of the term \"engineer\" is actually fairly regulated in France, but there are variants that are allowed for the \"abusive\" cases). That being said the general trend goes like this:  A programmer position is usually the one of a professional hired to to produce the code of a computer program. It will imply that you know how to write code, can understand an algorithm and follow specifications. However, it usually stops there in terms of responsibility. A developer position is usually considered a super-type of the programmer position. It encompasses the same responsibilities, plus the ability to design and architect a software component, and to write the technical documentation for it (including specifications). You are able to  - at least technically - lead others (so, programmers), but not necessarily a team (there comes the fuzz...) An engineer position would usually imply that you are a developer who has a specific type of degree, some knowledge of engineering, and is capable of designing a system (as in: a combination of software components/modules that together form a whole software entity). Basically, you see a wider picture, and you are capable of designing and explaining it and separating it into smaller modules.  However, all this is arguable, and as I said, there's no legal requirement that I am aware of in US/UK countries. That being said, in France you can only call yourself an \"engineer\" if you come from an engineering school (recognized by the Commission des Titres d'Ingenieurs or something like that). You cannot say that you have an \"Engineer Degree\", but you can say that you have a \"Degree in Engineering\" if you have studied a discipline that falls under the portemanteau of engineering and technologies. It might be that some countries have a similar distinction, I just don't really know. Back to the software engineer title... Once, one of my teacher told our class - and rightly so - that there's no such thing, as of today, as so-called \"software engineering\". Because engineering something (be it a building, a vehicle, a piece of hardware...) means you are capable of envisioning its design and all the phases of its production, and to predict with accuracy the resources you will need, and thus the cost of the production. This is true of most \"true\" engineering disciplines. There are fluctuations, of course (the prices of the materials will vary over time, for instance), but there are very finite theoretical models (for design and planning) and empirical models (for pretty much keeping any of the former within accessible constraints) that allow you to predict the termination date of a project and its resource usage. The major problem with software is that it is not there yet. We want to aim for software engineering, but we're not there yet, really. Because we have a very fluid and dynamic environment, very variable constraints for projects, and still a lack of maturity in retrospect in our processes. Sure we could say we get better at it (highly arguable with hard-data, though), but we've only been at it since the 60s (earlier projects were actually closer to hardware-only computers, thus closer to real engineering, ironically). Whereas we've been building motored vehicles for more than a century, vehicles in general for a few millennias, and building for even more millennias (and have been pretty damn good at it actually in some part of the world, making you feel like we're ridiculous kids playing with our new flashy software toys in comparison). We fail to systematically predict deadlines accurately, we fail to systematically predict costs accurately, we fail to systematically identify and mitigate inherent and external risks efficiently and deterministically. The best we can manage to do is produce good enough guesstimates, and accommodate for some buffer, while trying our best to optimize the processes to reduce cycles and overhead. But see, maybe that's what engineering is. And that's what, when someone talks about a \"software engineer\", they should think of and aim for. So that seems hardly interchangeable with the simple act of programming routines, or the more advanced act of developing applications. Still, everything is a matter of trends. Lately it's pretty common to have an horizontal dev team where everybody on the team is a Senior Software Developer (yes, capitals, because that makes us feel special, doesn't it?), without real distinction of age (fair enough, in my opinion) and not so much distinction of skills (uh-oh...) and responsibilities (now that can't be good, apart purely for PR buzz). It's also sometimes just a force of habit and specific to an industry's culture and jargon. More positions for embedded software production use titles for software engineers. Mostly because it would probably imply that you will always have to deal to a certain extent with the hardware as well in this field, so you obviously deal with other aspects of the production and of the whole \"system\" you produce. Not just the bits going nuts inside it. On the other hand of the spectrum, you don't really see the term engineer being used in financial software production positions. It's either because is a mimetic evolution of this industry from one of its predecessors (say, embedded engineering find its roots in automobile engineering, for instance), or because they just want to give more or less credit/weight to a position. And to be sure to loose everybody in the fog, you'll then find other titles mixing both (like \"Software Development Engineer\" or \"Software Engineer in Test\"!), and then other ones emphasizing even more crazy bridges with other domains (think of \"Software Architect\" and how \"software architecture\" might be a shameless theft of vocabulary). And keep them coming: Release Engineer, Change Development Manager, Build Engineer (that one goes ffaaarrrrrr out there as well). And sometimes just simply \"engineer\". Hope that helped, though it's not really an answer. Oh, and that means your new company is either trying to lure you in with a new title or that they don't really care about titles, or that you really are going to have a higher-level position. The only way of knowing is to read your job spec, talk to them and eventually give it a shot and judge for yourself. I'd hope it's the latter option and that you're happy with it (and potentially cash more in on it). ;) "
    },
    {
        "ID": "5015",
        "Question": "I'm currently using Planning Poker to do our detailed estimates. This works great but relies upon a fairly detailed work breakdown. Often it takes 6-8 weeks to get a sufficiently detailed design and work breakdown. I've found the 6-8 weeks of analysis are often wasted as the estimate comes out so high it doesn't make economic sense to continue the project. I think providing a high-level estimate up front with a wide range might be better to weed out these shaky business cases. What tools and techniques exist for high-level initial estimates? Right now I just pick a previous project that \"feels\" the same and provide a -50%/+100% range. ",
        "Best answer": "If you are doing detailed planning poker sessions for all of the requirements up front, you are wasting a lot of time, as in my experience, detailed project requirements simply aren't that fixed, so you spend a lot of time estimating items that you never build, or are so greatly changed by the time you build them that the initial estimate is not valid. All estimates are guesses, but you can get better at estimating if you do it often and keep data about how accurate your estimates are. Estimation is best done at two levels, once initially on the project and another as an ongoing process within the project. First, when asked for a project estimate - estimate at the feature level, using your experience on previous projects. Keep the data on your previous initial estimates and see how you track against them. You can do this initial estimate similarly to planning poker, but don't break the work down into tasks. Simply give yourself some big buckets (increments of a half week or week for the features could work, but not much more granular than that) to estimate. If more than one team member is estimating, don't waste time on too much discussion at this point, just go with the most pessimistic estimate rather than getting down into the weeds. Second, as you work through your short project iterations (assuming that you do have short iterations), you pick the highest priority items and estimate them at the task level (and of course develop and deliver them). Once you've cycled through that first iteration you can see how accurate your detailed estimates are, as well as how they compare to your initial ballpark estimates. Now you can revise those initial estimates as you see how accurate they are, and once you have a few cycles under your belt you can give a confidence interval for the project completion date. The units for the ballpark estimate are a good communication tool for the precision of the estimate. Your initial units are in days or weeks, but your detailed estimates are in hours. "
    },
    {
        "ID": "5034",
        "Question": "I'd like to know at what point can be considerated an AI implementation?  I means, what is the minimal requeriment for that? Can you give a simple code example? ",
        "Best answer": "Any program in which the decisions made at time t are impacted by the outcome of decisions made at time t-1.  It learns. A very simple construct within the field of Neural Networks is a Perceptron.  It learns by adjusting weights given to different input values based on the accuracy of the result.  It is trained with a known set of good inputs.  Here is an article that covers the theory behind a single layer Perceptron network including an introduction to the the proof that networks of this type can solve specific types of problems:  If the exemplars used to train the perceptron are drawn from two linearly separable classes, then the perceptron algorithm converges and positions the decision surface in the form of a hyperplane between the two classes.  Here is a book chapter in PDF form that covers the topic.  Here is an Excel Spreadsheet that explains a bit more with a concrete example. And finally, here is a beautiful Javascript Example that you can watch learn.   "
    },
    {
        "ID": "5074",
        "Question": "What are the preferred use cases for the following sets of terms:  Log in / Log out Log on / Log off Sign in / Sign out Sign on / Sign off  From what I can guess, \"Logging in\" should be used for a long-lived session (like a website), whereas \"Sign in\" should be for something that you will be attending to (like IM or a financial transaction). I'm a little fuzzy here... ",
        "Best answer": "I've always used Login/Logout without the space.   I notice that Microsoft is preferential to Sign in/Sign out. "
    },
    {
        "ID": "5232",
        "Question": "I tend to understand things rather quickly, but after 2 years of programming in Python I still stumble across things (like Flask today) that amaze me. I look at the code, have no idea what's going on, and then feel very humbled. I feel like an absolute expert each time this happens, up until the moment it happens. Then, for about a 2 week period I feel like an absolute beginner.  Does this often happen, or does it indicated that I have so much more to learn before I can even be considered a \"good\" programmer? ",
        "Best answer": "You will never, ever, ever, ever, ever, in the entirety of your career, be in a position where you immediately understand every programming technology simply by looking at it.  There's just too much there.  Its the accumulation of research and knowlege of millions of individuals over many decades.  If you ever find yourself thinking you are at that point, seek a therapist to discuss your delusions. The trait you need most is the ability and willingness to learn.  If you have that, nothing will be beyond you. "
    },
    {
        "ID": "5341",
        "Question": "I've had a couple of times in the time I've working, moments when I get an error ocurring in just 1 computer and it often takes me hours or days to figure out because it is (or at least seems) an isolated incident as it is not being presented in any other instance of whatever I'm checking. How do you guys deal with this? I've often had to just change the computer per se (like formatting, or stuff like that) because I simply cannot replicat the issue. ",
        "Best answer": "You have to try to isolate what's different about that machine/environment to every other machine/environment where your application works. That will involve checking the state of your application by adding diagnostics, checking the state of the machine - which may involve remote logging or even physical access, and checking what the user is doing at every step of the way. I've had many problems that only repeated for one user or on one machine and it was only by understanding what they were doing and how they were doing it were we able to resolve things. "
    },
    {
        "ID": "5372",
        "Question": "Being a programmer is not a very healthy profession - long hours of sitting in front of a computer, with impending deadlines just over the cubicle. This takes a toll on the body and mind. So what tips do you have for programmers in order to stay healthy? ",
        "Best answer": " Join a gym that is close to work Walk/Bike to work Drink a lot of water at work (increase your water intake, and force you to take break to use the washroom, win-win situation)  "
    },
    {
        "ID": "5405",
        "Question": "When I get a new laptop, it usually takes me about two weeks to reinstall all my developer programs, utilities and tweak the O/S settings to how I like them.  I know there are utilities out there to backup/restore systems, but this is usually if it is on the same hardware.  What would you recommend? ",
        "Best answer": "Over the years I've come to this set of habits, which works well for me:  I stopped customizing so much. Before I used to tweak my desktop and Windows settings greatly. After a while I realized I grew dependent on these tweaks, and would get uncomfortable when working at a co-workers PC, on family members' PCs etc. Now I keep it down to just a few must-have changes, and generally keep my Windows and less important tools at default settings. I use multiple PCs, each dedicated to specific tasks. My work PC is a laptop, which I keep 'clean' for lack of a better word -- no private stuff, almost no games/multimedia/accessories, just my primary work tools. As such it rarely (actually, almost never) breaks, and I spend often keep the same Windows installation until it's time to replace the hardware (2-3 years). My home gaming PC on the other hand gets reinstalled far more frequently. But I don't care, it is easy to just reinstall and allow Steam to redownload all my games. Optional, use full-disk backup with system state. Actually I'm thinking about quitting this habit, because I haven't had to reload a system backup in ~3 years. But in the olden day Acronis Trueimage saved me a few times, by allowing me to just overwrite my full Windows + applications state with a known working backup. The built-in Windows Vista / 7 backup tool can AFAIK do something similar. Embrace Virtualization. I do all testing of new software in a VM, and I keep 'invasive' software (mostly enterprise server software) contained in VMs. I have my VMs on a external USB2 2.5\" HDD; it's not the fastest but it works for me.  "
    },
    {
        "ID": "5427",
        "Question": "Other than being annoyed at whitespace as syntax, I'm not a hater, I just don't get the fascination with Python. I appreciate the poetry of Perl, and have programmed beautiful web services in bash & korn, and shebang gnuplot.  I write documents in troff and don't mind REXX.  Didn't find tcl any more useful years ago, but what's the big stink about Python?  I see job listings and many candidates with this as a prize & trophy on their resumes.  I guess in reality, I'm trying to personally become sold on this, I just can't find a reason. ",
        "Best answer": "I've found Python to be the most natural programming language that I've ever written code in. I've coded in a lot of languages before and after Python, and to a greater or lesser extent, you have to fight the language to get it to do what you want. Python reduces this struggle massively. Eric S Raymond said it much better than I can in Why Python? As a related point, Python maintains its cleanness even while evolving rapidly. In most languages I've worked with, introduction of new language features introduces a lot of wrinkles. But with Python, even major language features (decorators come to mind) are added all the time, without feeling like ugly hacks. "
    },
    {
        "ID": "5466",
        "Question": "Sometimes I can't stand it when project managers ask me to estimate time to complete for various tasks.  An estimate is a guess, and guesses can be wrong.  Generally, bad requirements and documentation will lead to bad guesses. So I often wonder if the project managers were ever in my shoes trying to guess at how long task X and Y will take, and how difficult it is to assign a number to it based on what little is known and collected from the client. My question then is: Do good project managers need to have a programming background? Or maybe the question should be, do good project managers need to have been a good programmer before?  Is there any correlation? ",
        "Best answer": "Managing IT projects is definitely not the same as managing other types of projects. I once heard of a project manager with no IT experience. He ended up frustrating the programmers and basically scaring them away. On the other hand, a programmer that becomes a Project Manager may become a control freak, thinking he can fix things if (s)he can't get the programmers to do it properly (that has been my problem in similar situations) "
    },
    {
        "ID": "5473",
        "Question": "I was reading the wikipedia article on programming style and noticed something in an argument against vertically aligned code:  Reliance on mono-spaced font; tabular   formatting assumes that the editor   uses a fixed-width font. Most modern   code editors support proportional   fonts, and the programmer may prefer   to use a proportional font for   readability.  To be honest, I don't think I've ever met a programmer who preferred a proportional font.  Nor can I think of any really good reasons for using them.  Why would someone prefer a proportional font? ",
        "Best answer": "Common points against proportional fonts, commented.   You cannot precisely align code vertically with proportional fonts. I mean, you could precisely align code vertically with proportional fonts, if everybody was using elastic tabstops, but alas... Some proportional fonts make it hard to distinguish some characters groups. (e.g., mrnm). Not all programming fonts are perfect either, however: Courier New has identical 'O' and '0' and identical '1' and 'l'. Some IDEs have poor support for non-fixed-width fonts (like aforementioned Visual Studio or Python's IDLE). In some contexts, also, you just can't use one. (e.g., terminals.) Choosing a proportional font for coding will get you in endless holy wars. Here, however, the problem exists between the keyboard and the chair.  Points in favour of proportional fonts  Some characters are just wider than others. Having to cram an m in the same space of an n or an i makes it truly challenging to design a good, readable monospace font. Improved spacing between letters just right. Compare rnW and Ill in this Proggy Clear screenshot for an example of font spacing done wrong. Most programmer fonts lack italic or bold. This makes it hard to use effective syntax highlighting. Vertical alignment is a can of worms anyway. Tabs or spaces or tabs and spaces?  Personally, I've been using both the 'Ubuntu' font and WenQuanYi Zen Hei Mono with pleasure and find myself unable to prefer one to the other. :) Ubuntu 10 and WenQuanYi Zen Hei Mono 9, compared. There's no clear winner here, if you ask me. That said, fonts are like food. Some like them well rounded, some like them hot and spicy --  there's no one right font, or all of us would be using it right now. Yay for choice! "
    },
    {
        "ID": "5490",
        "Question": "At my first workplace we were using Digital Standard MUMPS on a PDP 11-clone (TPA 440), then we've switched to Micronetics Standard MUMPS running on a Hewlett-Packard machine, HP-UX 9, around early 90's. Is still MUMPS alive? Are there anyone using it? If yes, please write some words about it: are you using it in character mode, does it acts as web server? etc. (I mean Caché, too.) If you've been used it, what was your feelings about it? Did you liked it? ",
        "Best answer": "Intersystems sell a MUMPS derivative : http://www.intersystems.com Some of the most interesting people in MUMPS are probably here : http://www.outoftheslipstream.com/ I started blogging about Cache (the MUMPS derivative) a couple of years ago : http://cachetastic.blogspot.com/ (but then changed jobs) Having been out of that world for a couple of years, my thinking is that the NoSQL movement is probably the best and worst thing to happen to MUMPS. Ultimately it's likely to both vindicate and kill it. Because somebody, at some point, is going to reinvent MUMPS's database and query structure almost identically, but with no connection to the MUMPS tradition. Then people will rave about this new storage system. But no-one will ever choose a MUMPS derivative again. For example, a month or two ago, I was talking with a colleague about using redis to cache a look-up of something in our Django application. We had a large number of records addressed by a triple of three keys, and needed to quickly get subsets matching one or two of these keys (but different combinations at different times). This is the kind of thing that MUMPS eats for breakfast. But we were finding it hard to squash into redis's key,val pair structure. Even with dictionaries. (Same would be true of memcached etc.) For the first time in my life, I found myself actually regretting that I couldn't write this module in Cache ObjectScript. Some more thoughts on Cache here : Good : http://cachetastic.blogspot.com/2008/07/ok-after-mentioning-some-bad-things.html Bad : http://cachetastic.blogspot.com/2008/07/some-mumps-dissing-and-more-positive.html "
    },
    {
        "ID": "5513",
        "Question": "In the past I have worked with designers, BAs and project managers, all who regularly produce project artifacts, yet very really do they understand the concept of versioning.  When I try to explain it to them (even in its most simple form of multiple differently named files) they seem to have some kind of mental block.  Why do you think this is? ",
        "Best answer": "This is because the human has difficulties projecting himself in time. Use the time machine analogy. Your life is versionned. Every day you have a new version of your life: new things and lost things. Hopefully more assets, less debts,... but more fat, less hairs, ... hopefully more knowledge, less doubts, .... Then you will have to explain branching ;) And there you hope they are fans of Fringe ;) "
    },
    {
        "ID": "5531",
        "Question": "I define defect as :  \"something within the application design or code which prevents it functioning as per requirements.\"  I'm looking for ideas about the causes of defects, eg the human factor, lack of testing, lack of prototyping, and possible ideas to mitigate these. ",
        "Best answer": "The prime cause of software defects is interpretation. The customer interpretation of a feature differs from the designer interpretation. The designer interpretation differs from the programmer interpretation. Most methodologies have invented ways to counter this effect. But in the end, we are only humans and we are not flawless. Besides, often there is a time pressure and most methodology magic is often skipped while under pressure. Testing can only detect the problems early. But even testers are human, and it is imposible to test 100%. If you want to release before the universe ends. "
    },
    {
        "ID": "5540",
        "Question": "How should code in version control be stored?  Developer friendly? so that programmer can quickly take the latest and able to run from his editor without doing many changes? (like config files pointing to dev DB..etc) or  Should it be production friendly? source should be in a manner which is easy to deploy on production environment and when developer takes the latest, he should perform changes as per his development needs. ",
        "Best answer": "Why choose ? It should be both. Your development environment should be configured so it's as easy as doing a checkout, open, build, run, debug (eg: no absolute path!). You can do that easily with compilation directives, configuration class + dependancy injection, or even tricks like the perso.config in ASP.NET Your automated build script should be customized enought to take care of specific production configuration, clean up, packaging etc. "
    },
    {
        "ID": "5560",
        "Question": "It's all the rage nowadays. \"Everyone\" recommends it. That in and of itself makes me suspicious. What are some disadvantages you have found when doing test-first (test-driven) development? I'm looking for personal experiences from knowledgeable practitioners--I can read the hypothetical musings of a hundred wannabes elsewhere on the internet. I ask not because I am looking to hate TDD, but because it is my job to improve software development process, and the more we can learn about the problems people encounter, the better chance we have of improving the process. ",
        "Best answer": "There are quite a few, but the advantages far outweigh the disadvantages. There's a steep learning curve. Many developers seem to expect that they can be efficient with test-first programming right from day one.  Unfortunately it takes a lot of time to gain experience and program at the same speed as before.  You can't get around it. To be more specific, it's very easy to get wrong.  You can very easily (with very good intentions) end up writing a whole bunch of tests which are either difficult to maintain or testing the wrong stuff.  It's difficult to give examples here - these kind of issues simply take experience to solve.  You need to have a good feel of separating concerns and designing for testability.  My best advice here would be to do pair-programming with someone who knows TDD really well. You do more coding up front. Test-first means you can't skip tests (which is good) and means you'll end up writing more code up front.  This means more time.  Again, you can't get around it.  You get rewarded with code that's easier to maintain, extend and generally less bugs, but it takes time. Can be a tough sell to managers. Software managers are generally only concerned with timelines.  If you switch to test-first programming and you're suddenly taking 2 weeks to complete a feature instead of one, they're not gonna like it.  This is definitely a battle worth fighting and many managers are enlightened enough to get it, but it can be a tough sell. Can be a tough sell to fellow developers. Since there's a steep learning curve not all developers like test-first programming.  In fact, I would guess that most developers don't like it at first.  You can do things like pair-programming to help them get up to speed, but it can be a tough sell. In the end, the advantages outweigh the disadvantages, but it doesn't help if you just ignore the disadvantages.  Knowing what you're dealing with right from the start helps you to negotiate some, if not all, of the disadvantages. "
    },
    {
        "ID": "5564",
        "Question": "Need a Free, Fast(development and runtime) and Reliable(transactions and locking) tool set for creating an enterprise application for SMBs. I'm thinking of an application framework + UI framework + DB which will help me in developing the software faster.  As it is known, business softwares need lot of similar UIs to be created.  My idea is ...to create a new form with N number of fields and connecting it to the database for basic CRUD operations within 30min to 1hr.  I got the taste of Intersystems Cache technology stack with an app framework on top of it. To be frank...it is amazing...  I'm looking out for something similar to this in opensource. Any suggestions? ",
        "Best answer": " What is the best toolset for living a   fullfilling life for suburban   professionals? Need a convenient, functional toolset   for living my life.  I'm thinking of a   moral code + religion + culture which   will help me life happier and better.  Ok I couldn't help myself.  You're asking for religion and any answer you get will be based more on personal preferences than one being actually \"better\" than another.   That being said, go with Ruby.  It's free, fully functional and comes with a lot of stuff, like Rails and an active community.  And, you get to look down on everyone else as a bonus. My background is in Java and .NET, but if I had to start from scratch at this moment, I'd probably learn Ruby. "
    },
    {
        "ID": "5597",
        "Question": "I've been doing design and programming for about as long as I can remember. If there's a programming problem, I can figure it out. (Though admittedly Stack Overflow has allowed me to skip the figuring out and get straight to the doing in many instances.) I've made games, esoteric programming languages, and widgets and gizmos galore. I'm currently working on a general-purpose programming language. There's nothing I do better than programming. Is a university education really more than just a formality? ",
        "Best answer": "Hooboy.  This is a tough position to be in; you have my sympathies. I'm biased towards getting a degree, most likely because 1) I have one (BS in Computer Science) and 2) I've often found the knowledge gained pursuing it to be very useful.  But it's hardly a pre-requisite for a successful career; the IT world is rich with people who kick ass, are acknowledged as kicking ass, and who technically don't have more than a high school diploma. The nice thing about a university degree is that you can put it on hold and come back to it later when life permits.  (Though the dangerous thing about the previous sentence is that it's a good way to simply quit without admitting to yourself you're quitting.)  You can test the waters and see what kind of job you could get by sending your resume out today and seeing what kind of nibbles you get; you haven't committed to anything until you actually say yes to a job offer. And it sounds like your school is a bad fit for you, regardless.  If you're so consistently bored with everything they're throwing at you, then you may need to find a school that will do a better job of giving you your money's worth and making you work for that degree.  Have you considered transferring somewhere better?  Edit:  Based on your comments elsewhere, given how much you love the high-level theoretic aspects of programming, have you considered that the best way to continue to explore that and get paid may be a career in academia?  Which would definitely require you to get your degree.  :-) "
    },
    {
        "ID": "5613",
        "Question": "With all of the frameworks available these days, ORMs, dependency injection (DI), Inversion of control (IoC), etc., I find that many programmers are losing or don't have the problem solving skills needed to solve difficult issues. Many times, I've seen unexpected behaviour creep into applications and the developers unable to really dig in and find the issues. It seems to me that deep understanding of what's going on under the hood is being lost. Don't get me wrong, I'm not suggesting these frameworks aren't good and haven't moved the industry forward, only asking if, as a unintended consequence, developers aren't gaining the knowledge and skill needed for deep understanding of systems. ",
        "Best answer": "This is an argument that pops up regularly, in many fields and in many forms. The general form of this argument is: Does having [x:tool/technology] make people worse at [y:function affected by x]? For example:  Does CAD software make for worse engineers? Do calculators in high school make students worse at math? Does social software stunt people's in-person social skills? Does accounting software produce worse accountants?  From memory, the ubiquitous answer is almost always: not really. You'll always have people that are good and bad at doing [y] but now they're just bad at a different facet of the skill.  A deeper understanding of the fundamentals with any job is going to help, no matter what you do - even jobs that are considered 'remedial'. Knowledge always helps. "
    },
    {
        "ID": "5705",
        "Question": "I've used Emacs quite a bit, and I'm comfortable with the most basic commands/shortcuts, but I know Emacs has a lot more to offer than what I'm using. What are some of the lesser known features of emacs that can help me become a more productive programmer? ",
        "Best answer": "Learn elisp.     It allows you to program any shortcut you can think of.  Any time you find yourself performing a multi-step process for something useful, you make a function out of it and bind it to a hotkey combo. It lets you experiment with functional programming - learning a new paradigm is a good way to expand your horizons.  "
    },
    {
        "ID": "5727",
        "Question": "Especially when writing 'standard' (non-HPC) applications, do you consider what sorting algorithm to pick, or just settle with quicksort (which is what most libraries just call sort)? To some extent it may be profitable in specific situations, but on the other hand proper optimization requires some time to analyse the problem and make benchmarks.  ",
        "Best answer": "In general, using the default methods unless there is a specific need to do something more exotic keeps everything a lot more readable/understandable down the road IMHO. If you experience (or in some cases, strongly suspect) that you have a performance problem that is the time to add complexity. On the other hand, if you are using a low enough language that there is not a built-in sort for the kind of objects you need to sort try to pick one or two that cover all your bases and implement those. "
    },
    {
        "ID": "5748",
        "Question": "I'm not talking about a diff tool.  I'm really looking to see if a project contains code that may have been \"refactored\" from another project.  It would be likely that function names, variable names and whatnot would be changed.  Conditionals might be reversed, etc. ",
        "Best answer": "You might be able to use the PMD tool to find what you are looking for.  It is meant to detect cut and paste within a code base but if you include the suspected origin project source it might help you see where code was copied from it. "
    },
    {
        "ID": "5898",
        "Question": "In another question, it was revealed that one of the pains with TDD is keeping the testing suite in sync with the codebase during and after refactoring. Now, I'm a big fan of refactoring. I'm not going to give it up to do TDD. But I've also experienced the problems of tests written in such a way that minor refactoring leads to lots of test failures. How do you avoid breaking tests when refactoring?   Do you write the tests 'better'? If so, what should you look for?  Do you avoid certain types of refactoring?  Are there test-refactoring tools?  Edit: I wrote a new question that asked what I meant to ask (but kept this one as an interesting variant). ",
        "Best answer": "What you're trying to do is not really refactoring.  With refactoring, by definition, you don't change what your software does, you change how it does it. Start with all green tests (all pass), then make modifications \"under the hood\" (e.g. move a method from a derived class to base, extract a method, or encapsulate a Composite with a Builder, etc.).  Your tests should still pass. What you're describing seems to be not refactoring, but a redesign, which also augments the functionality of your software under test.  TDD and refactoring (as I tried to define it here) are not in conflict.  You can still refactor (green-green) and apply TDD (red-green) to develope the \"delta\" functionality. "
    },
    {
        "ID": "5916",
        "Question": "Someone once said we should prefix all our methods with the  /// <summary> comment blocks (C#) but did not explain why. I started to use them and found they annoyed me quite a bit, so stopped using them except for libraries and static methods. They're bulky and I'm always forgetting to update them.  Is there any good reason to use /// <summary> comment blocks in your code? I normally use // comments all the time, it's just the /// <summary> blocks I was wondering about. ",
        "Best answer": " Use them as much as possible.  Yes, those are special comments that become the documentation for the method. The contents of <summary>, the parameter tags, etc. that are generated show up in intellisense when you or someone else is getting ready to call your method.  They can essentially see all the documentation for your method or class without having to go to the file itself to figure out what it does (or try to just read the method signature and hope for the best). "
    },
    {
        "ID": "6042",
        "Question": "I offered to do a little bit training in F# at my company and they seemed to show some interest. They are generally VB6 and C# programmers who don't follow programming with too much passion. That being said I feel like it is easier to write correct code when you think in a functional matter so they should definitely get some benefit out of it. Can anyone offer up some advice on how I should approach this? Ideas  Don't focus on the syntax, instead focus on how this language and the idioms it promotes can be used. Try and think of examples that are a pain to write in an imperative fashion but translates to elegant code when written in a declarative fashion.  ",
        "Best answer": "Functional programming is an odd beast for me.  I learned F# and Haskell, wrote a few simple programs and love using them, but never had the \"flash of revelation\" that some people talk about.  But slowly, I noticed that more and more I was writing code that was meant to be immutable, breaking up tasks into more, smaller functions, and trying to use delegates a lot more.  Its a thing that, if you like it, creeps into your work because the value of those techniques are self-evident. Now, more practically for training: I find that two concepts really click Functional Programming as a style to me.   First, FP style is based on structure of data, not composition as in OOP.  I looked at something like List in C# as being a clever trick to generate type-safe lists, something that composed the type (string) into the other type (list).  After learning FP, I look at generics more like Monads now.  List is a structured form that code can take, and it decorates the strings.   Second, and perhaps more useful to C#/ASP programmers, is the idea that FP works on recursion and repetition, while OOP works on mutability and looping. I tend to think of the ASP page lifecycle as a kind of FP now: each request is processed from scratch through the entire life-cycle, so the entire page is, in effect, one big slowly recursing program.  If you can narrow that notion down, you get a better idea of how an imperative program can be structured around loops of functions that take in data, operate over it, and return out new data instead of modifying the old. The trickiest hurdle, at least for me, to overcome with this approach is that sinking feeling that you're wasting tons of resources when using mutable objects would save a ton of memory.  In GC we trust, and I just had to learn to let go of performance concerns until I'd actually seen the program run and verified if there even were any, and if so to use a profiler to see exactly where the problems were. "
    },
    {
        "ID": "6045",
        "Question": "Some projects we run internally using are Scrum, while still being \"fixed everything\" to the customer.  We're experiencing mixed success on our part (the customer likes the visibility of the burndown chart). Can the types of projects we work be successfully executed using the agile methods? ",
        "Best answer": "I would like to pose a counter-question: Can fixed scope + fixed deadline + fixed price contract ever be made to work, period? The \"good/fast/cheap - pick two\" saying isn't just some silly engineering joke.  Every project manager worth his salt knows about the Project Management Triangle:  You're telling us that the cost, scope, and schedule are all fixed.  That leaves no room for maneuverability or error.  None.  You could choose to view \"Quality\" as an attribute, but it's not a \"real\" attribute, it's more like a meta-attribute that's derived from the other attributes (cost/scope/schedule). The problem is that this never happens in reality as long as your project is being planned and executed by humans.  Requirements and specifications never cover every edge case unless they've been drawn up in immense detail by qualified architects and designers, in which case the project is already half-done; and even then there's still the possibility of error. Unexpected costs will pop up leading to budget overruns.  A subscription expired.  A manufacturer discontinued their support for a product you're using and you have to find a new one.  An hourly contractor raised his rate under threat of departure.  Your entire team just went on strike, demanding a 10% raise and an extra week of vacation. Schedules slip.  Unforeseeable problems crop up; that charting component you've been using for 5 straight years isn't compatible with Windows 95, which your client is still using.  An obscure bug in 64-bit Windows causes serious UI glitches and you spend nearly a week tracking it down and developing a workaround (this actually happened to me).  Your senior developer got hit by a bus and you have to go recruit and train a new one.  Your estimated delivery date is always wrong.  Always. See Hofstadter's Law:  Hofstadter's Law: It always takes longer than you expect, even when you take into account Hofstadter's Law.   Agile methods are all about juggling around the cost, schedule, and scope.  Most of the time, they're specifically about juggling around the scope and sometimes the schedule, which is why you start with nebulous user stories and plan revisions instead of full versions.  Different methodologies use different terminology but it's all the same basic premise: Frequent releases and a rebalancing of the schedule and scope with each release. This makes no sense with a project that is (or claims to be) either fixed scope or fixed schedule. If one project attribute (cost/scope/schedule) were fixed, I would tell you that it might not be a good fit for agile methodologies. If two project attributes are fixed, then your project is definitely not a good fit for agile methodologies. If all three attributes are fixed, then your project is probably going to fail.  If it actually ships, then either the original schedule was massively fudged, or the client has managed to delude itself into thinking that you actually delivered what was promised. If this contract is still on the table, I urge you to reject it.  And if you've already accepted it, may God have mercy on your soul. "
    },
    {
        "ID": "6190",
        "Question": "I am planning to create a utility, which will query the database and store some information (on another table in the database). Its a multi-threaded utility and require to run for every 5 or 10 minutes/later may be thrice in a day. I see two options to achieve this in C#/DotNet programming.   creating windows service having timer approach inside it. a console program and schedule it using windows task scheduler.  Which one do you prefer and why?  ",
        "Best answer": "Services are either used for administrative purposes or to offer a service to multiple applications. Schedules are used for running a task multiple times which don't necessarily require extra permissions. "
    },
    {
        "ID": "6255",
        "Question": "Joel Spolsky said in one of his famous posts:  The single worst strategic mistake   that any software company can make:    rewrite the code from scratch.  Chad Fowler wrote:  You’ve seen the videos, the weblog   posts and the hype, and you’ve decided   you’re going to re-implement your   product in Rails (or Java, or .NET, or   Erlang, etc.). Beware. This is a longer, harder, more   failure-prone path than you expect.  Have you ever been involved in a BIG Rewrite? I'm interested in your experience about this tragic topic, and in particular, in any big rewrite that was completed succesfully (if any). ",
        "Best answer": "I've been involved in a few rewrites over my career and they were all disasters. I think they all fail for the same reasons  Vast underestimate of effort required:  Every time someone wants a rewrite, it's because the old system is using old technology and difficult to maintain.  What they fail to consider is that because of it's age, it may have 30-40 man years of development effort into it.  Thinking you can then rewrite the whole thing in 6 months with a team of 5 is silly. Lost knowledge:  The old system has been around so long, it does a lot of stuff, and is hooked into everything.  There is no up-to-date documentation, and no single point of authority that actually knows all the things the system does.  There will be pieces of knowledge with particular users in particular departments, and finding them all is difficult or impossible. Poor Management Decisions: The rewrites I've been involved in had a similar expectations from management: The new system should be 'done', and the old system could simply be turned off on a particular date, period.  No other option was acceptable.  I think they get this in their head, because they are spending all this money to hire new people for this huge project.  In reality, the better risk mitigation strategy is to rewrite the major functions of the old system, say tackle 50-75% of the old system for a first release, and then see how it works!  Because of #1 and #2 above, this would probably work out much better, as we find out some of the features that were missed, and what's needed to actually turn off the old system.  "
    },
    {
        "ID": "6394",
        "Question": "When drafting a project proposal, do you use any standard template?  What features/information should be included? What is nice to have included? What sort of boiler plate information should I shove in? Do you find any design pattern or concept particularly helpful? ",
        "Best answer": "Have you ever looked at the Volere Requirements Template?  While it contains a little too much detail for my taste, particularly for a proposal (it's better suited for detailed up front requirements specification), the section headings are a great checklist to make sure you've thought about all of the different moving parts before giving an estimate or creating a proposal document. Here they are:  PROJECT DRIVERS  The Purpose of the Product Client, Customer and other Stakeholders Users of the Product   PROJECT CONSTRAINTS  Mandated Constraints Naming Conventions and Definitions Relevant Facts and Assumptions   FUNCTIONAL REQUIREMENTS  The Scope of the Work The Scope of the Product Functional and Data Requirements   NON-FUNCTIONAL REQUIREMENTS  Look and Feel Requirements Usability Requirements Performance Requirements Operational Requirements Maintainability and Portability Requirements Security Requirements Cultural and Political Requirements Legal Requirements  PROJECT ISSUES  Open Issues Off-the-Shelf Solutions New Problems Tasks Cutover Risks Costs User Documentation and Training Waiting Room Ideas for Solutions   "
    },
    {
        "ID": "6395",
        "Question": "What tools and techniques do you use for exploring and learning an unknown code base?  I am thinking of tools like grep, ctags, unit-tests, functional test, class-diagram generators, call graphs, code metrics like sloccount, and so on. I'd be interested in your experiences, the helpers you used or wrote yourself and the size of the code base with which you worked.  I realize that becoming acquainted with a code base is a process that happens over time, and familiarity can mean anything from \"I'm able to summarize the code\" to \"I can refactor and shrink it to 30% of the size\". But how to even begin? ",
        "Best answer": "How do you eat an elephant?  One bite at a time :) Seriously, I try to talk to the authors of the code first.  "
    },
    {
        "ID": "6417",
        "Question": "It's not uncommon for projects to fail. As a programmer, how do you deal with projects that fail? Some definitions of failure:  Misses deadline. Code and functionality does not do what it's supposed to. Software becomes vapor-ware or endless number of phases, essentially undeliverable.  Or maybe you have your own definition(s) of failure. Do you start pointing fingers?  Do you blame yourself, the requirements, the technology, the management, the client, etc?  Do you do a lessons learned session as a team? ",
        "Best answer": "You should do lessons learned for all projects, failed or succeeded. There is a lot to learn from a good project.  True failed projects have been very rare for me. In addition to understanding what happened, I do the \"ask why 5 times\" thing to try to get to underlying causes. There is also the matter of why I didn't notice what was happening and either do something about it or at least get out. I think everyone's first position is to blame everything - the client, the tech, the business problem being tackled, the methodology, the team members, the language, the platform, heck even the way we take our coffee in the morning. The nice thing about a retrospective (even if it happens only in your own head) is the chance to reconcile with some or all of those factors and realize they weren't the issue. In my only real failure of the last 30+ years, the project had been in requirements for literally years when we arrived. We got requirements settled. One came from management and hundreds from the end users. We wrote code, lots of code, some of it brilliant. There was testing and acceptance testing and changes and arguments and change requests and unpaid work and paid work and last minute bolt ons and surreal humour and escalations to VPs and all of that. Eventually it just all kind of stumbled to a halt. The reason for the failure was that the single management requirement was unacceptable to the end users. And no matter how many things they got their way on, they could not get past that one and would never accept the system. But management would not have it any other way. So that was that and though we got a lot of money it was, in the end, all horrible. I still work in that technology, I still use those processes and I still work with the same people. I would even do another project for that client. But when the end users say they don't like something their own management has injected into the requirements, I will remember that writing good code that works does not protect you from a failed project. And I will do something about it then, not a year or two later. "
    },
    {
        "ID": "6510",
        "Question": "What is a recommended setup in terms of source control, builds, testing, bug tracking, and deployment for a downloadable PHP application (a la Wordpress, phpBB, Magento, etc.)? As a developer working on the application, what would you expect? What about as a user of the application? Please describe a setup and workflow, as well as any resources you think might be relevant or helpful. ",
        "Best answer": "I've found the following approach to be workable for the app I maintain:  The app is distributed as a single zip, that just has to be unzipped on a working PHP server. Navigating to the app automatically runs a set of sanity checks (PHP environment, app configuration). Errors explain in simple terms how to fix any issue that crops up. At first run, if the sanity checks pass, a configuration wizard is launched. This is disabled as soon as a settings file exists. All customer modifications are done in a special subfolder of the app, that they can copy over from old installs (backwards compatibility is maintained for this folder). Being able to do upgrades by unzipping files and copying around a few folders is a useful feature. The zip file itself is built with a command-line PHP script that is checked into the source repository. Doing a build is as simple as svn update followed by a call to build.php. Developer builds are made the same way as release builds, only with a \"-no-optimize\" flag passed to the build script to prevent minification and concatenation. The fact that the build script is itself PHP means that any developer can make changes to how the builds are made, without even having to log into our build server.  "
    },
    {
        "ID": "6587",
        "Question": "Recently reading the question What languages do you use without an IDE? One question asked in a few answers was \"is Notepad++ and IDE?\" One answers to the original question said \"None, I use vim...\", implying that vim is an IDE. But then another answer suggested vim isn't an IDE. So where is the line? What about notepad, ed, or nano? Is the only non-IDE coding technique the butterfly technique? ",
        "Best answer": "Taken literally, IDE = Integrated Development Environment. This is the way i look at it:  Integrated: Means you can code / launch / compile / debug your app from the tool. Development: Means it can group files into projects, and does syntax highlighting for your language, maybe has refactoring tools, ability to generate files from templates (like unit test files, class files etc.), auto complete / intellisense Environment: Means both of the above are available from the same tool  Notepad++ allows for development (eg. you can write code), but the other areas of development are not covered. I've never used notepad++ for development, only for occasionally editing files.   "
    },
    {
        "ID": "6633",
        "Question": "I always had this question in mind but couldn't find a proper place to ask. There are some really nice and great open source free software available on the net. How do these products sustain themselves financially? It is one thing writing a small utility which does something nice but writing a complicated product with whole lot of features is a totally different ball game. So to repeat myself again, how do they work financially? ",
        "Best answer": "There's lots of different answers. Some projects are maintained by people who just want to do it for assorted reasons, including prestige or the knowledge that they're doing something good or because they thought somebody had to do it and nobody else was.  This section is almost certainly not as large as it was. Some projects are maintained by people who want to be paid for support and the like.  Most Open Source companies are like that:  they want to create a popular product for free so that they can charge for related things.  It's a form of advertising. Some projects are maintained by companies who aren't in that exact business.  Quite a few companies benefit from being able to use Linux, for example, or Apache, because they then have access to high-quality software that they don't have to write all themselves.   Suppose your company wants to sell web servers.  You want to have as much of the customers' money going to you as possible.  If you sell them Windows-based servers with IIS, a chunk of that money is going to Microsoft.  If you sell them Linux-based servers with Apache, you get to keep that money, and you have a lot more control over what you sell.  That may well be worth donating resources to assorted projects.  (Obviously, Microsoft has the opposite opinion.  They'd like the server people to produce cheap hardware that runs Windows and IIS.  Microsoft is likely the company most inherently opposed to Open Source, but even they take advantage of it in some ways.) Let's look at Apple's use.  Apple makes their money selling hardware, but the main distinguishing feature is their ability to make user interfaces.  The iPhone does nothing previous smart phones didn't do, it's just a lot easier to use, and so it sold millions really fast and redefined the market.  They have a good idea as to what they're selling.  Nobody's going to buy Apple for operating internals, so by having the Darwin part of the OS as Open Source they can get some outside help on it.  They also started with Open Source after failing to produce a top-quality operating system themselves.  Nobody's going to buy Apple for the printer software, so it was easier and faster to use CUPS.  They will for the interface, so that's closed down tight. "
    },
    {
        "ID": "6662",
        "Question": "I've been on the constant road of learning new concepts in OOP, Software Design, Architecture, etc. But there are times when you are in a team where those concepts are foreign to them and they don't have the time or the same eagerness to learn as you. The problem is if you design your code the \"right\" way, the people who code with 2kLOC classes won't understand it. Would you sacrifice good coding principles to support your team? What about a scenario where this will be a long term arrangement? ",
        "Best answer": "Welcome in the real world. I worked with hundred of different developers around the world, in startups and large enterprises. The vast majority of them doen't understand advanced concepts, and won't in the future. It's just too complicated to master something unless you spend over a decade in that particular field. Very few are able to do that. That's why I'm really upset when one of my developers is too \"CV driven\" and try to implement design patterns that do nothing better but allow him to put something new in his resume (or the title \"Architect\"), while the rest of the team is strugling to understand and maintain HIS code. That's why I think that a good developer is not the technically supperior, but the most pragmatic of the pack:  An excellent developer try to convert   a functionnality the business ask by   maximizing the ROI.  IMHO, keeping things simple, is the way to go. If you want to do the \"right\" stuff, do it at home. Your boss is especting something else from you. "
    },
    {
        "ID": "6665",
        "Question": "The class diagram is modeled on the system requirements, and it is important to create solutions based on those requirements. If I have said class diagram should I strictly adhere to it? What about refactoring? What if the diagram did not provide some design principle that I feel were left out? ",
        "Best answer": "Short Answer: No.  Your output should be working (hopefully tested) code that performs the business function it's supposed to do. How you accomplish that task shouldn't be mandated (again, unless you work for NASA).  A lame analogy: I get into a taxi and tell them where to go. I leave it up to them to drive me there. I trust them to get me there safely and in a timely manner. I am not going to sit there and micromanage the taxi driver and tell him when to turn on his turn signal, how much to press the accelerator, or when to get gas. That's his job.  "
    },
    {
        "ID": "6677",
        "Question": "The way I see it if you create one before you get the advantage of:  Planning ahead Overview of the project  but you lose:  Time (doing work you'll probably end up repeating when writing code)  On the other hand, I could just create them all after writing my code just to keep it as a reference for future developers. Which one serves the purpose of class diagrams and which is more advantageous? ",
        "Best answer": "When I've had them created before coding, we view them as \"temporary\" documents. That is, we create the diagrams and get our thoughts onto paper. We start coding from those class diagrams. We then throw them out. It's not worth spending the time to maintain them once coding has started. And if you want up-to-date class models, use a tool to create them from the code.  "
    },
    {
        "ID": "6815",
        "Question": "As I advance in my career, I have found that I do less technical work and more project management work.  I joke that I am getting dumber every day.  Each time I go back to doing technical work it seems to be a little harder to get things going.  What suggestions do people have for maintaining technical expertise throughout your career? ",
        "Best answer": "Keep on coding I've always tried to steer away from a position where I'm forced to do more management than coding.  In fact it's something I always point out in interviews - I'm a coder - always will be. I would say that's #1 on keeping your technical skills sharp - as simply as it sounds - keep on coding.  Whether or not that's what you want to do is a different story. You can also try being more involved with group code reviews.  Not only is this a great way of sharing knowledge and elimination key-person dependency, it will also show you what's going on in the codebase and keep your skills sharp. The problem (in my opinion) with programmers moving into project manager positions is that there is absolutely nothing that says if you're good at programming you will be good at project management.  In my experience the exact opposite is usually true. "
    },
    {
        "ID": "6827",
        "Question": "This includes architecture decisions, platform choices or any situation where a such a bad choice led to negative consequences. ",
        "Best answer": "Years ago, I was the lead developer on a database centered application that started throwing errors. I tracked it down to the fact there were duplicate values in a database field that shouldn't have allowed them.  I was beating myself up about forgetting to set a unique constraint on the database when I had pushed it to production because it was just so obvious that this field needed one. I commiserated to one of my fellow developers who corrected me... Other Developer: \"Oh you didn't forget, there was a unique constraint on that field. I just removed it.\" Me: \"Why did you remove it?\" Other Developer: \"I did that a few weeks back. I was getting data files from the customer and they wouldn't import because the unique constraint was blocking the new data. So I removed the constraint so that I could finish importing it.\" Me: \"Did you stop to consider that maybe there was a problem if we were getting new data that overlapped with existing data and think about mentioning it to someone before importing it?\" Other Developer: (blank stare) Me: Facepalm. "
    },
    {
        "ID": "6834",
        "Question": "Modern IDEs have a lot of tricks up their sleeves to help with code writing, refactoring,  searching. All those are very helpful, but rarely any of them looks like a real \"magic\" and makes me think \"Wow! How did it figure that out?\" Can you name any impressive IDE code automation (or other) features that blew your mind first time you saw them? ",
        "Best answer": "Backwards Debugging Visual Studio 2010 (and now 2012) lets me debug backwards with IntelliTrace. Never again will I have to re-live the moment where I hit F10 one too many times and have to restart debugging. "
    },
    {
        "ID": "6884",
        "Question": "I'm sure that many of you have encountered a bad client. I'm also sure you took some measures to prevent such encounters in the future. What is the most influential characteristic of a client that warns you to walk away? ",
        "Best answer": "Some time ago I read 6 Warning Signs of a Problem Client and found it a good 'bad client smell' list.  Have you ever had a project that turned out to cause way more stress that it was worth? Of course you have. We all have. Most of the time you’re left saying “Why didn’t I see this coming?” Here are some early warning signs of a problem project and some tips for upgrading them from hopeless to profitable. 1. “I tried doing it myself, but…” 2. There’s No Real Deadline 3. “Somebody Told Me I Should…” 4. Multiple Points Of Contact 5. “Trust Me, This Is Going To Be Huge!” 6. Repeated Meeting Cancellations ...  "
    },
    {
        "ID": "6905",
        "Question": "I've used TestNG and JUnit.  What other frameworks are out there? What makes them special and/or different from the rest? ",
        "Best answer": "Scala Specs gets my vote! :-) Specs is a behavior-driven-development testing framework written in Scala. It can be used to write tests for Java and Scala. It was inspired by RSpec - a testing framework very popular in the Ruby world.  An example test written in Specs: import org.specs._  object ElementSpecification extends Specification {   \"A UniformElement\" should {     \"have a width equal to the passed value\" in {       val ele = elem('x', 2, 3)       ele.width must be_==(2)     }      \"have a height equal to the passed value\" in {       val ele = elem('x', 2, 3)       ele.height must be_==(3)     }      \"throw an IAE if passed a negative width\" in {       elem('x', 2, 3) must throwA(new IllegalArgumentException)     }   } }  Impressive, isn't it? :-) "
    },
    {
        "ID": "6974",
        "Question": "What's the worst programming-related or technical book you've ever read? This can be any book which one way or another can be connected to programming, as long as it's not fiction. ",
        "Best answer": "Any book that allows you to teach yourself X in Y hours. I've read some in the past but once read you don't know any details whatsoever about X and you can't use the book as a reference for X either. After discovering that it seems better to go for the big books.  "
    },
    {
        "ID": "7000",
        "Question": "Processors are getting more and more cores these days, which leaves me wondering... Should we, programmers, adapt to this behaviour and spent more effort on programming for multiple cores? To what extent should we do and optimize this? Thread? Affinity? Hardware optimizations? Something else? ",
        "Best answer": "No matter how good you are, it will be unlikely that you'll come up with a better scheme of managing threads etc. than the teams developing the language and compiler you are writing your code in. If you need your application to be multi-threaded then create the threads you need and let the compiler and OS get on with their jobs. You do need to be aware of how those threads are managed so you can make best use of the resources. Not creating too many threads is one thing that springs to mind as an example. You also need to be aware of what is going on (see Lorenzo's comment) so you can provide hints to the thread management (or override it in special cases), but I would have thought that these would be few and far between. "
    },
    {
        "ID": "7008",
        "Question": "What different types of security do there exist? Why and when should they be implemented? Example: SQL Injection Prevention ",
        "Best answer": "Preventing Buffer overflow  a buffer overflow, or buffer overrun, is an anomaly where a program, while writing data to a buffer, overruns the buffer's boundary and overwrites adjacent memory. This is a special case of violation of memory safety. Buffer overflows can be triggered by inputs that are designed to execute code, or alter the way the program operates. This may result in erratic program behavior, including memory access errors, incorrect results, a crash, or a breach of system security. Thus, they are the basis of many software vulnerabilities and can be maliciously exploited...  I cannot count exploits that are based on this. "
    },
    {
        "ID": "7038",
        "Question": "I'm using VS 2010 since we're developing an app in .Net 4 and the performance is driving me crazy. It's mostly bad when I don't view the IDE for a while (such as when I get pulled away for a help desk call or come in in the morning).  I realize it's probably built in WPF which unloads its resources when unused, but the few minute delay while it loads everything back up is really annoying. I've also noticed some significant delays when opening files or compiling. ",
        "Best answer": "Get a faster machine? "
    },
    {
        "ID": "7055",
        "Question": "Which design pattern do you think is the most popular? ",
        "Best answer": "I'm pretty sure the most common is 'The Big Ball of Mud'.  Unfortunately for us all.  http://en.wikipedia.org/wiki/Big_ball_of_mud "
    },
    {
        "ID": "7057",
        "Question": "How should I be keeping track of a php script's versions/changes when it pertains to a single script in one file? Should I have a separate changelog, and if so (1) what should it be named, (2) how should the contents of the log be formatted, and (3) how should the log be associated with the php script/file? Or, should I include the changes in a comment at the top of the php file?  Currently, I have the following comments at the top of the file:  /**  * @author    John Doe <contact@John Doe.com>  * @version   1.0  * @copyright Copyright (c) 2010, John Doe  */   Thanks in advance for your response! ",
        "Best answer": "Aren't you using a VCS such as SVN, Git or Mercurial? There's no better way to keep track of the changes than being able to see the difference of the file. Of course, you would want to include the changelog on both: the main comment block and the last commit you do. "
    },
    {
        "ID": "7157",
        "Question": "I was wondering if there was a standard for laying out a class's regions.  I currently use Fields Constructor Properties Public Methods Private Methods  Fields being Private Properties and Properties being the public ones. I'll normally use subregions within that if needed, or will occasionally add other regions below (such as interface or baseClass members).  ",
        "Best answer": "Sub Regions? Does your class have a Single Responsibility? (implicit in that ... my answer is \"Rarely any regions, except maybe to group properties, constructors and methods\"... but even then, I don't use it that much) "
    },
    {
        "ID": "7166",
        "Question": "Thoughts on these?  Python is one example, and no this is not a stab against Python I like the language.   What languages have a indentation requirement? ",
        "Best answer": "In Makefiles, it's annoying. In python, I find it very apropos and it makes the syntax a lot cleaner. I think the thing that makes it better in python is that no special characters are required, the only requirement is that you be consistent. You should be doing it anyway, so you get no cost added by following it. "
    },
    {
        "ID": "7217",
        "Question": "Most programmers defending methodologies politically correct like Agile, Waterfall, RUP, etc. Some of them follow the methodology but not all of them. Frankly, if you can choose the methodology, you certainly would go to mainstream \"correct\" methodologies or you would prefer the \"easier\" methodology like cowboy programming? Why? I know it depends. Please, explain when you would use one or another. Please, say what advantages do you see on Cowboy coding. See about Cowboy coding on Wikipedia ",
        "Best answer": "I think almost every experienced programmer has gone through three stages and some go through four:  Cowboy coders or nuggets know little to nothing about design and view it as an unnecessary formality.  If working on small projects for non-technical stakeholders, this attitude may serve them well for a while; it Gets Things Done, it impresses the boss, makes the programmer feel good about himself and confirms the idea that he knows what he's doing (even though he doesn't).  Architecture Astronauts have witnessed the failures of their first ball-of-yarn projects to adapt to changing circumstances.  Everything must be rewritten and to prevent the need for another rewrite in the future, they create inner platforms, and end up spending 4 hours a day on support because nobody else understands how to use them properly.  Quasi-engineers often mistake themselves for actual, trained engineers because they are genuinely competent and understand some engineering principles.  They're aware of the underlying engineering and business concepts: Risk, ROI, UX, performance, maintainability, and so on.  These people see design and documentation as a continuum and are usually able to adapt the level of architecture/design to the project requirements. At this point, many fall in love with methodologies, whether they be Agile, Waterfall, RUP, etc.  They start believing in the absolute infallibility and even necessity of these methodologies without realizing that in the actual software engineering field, they're merely tools, not religions.  And unfortunately, it prevents them from ever getting to the final stage, which is:  Duct tape programmers AKA gurus or highly-paid consultants know what architecture and design they're going to use within five minutes after hearing the project requirements.  All of the architecture and design work is still happening, but it's on an intuitive level and happening so fast that an untrained observer would mistake it for cowboy coding - and many do. Generally these people are all about creating a product that's \"good enough\" and so their works may be a little under-engineered but they are miles away from the spaghetti code produced by cowboy coders.  Nuggets cannot even identify these people when they're told about them, because to them, everything that is happening in the background just doesn't exist.   Some of you will probably be thinking to yourselves at this point that I haven't answered the question.  That's because the question itself is flawed.  Cowboy coding isn't a choice, it's a skill level, and you can't choose to be a cowboy coder any more than you can choose to be illiterate. If you are a cowboy coder, then you know no other way. If you've become an architecture astronaut, you are physically and psychologically incapable of producing software with no design. If you are a quasi-engineer (or a professional engineer), then completing a project with little or no up-front design effort is a conscious choice (usually due to absurd deadlines) that has to be weighed against the obvious risks, and undertaken only after the stakeholders have agreed to them (usually in writing). And if you are a duct-tape programmer, then there is never any reason to \"cowboy code\" because you can build a quality product just as quickly. Nobody \"prefers\" cowboy coding over other methodologies because it isn't a methodology.  It's the software development equivalent of mashing buttons in a video game.  It's OK for the beginner levels but anybody who's moved past that stage simply won't do it.  They might do something that looks similar but it will not be the same thing. "
    },
    {
        "ID": "7230",
        "Question": "While the basic scenarios are white on black and black on white, most programmers find more varied syntax highlighting useful. What advantages do you find from a general setup?  (E.g. \"a dark background allows...\") What specific tweaks do you find most helpful?  (E.g. \"slightly off-white works to...\", or \"highlighting quote marks and escapes, like \\n, differently shows...\") One answer per person, please; list multiple points as part of your one response, if needed. ",
        "Best answer": "Either way though, I personally find that a white screen with dark text is too bright and hard on the eyes for long coding sessions.  The black is much less intrusive on my eyes.   "
    },
    {
        "ID": "7242",
        "Question": "Who here is learning Go? Are other companies looking at using it? Is it likely to become widely used? ",
        "Best answer": "When it comes to programming languages, the old adage, \"it's not who you are, it's who you know\" definitely holds true.  C and C++ were sponsored by AT&T, Java was brought to us by Sun, the .NET family came out of Microsoft, and all of them got very popular very quickly.  Then we have Objective-C and Python, which were around for quite a while and stayed really obscure until they were discovered and hyped up by Apple and Google, respectively, and then suddenly they really took off.  But languages without a major sponsor tend to languish in obscurity, no matter how good they are. Go is sponsored by Google.  It's not difficult to arrive at the right conclusion here.  Give it five years and it's gonna be huge. "
    },
    {
        "ID": "7245",
        "Question": "I had a question posed to me the other day by another programmer.  I remember (a very long time ago) wondering the very same.  Why was a browser-side include tag never considered?  Or was it? Specifically with a tag that instructed the browser to include additional HTML from other sources. e.g. <include src=\"http://server/foo/bar.html\">.  Many folks will make javascript calls and fill innerHTML to accomplish the same, when the same outside a the javascript engine could be accomplished by the browser. It would have been painful to have nested <HTML>s <BODY>s (i.e.) but we have to consider that aspect anywhere anyway. ",
        "Best answer": " Why was a browser-side include tag never considered? Or was it?  It was certainly requested by every newbie web author who hadn't worked out Server Side Includes yet, back in the early days on the www-html list. But in those days W3 were happy to completely ignore web author pressure. If cross-site inclusion were allowed it would be a security disaster. You could pull in a page from the user's bank and read content from it. (Originally, DOM scripting was limited, but you could still have read from document.links, document.images, scripting functions dropped by the target page, etc. Since then you can do what you like with imported content.) If cross-site inclusion weren't allowed... well then the feature wouldn't have any advantage over server-side includes. It'd be more, slower work for the client to do that the server could have dealt with better. Unlike <iframe>, an include would have to block page loading. SSIs would be in every way superior. "
    },
    {
        "ID": "7305",
        "Question": "What is your favorite method to declare a pointer? int* i;  or int *i;  or int * i;  or int*i;  Please explain why. see also: http://www.stroustrup.com/bs_faq2.html#whitespace ",
        "Best answer": "If you write: int* i, j, k;  you misleadingly suggest that all of i, j and k are pointers to int. So I contend it's superior to annex the * to the variable name. "
    },
    {
        "ID": "7325",
        "Question": "A typical curly brace programming lang has two types of AND and OR: logical and bitwise. && and || for logical ops and & and | for bitwise ops. Logical ops are more commonly used than bitwise ops, why logical ops are longer to type? Do you think they should be switched? ",
        "Best answer": "Probably a legacy thing. Bitwise operations may not be very common nowadays, but when coding on very low level you use them all the time. So when C was deviced in the 70's or whenever it was created, bitwise OPs were probably more common than logical OPs. And since C has it that way, I take it many other languages (such as Java, indirectly from C++) followed the same convention. Now that I've used the double notation (&& and ||) for so long, I'd only be confused if they were switched. But for completely new programmers, it would probably be a nice thing: you save a whopping 50% per logical operation! Imagine the productivity! ^^ EDIT: Provided these changed were done in a new language, of course, not in any existing languages. "
    },
    {
        "ID": "7347",
        "Question": "I'm talking about the way we write simple routines in order to improve performance without making your code harder to read... for instance, this is the typical for we learned: for(int i = 0; i < collection.length(); i++ ){    // stuff here }  But, I usually do this when a foreach is not applicable: for(int i = 0, j = collection.length(); i < j; i++ ){    // stuff here }  I think this is a better approach since it will call the length method once only... my girlfriend says it's cryptic though. Is there any other simple trick you use on your own developments? ",
        "Best answer": "insert premature-discussion-is-the-root-of-all-evil lecture That said, here are some habits I've gotten into to avoid unnecessary efficiency, and in some cases, make my code simpler and more correct as well. This isn't a discussion of general principles, but of some things to be aware of to avoid introducing unnecessary inefficiencies into code. Know your big-O This should probably be merged into the lengthy discussion above.  It's pretty much common sense that a loop inside of a loop, where the inner loop repeats a calculation, is gonna be slower.  For example: for (i = 0; i < strlen(str); i++) {     ... }  This will take a horrendous amount of time if the string is really long, because the length is being recalculated on every iteration of the loop.  Note that GCC actually optimizes this case because strlen() is marked as a pure function. When sorting a million 32-bit integers, bubble sort would be the wrong way to go.  In general, sorting can be done in O(n * log n) time (or better, in the case of radix sort), so unless you know your data is going to be small, look for an algorithm that's at least O(n * log n). Likewise, when dealing with databases, be aware of indexes.  If you SELECT * FROM people WHERE age = 20, and you don't have an index on people(age), it'll require an O(n) sequential scan rather than a much faster O(log n) index scan. Integer arithmetic hierarchy When programming in C, bear in mind that some arithmetic operations are more expensive than others.  For integers, the hierarchy goes something like this (least expensive first):  + - ~ & | ^ << >> * /  Granted, the compiler will usually optimize things like n / 2 to n >> 1 automatically if you're targeting a mainstream computer, but if you're targeting an embedded device, you might not get that luxury. Also, % 2 and & 1 have different semantics.  Division and modulus usually rounds toward zero, but it's implementation defined.  Good ol' >> and & always rounds toward negative infinity, which (in my opinion) makes a lot more sense.  For instance, on my computer: printf(\"%d\\n\", -1 % 2); // -1 (maybe) printf(\"%d\\n\", -1 & 1); // 1  Hence, use what makes sense.  Don't think you're being a good boy by using % 2 when you were originally going to write & 1. Expensive floating point operations Avoid heavy floating point operations like pow() and log() in code that doesn't really need them, especially when dealing with integers.  Take, for example, reading a number: int parseInt(const char *str) {     const char *p;     int         digits;     int         number;     int         position;      // Count the number of digits     for (p = str; isdigit(*p); p++)         {}     digits = p - str;      // Sum the digits, multiplying them by their respective power of 10.     number = 0;     position = digits - 1;     for (p = str; isdigit(*p); p++, position--)         number += (*p - '0') * pow(10, position);      return number; }  Not only is this use of pow() (and the int<->double conversions needed to use it) rather expensive, but it creates an opportunity for precision loss (incidentally, the code above doesn't have precision issues).  That's why I wince when I see this type of function used in a non-mathematical context. Also, notice how the \"clever\" algorithm below, which multiplies by 10 on each iteration, is actually more concise than the code above: int parseInt(const char *str) {     const char *p;     int         number;      number = 0;     for (p = str; isdigit(*p); p++) {         number *= 10;         number += *p - '0';     }      return number; }  "
    },
    {
        "ID": "7349",
        "Question": "As a general purpose programmer, what should you learn first and what should you learn later on? Here are some skills I wonder about...  SQL   Regular Expressions   Multi-threading / Concurrency   Functional Programming   Graphics   The mastery of your mother programming language's syntax/semantics/featureset   The mastery of your base class framework libraries   Version Control System   Unit Testing   XML   Do you know other important ones? Please specify them...  On which skills should I focus first? ",
        "Best answer": "In my experience, programmers who are \"trying to be good programmers\" by learning things like they would learn basic math are never as good as those who program with a purpose. Just learn what you need to do to accomplish an idea you have; learning any of the points you listed is useless if you're not going to use it. "
    },
    {
        "ID": "7455",
        "Question": "I mean, I still have a MSX2 with a Z80 processor and if you look at any Konami's game made for that computer in its time (roughly between '84 and '90) is amazing the high quality code of those games. I was a kid at the time, trying to learn how to program a computer and still today fascinated me how well made they are, mostly zero bugs or glitches, despite the really complex behavior. What hardware/software tools could they have used to accomplish that quality, which metodology? I know computers are really more complex today but at that time, even a stock control program I made in Basic was plagued with many bugs and was painful to debug. Any light you can shed will be deeply appreciated.  ",
        "Best answer": "I don't know anything about Konami, so I'm taking an educated guess here.  Games on machines like the MSX would have required direct access to the hardware, so that pretty much limits the choice of languages to either C or Z80 assembly language.  (There were C compilers for the Z80 back then, for example this one.) I doubt if the games were completely written in C, if at all; mostly likely a lot of assembly code for performance reasons. As far as platforms go, since the MSX didn't have a hard drive, I would further assume the programs were initially written on a larger Z80 system, perhaps running CP/M.  For debugging, the code could probably have been downloaded into a special game cartridge for the MSX that had RAM memory in place of ROM -- this would have allowed the developer to download the program over and over again without burning ROM chips.  Debugging (e.g. setting breakpoints) could have been accomplished by replacing the Z80 inside the MSX with an In-Circuit Emulator (ICE) interface. "
    },
    {
        "ID": "7502",
        "Question": "I have the \"user's side\" of the story, I think they want it better if it's on Spanish. But, what about the programmers? Do you make your programs multi-language? Why? Why not?  Who translate your software? Are you ok with paying somebody to translate your app or you prefer doing it yourselves? Is the benefit bigger than the costs? ",
        "Best answer": "There's really only one question you need to answer:  Is there a demand for the application to be translated into one (or more) other language?  If the demand is there then you need to look into translation. There may be political (both with a lower case \"p\" and upper case \"P\" - bilingual countries like Canada spring to mind), social or ethical reasons that need to be considered as well as the cost. As to the methodology - when I worked on software that was translated we got our German and French distributors to do the translation. The benefit of this is that you are using someone who knows the product and will (hopefully) choose the most appropriate translation when more than one is available. If you don't have distributors familiar with the language then there are services available that will do the translation for you. Failing that there's always Google Translate :) "
    },
    {
        "ID": "7505",
        "Question": "I've been handed the ER diagram for a quite complex database.  Complex to me at least, 849 tables. One of the architects says to understand the code, you need to understand the database and relationships. Now I am basically a Java programmer hence not too familiar with this How do I start? Should I start from something basic like USER or ORDER and see which relationships they have with other tables around them? Any tips or tutorials would help a lot ",
        "Best answer": "An ER Diagram is out there to visualize a design, which was made to accomplish a certain job. Understand what the job is first, then ask questions to the diagram.  \"How is an order stored?\", \"How do I retrieve user information\", \"How can I tell which user ordered a certain product between these intervals and with these options?\". Such questions might help you understand the design. If there is a requirements analysis documentation, it will help you a lot with coming up with these questions. You can explore how certain requirements are handled. I suspect that a database with 849 tables is highly normalized so most of those tables might be lookup tables which does not have much effect on the relationships of the entities. Try to focus on the parent tables holding keys to those lookup tables. "
    },
    {
        "ID": "7539",
        "Question": "I'm writing a small database-centric application in Java Swing that will be used internally in a small company. I haven't much experience with developing production-level applications. I don't use an ORM, just a few simple methods that run SQL queries with Java JDBC. On every method that access the database I use a try-catch statement for SQLException and IOException. Should I do any logging here? In case I should, how much should I log? Please give me examples of good practice in this case. ",
        "Best answer": "Two main things in production system are:  Don't blow up the logs with information that is not interesting Allow raising the log level for troubleshooting purpose.  Use some kind of a logging infrastructure. If the exception can occur during the normal operation, report it in debug level. If it is a real problem, report it in error level. Either way you will have a way to troubleshoot the system by raising the log level. Also - when you report an exception always make sure the the entire stack trace + inner exceptions are printed. "
    },
    {
        "ID": "7547",
        "Question": "I develop a web application and we test it on several browsers, including Firefox and Internet Explorer. During our development we tested the application on Firefox 3.5.2 and our testing team tested the same on 3.5.6 and found the UI looks good. Once in production, though, we've started receiving client complaints that it doesn't look good in 3.5.8. How do we handle these minor version issues? How can we check the browser compatibility during development and during testing? Is there any tool to test the application on 3.5.X, X being all the minor versions? ",
        "Best answer": "I would say stick with the age-old traditions in web development.   Stick with the standards.  Add small adjustments for the specific browsers your users are using if necessary. You can't really do any better than that.  There is no tool that will ensure your site works on every single browser and version.  Stick with the standards.  Add small adjustments. "
    },
    {
        "ID": "7551",
        "Question": "Why are there so many programming languages? And what prompts someone to create a programming languages in spite of the fact that other languages already exist? ",
        "Best answer": "Programming languages evolve New programming languages often learn from existing languages and add, remove and combine features in a new way. There is a few different paradigms like object oriented and functional and many modern languages try to mix features from them both. There is also new problems that needs to be solved, e.g. the increase of multi-core CPUs. The most common solution to that have been threads, but some programming languages try to solve the concurrency problem in a different way e.g. the Actor Model. See Erlang - Software for a Concurrent World "
    },
    {
        "ID": "7565",
        "Question": " Possible Duplicate: What good book shoud I buy to learn Agile from scratch?   It seem that Agile for the solo developer is a good idea. But how to learn it?  Is there any good book, web reference or course that a solo developer could start from? ",
        "Best answer": "If you have to purchase only ONE book. Buy Practices of an Agile Developer.  "
    },
    {
        "ID": "7581",
        "Question": "Is Java becoming the de facto standard from Linux application development in the same way .NET is the standard for Windows application development?  If not why not?   ",
        "Best answer": "In short: No. It really depends on what sort of application you are writing. For many the answer is still regular old C/C++ (if doing, say Qt or GTK+ GUI development). Many doing GTK+ development may also be using Python + PyGTK. If doing web or web services development, you see lots of Ruby, Python, PHP, and Java. "
    },
    {
        "ID": "7618",
        "Question": "Perhaps the greatest promise of using object-oriented paradigm is the code reuse. Some dispute that this was achieved. Why was it (not) achieved? Does code reuse as OOP defines it, make projects more productive? Or more manageable? Or easier to maintain? Or with more quality? Probably we all agree that code reuse is a good thing, but there are several ways to achieve this goal. The question is about the method of code reuse offered by OOP. Was it a good thing? Are there better methods to achieved code reuse than object orientation, sub-classing, polymorphism, etc.? What ways are better? Why? Tell us your experience with OOP reuse or other paradigms reuse. ",
        "Best answer": "Code re-use is achieved in OOP but it is also achieved in functional programming.  Anytime you take a block of code and make it callable by the rest of your code such that you can use this functionality elsewhere is code re-use. This type of code re-use also makes code more manageable because changing this one callable block changes all places that it is called.  I would say this result increased quality too and readability.   I am not sure OOP is simply there to provide code reuse.  I look at OOP as more of a way to interact with objects and abstract away the details of the data structure.   From Wikpedia:  Object-oriented programming has roots that can be traced to the 1960s. As hardware and software became increasingly complex, manageability often became a concern. Researchers studied ways to maintain software quality and developed object-oriented programming in part to address common problems by strongly emphasizing discrete, reusable units of programming logic[citation needed]. The technology focuses on data rather than processes, with programs composed of self-sufficient modules (\"classes\"), each instance of which (\"objects\") contains all the information needed to manipulate its own data structure (\"members\"). This is in contrast to the existing modular programming that had been dominant for many years that focused on the function of a module, rather than specifically the data, but equally provided for code reuse, and self-sufficient reusable units of programming logic, enabling collaboration through the use of linked modules (subroutines). This more conventional approach, which still persists, tends to consider data and behavior separately.  "
    },
    {
        "ID": "7629",
        "Question": "What coding standards do you think are important for .NET / C# projects?  This could be anything from dealing with curly braces and spacing and pedantry like that.  Or it could be more fundamental questions such as what namespaces in the .NET Framework to avoid, best practices with config files, etc. Try to avoid creating a post that is simply the corollary to another.  For example, it would be fine to have one post focusing on curly braces.  We don't need two to support one style vs. the other.  The idea is not to vote for your pet standard, but rather to flesh out what should be thought about when creating standards. ",
        "Best answer": "Here is the official Microsoft Guide on coding standards for the .NET framework Version 4.0. If you want the older version for 1.1, try here. I don't necessarily follow this to a 'T', as they say.  However, when in doubt, this is the best place to start to be consistent with the current .NET framework, which makes it easier on everyone, no matter if they're new to your particular project or not. "
    },
    {
        "ID": "7686",
        "Question": "I am curious about experiences of programmers who have gone beyond college or university and now work in the industry. I am not talking about academia (you need PhD there anyway). Do you have a Master's degree? Has it helped your career? Are there any other benefits besides the knowledge one gains while pursuing the degree? ",
        "Best answer": "Yes it does. It helps a lot in getting your resume shortlisted by the HR who have no idea what programming is all about. "
    },
    {
        "ID": "7720",
        "Question": "I'm looking at licensing some open source software and am looking at the GPL.  What are the pros and cons of using this license? ",
        "Best answer": "Ok, my list of pros and cons of GPL: Pros  It makes people think hard about whether they really buy into Open Source; are you prepared to live by it, and let other people use what you've written, rather than just liking it because of what you can get out of it? It makes sure that when something has been developed by the Open Source community, it stays Open Source; no chance of someone taking all the work that others have been doing, repackaging it and selling it on.  Cons  It's a complete no-no for most corporate organisations; they can't afford the risk of GPL-licenced code getting into their products, so virtually all medium-large companies have clauses explicitly banning GPL-licenced code. It puts people off Open Source. Is it really fair, that because I use your Open-Source image picker control in my app, my whole app must now be Open Source too?  Even if I improved the image picker and contributed that code back to the community?  The terms are too onerous for many developers. Lots of people aren't aware of the stringent terms of GPL, so use it as it's the licence they've heard of without realising what restrictions they're placing on anyone else that wants to use it. Its extremely viral.  If your project contains a component that contains a component that contains a component that is under the GPL (phew!), your whole project is subject to the GPL too.  Ultimately for me the cons outweigh the pros.  To me it smacks of Open Source Evangelists trying to trick the world into going Open Source instead of persuading the world of its benefits. "
    },
    {
        "ID": "7804",
        "Question": "I tried programming Scala in Netbeans and Eclipse, but it is not what I'm looking for. In Netbeans suggestions for method names etc. are not working. In Eclipse I can't go to some classes sources by pressing F3. (e.g. scala List). Is support in IntelliJ IDEA any better? Are there other IDE's supporting Scala? ",
        "Best answer": "IntelliJ IDEA Same question has been asked on Stack Overflow. Check out Which is the best IDE for Scala development? "
    },
    {
        "ID": "7834",
        "Question": "I'm two months away from getting my degree in systems engineering, which is to say, I learned how to code and code well using managed languages. The only reason I got into this career was because I wanted to create video games. I know now that with my current skillset, I won't be able to create some of the things I have in my head. Would getting a second degree in pure mathematics would help me with this goal? ",
        "Best answer": "No type of degree will help you as a programmer more than programming. Experience trumps studying. If you want to be a good programmer then start programming. I don't have a degree but I've been programming on various projects for fun since I was around 15-16; needless to say I'm light years ahead of my friends who studied computer science at a university and ask me questions like \"is it better to check admin privileges for my website through PHP or the SQL GRANT option?\". "
    },
    {
        "ID": "7859",
        "Question": "As a solo developer, I think I'm using an Agile-like process, but I'd like to compare what I'm doing to real Agile and see if I can improve my own process. Is there a book out there that's the de-facto standard for describing best practices, methodologies, and other helpful information on Agile? What about that book makes it special? ",
        "Best answer": "Is there a canonical book? There is the agile manifesto, but for a canonical book? No. There are lots of books out there. Specific book recommendations: Agile Software Development, Principles, Patterns, and Practices by Robert C. Martin  Agile Software Development, Principles, Patterns, and Practices. This is focused on developer practices and coding and is a must read for any developer serious about agile software development. There is also a C# version of the book that he and his son Micah wrote, so if you are a .NET developer, that version might be the one for you.  The art of Agile Development by James Shore  For an insight into overall agile project practices look at The Art of Agile by James Shore & Shane Warden. It's focused on XP practices (but that's really because XP is where all the specific developer practices are defined), but has a big picture focus on how Agile projects work. A great thing about this book is that James Shore is publishing the whole text on his website for free, so you can try before you buy.  Practices of an Agile Developer: Working in the Real World by  Subramaniam and Hunt  Practices of an Agile Developer: Working in the Real World  Scrum and XP from the Trenches by Henrik Kniberg  It's a great book for getting a feel for how an agile team works, and it it's a very quick read (couple of hours). I give it to new staff in my organisation - technical and non-technical - and I've had consistently positive feedback. Amazon  Extreme Programming Explained by Kent Beck  Probably the oldest book I can remember which helped make Agile principles popular. Agile is fast becoming a buzz word in the world of Tech. I feel Extreme Programming (XP) is a good place to start before the term Agile just seems to lose meaning. Amazon  Agile Estimating and Planning by Mike Cohn  For \"the Agile process\" - look to Mike Cohn's \"Agile Estimating and Planning\" - bearing in mind that it's Scrum-centric. Cohn covers a lot of the basics as well as some of the things new Scrum teams often struggle with - estimation using Story Points vs. Ideal days, what do do if you fail a story in a sprint, when to re-estimate/size and when not to, etc. He also goes into some really interesting stuff that's mainly the domain of a Product Owner - things like how to assess and prioritize features, etc.  The Art of Unit Testing by Roy Osherove  Osherove presents a very pragmatic approach to unit testing. Presents a good approach on how to refactor code to become more testable, how to look for seams, etc. It is a .Net centric book, however. Amazon  The Agile Samurai by Jonathan Rasmusson  Just purchased this myself and found it to be a refreshing look on how to get started with agile. Amazon    Alistair Cockburns book on his Crystal methodologies is worth while reading - partly because it gives you an alternative the the usual Scrum methods, and partly because he was one of the original guys who came up with Agile in the first place, so I hope he know what he's talking about. Crystal is an interesting methodology as it scales from small teams to very large ones, he describes the changes required to make agile work in these different environments.  Unsorted books mentioned  Agile Adoption Patterns: A Roadmap to Organizational Success by Amr Elssamadisy  Agile and Iterative Development: A Manager’s Guide by Craig Larman  Agile Estimating and Planning by Mike Cohn  Agile Project Management: Creating Innovative Products by Jim Highsmith  Agile Retrospectives: Making Good Teams Great by Esther Derby and Diana Larsen  Agile Software Development by Alistair Cockburn  Agile Software Development with Scrum by Ken Schwaber and Mike Beedle  Becoming Agile: ...in an imperfect world by Greg Smith and Dr. Ahmed Sidky  The Business Value of Agile Software Methods: Maximizing Roi with Just-In-Time Processes and Documentation by David F. Rico, Hasan H. Sayani, and Saya Sone  Collaboration Explained by Jean Tabaka  Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation by Humble and Farley  Crystal Clear: A Human-Powered Methodology for Small Teams by Alistair Cockburn  Encyclopedia of Software Engineering edited by Phillip A. Laplante  Fearless Change by Linda Rising and Mary Lynn Manns  Growing Object-Oriented Software, Guided by Tests Freeman and Pryce  Innovation Games: Creating Breakthrough Products Through Collaborative Play by Luke Hohmann  Lean Software Development – An Agile Toolkit for Software Development Managers by Mary and Tom Poppendieck  Lean Solutions by Jim Womack and Dan Jones  Lean Thinking by Jim Womack and Dan Jones  Managing Agile Projects by Sanjiv Augustine  Managing the Design Factory by Donald G. Reinertsen  Planning Extreme Programming by Kent Beck and Martin Fowler  Scaling Lean & Agile Development: Thinking and Organizational Tools for Large-Scale Scrum by Craig Larman and Bas Vodde  Scrum Pocket Guide: A Quick Start Guide to Agile Software Development by Peter Saddington  The Software Project Manager's Bridge to Agility by Michele Sliger and Stacia Broderick  Today and Tomorrow by Henry Ford (From 1926)  User Stories Applied by Mike Cohn   Book lists  Agile Design Recommended Reading  "
    },
    {
        "ID": "7861",
        "Question": "I know we've covered what questions you should ask about a company before you would decide to work there.  But what do you do with the answers? In other words, what would you consider a dealbreaker?  I.e. what would scare you so much about a company that you wouldn't work there, even if everything else was great? For example, if they tell me they don't use version control, I wouldn't work there.  End of story. ",
        "Best answer": "Companies that feel the need to mention up-front that unpaid (for salaried employees) overtime is required 100% of the time. "
    },
    {
        "ID": "7912",
        "Question": "How do you endorse/support a code project that you find helpful, be it established, emergent or fledgling? I think there are some obvious answers, but hopefully there will be some novel suggestions too. ",
        "Best answer": "It is really going to depend on what state the project is in.  If this is code that is avaialble as a completed app that's offered as free to try/purchase to continue then I'll probably pay for the app if I think I'll use it. "
    },
    {
        "ID": "7927",
        "Question": "No one's perfect, and no matter what we do, we are going to produce code that has bugs in it from time to time. What are some methods/techniques for reducing the number of bugs you produce, both when writing new software and changing/maintaining existing code? ",
        "Best answer": "Avoid fancy coding.  The more complicated the code, the more likely there's bugs.  Usually on modern systems, clearly written code will be fast and small enough. Use available libraries.  The easiest way to not have bugs writing a utility routine is to not write it. Learn a few formal techniques for the more complicated stuff.  If there's complicated conditions, nail them down with pen and paper.  Ideally, know some proof techniques.  If I can prove code correct, it's almost always good except for big, dumb, obvious bugs that are easy to fix.  Obviously, this only goes so far, but sometimes you can formally reason about small but complicated things. For existing code, learn how to refactor:  how to make small changes in the code, often using an automated tool, that make the code more readable without changing the behavior. Don't do anything too quickly.  Taking a little time up front to do things right, to check what you've done, and to think about what you're doing can pay off big time later. Once you've written the code, use what you've got to make it good.  Unit tests are great.  You can often write tests ahead of time, which can be great feedback (if done consistently, this is test-driven development). Compile with warning options, and pay attention to the warnings.   Get somebody else to look at the code.  Formal code reviews are good, but they may not be at a convenient time. Pull requests, or similar if your scm doesn't support them allow for asynchronous reviews. Buddy checking can be a less formal review. Pair programming ensures two pairs of eyes look at everything.  "
    },
    {
        "ID": "7993",
        "Question": "During iteration retrospectives on agile projects, one of the topics that comes up most often for us is that the product owner is (or product owners are) not available or engaged in the project at a day to day level.  It seems to be a common theme that customers are unwilling to \"give up\" the necessary amount of their product owner's time to the project, but instead have them answer questions via email, or during product demos only. This has the effect of increasing the length of the feedback cycle and making the project less effective. Have you had to overcome this hurdle? How did you do it? ",
        "Best answer": "The product owner's presence in required meetings (Sprint Review and Planning) is (should be) non-negotiable.  Do what you need to negotiate a time that works, and then absolutely hold the product owner to it.  If something comes up, delay it but hold the product owner responsible for bringing the entire team to a halt.   If the product owner is actually (from) your customer and they are unwilling to participate in that capacity, then maybe it makes sense to find an internal Product Owner that communicates with the customer but is capable/authorized to make some calls on their own, on the customer's behalf.  That is the typical arrangement anyhow, since there are some things that a Product Owner is responsible for that shouldn't really pass by a customer's eyes. Otherwise, your only choice is basically to abandon agile.  You're not going to make it work without a PO at those meetings. "
    },
    {
        "ID": "8020",
        "Question": "Console app (my favorite), quick & sloppy form, MS Paint (for GUI); what works best most of the time for your standard application? why? ",
        "Best answer": "For me hands down it is Balsamiq I love it for a number of reasons.  Easy to use - The interface for Balsamiq is incredibly simple and fast for me to pull things together. Looks like a mockup - When showing it to clients/customers/vendors it looks like a mockup, so there isn't confusion that \"I'm almost done\" or something like that. Looks professional - In addition to the previous point, yes, it looks \"hand drawn\" but still looks professional. Common UI Controls - Are all available, to quickly build out mockups that resemble real apps.   "
    },
    {
        "ID": "8034",
        "Question": "In an open source project, a number of other open source libraries have been included to implement needed functionality, some as libraries (LGPL), and some as source code (non-LGPL). The new BSD license was selected for the project. The included open source libraries are licensed under the new BSD, MIT, Apache, and LGPL licenses, but no GPL licensed code. How should these other open source libraries be credited? Do all the library licenses need to be included in the main project license file? Is it sufficient to just provide links to the project web sites in the Help->About dialog and documentation? Is any credit really needed? ",
        "Best answer": "Each library that you use as a dependency should have a LICENSE file in their source code. I would just take these licenses and rename them to \"LIBRARY_NAME_LICENSE\" and include it with the source code. I know licenses (like the BSD license) require that the original license be included when any source code is reused. If you are just using these as linked libraries, I don't believe any of this is needed. But I may be wrong about this one. "
    },
    {
        "ID": "8055",
        "Question": "If I would start to focus on the .NET platform and be self-employed, then I probably would like to have some Windows 7, Windows Server 2008, Visual Studio 2010 licenses just for the development environment and for testing, and then a few licenses for the production environment (a Windows Server 2008 Web) and added to that upgrades when new versions is available. This will end up in a quite big amount of money. Is there any kind of bundle discount that I can get from Microsoft in such a case? And what is the requirement to be able to get that discount? ",
        "Best answer": "How about a 100% discount? If you are making software you intend to sell, you qualify for BizSpark, which gives all your developers MSDN subscriptions. If you intend instead to offer your services, you don't qualify for BizSpark, but you still don't need to buy separate licenses for dev, staging etc. You can get an MSDN subscription, which covers one developer across any number of machines other than production. You don't install dev tools on production, and your clients are responsible for the Windows, SQL etc licenses they need. It is generally useful to join the partner program. The Registered level is free and lets you buy an MSDN subscription at a dramatically reduced price, 80-90% off or so. The program names vary over time - Empower, Action Pack, etc so you would need to check the partner program to be sure what they are and what they cost at the moment. Finally, back to the free angle, don't rule out Visual Studio Express, SQL Express etc - absolutely no cost ever and almost all the features of the full products. "
    },
    {
        "ID": "8093",
        "Question": "What are some somewhat common math formulas you learned that helped you write better algorithms and become a better programmer? Example: I learned about the ecludian distance formula: sqrt((x1-x2)^2+(y1-y2)^2) which helped me understand how to find like objects by comparing 2 factors.  ",
        "Best answer": "Knowing the powers of 2 is handy, especially when dealing with low-level bitwise operations.   "
    },
    {
        "ID": "8098",
        "Question": "As I've implied through my other posts, I'm still fairly new to the workforce. During team meetings, I tend to be able to keep up with technical discussion, but when my project manager starts talking about how we've won a new contract, or we're involved in a new proposal bid, or... anything that's business rather than technical, really... I can get lost pretty quickly. What is the bare minimum all developers need to know about project management/business to function? ",
        "Best answer": "I think you need to understand how your company makes money. You also need to be able to evaluate if you should do anything to help it make more money, i.e. do your job better. Most importantly this means that you need to be able the tell which task you're working on you should put the most effort into and how to prioritize the tasks you've been given. Also, programmers need to understand that even though something might technically be the best choice, that choice might not be the best choice for the business. And that is a good reason to not take the technically best way. "
    },
    {
        "ID": "8104",
        "Question": "So I know everyone here is all about private offices, how many developers actually have them. I am sort of half skeptical. I can believe that lead developers have them, but that's normally just one person in your average office. That makes me wonder, how many developers have private offices. Which leads to the actual question: why should they have them? ",
        "Best answer": "In the management world, where concentration on a task is not an issue, offices are a means to represent status. They think \"private office == more status, big private office == even more status, etc.\" What most people fail to understand: Every time our concentration is broken, we create at least one bug and/or delay the deadline for another half-hour. Private offices is not a \"nice to have\" for developers but a must. This is not about status, this is about brain physics.  Working in an open space costs at least 30% productivity (I read that in a newspaper, start with this blog post if you want to know more). Worst part: This goes unnoticed. If you always work in such an environment, you'll never notice that it happens! Until you wonder why your neck is stiff, you feel tense/nervous all the time, etc. If you want another productivity increase, take the telephones away, too. Unless you're doing production support, the next day is always soon enough. To relax the team, supply free soft drinks. That costs $100-300/month for a team of 10 and makes sure they take regular breaks, drink enough (so they don't dehydrate). The funny thing is: These aren't a bunch of myths but hard facts. Still, most companies ignore these simple, cheap ways to boost productivity. Well, except for the successful ones, of course (Google, Microsoft, etc). See also:  Open Offices Reduce Productivity and Increase Stress The High Cost of Interruptions A study on unplanned interruptions in software development How to explain a layperson why a developer should not be interrupted while neck-deep in coding?  "
    },
    {
        "ID": "8119",
        "Question": "In what circumstances should an IT Consultant encrypt their hard drive to protect their code/data of their clients? I am thinking that if it does not add much to your work load you might as well use full disc encryption with a 'weak' password to at least prevent someone from accessing your email files and other documents if your laptop is stolen, even if they will not get access to any database files or other very sensitive data.  ",
        "Best answer": "I agree that full-disc encryption is good, especially if you have sensitive data on your a laptop (you probably do). So, with the new laptop models being plenty fast, I'd say \"always\". That said, there are caveats:  if you forget your password, this means all your data are as good as gone (until you remember the password again). (corollary: any encryption solution that has a \"recover password\" option is likely snake oil, not encryption) weak passwords == no protection (your cow-orkers probably won't try to break into your computer, but a stolen laptop's data could be worth some money; plus, pass-phrases are quite strong and easy to remember) the full-disk encryption could make sleep mode/hibernation impractical, if not impossible (check the product you plan to use) some data may be accessible from additional locations (e.g. your e-mails may be stored on a server, with a copy stored locally in your computer) full-disc encryption is not magical pixie dust - it doesn't provide security against other attack vectors, you still need to address those separately (backups, antivirus, firewall, phishing protection, social engineering, rubber hose cryptanalysis)  Note that encryption should not be seen as a way of securing the data forever from anyone - its goal is just to delay an attacker long enough to make the attack uninteresting. With strong encryption, it should take years before the attacker gets to the data by brute force, at which point the data is so old that it's useless. Although the National Security Agency (or similarly powerful entity) can probably crack the encryption much faster (as it can throw enormous amounts of computing power at it), full-disk crypto is still good protection against anyone else cracking it (e.g. your competitors or a random thief). As a bonus, encryption eliminates casual snooping: if you forget your (powered-off) laptop somewhere, an almost-honest person might decide to browse through your files before returning it to you, just out of curiosity. There's a proverb that says \"most locks are made to keep honest people honest\"; strong locks will do that, and also keep the really malicious people out for long enough. "
    },
    {
        "ID": "8187",
        "Question": "A recent question on stackoverflow provoked a discussion about the immutability of primary keys. I had thought that it was a kind of rule that primary keys should be immutable. If there is a chance that some day a primary key would be updated, I thought you should use a surrogate key. However it is not in the SQL standard and some RDBMS' \"cascade update\" feature allows a primary key to change.  So my question is: is it still a bad practice to have a primary key that may change ? What are the cons, if any, of having a mutable primary key ? ",
        "Best answer": "You only need the primary key to be immutable if it's linked to a foreign key, or if it's used as an identifier outside the database (for example in an URL pointing to a page for the item). On the other hand, you only need to have a mutable key if it carries some information that might change. I always use a surrogate key if the record doesn't have a simple, immutable identifier that can be used as key. "
    },
    {
        "ID": "8283",
        "Question": "What was a project or spec that got put on your desk that could not possibly be done? How did you explain the dilemma to the \"requester\"? More importantly, did they understand after you explained the fundamental issue?  ",
        "Best answer": "I was told to make the printer print faster. Serious, and I was written up for failing. The boss wasn't very tech savvy and didn't understand why I couldn't speed it up.  "
    },
    {
        "ID": "8301",
        "Question": "In my experience, software developers tend to wear multiple hats and fill multiple roles with different responsibilities.  From not only coding, but sometimes also writing SQL, designing the user-interface, designing the database, graphics manipulation, to even QA testing. If the primary role is to write software/code, what roles should the developer not take on?  Are there any? The intention of this question is not because a developer is incapable of filling another role-- but having the additional role actually works against the primary role, or should really be a dedicated role of someone who does not primarily program. ",
        "Best answer": "Sysadmin.  Developing software and handling the IT infrastructure are two different skillsets that look similar to an outsider.  (It's all just banging on computers, right?)  For a smallish company, the temptation will be very strong to make The Computer Guy responsible for all the machines in the office.   If you have the skills to actually wear both hats, awesome; but it's one of those things that can be a much greater time sink than people realize, and if you're self-teaching as you go, chances are you're not doing it very well. "
    },
    {
        "ID": "8352",
        "Question": "I really like using ClassNames and memberNames as convention but I am not sure how I would name the file containing a class. I like making my classes defined in a file with the exact same name as the class. But I also like making php files all lowercase. So I am conflicted. If I have a class called ProductGroup should that be defined in ProductGroup.php, productgroup.php, or product_group.php? I know there is no right answer, so I am looking for what is most common or your opinion of which to use.. which do you use? ",
        "Best answer": "At work we use underscores as folder delimiters and name the files exactly the same as the path. The autoloader is very simple, it just has to replace _ with / and add '.php' to the end. ProjectName_Models_ProductGroup() will always reside in ProjectName/Models/ProductGroup.php. It can make for some very long class names, but it doesn't really matter with an IDE. You could use the same convention, but just run strtolower() before including the file. "
    },
    {
        "ID": "8364",
        "Question": "Many developers recommend Firefox for web development for a variety of reasons. But, after looking at Opera, it seems to me that Opera has all of the same web development functionality that Firefox has built into it. So what is wrong with Opera for web development? ",
        "Best answer": "I think any browser you like to work in is the right browser to work in. I like Chrome--I think its developer interface is very nice indeed. Problem is, a very compliant browser is going to fool you when you switch to a less-compliant one (lookin at you, Internet Explorer). Things will be building nicely, and then your boss will look at it on IE6 and it'll be a calamity. So you've got to at least be looking very frequently at your work the browser that's simultaneously most popular and most breakage-prone. "
    },
    {
        "ID": "8391",
        "Question": "In a book I'm reading there is a chapter on documentation for your code. The book is about PHP and described some easy methods but also going for some complicated and time consuming methods (xml, xsl) like DocBook. At my current small company (5 people) we even rarely write comments, but I'm wondering if in a big company how detailed documentation do they write? Do they use such tools like DocBook? Is it complex or simple? ",
        "Best answer": "Working on PHP and NetBeans, the documentation style is pretty much PHPDoc way. Thus I write a little more than what the IDE generates. e.g. IDE generates: /**    * Description for ClassA    *    *    * @author Sam-Mauris Yong    */    class ClassA{      function __construct(){         echo \"5\";     }  }  I'll probably write: /**    * Class A Helper Class  * Some example class used here  *    * @author Sam-Mauris Yong  * @license GNU Public License v3  */    class ClassA{      /**      * Constructor for example class      * echos 5      */     function __construct(){         echo \"5\";     }  }  "
    },
    {
        "ID": "8416",
        "Question": "I have a Java class that handles import and export of data. It started out as a simple private void export() { } private void import() { }  Of course, I wrote the export first, committed it, and then went on to write the import. But import is a keyword in Java - I can't use that as a function name. So I go back and rename both methods. I usually end up with  private void doExport() { } private void doImport() { }  which is both ugly and feels contrived. What do you think of these names? Got any better suggestions? Note: I'm asking now, because it's now happened thrice and that keyword is getting quite annoying. ",
        "Best answer": "I would prefer something like this: private void importData(){} private void exportData(){}  If you now would use this Class as an API you can do something like this: dataUtil.importData();  With the CodeCompletion of the IDE, the differences are more visible if this methods wouldn't start the same way. "
    },
    {
        "ID": "8429",
        "Question": "Which way is more beneficial and productive? ",
        "Best answer": "I think you need both. You have to focus on your core competencies and improve your understanding of them, but at the same time it's beneficial to look outside and see what else is out there. Exposure to other approaches and other languages is very important to make one a better developer overall. There are many ways to skin a cat, as it were, and knowing as many of them as possible will make you  a psychopath better at picking the right tool for a particular task. So, spend most of your time getting better at your chosen proficiency and spend some of your time on learning something new. "
    },
    {
        "ID": "8445",
        "Question": "After 15 years of C++, I've still haven't learn to love using const. I understand it's use, but I've never actually been in situation where being const correct would have avoided the problem I was facing. So how did you come to love benefits of consts? ",
        "Best answer": "Well I wasn't convinced until I tried to embrace the philosophy. I first started by putting const to really read-only members of my most basic class members and member functions arguments. From there, I couldn't compile anymore. Then I persevered in going in the code using those basic classes, see if the previously const additions were really legitimate compared to the use I made of them. It helped me fix some bugs on the way as I added constness to other parts of the code. It's contagious. Most of the code got even more constness and I found easier to debug it because it makes you confident that the compiler will stop you if you start modifying something you shouldn't.  Once I got the application running again, it was faster (had to change some algorithms that I've discovered weren't right for the job), with a lot less bugs and easier to understand when reading the code. I was convinced.  Now, I think that it's even better when you're using a lot of assertions in addition to constness because it makes you feel confident when you have to write new code or modify the current code. You know the compiler will stop you if necessary. It lets you forget about having to check everything you shouldn't modify and then you have more thinking time for more business-specific thinking, or architectural thinking. "
    },
    {
        "ID": "8544",
        "Question": "For someone with .Net experience and wanting to develop for iPhone/iPod/iPad, is it worth learning Objective-C? Is MonoTouch a good alternative? What are some of the trade-offs when using MonoTouch vs coding in Objective-C? ",
        "Best answer": "MonoTouch is a fantastic alternative. I've been using it for pretty much one year now, and I can't ever imagine going back to objective-c. Highlights: LINQ, LINQ to XML, LINQ, C#, LINQ, Garbage collector, LINQ, MonoTouch.Dialog, and a lot of other things. Seriously, though.. nowadays most apps are always downloading data from the web, and you'll need to be converting that to objects, keeping them in lists, sorting and filtering them, and pushing them to databases. That stuff is so simple to do with C# and LINQ that I can't imagine people doing that in other languages.  The $400 dollar cost is not low, but you can probably make that money back in 1 month or so with no advertisement. And the amount of time MT will save you will offset that easily. Also remember, you don't have to pay until the app is ready for testing in a device, so its free for learning. With that said, yes, its still totally worthy to learn Objective-C. It'll make you a better developer for the platform, you'll understand why some things are the way they are, and so on. You'll also be able to read Obj-C examples in the web and translate them to C#.  Finally, here's my suggestion: if you're thinking of getting in iOS development, go to MonoTouch. Spend a few weeks using it, and then make the decision of jumping to Obj-C. The hardest part of going to iOS development is not the Obj-C language, is all the new frameworks you'll have to learn. Being able to learn them in C# is a HUGE deal, you'll be a lot more productive from the start up. Just after you know those things look into Obj-C: everything will be already familiar, so it'll be easy to understand everything. "
    },
    {
        "ID": "8564",
        "Question": "Obviously the size of the project you're working on will be a huge factor in how long you spend writing the design document/specification.  But do you go through everything, picking out every tiny detail? Or do you take a more agile approach and start writing the software quite early on and solve the problems as they come to you? I've always found that there's only so far you can go with designing.  There will inevitably be some things that are missed, and at that point how well you can adapt to the situation means more than the specification itself. Am I taking the right viewpoint on this? Is it actually an opinion, or is a perfect design spec always the best route to go? ",
        "Best answer": "It depends a bit on your target audience, but my experience ( more in small/medium scale development than very large scale work ) is that detailed design documents are arduous and boring to write, rarely read and tend to end up out of date by the time a project is delivered. This does not mean that they are worthless - if you are delivering something for someone, there needs to be an authoritative and agreed statement of what will be delivered sufficiently detailed that everyone can point to it in case anyone is dissatisfied with the deal and say \"this is what we promised\" and evaluate it against what was delivered. If I were setting up a company to build a product, however, I wouldn't worry so much about a detailed specification. I would want to document what we were going to do, but I wouldn't want to go into too much depth regarding how - that is the part that is most likely to change and leave the documents out of date and useless or even inaccurate enough to be actually obstructive. I would prefer to document the \"how\" stuff in code using whatever documentation format the language or IDE supports best, so that as the code changes it is easier to update the documentation at the same time. It won't stop it going out of date, but it will reduce it somewhat. Ideally you would want a design document that could double as your manual when your code is complete, but I don't know of anyone who has managed that successfully.  "
    },
    {
        "ID": "8588",
        "Question": "SQL is officially pronounced as /ˌɛskjuːˈɛl/ like \"S-Q-L\", as stated in   Beaulieu, Alan (April 2009). Mary E. Treseler. ed. Learning SQL (2nd ed.). Sebastapol, CA, USA: O'Reilly. ISBN 978-0-596-52083-0.  But often it is pronounced  /ˈsiːkwəl/ like \"sequel\", what is the history behind this second pronunciation? ",
        "Best answer": "SEQUEL = Structured English QUEry Language.  For a good historical perspective read   Don Chamberlin: ...A bunch of things were happening at about this time that I think we ought to mention just in passing. One was that we had to change the name of our language from SEQUEL to SQL. And the reason that we had to do that was because of a legal challenge that came from a lawyer. Mike, you probably can help me out with this. I believe it was from the Hawker Siddeley Aircraft Company in Great Britain, that said SEQUEL was their registered trademark. We never found out what kind of an aircraft a SEQUEL was, but they said we couldn't use their name anymore, so we had to figure out what to do about that. I think I was the one who condensed all the vowels out of SEQUEL to turn it into SQL, based on the pattern of APL and languages that had three-lettered names that end in L. So that was how that happened. ...  "
    },
    {
        "ID": "8631",
        "Question": "When you are defining a function/variable/etc and are not sure what to name it, what do you name it? How do you come up with a name? If you use a temporary name as a place-card until you give it it's real name, what temporary name do you use?  update I have been using things like WILL_NAME_LATER, NEEDS_NAME, or TO_BE_NAMED. I was hoping there was an adopted convention, I was actually hoping that if I used this adopted convention my IDE would highlight the name until I changed it. ",
        "Best answer": "I always try to give my variables and functions great names. If I can't think of a great name, I'll settle for a good name. If I can't come up with a good name, I'll use an okay name. I have never, in 15 years of professional programming, been unable to come up with a decent name. "
    },
    {
        "ID": "8677",
        "Question": "I'm not sure if I'm using the correct term, but would you program using High-level abstractions like Powerbuilder, or some CMS like MODx or DotNetNuke? I haven't dabbled in any of these yet. The reason I'm asking is that I kind of feel intimidated by the whole notion of using any abstraction over the languages I'm using. I'm thinking that my job might be over-simplified. While it may provide business solutions faster, I'd rather be coding straight from, in my case, .NET.  Do/Would you use abstractions like these or prefer them over programming in lower level languages? ",
        "Best answer": "I've got no problem with using high-level abstractions, with two caveats:  Any abstraction that you can't get underneath when necessary is evil, because it will occasionally be necessary.  Avoid these. Don't ever use any abstraction without a solid understanding of what's really going on under the hood.  Not doing this will frequently cause performance problems, and occasionally cause correctness problems, both of which are very difficult to debug since you don't know what's really happening.  "
    },
    {
        "ID": "8758",
        "Question": "Disclaimer: I by no means condone the use of pirated software. Have you ever witnessed the use of pirated software for development purposes? May be a company didn't have enough money to buy a piece of software and there were no free alternatives? May be a company wanted to try something out before buying and there were no trial licenses for that product. Whatever the circumstances, have you worked at a company where using pirated/cracked software was accepted? Were there any consequences to doing this? ",
        "Best answer": "While I don't have any problem when some companies or individuals use unlicensed software when they can't afford them (yet), I'm always amazed to see how commercial software development factories do it without shame. They are unrespectful to their own profession! Thanks to programs like Microsoft Bizspark (3 years of free Microsoft softwares for any startup that generate less than 1.000.000 a year in revenues), you can now get them legally. "
    },
    {
        "ID": "8805",
        "Question": "Assuming I've found an open source project I'm interested in, how do I evaluate the project(the members, the activity level, etc) to determine if joining the project will be a good use of my time and energy? ",
        "Best answer": "Look at the source control history. From that, you can see checkins, review code, gauge the activity level, and generally see the quality of commits.  You can learn the most from just looking at the source code activity. "
    },
    {
        "ID": "8886",
        "Question": "Interested in knowing from the more experienced ones if someone can find a job as a programmer without even a highschool degree. Consider the said person to be an average programmer. Would someone even consider giving him/her a chance on an interview ? The languages of interest would be python/php/java/c# Please answer for your region/city/country only. No \"go back to school\" answers please. ",
        "Best answer": "Your biggest difficulty is going to be getting through the HR filter. If you can do that, experience will trump education (most of the time).   In the meantime, try to find some small shop that just needs someone who can code. You should also try to join an opensource project (or two) to get some experience and show that you have some skills. You are going to have to start small and build on that. "
    },
    {
        "ID": "8955",
        "Question": "After working out a project, it might require too much tweaks and changes to get it working right... What can I do to prevent this? ",
        "Best answer": "There is simple rule, that I apply all the time because I tend to be a perfectionist too. And perfectionism leads you to mediocrity.  Set a deadline. Ship at that deadline.  To prevent your software from having an unfishined state of missing feature that will make it unusable, use this definition of done for each feature you produce. Don't start the next feature until ALL steps are completed:  Develop the feature Test is (80% code coverage) Commit/Integrate Document (both technical & end user documentation) Update Release Note (both in file and maybe for your website including screenshots) Update Installer (if necessary)  I assume you can create a release with one mouse click (using build scripts) "
    },
    {
        "ID": "8956",
        "Question": "If you are a solo developer, or have a leading role in a software company, you might have to decide how much your software will cost one or another day...  What is your experience with calculating how much your software costs for your customer? Why does this method it work for you? ",
        "Best answer": "While I don't agree with everything Joel says, his \"Camels and Rubber Duckies\" article on this topic is a must-read, IMO. The basic advice is \"charge what people will be willing to pay\", or:  If you can't be bothered to read this, just charge $0.05 for your software, unless it does bug tracking, in which case charge $30,000,000 for it.  "
    },
    {
        "ID": "8966",
        "Question": "Are there ways to unit test your multi-threaded code for race conditions and deadlocks? To see if they are performing the way they should be... ",
        "Best answer": "CHESS, a project of Microsoft Research.  Quoting their site:  CHESS is a tool for finding and reproducing Heisenbugs in concurrent programs. CHESS repeatedly runs a concurrent test ensuring that every run takes a different interleaving. If an interleaving results in an error, CHESS can reproduce the interleaving for improved debugging. CHESS is available for both managed and native programs.  Update (9/23/2015): For C, C++, and Go, you can use ThreadSanitizer. "
    },
    {
        "ID": "8988",
        "Question": "I don't know if it's just with me, but when I start refactoring some code I waste a lot time doing that and it never ends. Every time I read the code again, I find something that can be improved, code refactoring is like my brain into a infinite loop and the end conditional is reached randomly with no satisfaction. So, how much time should I dedicate to refactoring my code? ",
        "Best answer": "If you treat refactoring as its own thing, rather than as part of the process of development, then it can become never ending.  If you follow the test driven development approach of red, green, refactor, then you typically won't have to spend a lot of time refactoring for two reasons:  You'll be refactoring as you go, so it shouldn't become a mammoth task, just another small step in the TDD cycle. The internal \"pressure\" to get the next test written will keep your refactoring efforts to a sensible level.  If you are working with legacy code, then follow Uncle Bob's \"Boy scout rule.\" Timebox yourself to a short time period (say, 30 minutes) and make your goal to check in the code in a cleaner state than it was before. Each time you have to touch the code, you will leave it slightly better. Over time, the most modified code will be the most readable and well factored in your codebase. "
    },
    {
        "ID": "9006",
        "Question": "Functional programming is one of the oldest programming paradigms. However it isn't used much in the industry compared to more popular paradigms. But it have largely been emphasized in academia. What's your strongest opinion against functional programming? ",
        "Best answer": "The problem is that most common code inherently involves state -- business apps, games, UI, etc. There's no problem with some parts of an app being purely functional; in fact most apps could benefit in at least one area. But forcing the paradigm all over the place feels counter-intuitive. "
    },
    {
        "ID": "9043",
        "Question": "I have an external java application which use JDBC as to reach a MySQL database. This will be used by many people and is going to store all the people's data on the same server / MySQL. As I bet people will be able to crack the .jar and see the source, I expect them to be able to cheat the program by editing it and making it get information from other users even when they are not allowed to reach said information. Is there any standard way to protect a server so people should only be able to reach the information their account is connected to even if said people are able to change the source? ",
        "Best answer": "One way to solve the problem is by using a server-side program to get the information for the client instead of having the client connect directly to the server. This can be in the form of a webpage, service, or RMI. The advantages of this is that the user never can access the database directly (which is a big no no), and therefor can't get access to others information or write their own data. The disadvantage is that its complicated and requires external scripts. You have to figure out how your going to send information in between the server and the client thats not SQL based. Sure its the obvious solution, but its not always the best. It completely depends on your app.   As an example, in one of my apps I had a server side PHP script that took a bunch of mode and option parameters. This returned all the information in JSON format and was parsed by the app. It was pretty basic, but it worked for my purposes. For authentication when the app first started it asked the script for a session key (for you, you ask the script with a username and password). The session key was a really long randomly generated SHA512 key that was passed with all requests. For you, if the user shouldn't have access to something, then return an error.  Depending on how sensitive the information is, you could use HTTPS, text encryption, URL encryption, etc.  "
    },
    {
        "ID": "9081",
        "Question": "I am curios to use .NET for some web applications (e.g. ASP.NET MVC 2). However my budget is limited so I would like to use Mono on Linux. I have never used Mono before, and I haven't read about any bigger sites that is using Mono+Linux on the server in production. What's your experience using Mono on Linux in production? How is the performance and stability compared to .NET on Windows Server? Is there any popular sites that is using it in production? Any articles available online were they share their experiences? ",
        "Best answer": "I've been doing some mono mvc stuff on my mac (unix not linux) in the recent weeks and have found mono combined with monodevelop to be a pretty featured and useful environment. So far in a few prototype sites I haven't yet ran into any deal breakers or major issues. In the next few weeks I'll be deploying some sites to linux vps servers. So I'll comment on my experience. [Update] So this answer was a long time ago and it's taken me a while to get some stuff up and running. So my experience: mono-2.10.1 or higher is feature complete for asp.net mvc2 (mono-2.8 had some bugs that required a source compile) and it's now being used by a few cloud providers.  Running on mono-2.8 http://srvd.in/ AppHarbor: http://unwind.apphb.com/ DeployFu: http://unwind.deployfu.com/  I've found it stable and easy to develop with, like I said there were 2 annoying bugs in the 2.8 version but they are fixed in 2.10.1. I'm happy to elaborate based on comments. [Update] I've blogged here about deploying mono + xsp4 + nginx: http://www.thomasvjames.com/2012/07/asp-net-4-on-mono-part-2-automating-deployment/ "
    },
    {
        "ID": "9095",
        "Question": "F# and Scala are both functional programming langugages that don't force the developer to only use immutable datatypes. They both have support for objects, can use libraries written in other languages and run on a virtual machine. Both languages seem to be based on ML. What are the biggest differences between F# and Scala despite the fact that F# is designed for .NET and Scala for the Java platform? ",
        "Best answer": "Major Differences:  Both Scala and F# combine OO-imperative programming and functional programming into one language. Their approach towards unification of paradigms is vastly different though. Scala tries to fuse the two paradigms into one (we call it object-functional paradigm), whereas F# provides the two paradigms side by side. For example, algebraic data types in F# are purely functional constructs with no OO'ness in them whereas ADTs in Scala are still regular classes and objects. (Note: In the process of compilation to CLR bytecode, even F# ADTs become classes and objects but they are not visible to F# programmer at the source level.) F# has full Hindley-Milner style type inference. Scala has partial type inference. Support for subtyping and pure-OO-ness makes Hindley-Milner style type inference impossible for Scala. Scala is much more minimalistic language than F#. Scala has a very small orthogonal set of constructs that are re-used throughout the language. F# seems to introduce new syntax for every little thing, thus becoming very syntax heavy as compared to Scala. (Scala has 40 keywords, whereas F# has 97. That should tell you something. :-) F# being a Microsoft language has an excellent IDE support in the form of Visual Studio. Things are not so good on the Scala side. Eclipse plugin is still not upto the mark. Same goes for NetBeans plugin. IDEA seems to be your best bet at the moment, though it doesn't even come close to what you get with Java IDEs. (For Emacs fans, there's ENSIME. I have heard a lot of good things about this package, but I haven't tried it yet.)  Scala has far more powerful (and complex) type system than F#.   Other Differences:  F# functions are curried by default. In Scala, currying is available but not used very often. Scala's syntax is a mix of that of Java, Standard ML, Haskell, Erlang and many many other languages. F# syntax is inspired by those of OCaml, C#, and Haskell. Scala supports higher kinds and typeclasses. F# doesn't. Scala is much more amenable to DSLs than F#.   PS: I love both Scala and F#, and hope they become predominant languages of their respective platforms in the future. :-) "
    },
    {
        "ID": "9122",
        "Question": "I have done much development in Java using Eclipse, and now I have started to use Visual Studio for .NET development. So far I find the IDEs to be able to do the same things, but just different. What features in Eclipse isn't available in Visual Studio? ",
        "Best answer": "Eclipse has its Local History which is kind of like a super duper Undo.  I am not aware of a similar feature in VS. "
    },
    {
        "ID": "9161",
        "Question": "What are some strategies, methodologies and best-practices for managing a distributed team?  What can be done to encourage and enforce communication between the various team members?  Is it possible to adapt/implement existing agile development methodologies over a distributed team?  Related but not identical:  https://softwareengineering.stackexchange.com/questions/7942/what-tools-are-available-for-remote-communication-when-working-from-home-or-with Which (programming) specialities are needed in a good web development team?  ",
        "Best answer": "Andrew Kazyrevich's presentation is an excellent guide to agile development in distributed development teams. The main suggestions are:  Group chats & frequent calls Desktop sharing, pair programming Continuous integration Unit tests Work items tracking (TFS, JIRA etc) Short iterations Daily sync between teams through standups etc Travelling Organising teams by functionality  "
    },
    {
        "ID": "9175",
        "Question": "What do you do when malloc returns 0 or new throws exception? Just halt or try to survive OOM condition/save the user's work?  ",
        "Best answer": "Most people answering this question have probably never worked on embedded systems, where malloc returning 0 is a very real possibility.  On a system I'm currently working on, there is a total of 4.25K bytes of RAM (that's 4352 bytes).  I'm allocating 64 bytes for the stack, and currently have a 1600 byte heap.  Just yesterday I was debugging a heap walk routine so I can follow the allocation and freeing of memory.  The heap walk uses a small (30 byte) statically allocated buffer to output to a serial port.  It will be turned off for the release version. Since this is a consumer product, it better not run out of memory once the product has been released.  I'm sure it will during development.   In any case, about all I can do is beep the speaker a couple of times, and force a reboot. "
    },
    {
        "ID": "9200",
        "Question": "I'm interested in learning Scala, but due to is relative newness, I can't seem to find a whole lot of books about it. Is there a book out there that's the de-facto standard for describing best practices, design methodologies, and other helpful information on Scala? What makes that book special? ",
        "Best answer": "I can advise Programming in Scala because it's from  the creator of the Scala language: Martin Odersky. He describes most of the features of Scala very detailedly and explains, why he chose this feature instead of an other one. Therefore, the reader gets a deep insight into Scala. The book is fantastic. It is one of the best programming books I've ever read. "
    },
    {
        "ID": "9219",
        "Question": "I've heard it argued that design patterns are the best thing since sliced bread.  I've also heard it argued that design patterns tend to exacerbate \"Second System Syndrome,\" that they are massively overused, and that they make their users think they're better designers than they really are. I tend to fall closer to the former camp, but recently I've been seeing designs where nearly every single interaction is replaced with an observer relationship, and everything's a singleton. So, considering the benefits and problems, are design patterns generally good or bad, and why? ",
        "Best answer": "Design patterns are a language, not advice to write program or a contract.  Their primary use is an a posteriori explanation how a component or a system was (or is going to be) implemented.  Instead of going into too much details, you can just say a couple of words that can describe the implementation well enough for the listener to understand how it works and what was important in it.  Alex: Hey, how are the config files created? Bob: They're generated by a factory, which resides in config.h.  Now Alex knows that creation of config files involves non-trivial preparations, because otherwise their creation wouldn't be enclosed into a factory. However, if Bob was a pattern-headed phony, and just used patterns here and there, Alex couldn't tell anything about config creation, because Bob used factory just everywhere.   This would also lead to excessive complexity in the program. So, program first, then spot patterns in your code, not vice versa.  That's how they're effectively used. "
    },
    {
        "ID": "9236",
        "Question": "Sometimes, I see potential in a new technology eg. AIR (not new but I haven't tried it much), and I play around with it. After sometime, I feel inspired to try a new personal project out of it. It was during this project that I sometimes find another \"tech\" eg. Silverlight more suitable. Maybe its easier to get something done in that platform. But there are many alternatives to such things, and after a while, i feel that i have wasted my time just trying the \"new toy\", spending time learning it to get started etc. only to decide that I made the wrong decision. How long do you take to explore new technology? Or do you stick to what you already know? ",
        "Best answer": "No. Mostly. The real problem is that you can't know which technologies will pan out. You need to at least dabble with new technologies so you can keep up with the industry. However, even if the tech you just tried out fails in the market place, chances are you learned something new. Perhaps you learned a new way of looking at an old problem, or a way of structuring an API - or how NOT to do something. "
    },
    {
        "ID": "9290",
        "Question": "A quality what I would like to develop is to write more concise code. With writing more concise, at least in my opinion, the opportunity to add bugs to the code is smaller. It is easier to read the code for other. My question is if it is something that just comes with experience or is it something you can do explicitly for developing that quality? ",
        "Best answer": "One great way to write less code is to try to avoid re-inventing the wheel, and use existing software components when available. One common answer I get when I ask why people did their own ORM, or their own logging engine, or their own UI components, or their own everything:  But our is better  I believe this statement is correct most of the case, but the negative impact on the ROI is very high in most case. You mom does the best dishes right? But you can't ask you mom to come home and prepare them everyday. That's why I do think that developers should get interest in financial impact of their choices. Some of them are:  Extra work required to build the component Extra work for new comers to learn it Huge extra work to maintain it  I like to think that those component vendors are your extended team working for you for a tiny fraction of what you would have paid to build, maintain and improve it yourself. It's better for the whole company to maximum ROI rather than working on maximizing our ego satisfaction ;) The more money your company get, the more likely your work conditions and salary will increase. "
    },
    {
        "ID": "9320",
        "Question": "Lisp obviously is an advantage for the AI stuff, but it doesn't appear to me that Lisp is any faster than Java, C#, or even C. I am not a master of Lisp, but I find it incredibly difficult to understand the advantage one would get in writing business software in Lisp. Yet it is considered as a hacker's language. Why does Paul Graham advocate Lisp? Why did ITA Software choose Lisp over other high-level languages? What value does it have over these languages? ",
        "Best answer": "There are a few reasons I am working on becoming competent with Common Lisp.  Homoiconic code. This allows structured self-modifying code. Syntax-aware macros. They allow rewriting of boilerplate code. Pragmatism. Common Lisp is designed to get stuff done by working professionals. Most functional languages aren't, as a rule.  Flexibility. It can do a lot of different things, all at reasonable speeds. Wartiness. The real world is messy. Pragmatic coding winds up having to either use or invent messy constructs. Common Lisp has sufficient wartiness that it can get stuff done.  Arguably the only real reasons to choose against Common Lisp is that the standard libraries are dated. I will go out on a limb and say that in the general case, syntax should not be an issue to a professional software worker. "
    },
    {
        "ID": "9346",
        "Question": "One of my colleagues likes to use automatic code generators, which create large amounts of code that is poorly documented and very hard to maintain. Is the cost of using a code generator worth the hassle in maintenance, for the reduced time to creation? ",
        "Best answer": "Code generated by a generator should never be maintained by hand. If it needs to be changed, then the generator and/or its settings must be tweaked and run again. Considering that, it doesn't matter if the resulting code is incomprehensible and undocumented as long as the generation mechanism itself is crystal clear. (Be sure to document the fact that the code is generated, and where the generator is and how it works.) Analogy: while my computer's processor is always executing machine code, I don't need to know anything about it as long as I know how to create that machine code using high-level language and compiler. I've heard that GCC sometimes produces subpar machine code, but who cares, as long as it works perfectly. Database abstraction layers produce SQL to operate with the DB engine, but who cares what that SQL looks like, as long as the abstraction layer is clear and works? When properly used, code generators can definitely save not only creation, but also maintenance cost. "
    },
    {
        "ID": "9354",
        "Question": "Should a test plan for internal software account for the error states of all possible configurations? Ie: I have an app.config with some file paths.  Should the test plan have steps for testing what happens when those values are incorrect? ",
        "Best answer": "Two schools of thought on this:  If it's likely to cause an issue, then yes it should. On the other hand, since it's an internal app, you may not want to spend too much effort dotting every I and crossing every T.  The last one hinges on the scope of the software.  If it's just a simple utility that makes something a bit easier to do, then it may be fine without very robust error handling.  If it will get a good amount of use, especially by many different people, then it should be relatively robust as the users will likely come to depend on it. "
    },
    {
        "ID": "9371",
        "Question": "Right now my supervisor is creating requirements documentation / specs for me using bugtracking software.  This seems like a terrible idea to me, all the requirements are on these little tickets and I have to click around on this dumb webform to get at the requirements.  What is a sane software solution for requirements / software specs?   To be clear, I am building this large software component with quite a few features and these features are being set forth in this bugtracking software. ",
        "Best answer": "I am rather surprised that nobody so far has recommended the use of a wiki for tracking requirements. I've found it to be an almost perfect system, because:  It allows people to collaborate on the requirements and makes this aspect highly-visible; It enables you to easily keep the requirements up to date as the project progresses; You can go in and see the history at any time, in case of a \"that's not what we agreed\" dispute; Most modern wikis have decent formatting capabilities, so it looks almost as good as a Word doc; You can hyperlink directly from your requirements into actual documentation; You never have to worry about people working off of different/obsolete copies; Requirements can start to be treated as an iterative process, just like design/implementation; If the requirements start to get really large/complicated, it's easy to split them up across pages/topics. Most wikis accept HTML, so if you really need advanced formatting, you can probably use a tool like Windows Live Writer.  Given the choice, I almost always choose the wiki method these days, it's really quite painless compared to the old-fashioned Word documents or trying to cram it all into a bug tracker. "
    },
    {
        "ID": "9445",
        "Question": "I started learning WF 3.0 back a few years ago and never quite got the hang of it.  Has anyone learned WF 4.0 (now that it has been rewritten)?   Is it a viable platform worth considering?  What alternatives exist? ",
        "Best answer": "At my last job we had a huge plan to use WF4.  We were planning to build a single end-to-end platform to replace the 5+ disparate pieces that existed.   I will limit my comments to two areas:  It has not been around long enough.  With WF4 being such a drastic change from WF3, the product has not been in the wild long enough to establish a community that can sustain and help itself.  You will encounter issues that will be difficult to resolve, and you will find little help online (and dig through countless unhelpful pages relating to WF3.) Subjectively, it's just a new fangled hammer... You have to decide if the amount of time you will have to invest just to become comfortable with it is worth spending.  Instead of trying to shoe-horn the application into WF4, I would rather have spent that time building a home-grown workflow system that worked exactly how the application needed.  I admit that the State Machine concept is very appealing to me, but I'm also a proponent of building things from the ground up rather than using (and becoming dependent upon) existing frameworks.   "
    },
    {
        "ID": "9447",
        "Question": "Functions are not only used to minimize duplication of code - they are also used to split up a long function into smaller ones to increase readability, as well as making the code self-commenting. Yet this gain is not directly inversely proportional to the number of LOCs per function or method; otherwise we would have tonnes of functions, all of which only contains a single line or two of code. This lead me to wonder: Does there exist an optimal number of LOCs per function? If so, what is it, and does it deviate between languages? ",
        "Best answer": "Instead of number of lines, the criteria I would use is that each function should do only one thing and does it well. "
    },
    {
        "ID": "9481",
        "Question": "Why does Software Engineering not have union representation like other professional occupations, such as teaching? Are there any unions for software developers that exist and are successful? ",
        "Best answer": "Interesting question. We produce a product - code - but we're not like typical unionized laborers. We're also not professionals like doctors and lawyers and accountants. (Can you imagine some hospital administrator demanding that a surgeon work overtime on Saturday - with no extra pay - to push a few more patients through?) Really, we're highly skilled craftspeople, very similar to medieval stonemasons. The folks who built the great cathedrals of Europe varied tremendously in their abilities and qualifications, and job-hopped quite a bit - and still managed to have a Europe-wide guild. And woe betide the nobleman or bishop who screwed over a master mason... they could just kiss their project good-bye. I often think we, too, should have a guild - especially when I read so many questions on Stack* relating to software developers being underpaid, having no life due demands to work huge amounts of unpaid overtime, and having to put up with crappy working conditions. "
    },
    {
        "ID": "9498",
        "Question": "I'm looking at evaluating ORMs.  I've used SubSonic, Linq-to-SQL and Entity Framework. I've got a team of developers ranging from juniors to seniors.  What are the criterias for evaluating an ORM for.NET? ",
        "Best answer": "It's a loaded question. There are lots of very good ORMs approaching the subject with different philosophies. None are perfect through and all tend to become complex as soon as you stray from their golden path (and sometimes even when you stick to it). What you should ask yourself when selecting an ORM:  What does it need to do for you? If you already have a set of requirements for your application, then you should select the ORM that better matches these rather than an hypothetical 'best'.   Is your data shared or just local? A lot of the hairiness in ORM is caused by how they handle concurrency and changes to the data in the database when multiple users are holding a versions of the same data. If your datastore is for a single-user, then most ORMs will do a good job. However, ask yourself some hard questions in a multi-user scenario: how is locking handled? What happens when I delete an object? How does it affects other related objects? Is the ORM working close to the metal of the backend or is it caching a lot of data (improving performance at the expense of increasing the risk of staleness). Is the ORM well adapted for your type of application? A particular ORM may be hard to work with (lots of performance overhead, hard to code) if it's a used in a service or sitting inside a web app. It may on the contrary be great for desktop apps. Do you have to give up database-specific enhancements? ORMs tend to use the lowest-common denominator set of SQL to ensure they work with lots of different database backend. All ORMs will compromise on available features (unless they specifically target a single backend) but some will allow you to implement additional behaviours to exploit specific enhancements available in your chosen backend. A typical db-specific enhancement is Full-Text search capabilities for instance; make sure your ORM provides you with a way to access these features if you need them. How does the ORM manages changes in the data model? Some can update the DB automatically within a certain measure, other don't do anything and you'll have to do the dirty work yourself; other provide a framework for handling change that lets you control database updates.   Do your mind to couple your application to the ORM's objects or do you prefer to handle POCOs and user an adapter for persistence? The former is usually simple to handle but create dependencies on your ORM-specific data objects everywhere, the latter is more flexible, at the cost of a bit more code. Will you ever need to transfer your objects remotely? Not all ORMs are equal when it comes to fetching objects from a remote server, look closely at what is possible or impossible to do. Some are efficient, others not. Is there someone you can turn to for help? Is there good commercial support? How big and active is the community around the project? What are the issues existing users are having with the product? Do they get quick solutions?  A few ORMs that I looked at:  XPO From developer Express: is small and simple, code-centric. They use it for their application framework eXpressApp. NHibernate Is free, but the learning curve is rather steep. Lots of goodies but it's hard to find what is really relevant sometimes in all the fragmented documentation. LLBLGen Pro very mature project, not the simplest but a lot of thought has been put into it. Entity Framework GEtting there. The last releases are pretty good and MS is listening, although it's still a bit young compared to other more mature ORMs. DataObject.Net Looks promising but is also a bit new to risk an important project on it IMHO. Quite active though.  There are many others of course.   You can have a look at the controversial site ORM Battle that lists some performance benchmarks, although you have to be aware that raw speed is not necessarily the most important factor for your project and that the producers of the website is DataObject.Net. "
    },
    {
        "ID": "9521",
        "Question": "Imagine you were elected coroner of IEEE or somesuch governing body and you had to pronounce a programming language as dead.  What signs would you look for?  Are there any zombie languages out there that don't know they're already dead? ",
        "Best answer": "In linguistics, a dead language is defined as one which has no native or fluent speakers. I think this definition translates quite well in the programming sphere. A dead programming language is one that no programmer uses as their main language, or no system supports natively. "
    },
    {
        "ID": "9554",
        "Question": "Reading SQLite source code is IMO mission impossible. Yet it is a usable piece of quite complex software (it's a full-blown embedded database after all) that can be downloaded, compiled and used from others code and it is constantly updated. How do people manage to write and maintain such extremely complex and hard to read code? ",
        "Best answer": "In the specific case of SQLite, the main tool they've chosen to use in development and maintenance is automated testing. They pride themselves on 100% coverage (branch coverage, not statement coverage) in their test suite. According to them, it's one of the best-tested software products in the world. So they know right away when something they've added or changed causes a regression, and can develop pretty fearlessly as a result of that. http://sqlite.org/testing.html Pretty staggering numbers--they have around 640 times as many lines of testing code as of production code. EDIT: This question has been raised from the dead, it seems! And a little more than a year later, that same page reports they have 1177 times as many lines of testing code as production! "
    },
    {
        "ID": "9572",
        "Question": "What features are vital for good bug tracking software and why are they important? What in particular is necessary for you to consider these features done right? ",
        "Best answer": "Simplicity. If it's too complicated or too long to enter or sort bugs, nobody will want to use it. "
    },
    {
        "ID": "9576",
        "Question": "One thing that I've heard a lot over the years is that those working in the IT world generally don't make life time careers out of it, but tend to \"burn out\" and start a new career doing something else unrelated (e.g. going from software development to being an accountant). Have you found this to be generally true in your experience and if so, what is the general impression on how long people work as developers before starting a new career? ",
        "Best answer": "I've been in software development all my working life from junior developer, through senior developer to team lead/manager and now back developing (though hoping to get back into management sooner rather than later). My working life is now nearly 40 years and in that time I've changed domains and technologies as the companies I've worked for have changed. I've then used that new experience to find new positions when I've had to, which has in turn led to other new domains and technologies. All that time I've known developers as old or older than me. I think \"burn out\" happens if you try to do too much - working 12+ hour days and/or weekends for extended periods and happens in any industry not just computing. I know that if I had to do that I'd be looking for something less stressful to do. If you find a working style that fits your temperament then there's no reason why you can't continue working until you retire at 65 (or when ever). "
    },
    {
        "ID": "9598",
        "Question": "Do you think that only the programming pratice will help you to improve your logical programming skill or do you train your brain with puzzle games, trying imagine how universe works, playing instruments and so on? Devoting more time with programming, will do you get logical programming skills more fast? ",
        "Best answer": "I think full-time programming practices my logical skills quite enough, and they need rest  after work. Doing something else such as practicing motoric skills by playing musical instruments is good to the brain. "
    },
    {
        "ID": "9605",
        "Question": "There's a quotation by Alan J. Perlis that says:  There are two ways to write error-free programs; only the third one works.  I recently heard this quote from my friend, and was unable to understand the deeper meaning behind it. What is Perlis talking about here? ",
        "Best answer": "There is no third way.  There is no way to write error-free programs  "
    },
    {
        "ID": "9657",
        "Question": "I have very severe Dyslexia along with Dysnomia and Dysgraphia. I have known about it since I was a child. My reading and writing skills are pretty crippled, but I have learned to deal with it. However, with today's IDEs, I find it very easy to stay focused and in the zone when I code. But when I write text (like this post) I find it much harder to stay focused. In general, do dyslexics find it easier to read and write code compared to general reading and writing? What types of tricks and tools do dyslexics use to help them master programming better than normal reading and writing? ",
        "Best answer": "I have a severely dyslexic friend who reads quite well when she's reading through a sheet of yellow plastic. Or when she highlights the hell out of each page. For some reason, coloring the text somehow helps her brain grok the glyphs it's seeing. So maybe syntax-highlighting and code coloring helps? "
    },
    {
        "ID": "9714",
        "Question": "Is cloud computing mature enough to alleviate some of the pains of maintaining the IT infrastructure on site? If so, what are some of the drawbacks of adopting it? Is security and privacy a big concern? ",
        "Best answer": "It's not just privacy and security. Those are big enough concerns, while we wait for Craig Gentry to hurry up and make homomorphic encryption practical. There's also the matter of making your business dependent on something outside your control.  Example, and this doesn't even need cloud computing: I inherited a client who runs its business off a website. It's great... until the ISP has a routing loop, or the local exchange breaks, or the datacentre hosting the website breaks something local to it. (These have all happened in the last six months.) If your business is entirely dependent on things on the far side of the network, you run the risk of your business coming to a standstill every time something in the network goes wrong. It's not only a single point of failure; it's like a string of single points of failure. Yes, cloud computing's supposed to protect exactly against some of the problems I describe above. But if a backhoe goes through your exchange's cable, you're potentially in big trouble. "
    },
    {
        "ID": "9730",
        "Question": "I've heard a lot of talk about using functional languages such as Haskell as of late. What are some of the big differences, pros and cons of functional programming vs. object-oriented programming? ",
        "Best answer": "I would say that it is more Functional Programming vs Imperative Programming. The biggest difference is that Imperative programming is about Control flow while Functional programming is about Data flow. Another way to say it is that functional programming only uses expressions while in imperative programming both expressions and statements are used. For example, in imperative programming variables and loops are common when handling state, while in functional programming the state is handled via parameter passing, which avoids side-effects and assignments. Imperative pseudo-code for a function for calculate the sum of a list (the sum is kept in a variable): int sumList(List<int> list) {     int sum = 0;     for(int n = 0; n < list.size(); n++) {         sum = sum + list.get(n);     }      return sum; }  Functional pseudo-code for the same function (the sum is passed as a parameter): fun sumList([], sum) = sum  |  sumList(v::lst, sum) = sumList(lst, v+sum)  I recommend the presentation Taming Effects with Functional Programming by Simon Peyton-Jones for a good introduction to functional concepts. "
    },
    {
        "ID": "9795",
        "Question": "I thought of this today after a co-worker looked through the contract they had signed several years ago and was quite alarmed. What should one look out for before signing a contract, as most employers will get you to sign one. Please post ideas separately so they can be voted individually.  ",
        "Best answer": "Intellectual Property Clauses Such clauses may state that the employer owns all intellectual property rights for any creative work produced during employment. If this is something that is important to you then make sure all vagueness around the definition of intellectual property and creative work is clarified and/or removed. As programmers we write a lot of code and it is important to clarify before you start employment who owns the intellectual rights for a new algorithm or any patentable piece of code that you make come up with during your employment. "
    },
    {
        "ID": "9849",
        "Question": "I'd like a feature added to Eclipse, as a small plug-in, but:  It's a bit niche, so not high demand. So if I post it as a feature request it's unlikely to be followed-up. Still, I'm sure someone else would find it handy. I'm a programmer, but I don't know Java, and I don't think it's currently worth my time learning Java just to code this.  What might be a good way to find a programmer who could code such an Eclipse plug-in, and pay them to do the job? My example is specifically about Java and Eclipse, but what might be an answer to this question in general terms? ",
        "Best answer": "Take a look here: Hacker News Thread To summarize it briefly:   Hacker News Freelance Google Spreadsheet Craigslist FreelanceSwitch Odesk eLance Authentic Jobs  "
    },
    {
        "ID": "9872",
        "Question": "Every programmer has a primary language that he works with most of the time and is therefore very familiar with. But there are also languages that you kinda know, in the sense that you used to know them really well in the past, but hasn't used in a while, or that you use them infrequently and therefore are not as immersed in them as you are in your primary language. You can definitely bring yourself to be productive with these languages, but you might need to re-familiarize yourself a little, look up a few syntax rules, and such. My question is- will you write these languages in your CV as languages you \"know\"? ",
        "Best answer": "Of course, but only those that are relevant to the job you are applying for. they don't have to be limited to the ones asked for in the job requirements, but you shouldn't include everything. It shows that you are more than a \"one trick pony\" and have skills beyond those required for your current (and prospective job). It helps highlight the experience you have and shows that you can adapt to new technologies etc. as the need arises. You should indicate how long ago it was you last used that language, how long you had been using it and what you were doing with it. In much the same way as you indicate how long you've been using your current skill set and what you've been doing with that. "
    },
    {
        "ID": "9873",
        "Question": "Some companies, like Blizzard, make software that continues to work well in future versions of Windows and with newer versions of their other software dependencies. Other companies (mostly ones that are not hardcore software companies) sometimes write software that breaks with a release of a new OS or other software dependency. What do the hardcore software companies know that the others don't? What are the major causes for forward compatibility problems?  ",
        "Best answer": "Did Blizzard write software that works well with future versions of Windows (Starcraft still plays on 7 for example)...  or did Microsoft write \"future versions\" of software that is backwards looking? Something like WoW isn't exactly \"forward looking\" since it's still in active development. Other software, like Starcraft/Warcraft/Diablo was written for the time and happens to still work because MS goes out of its way to enable old software on new systems. Blizzard also has the ability, and reason, to update its old games. Very popular titles that drive its current software. Some software uses hackish/non-standard parts that don't work well. Unique setups, reliant on \"old\" bugs to operate, drivers that aren't ported to new systems, etc. "
    },
    {
        "ID": "9885",
        "Question": "How much technical (for future developers) documentation is enough? Is there a ratio between hours coding and hours documenting that's appropriate? Papadimoulis argues that you should  produce the least amount of documentation needed to facilitate the most understanding,  Is that a good guideline, or are there specific things I should be creating? ",
        "Best answer": "How about some hallway usability testing? Show the code and documentation to a developer unfamiliar with the project. When you can do that without an overwhelming urge to explain something while watching them review the code, you have enough. "
    },
    {
        "ID": "9936",
        "Question": "I've been thinking of releasing a few projects, and most authors in the community leave their email address right in the code, often as is or using commonly used schemes like \"meATNOSPAMgmail.com\", which sort of gets my paranoia flag up... When releasing source code publicly, does it often lead to your email address being parsed by spam bots, and if so how do you obfuscate it ? ",
        "Best answer": "I avoid the problem by leaving a website URL instead of an email address in the code, and then someone can contact me through my site without me needing to leave an email address sitting around where any random spambot could harvest it. "
    },
    {
        "ID": "9948",
        "Question": "A question with absolutely no relevance to myself: How would a programmer with a stellar SO reputation (like 30k+) market his or herself to poential employers/investors who have never heard of SO? In other words, how can one describe SO in a few sentences that will make a high reputation sound impressive at an interview? ",
        "Best answer": "In an interview, you wait for the right question. Something about \"how do you keep current with technology today\", or possibly \"would you describe yourself as active in the developer community\" (a question you are much more likely to be asked if it says \"active in the developer community\" on your resume) and then you say something about SO and how it's \"a question and answer site where other users award you reputation points for good answers and good questions\" and then give your score in really round numbers and then translate into English like \"which puts me in the top 1% of users on the site. I'm happy to be recognized as helpful in the technologies I use a lot.\" On your resume, you could simply include your SO handle in the contact section, along with your Twitter handle and link to your blog, assuming they're technically relevant. People who recognize it will go check your rep. People who've never heard of it won't learn anything from a simple sentence on the resume. "
    },
    {
        "ID": "9960",
        "Question": "When you have no clue about the question, how do you answer/act when you do not know the answer at all? Telling the truth is pretty obvious. But how could you try to transform this weakness into a strength? ",
        "Best answer": "\"I don't know how to do that, but if I ran into that problem in a project, here's how I'd go about figuring out how to make it work...\" "
    },
    {
        "ID": "9965",
        "Question": "I don't see any use for case sensitivity in a programming language, apart from obfuscating code. Why implement this in a programming language?  Update: It looks like someone you know made a statement on this. ",
        "Best answer": "Why would anyone WANT case insensitivity? In what scenario is it useful to be able to refer to a single variable as VARIABLE in one place, Variable in another, and variable in a third? Case insensitivity is exasperating. I’d much rather get a compiler error when I accidentally type VAriable instead of Variable rather than let case-typos like that slip into my code. In conclusion, many programming languages have case sensitivity not just for historical/inertial reasons but because case insensitivity is a Bad Idea. "
    },
    {
        "ID": "10002",
        "Question": "I often come across a class which has a single reference to another library and was wondering if its better to include the library or to reference the object by it's full name. Is one better then the other? Is one more of a standard then another? For example using System.Windows.Messagebox: Option A: using System.Windows;  public class MyClass {     SomeMethod()     {         MessageBox.Show(\"Something\");     } }  Option B: public class MyClass {     SomeMethod()     {         System.Windows.MessageBox.Show(\"Something\");     } }  If B, how many references do you need before you decide to add the entire library? ",
        "Best answer": "I would prefer 'Option A'. It clearly states, that this file indeed uses that library. And anybody new to the code won't be searching for signs of external references through the whole file. Don't know about such standards though, just reasoning. "
    },
    {
        "ID": "10021",
        "Question": "I have often seen people fighting over that their favorite language is more \"powerful\" than others. When it comes to describing a programming language, I can understand what an object oriented language is or what a dynamic language is, but I still can't figure out what exactly a \"powerful\" language is. What are your thoughts? ",
        "Best answer": "We can't define what a \"powerful\" language is without first defining the word \"powerful.\" The literal definition of power would be \"potency\", and I think we can all agree that the vast majority of compilers - and even many interpreters that aren't Turing-complete - do an equally good job of getting the processor to execute their instructions.  So as far as the literal definition, the answer to the question would be \"almost any language at all.\" Practically, we really ought to stop there; defining a \"powerful language\" is a bit like defining a \"good person\" or a \"quality product.\"  There is absolutely no objective definition of these words that you could get everybody, or even a majority of experts, to agree upon, and most definitions simply end up begging the question.  Depending on who you talk to, power could be any of the following:  A rich general-purpose framework or library for performing a wide variety of common tasks A sophisticated domain-specific syntax that \"does one thing and does it well\" Direct access to machine functions, i.e. the ability to write low-level code Abstracting away machine-level concepts, i.e. the ability to write high-level code A very rich type system allowing for advanced strategies like reflection, DI, and static analysis A very loose type system that allows programmers to just get it done (type coercion, etc.) The ability to treat everything as an object, which offers conceptual verification The ability to treat everything as a function, which offers mathematical verification Automatic memory and resource management (GC, RAII) leading to fewer bugs Manual memory and resource management, potentially leading to optimized performance A minimum amount of syntactic noise, leading to improved readability A more English-like syntax, which offers a shallower learning curve The ability to write very concise code (i.e. ternary operator, null-coalescing, null-extension) The inability to write potentially confusing code (i.e. no ternary operators, etc.)  Does everybody see what's going on here?  Virtually every bullet-point feature can be interpreted as a sign of \"power\", and so can its exact opposite! Somebody, somewhere, obviously thought that variable variables were an awesome idea that would make the language very powerful.  I won't judge; I'm not a PHP guy. I propose that instead of all this holy-war nonsense, we all use this Really Simple Definition: The most powerful language is the one that allows you to ship the highest-quality product at the lowest cost in the shortest amount of time. Vague?  You betcha.  That's why anyone who wants to call him/herself a professional has to understand both the programming concepts and the project domain.  That's the only way you're going to be able to decide what's \"powerful\" for you. Otherwise, you might just be bringing a really big knife to a gun fight. "
    },
    {
        "ID": "10032",
        "Question": "Do there exist studies done on the effectiveness of statically vs dynamically typed languages?  In particular:  Measurements of programmer productivity Defect Rate  Also including the effects of whether or not unit testing is employed. I've seen lots of discussion of the merits of either side but I'm wondering whether anyone has done a study on it. ",
        "Best answer": "Some suggested reading:  Developers Shift to Dynamic Languages (PDF) On the Revival of Dynamic Languages (PDF) Static typing where possible, dynamic typing when needed: The end of the cold war between programming languages (PDF) The Security of Static Typing with Dynamic Linking (PDF) Combining Static and Dynamic Reasoning for Bug Detection (PDF) Dynamic Typing in a Statically Typed Language (PDF) Turning Dynamic Typing into Static Typing by Program Specialization (PDF) Hybrid Type Checking (PDF)  Not exactly on static typing, but related:  Securing web application code by static analysis and runtime protection (PDF)  Some interesting articles or essays on the subject or on static analysis of programs in general:  Pluggable Type Systems (PDF) Strong Typing vs Strong Testing Linux Kernel Developer Responses to Static Analysis Bug Reports (PDF) Is Weak Typing Strong Enough? Correlation Exploitation in Error Ranking Improving Software Quality w/ Static Analysis  And for the ones who would be wondering what this is all about:  Introduction to Static and Dynamic Typing  However, I doubt any of these with give you a direct answer, as they don't do exactly the study you're looking for. They will be interesting reads though. Personally, I firmly consider that static typing over dynamic typing facilitates bug detection. I spend way too much type looking for typos and minor mistakes like these into JavaScript or even Ruby code. And when it comes to the view that Dynamic Typing gives you a boost in productivity, I think that mostly comes down to tooling. If statically typed languages have the right tools to allow for background recompilation and provide an REPL interface, then you get the benefits of both worlds. Scala provides this for instance, which makes it very easy to learn and prototype away in the interactive console, but gives you the benefits of static typing (and of a stronger type system than a lot of other languages, ML-languages aside). Similarly, I don't think I have a loss of productivity by using Java or C++ (because of the static typing), as long as I use an IDE that helps me along. When I revert to coding only with simple configurations (editor + compiler/interpreter), then it feels more cumbersome and dynamic languages seem easier to use. But you still hunt for bugs. I guess people would say that the tooling issue is a reversible argument, as if tooling were better for dynamic languages, then most bugs and typos would be pointed out at coding-time, but that reflects the flaw in the system in my opinion. Still, I usually prototype in JRuby and will code in Java later most of the things I do. WARNING: Some of these links are unreliable, and some go through portals of various computing societies using fee-based accesses for members. Sorry about that, I tried to find multiple links for each of these but it's not as good as I'd like it to be. "
    },
    {
        "ID": "10103",
        "Question": "What are your strategies to improve the flow experience when doing work? ",
        "Best answer": "It's a bit like creativity. You can't force it, you can only prepare conditions favorable for it to appear. Some of the most important requirements:   Distraction-free environment (you'll need hours of uninterrupted concentration). Clear goal. Genuine, intrinsic interest in that goal. Clear mind; not too much \"should do this, should do that\" issues nagging in the  background.  So, before starting to actually work to problem, be sure to define the goal clearly, complete or at least write down other to do -items so that they don't disturb your mind, and go somewhere where you can concentrate. If you're interested in what you're doing, you now have a good chance of getting into flow. "
    },
    {
        "ID": "10136",
        "Question": "Hai Friends,               Tell me or share with me , i am developer with one year experience, now i want to modify my old fresher resume to an experience resume, so i want to know what are the primary things must be in an experienced resume which is attracted by the firm. ",
        "Best answer": "One bit of advice I like (not mine, but I'm too lazy to look for sources) is to explain how each of your projects helped your customer/user/employer to improve or \"generate value\". Example, you might have an item that reads:  Implemented Winforms application with .NET 2.0/Infragistics 2.3/SQL Server 2005/Crystal Reports 10.2/C#/Visual Studio 2005.  Try to rephrase to something like:  Designed, built and delivered a desktop reporting application that helped the Accounting Dept. to reduce invoice payment time from a manual task that used to take 3 days to an automated process based on realtime updated reports that now takes 10 minutes end to end. Implemented as a Windows application built on the Microsoft .NET platform.  Explain what your employer achieved with your work.  The technology tag soup can be left at the end of your resumé just to please recruiters that like to do text search for buzzwords. "
    },
    {
        "ID": "10210",
        "Question": "I primarily work in C++.  My question is, for a C++ developer, what languages are beneficial to learn, from a job-profile point of view.  For example, I see a lot of work combining C++ and Php these days.  Conversely, I don't see a lot of work combining Ruby and C++ (unfortunately). Of course, learning another language is beneficial regardless of whether it compliments a language. ",
        "Best answer": "I think PHP compliments C++ nicely. In both cases, you're scraping the bottom of the barrel of languages, but PHP has so many annoyances that it makes C++ look like a diamond in comparison. As far as languages that complement C++, I'm not sure. "
    },
    {
        "ID": "10270",
        "Question": "Many companies, especially larger ones have mandatory professional development plans for their employees. Employees and managers set out a personalized professional development plan and follow up on the progress every so often.  As a developer, do you find such PDPs useful, do you follow through with your commitments?  As a manager, do you think such PDPs bring value to the company and improve the overall quality of the employees? It seems that good developers will continue to educate themselves and strive to be better regardless of companies' procedures while bad ones won't. Are there benefits of having PDPs or is it just something managers feel they need to do? ",
        "Best answer": "I love the idea that my employer is at least pretending to be concerned about my career development.  Frankly though, this is a good thing.  The more money you invest in your employee's self-betterment and overall career progression, the more you invest in your company. Smarter, healthy, more qualified employees = better work = more money = everyone happy.   "
    },
    {
        "ID": "10334",
        "Question": "Granted that Silverlight may make eye-popping websites of great beauty, is there any justification for using it to make practical web applications that have serious business purposes?  I'd like to use it (to learn it) for a new assignment I have, which is to build a web-based application that keeps track of the data interfaces used in our organization, but I'm not sure how to justify it, even to myself.   Any thoughts on this?  If I can't justify it then I will have to build the app using the same old tired straight ASP.NET approach I've used (it seems) a hundred times already. ",
        "Best answer": "Silverlight has not been designed to build websites. It has been designed to build web applications that run in a browser. "
    },
    {
        "ID": "10340",
        "Question": "Is it still worth it to protect our software against piracy? Are there reasonably effective ways to prevent or at least make piracy difficult? ",
        "Best answer": "Not really.  Any copy protection has to be 100% perfect, (which we all know is impossible,) or else all it will take is for one person anywhere in the world to come up with a working crack and post it on the Web. If you want people to pay money for your product, copy protection is not the answer.  It never has worked and never will.  The answer lies in Economics 101: people will pay money for your product if they perceive its value to them as being greater than the price you are asking for it.  Otherwise, they won't.  Period. "
    },
    {
        "ID": "10373",
        "Question": "I'm working on a cloud platform, and was curious what people prefer in terms of paying for web services. There's the option of very simple pricing (tiers and such), which is easy to project costs on, easy to compare, but (possibly significantly) more expensive for the average user, because you're always either underpaying or overpaying. The other option is a complicated pricing structure with many different factors, a lot harder to project costs on, but you're literally paying only for what you use, so it comes out to a lower price for the average user.  Which do you prefer? ",
        "Best answer": "I like simple and cheap. $20 a month is simple for what you get. Which is a 256MB VPS with a blank install of your choice of Linux. (This is Slicehost). Simpler in my mind is always better, and I'm willing to pay for the simplicity. That same server from Rackspace costs $0.0015 an hour, 24 x 30 = ~$11 a month. What do I get for the extra $9 from Slicehost? No bandwidth charges. I get simple hosting for what I pay for. "
    },
    {
        "ID": "10379",
        "Question": "I would like to know what other developers do when a situation arises to implement a couple of features in language unknown to you. You are familiar with Javascript, and you do not know jquery, how do you go by implementing some features in jquery assuming you have not much time. ",
        "Best answer": "Google it.* *Note: In order for this to work, you need to try to learn abstract concepts as a developer in order to know what to Google, as opposed to just learning how to do something in a particular way.  Learn how things work, not just that they work. "
    },
    {
        "ID": "10416",
        "Question": "My contract explicitly states that I can't render services to another company.  I would like to know if working on open source projects or freelancing count as such because I would like to work on some other projects without violating my contract.  Freelancing might count as rendering services, but would open source work count too? ",
        "Best answer": "Yes it is counting. Before contacting a lawyer, I highly suggest you to discuss with your employer to negociate that right. Most employers will see the value of you working in open source projects. However, freelancing in the same industry may be a problem. In any case, don't try to bypass the laws, talk with your employer first, then your lawyer if you can't get a written approval. "
    },
    {
        "ID": "10424",
        "Question": "I know the technical pitfalls of writing an ORM are pretty well-known nowadays, but what are some non-technical factors (e.g., Scope) that make it difficult to writing a good ORM?  I have already read the technical reasons for why ORMs are so difficult to write from this Stackoverflow post: https://stackoverflow.com/questions/404083/is-orm-still-the-vietnam-of-computer-science ",
        "Best answer": "I think the Object relational impedance mismatch is the big problem. OO is not relational. For example, where does relational theory describe inheritance. Also smaller (but big issues) like if the programming language doesn't support nullable types. How do you map that to a nullable column in the database? "
    },
    {
        "ID": "10462",
        "Question": "I've been programming for a while now, and I've covered a lot of languages. And this trend that I noticed is that all HDL languages have so painful IDEs! In general, any development environment having some Hardware related development has very crappy UI. I'm talking about uVision, ModelSim, VHDL Simili, Xilinx etc, compared with Netbeans, Eclipse, Visual Studio etc. Why do hardware-guys hate their developers? NOTE: There are exceptions (LABView is awsome!). Can you think of any more? ",
        "Best answer": "It's not that hardware guys hate their developers. It's that they're hardware guys, so they're not really very good at designing or writing software. Most of them simply don't think enough like \"normal\" people to produce software that most people will find attractive or easy to use. The other part of it is that most of these tools assume that anybody using them uses them  constantly; the emphasis is primarily on making them easy for an expert to use, as opposed to easy for a beginner to learn. Of course, it's possible to combine the two, but it takes even more of the UI design skills that (as I just pointed out above) they mostly lack. Worse, along with lacking the skills, many think in terms like: \"only a [insert perjorative term here] would care about changing colors.\" "
    },
    {
        "ID": "10512",
        "Question": "I started, like many others, with console-based (as in terminal, not Playstation) programming. But sooner or later, one needs to touch upon GUI-based programming, whether you want to or not. This transition holds many changes in how you need to think about the frontend (and possibly also the backend). So, what are the major differences when moving from console-based programming to GUI-based programming? ",
        "Best answer": "The biggest difference is the design of the UI. A good GUI can make or break an application. Mac fans would draw attention to the beautifully designed GUI's of the average Mac OS X app and they've got a point, but this isn't a technology issue - it's a design/ethos/usability issue. As for technical issues, in no particular order:  The user can do anything they want in any order at any time, unlike console program in which you're either asking for input or telling them the output. You cannot assume that they'll follow the order you hope, unless you enforce the workflow Wizard-stylee. As already mentioned, events play a big part in this, and you can get multiple events happen while you're servicing the last one, so you can't really construct your state based on the 'current event'. Use closures or a similar mechanism to maintain context across different events. In a console app, your FSM is usually self-contained around the 'get input, process input, update output' loop. There isn't the same kind of structure in GUI programming - the 'main' is a re-entrant event-driven thing, often a ginormous switch() statement. You need to consider different screen sizes/resolutions and allow the GUI to resize from 800x600 up to the users' monitor maximum. You need to consider different input strategies - mouse, keyboard, touch, etc. Some technologies come for free (Mouse-wheel scrolling), others require some integration work (Ink). Accessibility - a GUI is much more suitable for less able users who have restricted vision, hearing, motor skills or cognitive skills. A 'ding' noise is nice and obvious compared to a cryptic error message on the console. Internationalization - i'm assuming your console app is US/ANSI only, but when you get into GUI, you can have language/resource packages that can target other languages and regions with no change to the coding, if you prepared for it from the start. For example, no hard-coded language strings in the code - everything as resource lookups. You have lots more options for implementation technology - web-based, various GUI kits, Flash/WPF, etc. Use of colour and animation. Console programs are generally monochromatic and don't animate much. Many modern GUI frameworks provide themed widgets and have move/size/show/hide animation effects, often for free. Graphics. Console apps sometimes use ASCII art for diagrams, but a GUI app gives you full graphical ability. Lovely art can make a big difference too.  "
    },
    {
        "ID": "10569",
        "Question": "It seems that some rules of writing code are in direct contradiction with the rules of human writing.  For example in code it's advisable to define each piece of information in only one place and in writing for humans it's normal to repeat important points (though usually phrased differently). What rules of writing good code contradict the rules of good writing? ",
        "Best answer": "Indentation rules (most coding standards impose) really contradict     rules of good writing,     the way people perceive information,     and the grammar rules. Making things (that group naturally, but not syntactically) inside  parenthesis also contradicts how texts are usually typed. If (you try to type text that way)     you'll face misunderstanding even (if programmers read you) otherwise     your text will be easy to read,     and your writing will be productive !  "
    },
    {
        "ID": "10574",
        "Question": "I'm a .NET developer, but I've never been to a .NET user group meeting or anything like that. I might have the opportunity soon to start attending one, but I'm wondering. What happens at user groups? I'm not looking necessarily for info specifically about .NET ones, but they are preferred. What do you get out of it excluding the opportunity for networking? ",
        "Best answer": "I attend my local .NET User Group every month.  Here are some things that go on besides networking:  Giveaways: Win books, licenses of ReSharper or CodeRush, etc. Presentations: Speakers from all over the country (and world) give talks on development best practices, new technologies, etc. Eat food:  Pizza...of course! (Sponsored by a local business)  "
    },
    {
        "ID": "10580",
        "Question": "Assume that you're a sole developer leaving a job. What kind of information/material, outside of the code itself, should you create and leave behind for your replacement? An obvious answer is \"whatever you would want at a new job\" for sure, but it's been a while since I started a new job, and I forget what the most important things that I needed were back then. I'm thinking:  accounts/passwords location of equipment, backups, software CDs  What else? ",
        "Best answer": " Accounts & Passwords Server information Good code Documentation  Database diagrams & explanations are amazing List of oddities in the code  Procedures Explanation of manual processes, or occasional, non-obvious, work List of programs they used or found helpful Contact information ;)  "
    },
    {
        "ID": "10581",
        "Question": "Should items like Foreign Keys, Constraints, Default Values, and so on be handled by the database management system (in this case, MS SQL 2005) or the application? I have heard opinions from both sides and I'm honestly not sure which way to go. There is a chance we will be spanning multiple servers/databases and I don't think Foreign Keys can be used across linked servers. In addition to that, there are some circular references in the database design which prevents me from using ON UPDATE CASCADE on everything. The database is MS SQL 2005 (possibly 2008) and all interactions with it should go through the application. ",
        "Best answer": "If there's any chance that the DB will be modified outside your application, you want the constraints in the database.  If the DB is and always will be nothing more than the back end of the application, you can leave them out, although I'd document them just in case and probably just keep them in if the performance hit wasn't too bad.  (Peoplesoft software works this way - the constraints are in the software, and (I'm not making this up) it runs everything as SYS on Oracle.) You want things like that to be monitored by the application, so it can react intelligently and not at best parrot back a database error message to the user. And, yes, this is double coverage, but without it you're probably going to get preventable data corruption or a bad user interface. "
    },
    {
        "ID": "10605",
        "Question": "As programmers we can solve very complex problems, but then, when we have to design a user interface we tend to fail on making them easy to use.  In small companies they can’t afford having designers and UX experts, programmers have to do almost everything in the software. But these interfaces are rarely intuitive (the classic example). What is the problem? How can developers improve their skills in designing good user experiences? ",
        "Best answer": "I have encountered this problem many times in my career - the trick is to first be aware that it is a problem, and acknowledge it. Once you've done that, it's easier to stop making overly complex interfaces. The user interface is also a part of software engineering, but perhaps for many software engineers not as much interesting. However, there are many interesting challenges related to this, and they can probably be as interesting as more technical challenges, in my experience. Usability, user experience design (UX), human-computer interaction (HCI) - it's not magical, and it is a part of the software development process. My tip is to:  acknowledge your limitations ask and listen to people who claim to know about these things when unsure, google it and look for authorative answers  By following these simple principles over the years, I have actually accumulated useful information on how to build user interfaces, how people interact with software, and how they think when they're using it. I am by no means an expert, but I probably know a little bit more than your average programmer. Tl;dr: KISS "
    },
    {
        "ID": "10614",
        "Question": "I see this problem quite often.  I like a certain value proposition of an open-source project. I try out the basic tutorials. Great. It works! But if I move on to more complex problems, I spend hours of doing research, debugging, frustrations, etc. What are your strategies to keep motivation in open-source going?  What is the reward of open-source after success of basic tutorials?  What \"success\" of open-source did you experience? ",
        "Best answer": "I am assuming you are looking at small open source libraries like those found on github.  In my case I am often using one to solve a specific problem.  If it doesn't solve it cleanly, then I dig in, learn how the code works and make changes as necessary.  If my change is for something useful or a bug fix I attempt to contact the open source owner or fork my own branch.   Other times I am just adapting something close to my own needs, in those cases I just keep my changes and move on.  I add watches or check back in regularly to see what has been updated. As in the notes though, this is the life of software development.  It's an ever changing environment. "
    },
    {
        "ID": "10651",
        "Question": "I'm beginning to think that I might be best suited for a job in R&D.  The reason being that I like jobs that allow me to be creative and I generally am more interested in solving puzzles than building things.  I really dislike jobs that just boil down to translating requirements into code. Here's the kicker though:  I don't have a bachelor's much less a masters or phd.  Is it possible for me to land a job like this? ",
        "Best answer": "I work at R&D. For one thing, most of it is about the \"D\" part, that is, building things. You can't escape it. The primary goal of any commercial R&D department is to develop things that work (and that can be sold to customers in order to pay your salary). As Edison put it: 1 % inspiration and 99 % perspiration. Then there's of course the rare \"R\" part, tinkering, which is quite enjoyable. The proportion between R and D certainly depends on the field and firm where you're working, but it's typical that making things that work (D) is the high priority, and trying to come up with something completely new (R) gets the remaining time, which may not be much. I can only talk about myself, but if you're a decent programmer, you have a good chance of getting a R&D job via ordinary job interview. Look for firms that inspire you, and let them know about yourself. Don't ignore small, innovative firms; if you get to work at such, you can actually affect what they and you are doing (not so with giant corporations). If you meant academic research, then it's definitely less work to first get the degree and then a job at the university than trying to do any meaningful research alone, without a degree, unless you're a genius. "
    },
    {
        "ID": "10672",
        "Question": "Ever since my very first programming class in high school, I've been hearing that string operations are slower — i.e. more costly — than the mythical \"average operation.\" Why makes them so slow? (This question left intentionally broad.) ",
        "Best answer": "\"The average operation\" takes place on primitives.  But even in languages where strings are treated as primitives, they're still arrays under the hood, and doing anything involving the whole string takes O(N) time, where N is the length of the string. For example, adding two numbers generally takes 2-4 ASM instructions.  Concatenating (\"adding\") two strings requires a new memory allocation and either one or two string copies, involving the entire string. Certain language factors can make it worse.  In C, for example, a string is simply a pointer to a null-terminated array of characters.  This means that you don't know how long it is, so there's no way to optimize a string-copying loop with fast move operations; you need to copy one character at a time so you can test each byte for the null terminator. "
    },
    {
        "ID": "10675",
        "Question": "What do you think? What is the ideal programming language learning sequence which will cover most of the heavily used languages and paradigms today as well as help to grasp common programming basics, ideas and practices? You can even suggest learning sequence for paradigms rather than languages. N.B. : This is port of the question I asked in stackoverflow and was closed for being subjective and argumentative. ",
        "Best answer": "Python, Lisp, C, Haskell Assuming the question was about an ideal learning sequence for newcomers to programming (since old hands at programming will have had their own (likely accidental) learning sequence), I'd suggest reading Norvig's essay on how to learn programming in 10 years, then:  Python: Start with a dynamic, high-level, OO & functional language.  Because it's really important for beginners to feel productive ASAP, and not be turned off by alien syntax, lack of libraries, lack of multi-platform, lack of learning resources, and lack of community.  Python is highly readable, has tons of good libraries (esp. scientific libraries - a modern scientist/engineer must know how to program), is easily run from most OSes, has tons of tutorials and entire free books, and is generally user-friendly — all while still being powerful enough to grow with you as you become an advanced programmer working on large problems.  It's also important to reinforce important+useful conventions for a beginner: code readability counts a LOT, and you should be writing code for yourself and others to readily understand. Lisp: In particular, at least skim The Structure and Interpretation of Computer Programs or watch the SICP videos, and have one's eyes opened very wide by seeing the foundations and expressive power of programming - one can do so much with so little.  Learn how Lisp can express not just the functional paradigm, but OO, logical/declarative, and more - like the creation of new domain-specific languages.  Read Norvig's PAIP to witness what a master can do with the language.  Then check out Clojure, a modern Lisp dialect that could be one of the Next Big Things. C: Only because it's the lingua-franca of computing. :)  Possibly optional these days if one is primarily a programmer in a particular non-software domain.  I find it ugly but worth knowing about to get an appreciation for the underlying hardware.  Go with K&R, of course. Haskell: Pure functional power.  Where current Com.Sci. theory and practical expressive power meet.  See Real World Haskell.  After the above tour, one would be very adept at tackling problems and expressing solutions in code, and be totally comfortable with the paradigms here:  "
    },
    {
        "ID": "10723",
        "Question": "I am using a java library licensed under AGPL3.  However, I would like to license my own work as BSD, such that my code could then be integrated in proprietary projects if one strips the library. Will I be allowed to do that? So far, I couldn't find a clear answer for my problem on the net or in the license text(not a lawyer). Dual-licensing seems cumbersome and ugly. EDIT: What happens if I use other libraries which are licensed under BSD or anything else? ",
        "Best answer": "Here's a quote from the Licensing section of the GNU website:  It has always been the FSF's position   that dynamically linking applications   to libraries creates a single work   derived from both the library code and   the application code. The GPL requires   that all derivative works be licensed   under the GPL, an effect which can be   described as “hereditary.” So, if an   application links to a library   licensed under the GPL, the   application too must be licensed under   the GPL.  (the specific page is discussing Java and LGPL, but the general point applies here)  If you want to license your original application as BSD, you must first create it without using that library at all. Then, you can create a derivative work by combining your BSD application and the AGPL3 library, with the whole thing licensed as AGPL3. This would effectively be a new, different application, and is not dual-licensing. Since you want your BSD application to be used commercially, you would need to release it separately from the extended AGPL3 version of the application - which means two distinct links on your download page, with a clear explanation on the difference. (NOTE: Anywhere I use \"BSD\" in this answer, it's important to note this refers to the \"modified BSD\" not the original BSD.)  What happens if I use other libraries which are licensed under BSD or anything else?  That's fine. So long as the license allows for the code to be re-released under a compatible license and the AGPLv3 is considered compatible (which the BSD is - see a full list here), then you've got no problem. If you plan on using lots of libraries/etc then it's a good idea to maintain a distinct credits file, where you can list each specific component you use, its website address, licensing requirements, and so on.  (usual disclaimer: I'm not a lawyer, follow at own risk, etc) "
    },
    {
        "ID": "10735",
        "Question": "Bugs creeping into code can be minimized, but not entirely eliminated as it is written - programmers are, although many would disagree, only humans. When we do detect an error in our code, what can we do to weed it out? How should we approach it to make most effective use of our valuable time and enable us to spend less time trying to find it and more time coding? Also, what should we avoid when debugging? Note here that we're not talking about preventing bugs; we're talking about what to do when bugs do appear. This is a wide field, I know, and may be highly dependent on language, platform and tools. If so, keep to encompassing answers such as mindsets and general methods. ",
        "Best answer": "The mindset and attitude to debugging is perhaps the most important part, because it determines how effectively you'll fix the error, and what you'll learn from it — if anything.   Classics on software development like The Pragmatic Programmer and Code Complete basically argue for the same approach: every error is an opportunity to learn, almost always about yourself (because only beginners blame the compiler/computer first).   So treat it as a mystery which will be interesting to crack.  And cracking that mystery should be done systematically, by expressing our assumptions (to ourselves, or to others) and then testing our assumptions, one-by-one if need be — using every tool at our disposal, especially debuggers and automated test frameworks.  Then after the mystery is solved, you can do even better by looking through all your code for similar errors you may have made; and write an automated test to ensure the error will not happen unknowingly again. One last note - I prefer to call errors \"errors\" and not \"bugs\" - Dijkstra chided his colleagues for using the latter term because it's dishonest, supporting the idea that pernicious and fickle bug-fairies planted bugs in our programs while we weren't looking, instead of being there because of our own (sloppy) thinking: http://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html  We could, for instance, begin with cleaning up our language by no longer calling a bug a bug but by calling it an error. It is much more honest because it squarely puts the blame where it belongs, viz. with the programmer who made the error. The animistic metaphor of the bug that maliciously sneaked in while the programmer was not looking is intellectually dishonest as it disguises that the error is the programmer's own creation. The nice thing of this simple change of vocabulary is that it has such a profound effect: while, before, a program with only one bug used to be \"almost correct\", afterwards a program with an error is just \"wrong\" (because in error).  "
    },
    {
        "ID": "10791",
        "Question": "Right now, I use visual svn on my server, and have ankhsvn/tortoise on my personal machine. It works fine enough, and I don't have to change, but if I can see some benefits of using a DVCS, then I might give it a go. However, if there's no point or difference using it without other people, then I won't bother.  So again, I ask, are there any benefits to using a DVCS when you're the only developer? ",
        "Best answer": "Yes! I think the biggest benefit is the better branching + merging support offered by many DVCSes. Branching and merging is kind of a pain in the ass in SVN; it's annoying enough that it's not worth the time to create small, short-lived branches for quick feature additions, bug fixes, or experimentation, but merging is also annoying enough that it's a pain to create long-lived branches as well. On the other hand, branching and merging is a breeze in Git, so much so that I create a (local) branch for nearly every bug fix or feature I work on. I think the tools offered by Git for visualizing repos, grepping logs, etc., are a lot better than in SVN, too (although that's more a Git thing than specific to a DVCS). A DVCS also doesn't require a central server; when using SVN as a developer, you have to create a local repo to push into, which isn't a requirement with Git, since every repo contains the full history. As a corollary, archiving a repo is just a matter of zipping up your project -- there's no \"central database\" to back up. I started using Git nearly four years ago, after using SVN for a while, and I haven't looked back. "
    },
    {
        "ID": "10832",
        "Question": "I'm trying to advocate unit testing in my workgroup, but an objection I often get is that it should be used only for externally exported API (which is only a minimal and non critical part of our system), and not on internal and private code (which now has only functional testing).  While I think that unit test can and should be applied to all the code, how can I persuade my coworkers? ",
        "Best answer": "The reasons for using unit tests on internal/private code are precisely the same as for externally supported APIs:  They prevent bugs from recurring (unit tests form part of your regression test suite). They document (in an executable format!) that the code works. They provide an executable definition of what \"the code works\" means. They provide an automated means of demonstrating that the code does indeed match specs (as defined by the point above). They show how the unit/class/module/function/method fails in the presence of unexpected input. They provide examples on how to use the unit, which is great documentation for new team members.  "
    },
    {
        "ID": "10835",
        "Question": "In my battle to introduce unit testing in my workgroup, I find a lot of people that have little to none knowledge of the concept. Can you suggest:  best articles or tutorials for quickly introducing people on the topic best comprehensive book(s) for learning unit testing in depth academic works and studies that prove the effectiveness of unit testing  ",
        "Best answer": "I found Test Driven Development By Kent Beck to be a  good introduction. "
    },
    {
        "ID": "10865",
        "Question": "Functional programming is a declarative paradigm. One of the strenghts with FP is that side-effects are avoided. It's said that for some problems FP isn't a good fit. For what common problems isn't functional programming a good fit?  ",
        "Best answer": "Real-time embedded programming is all about the side effects.  Interacting with digital and analog io, timers, serial and parallel ports, everything interesting is done by calling functions with side efffects.   "
    },
    {
        "ID": "10899",
        "Question": "What are questions to test a programmers knowledge of SQL? What is the answer to the question? And what would a lack of an correct answer mean in terms of time likely to understand the concept(s) related to the question? GOOGLED: sql challenge ",
        "Best answer": "It depends on how difficult you want it to be. Also, I'm a little wary of giving you the answer because most SQL problems have multiple acceptable ways to do things and there are also ways of solving SQL problems in sloppy ways that will cause other issues. The person \"grading\" the answer definitely needs to be able to solve it on their own.  That said, here are a few I came up with off the top of my head. Extremely Easy Level: Given an employees table with the columns EmpID, FirstName, Lastname, HireDate, and TerminationDate: Write a query to return all employees still working for the company with last names starting with \"Smith\" sorted by last name then first name. Easy Level Given the Employee table above, plus a new table \"AnnualReviews\" with the columns EmpID, and ReviewDate: Write a query to return all employees who have never had a review sorted by HireDate. Medium Level Given the employee table above, write a query to calculate the difference (in days) between the most and least tenured employee still working for the company? Hard Level Given the employee table above, write a query to calculate the longest period (in days) that the company has gone without a hiring or firing anyone. Harder Level Again using the same tables, write a query that returns each employee and for each row/employee include the greatest number of employees that worked for the company at any time during their tenure and the first date that maximum was reached. Extra points for not using cursors. "
    },
    {
        "ID": "10916",
        "Question": "I'm curious if speakers (bloggers, google employees like Matt Cutts) get paid for speaking at conferences. If they do, how much? If they don't, or they get paid so little that it's as though they don't, why do it? What are the attractions beyond money that lead a developer into public speaking? ",
        "Best answer": "It varies. I have had speaking gigs where I spent my own money to get there, to eat, and so on, and didn't get paid to speak. I would say my norm for a conference where attendees pay is free admission, economy travel and hotel covered (but not meals, cab fare and the like), and possibly a small honorarium (eg $500 per talk.) For a conference where attendees do not pay there is no honorarium and often no travel covered either. Some large conferences offer a few selected speakers a pre-conference which can pay a full week's billable rate or more for a single day (because, of course, you put a lot of time into preparing that day of training). I have heard of conferences that actually ask speakers to pay to get in the door in addition to covering their own travel expenses. For me, the benefit of being there for the week and the benefit of being able to say \"I've spoken at ten Tech Ed US, four Tech Ed Europe, and three Tech Ed Africa\" (so far - fifth Tech Ed Europe is next month) is what has to balance against my time spent preparing the talks and being there. The honorarium rarely factors into it. (How much speaking do I do? http://www.gregcons.com/KateBlog/CategoryView.aspx?category=Speaking) "
    },
    {
        "ID": "10989",
        "Question": "In a debate with Andrew Tanenbaum over microkernel vs. monolithic operating system architecture, Linus Torvalds said,  Portability is for people who cannot write new programs.  What did he mean by that? ",
        "Best answer": "As Linus writes in the debate, it's with tongue in cheek (i.e. not to be taken too seriously). Then, he goes on to explain that while portability is good thing, it's also a trade-off; unportable code can be much simpler. That is, instead of making the code perfectly portable, just make it simple and portable enough (\"adhere to a portable API\"), and then if it needs to be ported, rewrite it as needed. Making code perfectly portable can also seen as a form of premature optimization - often more harm than good. Of course that's not possible if you can't write new programs and have to stick with the original one :) "
    },
    {
        "ID": "11002",
        "Question": "What free UML authoring tools do you use and why is it better than others? ",
        "Best answer": "ArgoUML - I use it for its simplicity.  "
    },
    {
        "ID": "11007",
        "Question": "Most of my fellow students that I've talked to claim that aiming for good grades is useless as the companies don't care about them when hiring programmers. To them, it's enough to have simply attended courses which may be important, and that's that. Is this true? Are university grades useless when leaving campus, or do employers ask to see them for an interview? ",
        "Best answer": "Incorrect. Grades are important especially if you have no or little professional programming experience. It's the bulk of your resume until you have professional experience. "
    },
    {
        "ID": "11120",
        "Question": "The company I am leaving has asked that I make myself available to answer questions and/or debug programs occasionally should the need arise. I'm not opposed to this. After searching google for some kind of standard contract for this sort of thing, I didn't see any.  Is there a standard contract for this sort of thing that you use? Are there any other steps I should take to ensure this kind of arrangement works smoothly? ",
        "Best answer": "You are in a good position here as your old company has asked you for help. Take the following steps;  Get the agreement of your new employer Decide on how much time you are prepared to spend on this and when you want to spend that time. Pick a sensible hourly rate - ask a recruitment agency in your area what the average is and charge that. Agree on how much notice your old employer must give you for a request for work. Agree when and how your old employer can contact you. You don't want them ringing you at your new employer so e-mail conversations is probably best.  Be prepared to negotiate - while you are in a good position if you ask for too high a rate (for example) they might suddenly find that they have the skills in-house after all. "
    },
    {
        "ID": "11222",
        "Question": "I want to be a successful enterprise Java developer. With what algorithms and data structures should I be well versed? What books would you recommend to me? To be a successful Java developer, do I need to know all the advanced algorithms such as those given in CLRS? Can you suggest a list of the topics that I should learn in the descending order of their priority? Must I know:  Searching? Sorting?  Graphs/Trees?  etc? ",
        "Best answer": "Cormen's Introduction to Algorithms is a good book for some basic theoretical foundations. That said, most algorithms and data structures have been coded, tested and optimized by others already, and there's no reason to re-implement them once more. To be a successful Java developer you need to know The Collections Framework inside out. "
    },
    {
        "ID": "11275",
        "Question": "If you know of an open source project whose organizer   has gotten really busy that he doesn't maintain it that much  and he's the type of guy that doesn't accept contributions until a few months later (busy I guess) if at all and the project is starting to flounder   would you fork this project, give it a new name, and continue working hard on it and build a new community around it that's more encouraging to code contributions? Basically a better managed project since the concern now is bad management. ",
        "Best answer": "One should start by seeing if the current maintainer would be willing to let you take over. Since the issue is that he his busy, I would expect him to be willing to hand it off. If that works it is better for everyone involved. If that fails, I'd suggest collecting contributions into a distribution that you maintain without making it a separate project. That way you work within the existing community. The hope is that eventually the maintainer will see the light and accept your offer to take over. If that fails, fork it. At that point you have gained some visibility and I think you'll attract some of the original community. But at the same time, you have made it clear its not your desire to split the community.  "
    },
    {
        "ID": "11312",
        "Question": "I've been tasked with creating a fun and relaxing environment, one thing I know that I want is ergonomic mice and keyboards, others have suggested exercise balls and bands. What is it that every programmer needs while working? What might not be necessary but would be nice to have anyway? Note: this question was asked previously, but has been recommended to be posted here. See this link for the previous responses: https://stackoverflow.com/questions/3911911/stuff-every-programmer-needs-while-working-closed ",
        "Best answer": "The Internet As Joel Spolsky said, \"The internet should be as freely available as air.\" "
    },
    {
        "ID": "11334",
        "Question": "Does your company have a written policy about contributing to open-source projects?  We've been contributing \"don't ask don't tell\" style, but it's time to write something down. I'd appreciate both full written policy text and bits and pieces. Update: we've made some progress since I asked this question and now have such a policy - read this. ",
        "Best answer": "We do not have a direct policy regarding this.  Long story short: work created at the university is owned by university unless declared prior to start of coding.   I have discussed with my boss from time to time about this and we have not gone far with it as it becomes a political nightmare as the people who need to approve this do not quite understand open source software aside from the \"it is usually free\" idea. /me eagerly awaits others responses!  Update: This topic surfaced at work recently, we met with our legal team and all appears well.  My understanding (from my employer) is this is a case by case basis but they are willing to work with employee's interested.   "
    },
    {
        "ID": "11342",
        "Question": "I want to contribute to an open source project, but I don't know much about unit testing. I want to learn how to test and then practice my skills on an open source.  Will this also be acknowledged as a contribution. I want to first get my name out there and then conc. on development. ",
        "Best answer": "Other than writing tests, here are other works that are relatively easy to get started with while you're familiarizing yourself with the project:  triaging bugs testing patches subitted on the bug tracker  preliminary patch review writing docs for smaller projects, packaging for package managers that they had not supported yet  "
    },
    {
        "ID": "11400",
        "Question": "When is it useful to use a captcha? When is it an unnecessary hindrance? Is a captcha just a quick fix for the lazy/unexperienced programmer, or are they really the best way to prevent spam and bots?  ",
        "Best answer": "ReCAPTCHA seems to be pretty secure, and will probably outlast any other OCR based CAPTCHA solution. CAPTCHAs are useful when you aren't sure if it's a bot or a human - ie, after the second or third login attempt, or if you allow anonymous commenting. Once a user has authenticated, dump the CAPTCHA. An alternative that hasn't come up yet is the \"SAPTCHA\". "
    },
    {
        "ID": "11512",
        "Question": "I am curious if anyone knows of any methodologies that are significantly different (not a recombination) and I would especially appreciate anyone who brought forward any experience with alternatives. ",
        "Best answer": "Wikipedia lists these as methodologies/development processes:  Agile - based on iterative and incremental development, where requirements and solutions evolve through collaboration between self-organizing, cross-functional teams.   Cleanroom - the focus of the Cleanroom process is on defect prevention, rather than defect removal.   Iterative -  a cyclic software development process developed in response to the weaknesses of the waterfall model. It starts with an initial planning and ends with deployment with the cyclic interactions in between.  RAD -  uses minimal planning in favor of rapid prototyping. The \"planning\" of software developed using RAD is interleaved with writing the software itself.   RUP - The Rational Unified Process (RUP) is an adaptable iterative software development process framework, intended to be tailored by selecting the elements of the process that are appropriate.   Spiral - combining elements of both design and prototyping-in-stages, in an effort to combine advantages of top-down and bottom-up concepts. This model of development combines the features of the prototyping model and the waterfall model.  Waterfall - sequential through the phases of Conception, Initiation, Analysis, Design, Construction, Testing and Maintenance.  Lean -  a translation of Lean manufacturing and Lean IT principles and practices to the software development domain; everything not adding value to the customer is considered to be waste.   V-Model -  Instead of moving down in a linear way, the process steps are bent upwards after the coding phase, to form the typical V shape. The V-Model demonstrates the relationships between each phase of the development life cycle and its associated phase of testing.  TDD -  relies on the repetition of a very short development cycle: first the developer writes a failing automated test case that defines a desired improvement or new function, then produces code to pass that test and finally refactors the new code to acceptable standards.  "
    },
    {
        "ID": "11546",
        "Question": "Take the example of the recent ASP.NET (and Java Server Faces) vulnerability disclosure at a Hacker conference in Brazil.  It's my understanding that the poet tool was demonstrated before Microsoft was even aware of the issue. Are there laws to protect legitimate customers from people who encite the hacker community to start hacking all the ASP.NET servers they can?  Who knows how many legitimate businesses were compromised between when the tool was demoed and the patch was applied to the server. ",
        "Best answer": "None that I know of, and there probably shouldn't be, either.  Who knows how many legitimate businesses were compromised between when the tool was demoed and the patch was applied to the server.  Who knows how many legitimate business were compromised before the tool was demoed?  You seem to be making the assumption that because of this demo at a conference in Brazil that that was the first any bad guys heard of it. "
    },
    {
        "ID": "11624",
        "Question": "I need to hire a web designer. How do I test his creativity and skills before I hire him. Ideally I'd like to give him a fairly open-ended task i.e. I give him a problem statement and ask him to design a PSD file.  Any suggestions on the problem statement I might give him? Also, is there a better way to hire /test a web designer? ",
        "Best answer": "You can always ask for:  Previous work Examples of design Mockups / thoughts / ideas before you commit to a design  "
    },
    {
        "ID": "11667",
        "Question": "I'm looking for a 'term' that describes holistically the set of various software development tools/services that most developers use on a day to day basis. While each tool by itself has a specific purpose and can be described, is there a plural term when they are grouped together and running as a holistic service. If a person said a server was a production test box, I'd expect it to be running the application that company are writing. If a person said box A is the office ware server, I'd know to expect email/communication related services. ",
        "Best answer": "I've been using the term \"build environment\" or \"build server\" which covers our:   SVN Server Issue tracking CCNet  Test case management  Development wiki etc  It was the best term we could come up with at the time.  "
    },
    {
        "ID": "11669",
        "Question": "I saw some projects require BISON to generate C++ code. What is the advantage of having BISON/Yacc etc. generate C/C++ code instead of writing it yourself? ",
        "Best answer": "The most important thing about BISON and YACC is that they generate a state machine which would parse exactly your grammar. This is for speed reason, since complexity of generated code is not O(n) with n = number of rules as it would be in most first-attempt implementations.  The state machine approach causes the number of states spawned to be very high  and thus it would be very error prone to write that code. Manteinance of that code would also be an hell (where hell = I would quit the job if someone suggested me to do that).  Bison and Yacc make you concentrate on the grammar instead of its implementation. "
    },
    {
        "ID": "11726",
        "Question": "A common task in the working world is dealing with existing, but buggy code. What are some tips to improve your skills as a debugger? ",
        "Best answer": "Don't Assume Anything It's often tempting to just say \"oh, I know what this bit of code is doing, it's fine\". Don't do that. Test every assumption and step through everything carefully. "
    },
    {
        "ID": "11769",
        "Question": "All, Given a problem statement, when I start of with the analysis and design, I tend to think too much about the various functionalities associated with the task (sometimes not even mentioned in the task). My point is, I get diverted from the main task at hand thinking about various scenarios that I should consider while coding and this tends to demotivate me because I am often very confused in the end. What approach should I take in this case since thinking a lot on all possible scenarios comes naturally and I only try hard not to. ",
        "Best answer": "What you appear to be describing is a problem in defining your scope. My advise is as follows:  By all means think about all the different possible tasks, you may well come up with some useful additional features. Write headings for all the different ideas you've had, but nothing more than headings. Review and prioritise the headings. Some at the bottom can probably be lost, concentrate on the functionality requested. Expand the top headings into their sub-tasks until you have tasks that are only a few hours or days long each. Review these with the person requesting the change (possibly before expanding the detail if the person is non-technical). Begin work on the top task and continue downwards, producing prototypes and demonstrations (if not releases) at regular intervals for review with the requester. Add to and review your task list between task completions; more tasks will always result from user reviews, and some may become obsolete.  "
    },
    {
        "ID": "11775",
        "Question": "It seems like after I've written a significant amount of code I get this anxious feeling like I haven't done it in the best possible way, and I end up continuously refactoring and spending way too much time on the project, or never getting it done sometimes. Does this ever happen to anyone else, and how do you deal with this? ",
        "Best answer": "I enjoy these moments when they happen to me. The reason: if I did not look back at my work and think there is something I could have done better than I would not be advancing as a developer.  Therefore, when these moments of enlightenment occur, embrace them and take note of what you learned.  Consider your timetable for current project and if possible refactor the code, if not possible take the lesson and use it in future implementations for projects.   Either way, learning from your own mistakes is a great thing!   "
    },
    {
        "ID": "11889",
        "Question": "I code primarily in .net at work, but was wondering if home MSDN subscriptions were available/worthwhile as a tool to stay in touch with the latest technology for Microsoft development? ",
        "Best answer": "Probably not, unless you're actively working on long-term projects and/or making money on the side. For everything else, there's the express versions of most of the software. "
    },
    {
        "ID": "11915",
        "Question": "I am using MAMP (just and example) to develop web applications without having to constantly upload and download via FTP. I´m sure this is the correct way to test a project but I´m no sure if I´m doing it right.  After I´m satisfied with the project I have to change in every page the MySQL databases and most of the links etc. Is there a workaround this? How should this be used? Thanks in advance! ",
        "Best answer": "Whether it is local or not, you need a development environment outside of production.  I'm no PHP expert, but every page should not have its own connection code to the database. Create a class that can be reused. Your application should be able to determine if you are on local host or example.com and connect to the appropriate database. And for hyperlinks, look into Relative URL instead of Absolute URL. "
    },
    {
        "ID": "11951",
        "Question": "We have an enormous number of nested SQL stored procedures, and a number of different ways of debugging them (varying from developer to developer). So far, our methods include:  An optional @debug parameter, that causes the procedure to printmessages as it runs (passing the variable down to called procedures). Checking @@servername against a table of test server names, and printing as above Writing everything the procedure does to a log table (in production and test)  Which of these is preferable (and why), or is there a better method we've overlooked? ",
        "Best answer": "You should also consider SQL Server Profiler.  In SQL Server Management Studio, select Tools -> SQL Server Profiler.  When the profiler window opens, select File -> New Trace...  (I don't know specifics about other RDBMSs, but they've got to have tools with similar functionality.) As the long-term solution, you should of course move the business logic out of stored procedures. "
    },
    {
        "ID": "11975",
        "Question": "I never used an automated test mechanism in any of my projects and I feel I'm missing a lot. I want to improve myself, so I have to start tackling some issues I've been neglecting like this and trying Git instead of being stuck on SVN. What's a good way to learn TDD? I'll probably be using Eclipse to program in Java. I've heard of JUnit, but I don't know if there's anything else I should consider. ",
        "Best answer": "You could start by working on coding katas.  Pick an algorithm (e.g. decimal-to-Roman numeral conversion, scoring a bowling game, Conway's game of life, etc.) and try to use TDD to work on the solution. Your solution structure will likely to be very simple (much simpler than your real-world production code): one class for the test fixture and one class containing the algorithm under test.  And the class under test having no dependencies is another plus.  You could use the simplicity of this setup to quickly get the hang of the red-green-refactor loop. Which tool you use for your TDD katas doesn't really matter as long as you stick to the principles.  However, JUnit plugin for Eclipse is very easy to use, so it's an excellent choice. "
    },
    {
        "ID": "11998",
        "Question": "If you are developing your own architecture or heavily use anothers vendors API, how do you promote awareness to the programming staff? For example, say you use Infragistics controls for you C# winforms apps - the controls have boatloads of methods, some quirks, and a best way to use them for your application. How do you promote the knowledge transfer thru your programming staff? Wiki's? Email? Blogs?. ",
        "Best answer": "Documentation seems like the most reasonable place to inform your development team regarding implementation and usage of external libraries/API.s  Emails get lost, are not read. Wikis are for interlinked content. Blogs are usually for individuals. Documentation is for communicating evidence of something.   Although I suppose you could have a meeting initially when a project starts to discuss a chosen API/library to make sure 1. everyone is on the same page and 2. no one has a concern with using it that may have been missed.  "
    },
    {
        "ID": "12005",
        "Question": "Seriously, what's the big deal about having a few cycles in one's import graph?  I see it as a lesser evil than having super fine grained imports that make keeping track of where to look for what code an absolute nightmare. ",
        "Best answer": "Depending on your environment, cyclic dependencies across modules can make static linking impossible.  Module A cannot be linked until Module B is linked; but Module B depends on Module A and cannot be linked until Module A is linked.  This is why some environments - such as .NET and Delphi - don't allow this sort of thing at all. Other environments/compilers/linkers may be more forgiving, but that does not make it better design.  While it seems to be generally accepted that cyclic dependencies within data structures are sometimes necessary (entity models, doubly-linked lists, certain kinds of trees... you get the picture), it is almost always a serious design flaw to have a cyclic module dependency. Why?  Try to picture the initialization phase of an application, while modules are still being loaded.  Most modules count on the fact that all of their dependencies have been fully-loaded before running any initialization code.  Cyclic dependencies invalidate this assumption, because it's physically impossible (at least in this universe) for both modules to be loaded before each other.  Similarly, in the teardown phase, finalization code that makes perfectly reasonable assumptions may crash because a dependent module has already been unloaded by the runtime. The worst part is that on most operating systems, when you have cyclic module dependencies, the load order is deterministic but impossible to predict.  So an assumption about initialization order that turns out to be correct today may suddenly break tomorrow due a totally innocent change in a totally different part of the application.  Tracking down these bugs can be an excruciatingly painful process.  Cyclic imports within a single package/module/assembly are really another story entirely.  An \"import\" can mean so many different things depending on the context.  In many cases they are \"Considered Harmful\" simply because early compilers couldn't handle them, so you had to do your own cycle detection with a bunch of #ifdef directives (or similar) and obviously, after a certain number of these, you start to lose your mind.  But in modern-day object-oriented programming it's often considered good practice to have one file per class, which means that a cyclic import is merely a sign of a cyclic class dependency, which as I mentioned above, isn't necessarily such a bad thing unless it's unnecessary. If it's unnecessary, then it's a Bad Thing simply on account of the fact that any unnecessary complexity is a Bad Thing, and cyclic dependencies add complexity. "
    },
    {
        "ID": "12070",
        "Question": "If applying for a new job, would you put your Stack Overflow profile link on your resume? This would show the employer you're an active member of the development community and also provide an insight into your knowledge + how well you convey your ideas. However it would feel a bit gimmicky to me? ",
        "Best answer": "It Depends When I was looking for a job a month ago, I didn't put a link to SO on my resume, but I did mention that I participate on SO and added a link to my blog that contains the SO \"flair\" on the About page. At that point I had about 3000 rep. I wouldn't try to leverage rep, but I would leverage intelligent participation. If you act like a moron on SO and draw attention to it, that's obviously a bad move. But if you say \"hey, I participate on this dedicated Q&A site\" and you have been giving good answers and asking smart questions, it can work in your favour as it shows passion for your work and fellow developers. "
    },
    {
        "ID": "12073",
        "Question": "This is my first question here, so i'll make it short and to the point. When Writing unit tests, should you write them before or after you have made the underlying framework for them? This came up in a discussion between my CEO and President, and I wanted your opinion on this. Personally, I have always wrote the code first, then wrote the unit tests, but what about you? ",
        "Best answer": "During. You're not going to get an entire framework done in one iteration. Your framework and unit tests should co-evolve simultaneously. On a more micro scale, tests first, code second. "
    },
    {
        "ID": "12171",
        "Question": "Is there a size where you can copy under fair use for code you don't have a license for the purpose? For example, what if I copy a snippet that is (normally) 3 lines of code? Is that fair use? If it is fair use, what length is required before I need a license? ",
        "Best answer": "I am not a lawyer. However, you are absolutely free to use the following 3 lines of code in anything that you write: for (i = 0; i < 5; i++) {     printf(\"I am not a lawyer!\\n\"); }  .... That's purely functional code (not functional in the sense you might think, but functional by what it does). It doesn't actually accomplish anything; it simply exercises the constructs of the language. It supports stuff that does actual work, you type it often enough that you have a macro to produce it. But what you're talking about isn't 'fair use', it's is this even copyrightable or licensable in the first place? The only correct answer here is ask a lawyer. Ten lines of code from what? A highly specialized sorting algorithm? Some kind of firmware? One of the millions of configuration file parsers that have been floating around since the dawn of usenet? Code generated by another program that generates database classes based on your DB schema? But, prove to me that you wrote that for loop, or switch statement, and didn't just copy mine. At the point where assertions stop sounding absurd, you're probably at the point where you should talk to a lawyer, or at least your engineering manager / PM / etc.  I participate in several free/open source projects, some of them require a copyright assignment for anything not 'trivial'. All of them that have this requirement define 'trivial' to be ten lines of code added or modified. But that's just an arbitrary litmus test to establish some kind of standard; one line of mind-bending byte comparison is much more likely to be subject to copyright than a 10 line switch.  My snippet gallery consists of hundreds of functions, all of them have the original author's information in doxygen style comments, as well as license info (if any, most are just public domain). Unless clearly trivial (as in my humorous example), I would not re-use code unless I know that I have permission to do so. I also, always, follow the rules of whatever license applies. And to help my future self (or future maintainers), I like leaving a link to where I found stuff in a comment, even if that's the name of a tar ball I turn in on a USB stick if I leave the company.  "
    },
    {
        "ID": "12182",
        "Question": "I've seen a lot of other framework/library developers throw the phrase 'we write opinionated software' around, but in practical terms, what does that really mean? Does it mean that the author of the 'Opinionated Framework X' says that because they write code a certain way, you should be writing the same type of code that they write? Isn't that a bit pretentious? ",
        "Best answer": "The framework imposes a certain way of working on you. Put another way, there's clearly one right way of using the framework which is nice and easy, and any other way of using the framework makes your life difficult. I'm no Rails expert, but I'm told that it's opinionated because it's awesome for simple CRUD stuff, but when you try deviate from the \"Rails way\" things get tough. (This isn't necessarily a bad thing; I don't mean it as criticism.) "
    },
    {
        "ID": "12186",
        "Question": "I've seen a lot of different ways to [or not to] comment out the code, specially patterns on desciptions of function details and file purposes. I'd like to know what is most used pattern to do that [I'm specially interested on documentation generators and their patterns] ",
        "Best answer": "Developers in Visual Studio tend to use the /// <summary /> because apart from being able to generate an Xml usable as input for document generators, you also get tooltips during development when using the commented method/class. "
    },
    {
        "ID": "12229",
        "Question": "Does your company have a written policy about personal (technical) blogging? Care to share? We encourage our developers to keep personal blogs and talk about technical problems they've encountered at work that are not core competency or core IP. We've been doing it \"don't ask don't tell\" style, but it's time to write something down. Update: thank you all for your answers, we've implemented a policy for this last week - the full text is here. ",
        "Best answer": "No such policy here I do keep a personal / technical blog (more technical then personal), however it's not related in any way to companies I work for.  Publishing internal protocols, worksheets, code, or other thing would be a serious do not do in my opinion. Other then that I can see for very few reasons where keeping a technical blog would not be allowed. One might be in a case where the contract stipulates any and all material you create is owned by the company. There was an interesting article I read about someone who worked at Microsoft where this was the case. Can't find it again for the life of me though. "
    },
    {
        "ID": "12279",
        "Question": "How necessary or important is it? I try to keep a running list of blogs or sites to follow, but a lot of the time I pull up someone's profile and notice there isn't anything there. Is it really important?  I understand are different levels of programming (from C/C++ system programmers to Rails and even Haskell and J) and not everyone works in a language easily worked with for web based applications. Not everything is web-centric, however with the advent of many popular and sometimes free services I don't think it's unreasonable to expect a majority of programmers to have a personal site. ",
        "Best answer": "To what end? Why would you want one? Some people use it as a way to sell themselves to employers. Others do it to help enrich the community and share information. Maybe both. I think you need to ask why you want a website before you determine how important it is. If you want to sell yourself to employers, then yes a site or blog can definitely be of help. If you want to share something with the community then yes, a blog is a great way to do that. But you should only contribute if you want to contribute. Don't feel compelled to have a blog just because all the other \"good developers\" have one. If you like to write and have something good to say, go for it. Otherwise don't bother. "
    },
    {
        "ID": "12292",
        "Question": "Let's say you have a static method that looks something like this: public static bool Foo() {      var bar = new Bar();     //do some stuff here }  This method as it stands can be a real headache to unit test. What is the best practice to refactor this so that it can be testable, without turning it into an instance method or changing the method signature? ",
        "Best answer": "It really depends on what Bar is.  If it's something simple then your Foo method is already testable, you just need to specify your expectations and invoke it statically, e.g.: Assert.IsTrue( FooContainer.Foo() );  But if Bar encapsulates, say, your database access layer, then you can't test Foo without a real database, which is why (thanks @ysolik), static methods are death to testability.  Or, in the words of Michael Feathers, \"don't hide a TUF in a TUC\" (TUF stands for a test-unfriendly feature, TUC stands for a test-unfriendly construct).  If Bar is indeed test-unfriendly, then sorry, it doesn't work well without making Foo an instance method.  You would need to redesign your code first: public class FooContainer {     public bool Foo() {         var bar = new Bar();         //...     } }  When Foo is no longer static, you can invoke it on an instance of FooContainer: var container = new FooContainer(); Assert.IsTrue( container.Foo() );  The next step is to extract an interface from Bar (let's call it IBar) and inject it into FooContainer: public class FooContainer {     private readonly IBar m_bar;     public FooContainer( IBar bar ) { m_bar = bar; }     public bool Foo() {         // don't create another Bar, use m_bar     } }  Now you can mock/stub IBar with your favourite isolation framework and test your FooContainer code in isolation from its dependencies. "
    },
    {
        "ID": "12343",
        "Question": "I was going through this article: http://en.wikipedia.org/wiki/Subculture Which got mee thinking is programming a subculture? After the a while I started thinking it really hard, and if you go really in depth this is a very complex and interesting question to ask. YOu can even ask yourself if (heavy) internet (social) users are an subculture and programmers a culture within. I think it might be an interesting discussion, hope you like it! NOTE: I linked the wiki article because it might be a good baseline, maybe you can base you answer  on Ken Gelder´s proposal to distinguish subcultures. But it should be based on a little bit more that intuition. Thanks in advance! Trufa ",
        "Best answer": "No, programmers are not, in themselves a subculture.  There isn't a sense of belonging shared among programmers as a group, or really any of the elements of a shared identity.  However -- and this is a big \"however\" -- a not-insignificant number of programmers are members of hacker culture. Hackers have a shared identity, shared rituals/holiday, shared humor, a distinct shared language, they differentiate themselves from other cultures, have their own folklore, and modes of dress, foods, habits, etc. Hacker culture is not the same as programmer culture.  Many programmers don't grok hacker culture at all, and being a hacker isn't really defined by the activity of programming itself.  It's more about the mind set. You may be wondering why I call hackerdom a \"culture\" rather than a \"subculture\".  Part of being a \"subculture\" is being somehow distinct from the \"dominant culture\".  However, hackers don't have a specific term for non-hackers, despite having developed an expansive distinct vocabulary.  This is because the hacker doesn't consider him/her-self not to be part of other cultures due to his/her membership in the hacker culture.  It is not only acceptable that each hacker also belong to one or more other cultures, and that one hacker's \"other\" culture(s) may conflict with a fellow hacker's \"other\" culture(s), but it is expected, and believed by some to be the reason hacker culture exists. One 20th century definition of \"culture\", however, is a much better match: \"the universal human capacity to classify and encode their experiences symbolically, and communicate symbolically encoded experiences socially\".  Hackers share a common way of classifying and encoding experiences, a shared symbology and language, and a shared way of communicating all of these things within the hacker social group. \"Programmers\" as a group doesn't seem to fit either mold to me. --Susan P.S. - I registered just to answer this question, but since I am new I can't link all of my references here.  Please see my comments for URLs (if the system lets me do that). Links:  Hacker humor and ritual observances (holidays): http://esr.ibiblio.org/?p=2520 Hacker lexicon (dictionary of hacker language): http://catb.org/jargon How hackers differentiate themselves from other cultures/subcultures: http://catb.org/jargon/html/crackers.html Hacker folklore: http://catb.org/jargon/html/appendixa.html Hacker dress, food, habits, etc.: http://catb.org/jargon/html/appendixb.html  "
    },
    {
        "ID": "12359",
        "Question": "I am editing some c++ and all of the built in types have been wrapped in a corresponding wrapper class. All of the operators have been overloaded to do bizarre things like operator[] coded to ignore the argument or to exactly what the underlying type would do.  Is this malevolent or is there some possibility this is a good idea in some other universe.     ",
        "Best answer": "IMO, your question is made of 2 questions:  Is a good idea to wrap built-in/3rd party types in own types?  Depends. Having own types around existing types is great when you change the underlying type because the existing logic can be preserved (ideally) with no changes. Is like having a 3 tire architecture and you can change the database without changing the business logic. However, creating all those types takes time and developers also need time to learn them...  Is a good idea to code own types that behave different from the original types when using the same syntax?  No. Is bad because it has side effects. Strange bugs may appear because one wasn't aware of the new behavior. Also makes debugging complicated and increases the time/cost of the development process. "
    },
    {
        "ID": "12423",
        "Question": "I have heard this time and again and I am trying to understand and validate the idea that FP and OO are orthogonal. First of all, what does it mean for 2 concepts to be orthogonal ? FP encourages immutability and purity as much as possible. and OO seems like something that is built for state and mutation(a slightly  organized version of imperative programming?). And I do realize that objects can be immutable. But OO seems to imply state/change to me. They seem like opposites. Does that meant they are orthogonal ?  A language like Scala makes it easy to do OO and FP both, does this affect the orthogonality of the 2 methods ? ",
        "Best answer": "The term \"orthogonal\" comes from maths, where it has a synonym: \"perpendicular\". In this context, you could understand it as \"the two things have nothing to do with each other.\" When people compare FP and OO they often confuse two separate axes. On the one hand you have functional programming versus imperative programming. Jonas gives a good comparison of the two. The one-sentence version says that \"data flow versus control flow\". The other axis is data abstraction. Languages like Haskell use abstract data types to, well, abstract data. Smalltalk uses objects, which fuse the data and operations on that data into a single unit. William Cook explains better than I could in his paper On Understanding Data Abstraction, Revisited. It's perfectly understandable that most people end up thinking that FP and OO are opposites: most OO languages are imperative, so if you compare, say, Haskell and Java, you have data flow + ADT versus control flow + object. But there are other possibilities! Matthias Felleisen explains how to happily marry FP and OO in his talk Functional Objects. "
    },
    {
        "ID": "12439",
        "Question": "It's been generally accepted in the OO community that one should \"favor composition over inheritance\". On the other hand, inheritance does provide both polymorphism and a straightforward, terse way of delegating everything to a base class unless explicitly overridden and is therefore extremely convenient and useful.  Delegation can often (though not always) be verbose and brittle. The most obvious and IMHO surest sign of inheritance abuse is violation of the Liskov Substitution Principle.  What are some other signs that inheritance is The Wrong Tool for the Job even if it seems convenient? ",
        "Best answer": "When inheriting just to get functionality, you're probably abusing inheritance. This leads to the creation of a God Object. Inheritance itself is not a problem as long as you see a real relation between the classes (like the classic examples, such as Dog extends Animal) and you're not putting methods on the parent class that doesn't make sense on some of it's children (for example, adding a Bark() method in the Animal class wouldn't make any sense in a Fish class that extends Animal). If a class needs functionality, use composition (perhaps injecting the functionality provider into the constructor?). If a class needs TO BE like other, then use inheritance. "
    },
    {
        "ID": "12444",
        "Question": "In maths, a variable means you can put any number there, and an equation will still be true: root(square(x)) = abs(x)  In programming languages, this is not so: a var can change. In Python: y = (x**2)**.5 x *= 2 assert y == abs(x)  will raise an exception, since x in the last line is not the same. Are there programming languages that use immutable variables? ",
        "Best answer": "Purely functional programming languages, such as Haskell, enforce immutable variables. I like to call them identifiers though, instead of variables. "
    },
    {
        "ID": "12450",
        "Question": "Say, for example, I wanted to pay somebody to create a programming language or scripting language for me. What type of document would they need, in order to fully understand what it is exactly that I want. I mean, are there standard documents that describe the new programming/scripting language in question? ",
        "Best answer": "What you need to write is called a language specification. It should contain a description of the language's grammar (preferably in Extended Backus-Naur-Form) and its semantics. For the latter part you could either write a description in your own words (but take care to be precise) or a formal semantics. "
    },
    {
        "ID": "12475",
        "Question": "Most programming languages are Turing complete, which means that any task that can be solved in one language can be solved in another one, or even on Turing machine. Then why aren't there automatic translators that can convert programs from any given language to any other language? I've seen couple attempts for two languages, but they always work only on a limited subset of a language and can hardly be used for converting real projects. Is it possible, at least in theory, to write 100% correct translator between all languages? What are the challenges in practice? Are there any existing translators that work? ",
        "Best answer": "The biggest problem is not the actual translation of program code, but the porting of the platform API.  Consider a PHP to Java translator. The only feasible way to do that without embedding part of the PHP binary is to reimplement all of PHP's modules and API's in Java. This involves implementing over 10.000 functions. Compared to that the job of actually translating the syntax is easy as pie. And even after all that work you wouldn't have Java code, you'd have some sort of monstrosity that happens to run on the Java platform, but that was structured like PHP on the inside. This is why the only such tools that come to mind are all about translating code to deploy it, not to maintain it afterwards. Google's GWT \"compiles\" Java to JavaScript. Facebook's hiphop compiles PHP into C. "
    },
    {
        "ID": "12524",
        "Question": "I'm looking forward to delving into the .NET framework but most mobile applications seem to be developed in Java. I'll most probably be targeting the less pricier phones like Nokia and motorolla not android or iphone OS. Can I do these stuffs in C#? ",
        "Best answer": "No, C# won't do. You need to learn Java, and not J2SE but the J2ME because most low-end mobile phones only support J2ME. However, you might want to consider Android again because it's really becoming popular even with low-end phone. Anyway, the language for J2ME and Android is both Java. "
    },
    {
        "ID": "12683",
        "Question": "Pseudocode helps us understand tasks in a language-agnostic manner. Is it best practice or suggested approach to have pseudocode creation as part of the development lifecycle? For instance:  Identify and split the coding tasks Write pseudocode Get it approved [by PL or TL] Start Coding based on Pseudocode  Is this a suggested approach? Is it practiced in the industry? ",
        "Best answer": "I use comments as a kind of very high-level pseudo code, in that I write the comments before I write the code. I find that thinking through the entire problem, even at a high level, improves my code considerably. "
    },
    {
        "ID": "12773",
        "Question": " Possible Duplicate: Is there a canonical book on design patterns?   I'd like to read a design patterns book. GoF for sure is the one to read. But after 15 years is it still valid, I mean isn't there an updated one? Something like \"Charles Darwin On the origin of species\" is a very important book, and some main concepts are still valid, anyway today (2010) one would read another book to study the subject. In this question the main suggestions are the Head First and GoF books. But Head first is not a replacement for GoF. So any suggestion or should I stick with GoF? ",
        "Best answer": "I recommend reading the Head First book first, as it does a better job of explaining why design patterns are needed. After that, you can use the GoF book for examples and as a reference. "
    },
    {
        "ID": "12807",
        "Question": "There is a colleague of mine who constantly writes: if (someBool == true)  It drives me up the wall!  Should I make a big deal of it or just drop it? ",
        "Best answer": "It's only redundant code, not life or death.  However.... If it's happening a lot, it could be a problem with how someBool is being named. A good name can go a long way towards eliminating the need for the ==true if(IsSomeCondition)  or if(hasCondition)  or if(somethingExists)  for example. "
    },
    {
        "ID": "12808",
        "Question": "Say you've started an open source project and are posting it on a public repository.  (like I have, using Codeplex.) One of the key files just makes the connection to the database, contains the login/password, and is just included from any other source file that needs a database connection. What's the best way to share the project without giving out your password? So far I've specifically removed it before committing any changes, but I'm thinking there has to be a better way. ",
        "Best answer": "One trick learned in the ASP classic days, was to have the connection string in an outside file. ''// Init the ADO objects  & the stored proc parameters cmd.ActiveConnection = \"file name=c:\\udl\\mydb.udl\" cmd.CommandText = sqlstmt cmd.CommandType = adCmdStoredProc ....  It works in ADO, don't know your environment "
    },
    {
        "ID": "12958",
        "Question": "As a programmer what do you think that is the thing that your manager does that mostly decreases your motivation? My manager insists in blocking web content (this week was msdn content and Microsoft domain sites) This is so stupid, make me think I am not a reliable professional or that I am stealing his internet. And not, it is not a small business. It is a huge enterprise where such dinossaurs should not exist anymore. ",
        "Best answer": "You need to find a new job. What do base such a harsh conclusion on?  I just think about some things you need from your work environment:  it lets you do your job. it lets you grow professionally. surround yourself with smart people - who may be leaving or have left already.  "
    },
    {
        "ID": "12971",
        "Question": "I want to create a list of ranks for the employees in my company. We are an open source integrator that works usually with existing solutions and also building custom solutions. We don't want to name our employees as normal senior consultant, trainee and we would like give them ranks as forums do to the users. Does anyone have any suggestion for this? ",
        "Best answer": "This is a tricky situation. You are straying into some seriously ego-driven territory here, particularly with people who identify very strongly with their work. I have seen people get VERY twisted over being a Developer III when somebody else they consider inferior is a Developer IV.  Usually when a system with grades like this is put in place it is because there is a push for some kind of a standardization. It usually appears with a pay scale. Almost always it is a larger organization. In smaller organizations you usually wind up with a title prefix like \"Senior\" or \"Chief\" as a way to differentiate. In all cases what you are doing is making a hierarchy visible and using it as a way for people to measure themselves. If this is your goal, then go ahead and put in some kind of a system.  Be aware that you will immediately ruffle feathers when people find out they are not seen by management as the best on the team. Also be prepared for the demands of \"When will I make the next grade?\" This will come hand in hand with a demand for more compensation. If the compensation winds up tied to the title then people will be able to figure out what you are paying the new hire and this will spawn more issues. If it's internal, maybe tie it to some kind of objective metric like number of releases authored, bugs resolved, etc. This would feed the need for recognition but not be something you'd have to put on a business card. In my own experience I just called everyone \"Software Developer\" and made sure that I communicated with everyone all the time so that I could head off any feelings of ill will, etc. It also cut down somewhat on \"that's not my job\" comments. The only person who wound up as a \"Senior Software Developer\" had minor management duties. It was not perfect and had its own issues but overall worked for me. YMMV. "
    },
    {
        "ID": "12996",
        "Question": "I am a fairly adept .NET and PHP programmer. These days my pattern of choice for complex web apps is an SOA approach where I use PHP as the front end, and WCF web services as the middle tier. My ajax calls are usually to JSON webHttpBinding WCF endpoints. On the .NET side of things I am absolutely in love with the Visual Studio plugin ReSharper. So my question is as follows, I want to rethink my tool chain on the PHP side of things. On one side JetBrains, the makers of Resharper, have a PHP IDE called PHPStorm. On the other side, I can use VS.PHP and ReSharper. Does anyone use the later combination fo VS.PHP and Resharper? Can you recommend it? ",
        "Best answer": "Since you're also working with WCF Web Services, I'd recommend you stick with Visual Studio. I've done quite a bit with that platform and it works well.  PHP Storm is probably very nice, but it's going to cost extra and be a new environment that you'll have to get used to.  If it's not broke, don't fix it :) "
    },
    {
        "ID": "13010",
        "Question": "When you are under pressure, the deadline is approaching, and a manager is breathing down your neck do you find yourself starting to write bad code? Do TDD and best practices slip by the wayside in order to get things done? What do you do in situations like that? What were your experiences? ",
        "Best answer": "In a word, yes.  Anyone that tells you otherwise is probably, at best, mistaken. However, the key is to build on your experience to write code that is less bad.  Resist the temptation to put in something to make it \"just work\" if at all possible, because it won't.  You still need to follow some sort of process (be it your own, or your company's, or some mix thereof). Experience tells me that its much better to (gasp) slip the schedule a couple of days to prevent a week's worth of fixes, especially when \"under pressure\" means an expedited release to production.  If you are hurrying to release code, the testers are probably going to be in a hurry to rubberstamp it as well. "
    },
    {
        "ID": "13034",
        "Question": "Recently I worked with a group to create a logging system for tutoring labs to track Tutor hours, and usage of the labs. We had to change the design a fair amount to be in compliance with FERPA.  My question stems from the curiosity that in my course of study there has never been a real mention of how people in this field have to deal with complying with the law in their work. So I would like to know how much programmers have to consider the law in the work they do.  ",
        "Best answer": "In many fields dealing with legal or regulatory requirements are a daily thing. Sarbanes Oxley compliance is critical for large businesses, health care organizations have to consider HIPAA, programming for state, local or federal government projects tends to have lots of regulatory requirements. You may also need to consider Government Accounting Standards to make sure you have what auditors need. You may have to deal with privacy considerations and there are laws about how the disabled must have ways to access the data.  In our business we have government regulations that vary by state that our clients must follow and spend a fair amount of time making sure the compliance rules are followed and in creating reports to show that we have complied with regulations. If you are selling anything, at a minimum you need to consider taxes. You may also get stuck dealing with things like ISO certification requirements which aren't legal, but are simliar in their pain in the posterior factor. In general if you are doing business programming (especially anything to do with health care or finances) and you are unaware of any legal or regulatory requirements, you have a good chance of doing something wrong. If you (or your company) have never asked about them, then you are definitely doing something wrong as it should be one of the first questions that almost every project starts with. You don't know if the answer is no unless you ask.  If you are selling software to businesses (not so much individuals), regulatory compliance is often a key selling point as not everyone does a good job with this. We get a lot of our clients specifically because we do a much better job of this than our competition and regulatory compliance is critical for our clients. If you are in Gaming, I would expect less of a need for this although, I'd bet there are still disability laws to comply with. I personally have never worked on a major project that didn't have some regulatory or legal needs to consider.  "
    },
    {
        "ID": "13045",
        "Question": "I was told not to use Jquery as a beginner because it would hamper my learning of Javascript.  Now I've read a couple books on Javascript, read loads of sites, and made a Javascript web app.  Am I ready for Jquery? If not, then how will I know when I'm ready? ",
        "Best answer": "Wow, I know this may go against some people's ideas about crawling then walking and then running.  But Javascript is not some awesome language you need to totally understand and then you'll be a whizbang programmer.  Just use JQuery and have fun.  Use it wiseley, use it cleverly! I did my company's website 3 years ago with just Javascript and replaced nearly everything I did with JQuery over the last year.   Knowing every last IE6 browser quirk is not going to help in the long run, so just go for it man! "
    },
    {
        "ID": "13212",
        "Question": "Our team is switching to Scrum. I would be the ScrumMaster (in addition to being a developer), and another developer would become Product Owner (in addition to our product marketing guy). All members of the team, including me, would be managed by the would-be Product Owner. By that I mean that the guy would be the one deciding about our yearly evaluation, raises, etc. Would this hierarchical link be prone to introduce issues? How do organizations typically map hierarchical structure onto agile teams? I suppose it's quite common that the ScrumMaster has a hierarchical link to the other developers in the team.  Here it would be the Product Owner.  Is this different? ",
        "Best answer": "The product manager doesn't manage the team, but the product backlog. Having any hierarchical link will make things harder. I also suggest you to remove yearly evaluations from your company. They are individual, and Scrum is focused on a team rather than individuals. There is no hierarchical link between the Scrum Master and the rest of the team. The team is self managed. The ScrumMaster ensure that the scrum process is properly implemented and help the team on a daily basis. The team reports to the team during the daily stand ups. The team reports to the product owner during the sprint review. Simple and effective. "
    },
    {
        "ID": "13259",
        "Question": "Scrum being a project management methodology, how would you 'sell' it to the developers in a team that is reasonably happy with its current situation? It seems easy to me to explain to our Product Manager how Scrum will allow him to get regular releases, to amend requirements, and get the team to focus on the high-priority stories first.  I found it easy to explain what TDD or Continuous Integration bring in a developer's day-to-day life.   But how can developers be convinced to embrace Scrum?  How would Scrum make their life easier? ",
        "Best answer": "Scrum will provide much more visibility on what is going on. Bad management, last minute changes, pressures, and all kind of stuff a developer usually face. However, it will also bring a lot of visibility on procrastinators, bad faith developers, insane individualists, ... in other words, bad developers.  Scrum is a double edged sword  Scrum will bring you with opportunities to solve those problems. That's why it's so powerful. "
    },
    {
        "ID": "13376",
        "Question": "When there is too much work for one scrum team to manage, multiple teams are created. A scrum team has a stand up meeting everyday. How often should the intra-scrum team meeting occur and who should attend? What should they talk about? ",
        "Best answer": "We have Scrum-of-scrums daily. The typical day goes like this... 9:15 - individual teams have their stand-up meetings. The usual stuff: what you did yesterday, what you're doing today and (most importantly) any blocks 9:30 - Scrum Masters from each team have their Scrum-of-scrums. Same stuff: what the team did yesterday, what they're working on today and most importantly (even more so than for the individual team Scrums) any blocks and/or cross-team issues Doing it this way requires a lot of discipline to stay focussed on the important stuff, but that's Agile in general for you. For me, the most important thing, regardless of how or when or who is that blockages and cross-team issues are raised and resolved quickly.  Finally as Casey mentioned, don't be dogmatic. If it doesn't work - do something about it.  "
    },
    {
        "ID": "13381",
        "Question": "Coming from a scrum environment with no integrated testing support and an independent minded QA staff, how is a tester (QA person) best integrated with a scrum team? What should they do? For reference some test functions are: Unit Testing Integration Testing Functional Testing Performance Testing Acceptance Testing ",
        "Best answer": "Scrum Master Because the team must have people with cross-funcional skills.  Scrum is facilitated by a Scrum Master, who is accountable for removing impediments to the ability of the team to deliver the sprint goal/deliverables. The Scrum Master is not the team leader, but acts as a buffer between the team and any distracting influences. The Scrum Master ensures that the Scrum process is used as intended. The Scrum Master is the enforcer of the rules of Scrum, often chairs key meetings, and challenges the team to improve. The role has also been referred to as a servant-leader to reinforce these dual perspectives. The Scrum Master differs from a Project Manager in that the latter may have people management responsibilities unrelated to the role of Scrum Master. The Scrum Master role excludes any such additional people responsibilities...  "
    },
    {
        "ID": "13384",
        "Question": "Change is not uncommon, change in requirements, change in specs change in workflow. I've accepted that there will be change, but I wonder: knowing that change is going to happen, how short of a planning period is too short? (Justifications are encouraged)  An iteration (2-4 weeks)? A week? A 2-3 day period? A day? 1/2 a day?  Assume that the company 'plans' 1 [time interval (from above)] in advance from the current so that any plan sounds like: \"[this morning/today/this week/etc.] you'll work on this and [this afternoon/tomorrow/next week/etc.] you'll be working on that. Also assume that the changes in focus/direction will consistently occur every second to third time interval. ",
        "Best answer": "I'm a Scrum Practitionner so I'll will suggest you to use it.  Define the duration of your iteration. I like two weeks iteration in startups, and one month in large enterprise projects At the beginning of an iteration, select from the features you will develop from the product backlog. Nobody has the right to change the iteration plan, not even the product manager. Changes occur in the product backlog, not in the iteration plan. Therefore, you are never affected in your work.  More details about Scrum "
    },
    {
        "ID": "13439",
        "Question": "At what  point does a developer stop being simply a developer and turns into a software architect. Or in other words, what's the minimum job description of a software architect?   ",
        "Best answer": "I could probably come up with 2 pages of humor here, but I'll assume you are taking this seriously. So, I'll respond as seriously as I can. First thing you must understand: @Craig is right... differnet organizations mean different things. For some, it's just part of their advancement track and doesn't really mean much beyond that. For others, its a distinct role and very often, they get subverted out of doing any code or other hands on work and so lose their efficiency. The point at which you stop being a developer and start being an architect is the point at which you spend 90%+ of your time engaged in the following  Writing specs / drawing pretty pictures Meetings with management/Stakeholders Researching what technologies you should be using instead of learning what your team is using today. Wondering what the code for your apps looks like because you haven't seen the code in 4 months  Short of it is that an architect is the interface between the dev team and the stakeholders (the BA is the opposite). They need to be able to understand the business side and the technical details, but truth is, they are likely to not \"get their hands dirty\" all that often. Their primary 'technical' skills need to be with UML, a word processor, other technical drawing tools, and presentation software. So, in most cases, within a few years, they will start to become less and less effective as their knowledge of coding becomes dated (e.g. they are trying to think in C++ while the project is in C# or Java). At that point, the smart people learn how to lean on and learn from the hands on folks. The others become a pain in rear because they have a harder and harder time relating to state of the art. "
    },
    {
        "ID": "13556",
        "Question": "What do you think about using exclamation points in error messages?  i.e. \"The address is not valid!\". Personally, I think it adds nothing, insults the user, and makes the product look like it was written in TRS-80 BASIC by a 12-year-old. ",
        "Best answer": "On error messages meant to be displayed to the user?  Yeah, I'm with you.  Anything that makes the user feel like their machine is yelling at them for being an idiot should be filed under \"Bad Idea.\" But, error messages to be sent to the log file, particularly on the server?  I know you're supposed to still maintain a sense of professionalism, because those can still wind up on the user's screen.  But I gotta admit, snarking it up is awfully tempting.... "
    },
    {
        "ID": "13614",
        "Question": "If a software company loses the source code to one of the products they are selling, how serious would that be, in terms you could explain to a layman?  Would the term \"gross negligence\" be too strong?  Or \"gross incompetence\"?  Obviously no one got killed but isn't it as serious as some financial negligence that people get jail time for? EDIT: Let's say it's not a case of a disk drive crashing, natural disaster or anything like that.  Just they misplaced it. ",
        "Best answer": "Let's say MS loses the source for Windows Phone 7... people have been killed for waaaaay less than the estimated $400 million it cost to develop it. Depending on the product, there isn't a term that I can think of that is 'too strong'. "
    },
    {
        "ID": "13616",
        "Question": "I happened to know some system admin and according to him, testing guys are not given preferences in an organization in comparison to developers. No doubt software releases are not possible without testers but I've never laid my hands on testing so not much aware of it. No offense intended.  ",
        "Best answer": "In my experience, unfortunately, they are often treated like second class employees and even worse a frivolous perk for programmers. It stems from a number of things:    When the testers are doing their jobs correctly, it is easy for everyone but the programmers to forget they even exist. Much like a network admin, you only notice them when they are not doing their jobs, or doing them badly. So from the perspective of the rest of the organization, they are remembered only for their mistakes. It is mistakenly seen as an entry-level job for people who aspire to be programmers, but aren't qualified yet for those jobs. In fact, at one company I worked they were given Jr. Programmer job titles despite their pleas to get a Q&A job title. Even the fact that they were in a QA department wasn't enough to get HR to budge on that. Because of #2, it is assumed that testers are all entry-level folks, and should be paid accordingly. Nobody likes to be criticized, and it is all too common for defensive programmers to dislike testers because their jobs require them to point out programmer mistakes all day. As a manager, I was constantly on a PR mission to remind programmers that the QA team was there to make them look good, not narc them out. It tends to be a job people get into by accident and not choice, at least initially. I don't remember any degree plan at any of the schools I attended that prepared people for software Q&A. They do exist, but usually at the lower-end vocational schools, which only contributes to the idea that they are less skilled professionals. Testing jobs are much more likely than programming jobs to be sent offshore. At least the programmers can argue that it is more efficient to communicate design needs locally and that it is valuable to keep the knowledge of how the company's flagship app works inside the company. Testing, however, is much easier to modularize and thus easier to outsource. For all of the reasons above, testers tend to see the writing on the wall and move into other jobs (like programming), especially the really good ones. This means that most testing jobs tend to get staffed with more entry level people who haven't burned out on it yet or moved on to other things, which unfortunately reinforces several of the above ideas.  "
    },
    {
        "ID": "13641",
        "Question": "As IT manager what goals would you set for your programmers in your team. These goals would be used in the annual performance review. Any recommendations? How would you create goals that actually measure programmers technical abilities. ",
        "Best answer": "This can be really difficult, because good goals for employees should be specific and quantifiable. As we know, quantifying a programmer's productivity is definitely a conundrum.  In my experience managing programmers, I try to create goals based on the long term goals of the individual employee. For example, if the programmer aspires to move into management I'll set a goal for them to seek out and take on leadership roles on teams or projects to help them build some informal authority that will be necessary for the transition.  Usually, however, to create measurable goals they tend to be fairly task oriented. For example getting a MCDBA certification for someone who wants to move into a DBA role. For someone who claims they want to improve/learn on a particular technology I'll usually pick a goal like reading x books on the topic or teaching a brown-bag class to the other developers on the technology. "
    },
    {
        "ID": "13676",
        "Question": "I had many bosses, each one had a different approach about allowing or not use of Windows Live Messenger, Facebook, and many other Internet sites. Of course Internet is really needed to research about the best way to solve a specific task. Sometimes you could have a friend online, also a programmer, who knows better about something. For some managers, internet access would slow down project progress, and on the other hand, allow people to interact and find out brand new solutions. What would you do? ",
        "Best answer": "I wouldn't make it an issue unless it became a problem. I prefer to treat my employees as adults and assume they will act professionally unless there is evidence to the contrary. For example, if someone is continually missing deadlines without a good reason I might check in on them once in a while and if they are wasting time online, THEN I would deal with that individual as needed.  Also, since none of my employees are paid hourly, I don't see the sense in policing every minute they spend at the office as long as they are getting their work done. The exception might be if they were doing something online that is otherwise problematic (porn, leaking company secrets, badmouthing the company publicly, etc.) For those things we would have specific policies against it and deal with infractions also on an individual basis.  "
    },
    {
        "ID": "13693",
        "Question": "Although I've seen many discussions on how to do an interview and develop your brand, I haven't seen many discussions on how to determine if your hiring & interview process is actually working well. I think this question has two parts:   How do you know your hiring process is getting the right candidates to apply and go through the interview process? Of the people that you end up interviewing, how can you tell that the ones you choose to hire are better (on average) than those that you rejected?  I suppose the \"extreme\" cases - when you end up with a superstar or a total dud - are pretty obvious, but what about the rest? ",
        "Best answer": "One thing that has been really useful for me in this regard is to review my interview notes on the one year anniversary of hiring an employee. I have a standard interview script, although I usually tweak it for the particular candidate. In any case, I type up the questions that I asked and summarize the answers into a word document directly after the interview while the information is still fresh. I also have a section for my general gut feel about the candidate and what stands out about them either positively or negatively. Of course I use this document extensively throughout the rest of the hiring process, especially when I am talking to a lot of candidates when it is easy to get confused about who said what, but it really comes in handy for fine tuning my interviewing process. On or about one year after the hire, I think about how well the person has worked out. I look at both the positive and negative surprises over their first year. Then I go back to my interview notes and evaluate how well my questions drew out that information. Finally, I use this analysis to tweak my template interview script so that I can make sure to extract that information in future interviews. Also, I use this to get rid of questions that aren't contributing useful information and wasting valuable time during the interviews. Over time this has really honed my recruiting process and the quality of my hires has improved steadily. I think the most important lesson I have learned from doing this is to never ignore your gut when you have qualms about a candidate, even when you can't put your finger on what the specific issue is. Not once did I have a concern from an interview that didn't manifest itself over that first year. I blogged about this a while back in my article \"19 Tips For Recruiting Great Developers\". "
    },
    {
        "ID": "13746",
        "Question": "I keep coming across this term hooks in various programming articles. However I don't understand what they are, and how can they be used. So I just wanted to know what is the concept of hooks; if someone could link me to some examples, particularly in the context of web development, it would be great. ",
        "Best answer": "My answer pertains to WordPress which is written in PHP, but this is a general development mechanic so it shouldn't really matter, despite the fact that you put 'python' in your question title. One good example of usage of hooks, coincidentally in web development, are WordPress' hooks. They are named appropriately in that they allow a way to 'hook into' certain points of the execution of a program. So for example, the wp_head is an 'action' that is emitted when a WordPress theme is being rendered and it's at the part where it renders the part that's within the <head> tags. Say that you want to write a plugin that requires an additional stylesheet, script, or something that would normally go within those tags. You can 'hook into' this action by defining a function to be called when this action is emitted. Something like: add_action('wp_head', 'your_function');  your_function() could be something as simple as: function your_function() {     echo '<link rel=\"stylesheet\" type=\"text/css\" href=\"lol.css\" />'; }  Now, when WordPress emits this action by doing something like do_action('wp_head');, it will see that your_function() was 'hooked into' that action, so it will call that function (and pass it any arguments if it takes any, as defined in the documentation for any particular hook). Long story short: It allows you to add additional functionality at specific points of the execution of a program by 'hooking into' those points, in most cases by assigning a function callback. "
    },
    {
        "ID": "13757",
        "Question": "How do you correctly or erroneously pronounce programming terms?  Any that you find need strict correction or history into the early CS culture? Programming char = \"tchar\" not care? ! = bang not exclamation? # = pound not hash? Exception #! = shebang * = splat not star? regex = \"rej ex\" not \"regg ex\"? sql = \"s q l\" not \"sequel\" (already answered, just i.e.) Unixen | = pipe not vertical bar? bin = bin as in pin , not as in binary? lib = lib as in library , not as in liberate? etc = \"ett see\" , not \"e t c\" (as in /etc and not \"&c\") Annoyance / = slash not backslash LaTeX = \"laytek\" not \"lay teks\" ",
        "Best answer": "To summarize my own (North American) experience:  char may be pronounced both ways: like \"char\" in \"charcoal\", or close to \"care\"; never like \"car\" ! is an \"exclamation point\" # is a \"pound sign\" (probably an Americanism) * is an \"asterisk\" (never in 15 years heard anyone call it \"splat\") RegEx with a hard \"g\" because it's regular, not rejular. SQL: heard both \"ess-queue-ell\" and \"sequel\"; but (Microsoft's) SQL Server is much more often pronounced \"sequel server\". bin is like a recycle bin; not \"bine\" LaTeX's ancestor is TeX.  The \"tech\" pronunciation goes all the way back to TeX's creator Donald Knuth.  Actually the X was supposed to sound more like the German \"ch\" in Bach.  \"La-tech\" is the only way LaTeX is pronounced in the US research community; if you pronounce it like a sort of rubber, you give away that you don't belong to it.  "
    },
    {
        "ID": "13778",
        "Question": "I'm pretty sure about who, but when? The one with proficient knowledge of programming and software development or the one who's just a beginner in programming? I'm pursuing bachelors right now, when is it preferable for folks like me to read this Must read for programmers book? ",
        "Best answer": "Like Robert Harvey said, it's probably best to read it as an intermediate programmer. I read it after having programmed, more or less, for 10 years. After having read it, I wished I had read it at least 5 years sooner. Code Complete is a bible of best (or at least very good) practices which you only really appreciate after first having tried out a few of your own. It's like doing a math exercise - you won't appreciate the solution unless you've first made your own stab at it. Maybe the book's solution solved the problem differently compared to your solution, and while the final answer may be the same it is the approach which is interesting. Some approaches are better than others, are more intuitive and yield a better understanding of the problem. The same goes for programming. The following quote of Robert Harvey really says it best:  If you're a beginning programmer you won't understand a lot of the material, and if you are experienced, the book will only confirm what you already know.  "
    },
    {
        "ID": "13782",
        "Question": "I've worked at two companies, who each had a different methodology when it came to code reviews: In the first company, a code review was conducted by the team leaders and was required after the completion of every module. However, in the second company, team leaders weren't required to conduct any code reviews, and just checked for functionality and design issues. So I am confused. Is the code review process really needed? If it is, why? And if it isn't, why not? ",
        "Best answer": "I personally think that every piece of code should go through a code review, it doesn't matter if you are junior or senior developer. Why? For starters your title doesn't state anything about how you develop, and a senior developer could learn something from the junior. At our company we shift around so one of the other members of the team review your code...mostly we are teamed a \"junior\" and a senior together, so all the stuff that doesn't get said on a daily basis can be caught in a follow up. If the senior doesn't like the junior code he should listen to why the junior did as he did and look at it and see if that's a feasible solution that might be used in the future...it's a matter of getting wiser no matter who you are. One important thing about code review is not being too nice, if you are being a nice guy you'll just allow more and more messy code to evolve in the system. Just as yesterday I started reworking a complete application that a former employed juniordeveloper wrote, and my god that code could have needed a review before he left. I don't see why it should be the teamleader only doing reviews but it requires a person that's not affraid of picking a \"fight\" over a piece of poorly developed code, and it has to be a person that cares about how the code should be. Not all companies hire people that actually care about what they do, and those bad eggs should IMO not be allowed to do code reviews as they are likely just to shrug their shoulders and say \"OK\" to bad code. "
    },
    {
        "ID": "13786",
        "Question": "I was asked to make some small technical presentation about specific application scalability. The application is developed using Java, Spring MVC, Hibernate. I have access to the application source code. How can I measure software scalability (using sources) and what metrics do I need to look after when measuring software scalability?  ",
        "Best answer": "I would start with reading Wikipedia article on the subject. In short, scalability is how system performance grows with adding more resources or, alternatively, how the resource utilization grows with increasing load. For example, how many concurrent users can your site handle until response time grows beyond 0.3 sec? The same question after you double the available RAM/disk/CPU/etc. You probably can use your knowledge of the application internals to decide which parameters are worth checking.  Setup a test bench with a server machine and one or more client machines. Use some tool to limit the amount of resources available to the server (e.g. ulimit) or run some interfering application on the server. Measure how the server deals with client requests. Repeat the above gradually increasing/decreasing interfering load/available resources. At the end you get n-dimensional space with  dots in it. It may be simpler to change only one parameter at a time while fixing all the others at some typical value (or a couple of values). In this case you can represent the result as a bunch of 2D graphs with server performance (e.g. number of users/requests) on one axis and resource utilization/availability on the other. There are more complex scenarios where your application uses several servers for several part of the application and you can vary their amount and ratio, but I guess it's not your case. At most, you probably may want to vary the number of threads/processes, if this matters. If you measure the whole application you usually don't need source code access. However, you may be interesting in measuring some specific part of the code (e.g. only DB or UI). Then you can use the source code to expose only this module for measurements and run your tests. This is called a microbenchmark. If you're looking for examples, there is a plenty of them in academic articles. Search the google scholar for performance evaluation + your preferred terms. "
    },
    {
        "ID": "13793",
        "Question": "While travelling I met a mathematician who was sitting near me. In a discussion he said: \"...there is nothing like engineering in IT or rather programming\". A true engineering is what Architecture is, what Electrical and Mechanical is. It made me think and I was puzzled. A percent of my brain agreed also because in Indian Army, there is no subject like Computer Engineering in the Engineering Corps. They don't consider programming as engineering. This is what I heard few years back, I don't know what Indian Army thinks now. What are your views? ",
        "Best answer": "If we follow this description, we are 75% engineers, or 75% of us are engineers :) Unlike other engineering disciplines:  software developers don't need formal education and/or licences is a relatively young discipline and don't have proven best practices (like construction, for example)  I think this is going to change in the future. Someday we'll be 100% engineers.  Engineer source: http://en.wikipedia.org/wiki/Engineer An engineer is a professional practitioner of engineering, concerned with applying scientific knowledge, mathematics and ingenuity to develop solutions for technical problems. The word engineer is derived from the Latin root ingenium, meaning \"cleverness\". Occupation  Names  Engineer  Type  Profession Activity sectors  Applied sciences  Description  Competencies  Mathematics, scientific knowledge, management skills Education required  Engineering education  Fields of employment  Research and development, industry, business Related jobs  Scientist, architect, project manager  "
    },
    {
        "ID": "13796",
        "Question": "In hopes of getting myself and my coworkers to adopt automated unit testing methods in future projects, I would like to see a proper example on how it's done. Simple introductionary lessons to unit testing only cover very basic examples and they don't seem to convince anyone of the benefits unit testing can offer. What are some open source projects written in PHP that come with an extensive array of unit tests that are done \"properly\", and can be used as an example of how testing should be done? ",
        "Best answer": "Most PHP frameworks these days are unit tests, for a good example of the difference in tools, Zend Framework uses PHPUnit and CakePHP uses SimpleTest. Their repos should be enough to get you started or writing \"proper\" unit tests. "
    },
    {
        "ID": "13798",
        "Question": "Following reading the latest CodeProject newsletter, I came across this article on bitwise operations.  It makes for interesting reading, and I can certainly see the benefit of checking if an integer is even or odd, but testing if the n-th bit is set?  What can possibly be the advantages of this? ",
        "Best answer": "Bitwise operations are absolutely essential when programming hardware registers in embedded systems.  For example every processor that I have ever used has one or more registers (usually a specific memory address) that control whether an interrupt is enabled or disabled.  To allow an interrupt to fire the usual process is to set the enable bit for that one interrupt type while, most importantly, not modifying any of the other bits in the register. When an interrupt fires it typically sets a bit in a status register so that a single service routine can determine the precise reason for the interrupt.  Testing the individual bits allows for a fast decode of the interrupt source. In many embedded systems the total RAM available may be 64, 128 or 256 BYTES (that is Bytes not kilobytes or megabytes)  In this environment it is common to use one byte to store multiple data items, boolean flags etc. and then use bit operations to set and read these. I have, for a number of years been working with a satellite communications system where the message payload is 10.5 bytes.  To make the best use of this data packet the information must be packed into the data block without leaving any unused bits between the fields.  This means making extensive use of bitwise and shift operators to take the information values and pack them into the payload being transmitted. "
    },
    {
        "ID": "13927",
        "Question": "I live in Manchester, UK, and while there are lots of developer communities (regular events, get togethers etc) in London, there are not many \"up north\".  I know of a few (NxtGenUg and ManchesterGeekNights) but there must be more than this. What's the best ways of finding out about these? How have people got involved with communities in there area? ",
        "Best answer": "Get in touch with Manchester Uni or Manchester Met student's unions... they will have programmer's societies ran by the student's union. They welcome non-students and have regular meetups. Here is a list of societies Also check out ManLug as there is bound to be coders there, and they may be able to point you in the right direction. Manchester also has a rich hacker community. "
    },
    {
        "ID": "13956",
        "Question": "I like \"red/green/refactor\" for RoR, etc. just fine. My day job involves batch processing very large files from third-parties in python and other custom tools. Churn on the attributes of these files is high, so there are a lot of fixes/enhancements applied pretty frequently. Regression testing via a known body of test data with expected results does not exist.  Closest thing is running against the last batch with new test cases hand coded in, make sure it does not blow up, then apply spot-checking and statistical tests to see if data still looks OK. Q>> How to bring TDD principles into this kind of environment? ",
        "Best answer": "Just an FYI: Unit testing is not equivalent to TDD. TDD is a process of which unit testing is an element. With that said, if you were looking to implement unit testing then there's a number of things you could do: All new code/enhancements are tested This way you don't have to go through and unit test everything that already exists, so the initial hump of implementing unit testing is much smaller. Test individual pieces of data Testing something that can contain large amounts of data can lead to many edge cases and gaps in the test coverage. Instead, consider the 0, 1, many option. Test a 'batch' with 0 elements, 1 element and many elements. In the case of 1 element, test the various permutations that the data for that element can be in. From there, test the edge cases (upper bounds to the size of individual elements, and quantity of elements in the batch). If you run the tests regularly, and you have long running tests (large batches?), most test runners allow categorization so that you can run those test cases separately (nightly?). That should give you a strong base. Using actual data Feeding in 'actual' previously used data like you're doing now isn't a bad idea. Just complement it with well formed test data so that you immediately know specific points of failure. On a failure to handle actual data, you can inspect the results of the batch process, produce a unit test to replicate the error, and then you're back into red/green/refactor with useful regression cases. "
    },
    {
        "ID": "14033",
        "Question": " Possible Duplicate: How can I get the word out about a new (open-source) library I've developed?   I have hosted my latest project, a JVM-based MIDI processor/API called Mjdj MIDI Morph, on Github (here and here). Now I need to bring some interest to it, even if it's negative interest (so I can improve it). I've looked up open source list on Google and end up with such things as this page on Wikipedia, which makes it quite clear that they don't want your project if it's new. Where should I list my project? Short of adwords and talking it up in forums and trade shows, where should I submit my URLs? ",
        "Best answer": "Post it on Hacker News - you'll get a lot of feedback.  Hacker News is a social news website about computer hacking and startup companies, run by Paul Graham's investment fund and startup incubator, Y Combinator. It is different from other social news websites in that there is no option to down vote submissions; submissions can either be voted up or not voted on at all, although spam submissions can be flagged. In contrast, comments can be down voted after a user accumulates sufficient \"karma\" or points gained when submissions or comments are voted up. In general, content that can be submitted is defined as \"anything that gratifies one's intellectual curiosity\"...  "
    },
    {
        "ID": "14089",
        "Question": "When you write wrappers for APIs how do you handle usage errors? Do you assume that the user uses it right and write the wrapper such that it makes the call to the server and let the server return the error? Or do you check for the missing parameters and return an error? If yes, how do you convey the error? a string or an error code? ",
        "Best answer": "Think wrapper vs adapter vs bridge from a design pattern point of view. You might have a new perspective and find your answer from there. E.g. Adapters and Wrappers from Andy Kramek and lots more if you google. "
    },
    {
        "ID": "14200",
        "Question": "I want to download Visual Studio 2010 Express Edition to learn all that is new in it. I find it insufficient as there is no Crystal Reports integration in it. Apart from that I want to learn other enterprise features that are bundled with the Professional Edition. If I download the VS 2010 Professional Edition, it is just limited to 90 days. Is there any other alternative for learners?  Also, is there any FREE alternative for reporting in Express Edition. ",
        "Best answer": "You can't do reporting with Express. There isn't some trick to turn the free product into the paid product, and there isn't some obligation on Microsoft that they should have all the same features in the free and paid product. As you are saying, there's great value in those reporting features, and that's why people buy the versions that cost money. Just because Microsoft says you're allowed to sell the products you make with Express, they're not saying you can have any and all features in Express that you might use to make a product you will sell. That said, you can probably get the versions that cost money for free. If you're writing software you intend to sell, if you make web sites for customers, or if you're a student (high school or college/university) then there are programs from Microsoft that give you free licenses of non-Express editions.  See Can a developer get a discount for Microsoft products? which has several answers that have nothing to do with \"volume discounts\" because they actually explain how to get the full featured product for free. "
    },
    {
        "ID": "14206",
        "Question": "I'm a student of web development languages like PHP, Ruby, and Python. Currently I'm working on some school projects but I'm trying to work on some weekend projects which I can use to showcase for potential employers, what's the best way to set this up? Specifically, are there any tips you have for a new programmer because I don't want opinions on how this or that should or shouldn't be set up. If possible please give real examples.  Edit: what about Git? Is this a good place to show my projects? ",
        "Best answer": "I honestly think you can do no better then shelling out a few bucks to buy a domain name that has your name and a monthly server. It's not expensive. This way, you can do anything you want on it, show case your projects and let them be viewed and downloadable by employers or the friends your trying to impress =)   The way you want people to see them. Small portions or full downloads, your in control of the name and the content. "
    },
    {
        "ID": "14281",
        "Question": "It's easy to find resources for learning WPF, similarly it's pretty easy to find resources for F#.  But I feel that I could save some time if I could learn them both at the same time. So can anyone recommend any books, blogs, articles , something else?  (I'm familiar with functional programming, winforms and c#)   ",
        "Best answer": "Learning WPF through F#, and vice versa, by John Liao  Link "
    },
    {
        "ID": "14293",
        "Question": "Many people praise Sinatra's ability to create simple projects in a single file.  This is great and all but what if I want to use Sinatra for slightly larger project? I'm fairly new to Ruby as a whole but have already found a couple ways to create a Sinatra application with separate models files, partitioning different groups or URLs into individual files, etc... But is there any best practice for this? ",
        "Best answer": "The step to take is to partition the application into multiple parts at the file level. Instead of having all of the routes in one file, split them up into logically separated files with different functionality groups in different files, which are all then require'd back into the main Sinatra application/Rackup file. Although this makes your file structure prettier, it does not implement any true compartmentalization or organization beyond superficial divisions. The solution to this that seems to have garnered the most popularity and acceptance is the Padrino framework, which runs on top of and expands upon the Sinatra core to add controllers (for compartmentalization) and a bunch of Django-like features (drop-in authentication, administration, etc.). "
    },
    {
        "ID": "14326",
        "Question": "The title pretty much gives you the gist of the question, but allow me elaborate a bit nevertheless. Apart from Google's involvement on Python, and Activestate's on Perl development (Strawberry AFAIK doesn't make any money on its own product) I'm interested in what are the major sources of funding of such.  Are there any texts that cover this? I tried searching, but found nothing apart from the \"history of\" and \"it's opensource, everyone gives in\" ... ",
        "Best answer": "\"It's opensource, everyone gives in\" pretty much is the state of the funding for these languages.  (Aside from Google, of course.)  Your question seems to be based on the unspoken assumption that in order to develop the language, it has to be funded by someone with deep pockets, and this simply isn't true. Development (of anything) doesn't require money, it requires time, effort and raw materials.  We've got an economy where money is commonly used to buy raw materials and motivate people to put time and effort into something, and the concept is so prevalent that so we tend to equate them, but they're separate and separable concepts. People who contribute to an open-source programming language already have the raw materials (a computer, an Internet connection and basic development tools,) and usually have a different motivation for putting time and effort into it:  They don't do it for money, they do it because they're using the language and they want to help shape it into a better tool for whatever it is they're using it for. "
    },
    {
        "ID": "14411",
        "Question": "I am still having a issue getting over a small issue when it comes to TDD. I need a method that will get a certain record set of filtered data from the data layer (linq2SQL).  Please note that i am using the linq generated classes from that are generated from the DBML. Now the problem is that i want to write a test for this. do i: a) first insert the records in the test and then execute the method and test the results b) use data that might be in the database.  Not to keen on this logic cause it could cause things to break. c) what ever you suggest? ",
        "Best answer": "Variation on (a).  Have a test db or subsection of the db that can be used for testing. When you are setting up  your tests, have a routine that initializes the DB as you need it to begin testing (this can be done before each test, as appropriate). This may include deleting data, inserting data, etc. Then run your tests. In the Teardown phase, clean up after yourself. Can be repeated as much as is needed, without risk of disrupting live system (not a good idea to test using data in the database that is needed for anything else). "
    },
    {
        "ID": "14441",
        "Question": "When my Manager told to team that \"from now on successful user stories will be considered for appraisal!\" We sat there for while shocked and that was one of the several jaw dropping moments he gave us. We felt that was stupid idea, since this will ruin all concept and goal of agile development methodology. Let me know what you people think? And how can we convince him? ",
        "Best answer": "Sandy, unfortunately your manager's statement is a classic misunderstanding of scrum in particular and agile in general. The proposed approach kills collaboration and counters the principle of collective code ownership.  User stories in agile (if it is a real agile) rarely get completed before being touched by multiple people.  Also, you will have user stories from time to time that need swarming in order to be finished within the iteration.  How are you going to all get that when the individual incentives are aligned 180 degrees in the opposite direction? Your teams instincts are correct.  What sources would I suggest in the short term for you to read as you brainstorm the response to your manager?  Look at blogs of renowned agile experts like Mike Cohn, Martin Fowler, Elizabeth Hendrickson, Jurgen Appelo, Esther Derby and several others and look for articles about agile team organization. "
    },
    {
        "ID": "14467",
        "Question": "I have been hearing about this term for close to 5 years now. I have read about services from Microsoft (Azure), but I have never saw it adopted in the mainstream usage. The thing I am trying to understand is this:   What happened to cloud computing? Is the infrastructure at present insufficient to implement this? Is it still in its infancy? Is it being used in other forms, like all the services Google seems to provide (plus Plus Google OS, etc)? If it has failed, then why?  ",
        "Best answer": "Cloud Computing, like most new technologies, was painfully over-hyped by the industry media. As it matures and is adopted -- or not -- as a working strategy, it is finding its valid place in the ecosystem. It is neither a panacea for all infrastructure problems nor a failure.  "
    },
    {
        "ID": "14472",
        "Question": "I am looking for a theme with gadgets that will make my life easier as a programmer. Googling from the desktop is one feature I am looking for. Integration with SharePoint or other bug trackers are the second. Any other idea that might make me more productive is a good one. ",
        "Best answer": "TFS gadgets:  See http://www.gregcons.com/KateBlog/SidebarGadgetsForTFS.aspx - it's about two years old, so ignore the Vista comments. "
    },
    {
        "ID": "14474",
        "Question": "Let's say you know of an anonymous noobie that wants to be lead upon the path of righteousness.  This noobie wants to use some sort of source control tools simply for the experience of using source control tools (and possibly for whatever benefits they bring along with them). To constrain things further (and to make this possibly even more noobie-tastic), let's say they're stuck in windows developing in visual studio. How would you guide your neophyte? ",
        "Best answer": "First, show them VisualHg, which really fits Visual Studio like a glove to a hand, so they fall in love with it on first sight. Then show them where they can download it, and then, although it's lemon easy to use, also show them these two really swell tutorial sites, that shows a little more than just pretty pictures (although it's got some pretty pictures in there as well): Hg Init and Hg Tip. As soon as they're across that, they can stop calling themselves newbies for sure --> here be dragons. "
    },
    {
        "ID": "14497",
        "Question": "What benefits for the developer does the Ruby language have that are not available in C# (preferably with code examples)? ",
        "Best answer": "Check this articles: What Is So Great About Ruby?, also don't miss Poor Man's Singleton Methods via Dynamic Wrappers in C# as answer to first article. "
    },
    {
        "ID": "14524",
        "Question": "Being a freelancer, I don't have access to corporate training programs where employees learn best practices. Most of the time I am advised to look into the available code on the Internet. Ideal places would be: CodePlex and SourceForge. But this proves to be of limited rather very little help. I want my existing code to be analyzed and a better solution be suggested to improve the quality of the code. How to learn coding that matches standards? ",
        "Best answer": "The ideal solution is near you. There is a website called stackoverflow.com where thousand of fanatic developers are answering questions for free. Just for what is called reputation points. That website is full of extremely experienced professionnals and it's not very common to meet people like Jon Skeet the author of C# In Depth. Your strategy? Try to answer to questions you know the answer, but also consult questions you may be interested in, such as the ones that talk about industry standards. It is becoming so huge, that now I skip google, and search directly on stackoverflow.com. "
    },
    {
        "ID": "14525",
        "Question": "Here we are in 2010, software engineers with 4 or 5 years or experience, still designing tables with 96 fracking columns. I told him it's gonna be a nightmare. I showed him that we have to use ordinals to interface MySQL with C#. I explained that tables with more columns than rows are a huge smell. Still, I get the \"It's going to be simpler this way\". What should I do? EDIT * This table contains data from sensors. We have sensor 1 with Dynamic_D1X Dynamic_D1Y [...]   Dynamic_D6X Dynamic_D6Y [...] EDIT2 * Well, I finally left that job. It is a sign when the other programmer goes dark for months at the time, it is another sign when management does not realise this is a problem ",
        "Best answer": "Maybe he did that for a good reason, such as performances or ROI ?.  The best thing to do, is asking him questions. With a certain amount of \"why\" you will certainly make him understand he is probably wrong by itself (if he really is).  I had one case myself that is not related to performances but return on investment (ROI). I had a table containing objects that had a specific value for each hour of the week (168h in a week). We had the choice to create a ObjectHour table that would contain the value, but also a key to the Object and the day number of hour number. But we also had the opportunity to put the 168 values right in the row. Probably like what your coleague did. The developers estimated both solutions. The simple solution (168 columns) was a lot cheaper to do than its well designed counterpart. For the exact same result for the customer. We decided to go for the simple/cheapest solution to focus our efforts of more important stuffs such as security.  We will have many opportunities to improve that in the future. Time to market was the priority for us at the time. "
    },
    {
        "ID": "14584",
        "Question": "When you're coding, do you actively think about your code might be exploited in ways it wasn't originally meant to do and thus gain access to protected information, run commands or something else you wouldn't want your users to do? ",
        "Best answer": "Kindof. Disclaimer: I'm a security guy ;) So the way I work is that I have my threat model, which describes what kind of attacks by what kind of attackers are likely. That helps to work out the security requirements. When I'm actually coding, I do the usual \"safe coding\" practices like being careful that cursor variables are always within bounds, tainted input is sanitised, error conditions are handled. Then I go back to my threat model to see which modules are most likely to get targeted by attackers; those get some extra review.  "
    },
    {
        "ID": "14596",
        "Question": "My manager has talked with me and says that the quality of my work is excellent, but I need to step up the quantity of how much I crank out. I've only been working for a couple of years, so I still feel like I have a lot to learn. Which route do I take (or is there another?): Quantity: If they want crap, I can give them lots of it. Quality: Ignore the request, and hope that my productivity will increase with experience. ",
        "Best answer": "Remember that you need to actually ship software for it to be useful: there's no point in writing the most awesome code in the world if it isn't available when it's needed.  It's one of many requirements that you need to balance. Assuming your manager is competent I'd be listening to them: they're more likely to know the quality/speed ratio required for the project than you (as a fairly new dev) are.  Or to use a common phrase: the perfect is the enemy of the good.  "
    },
    {
        "ID": "14635",
        "Question": "... that is, the inputs and outputs of service functions ? Are entities, or aggregates, allowed to leave service boundaries? ",
        "Best answer": "Yes (depending on your definition of service boundary) but in practice it's going to depend on the consumers of the services. If it's within your own application then it won't be a problem (for example if a service is calling a couple of other services and returning a result based on those results).  If it's outside your application then you will want those services to be returning either simple values or more usually DTOs composed of simple values. If you don't do this then your consumers will typically break any time your domain changes (and DDD is best applied when you have a regularly changing domain). In all this I'm assuming you're talking about domain services. "
    },
    {
        "ID": "14650",
        "Question": "For example, being a beginner, I find a lot of inspiration and direction from reading this post by Bryan Woods. ",
        "Best answer": "I'm surprised no one has mentioned The Pragmatic Programmer. It's a must-read if you are at all interested in your craft. "
    },
    {
        "ID": "14673",
        "Question": "Since Oracle bought Sun and Apple have decided not to continue developing their JVM, is the \"write once and run everywhere\" model still relevant or has web services/SOA reduced it to an edge case? ",
        "Best answer": "Write once, run anywhere always had a couple of problems:  \"write once, debug everywhere\": developers coming from a particular platform would occasionally make unwarranted assumptions about how all platforms worked, e.g. that configuration could always be stored in a registry, that all execution environments have virtual memory, or that all mobile phones that claim to support JPEG can actually render JPEGs. So in fact you would need to test your application on every platform you claimed to support, bringing the development costs back up. \"write once, run away\": user interfaces in cross-platform apps tend to borrow the HCI paradigms of the platform in which the designer was most comfortable, and therefore don't fit well in the platforms to which they're ported. Even where the widget library has been engineered to produce native-looking controls (Apple, in particular, invested heavily in making Swing look native) the placement, behaviour and workflow can still be different.  On the server side these problems are less significant - the browser is the UI, and people tend to choose one of either Win or *nix and stay with it. In that instance, \"write once, run anywhere\" was what allowed numerous companies to ditch Solaris for Linux after the dot-com crash. "
    },
    {
        "ID": "14720",
        "Question": "Imagine yourself hired by a new startup backed with few millions coming from venture capitalists. Your mission: organize the development of the next killer app. 25 developers is too much to take care of each individually, so what decision(s) you would make to motivate them? I will appreciate any answers from stock options to free cookies ;) Of course the trick here (unless you are really a manager of a such startup), is put yourself in the shoes of one of those programmers. EDIT: it's an imaginary context. The purpose of this story is to stimulate your wishes. I want to capture what motivates developers. ",
        "Best answer": "Here's my checklist, in no particular order:  Awesome computers to develop on. At least double the power of the target user,  with plenty of RAM and large/multiple monitors... ~$3 to 5k budget. Nice headphones for whoever needs them, when they prefer to work to music. Excellent development tools to work with. This depends somewhat on your target environment, but Visual Studio / Eclipse / whatever is the best for the job. This includes things like continuous integration/build servers. Fast internet access - perhaps with a caching proxy server to pre-cache things like SO, TheRegister, Reddit, etc Very few meetings - only what is absolutely necessary and a hard limit on their length (we use a timer); think 'stand-up meeting' like Scrum. Healthy atmosphere in which to work. Daylight, fresh air options, stable aircon, plants, pictures, good lighting. 10 to 20% downtime to learn something new or flex your skills a little. A water cooler for each group of desks that is regularly maintained. Market-competitive salaries with performance-related bonuses, where performance and the remuneration are clearly defined. Performance bonuses would likely be company profit share. Encourage a collaborative work ethic; have tech debriefs to share learning, rotate people around teams to build their experience. Free drinks (non-alcoholic). A fruit basket for healthy snacks that don't ruin lunch. Establish a level of professional respect from the other parts of the business for the software development department and vice versa. This is a long-term, fuzzy target, but there are ways and means of establishing it. Clear communication to and from management of expectations and delivery on those expectations. Clear priorities for work items, reviewed regularly. Use of best practices in terms of SDLC methodologies - Agile/Scrum, etc. Clear and documented procedures on what has to be done, why and how for important stuff like release management. Whatever can be automated would be, so this is just the manual bits - there's always some. Supportive environment for when things don't go so well. No kicking people when they cause bugs, but helping them learn from their mistakes. 24x7 access to the building and remote access for when team members get inspiration outside of normal hours. Whiteboards for prototyping/thinking out loud. Celebrations of success - whether a team lunch or a trip to the Grand Prix at the weekend, it's important to recognise great effort and great results.  I would not have:  Nerf guns/frisbees/pool table/toys. The work environment is where we work. There's lots of fun to be had while doing the job without playing soldiers around colleagues that are trying to focus. Free food - people should take a break to go out and get something to eat. Internet censorship - I'd leave it up to the individuals to exercise their judgement.  "
    },
    {
        "ID": "14727",
        "Question": "When your programming you often have to go and look at other pieces of code for reference, search for something etc. I'm looking for some numbers on the ratio of time spend actually typing the code(coding) and the time spend going back and forth between other parts of the system, not even counting browsing the internet, reading documentation. Ofcourse this depends on a large number of factors, probably also on your IDE, programming language etc. I just need some rough estimates, or researches on the subject. Background: I want to investigate how much network latency is tolerable for a programmer to work on virtual machines (if any).   ",
        "Best answer": " I want to investigate how much network latency is tolerable for a programmer to work on virtual machines (if any).   This is simple. I'm going to throw out the \"scrolling/navigating\" part of this question and go directly to the root here: typing vs. latency.  When I hit the key, I expect the character to appear immediately. Anything less than immediate is unacceptable. When it comes to typing, I expect results. I type something approaching 100 wpm. Latency negates that.  The biggest problem I have with latency when typing is this: if I misspell something, on a live computer I can backtrack and correct it at 100 wpm. So it's taken care of and corrected immediately. If I am typing with latency, then what happens is I misspell something and don't see it for a word or two. Now I have to stop, erase, retype, all slowly because of the latency. It just flat-out slows down the whole process of composition.  When it comes to typing latency, you just have to decide what you can reasonably live with.  "
    },
    {
        "ID": "14745",
        "Question": "I'm on the way of learning Java myself. I find most of texts giving emphasis to Java applets. I got confused about the importance. Is it something widely used?  Do I need to spend more time on it? ",
        "Best answer": "Applets aren't used widely these days, so the knowledge won't be that useful.  That said, there is little in Java that is applet-specific.  It makes no difference if you learn AWT and Swing by writing applets or by writing desktop applications.  It's fairly easy to write an application that will also function as an applet. The main distinction is that you use a different top-level container for applets than for applications.  Unsigned applets also have some security restrictions, particularly around IO and networking. "
    },
    {
        "ID": "14914",
        "Question": "Other than title and pay, what is the difference?   What different responsibilities do they have.  How knowledgeable/experienced are they? What is the basic measure to determine where a developer fits into this basic structure?  ",
        "Best answer": "This will vary but this is how I see it at a place large enough to have distinctions between types of programmers. I would say entry level and Junior are the same thing. They are just out of school and have less than two years of work experience. They are assigned the least complex tasks and should be supervised fairly closely. Generally they know about 10% of what they think they know. Usually they have not been through the whole development cycle and so often make some very naive choices if given the opportunity to choose. Sadly many of them don't actually care what the requirement is, they want to build things their way. They often have poor debugging skills. Intermediate level is where many programmers fall. They have more than two years experience and generally less than ten, although some can stay at this level their whole careers. They can produce working code with less supervision as long as they are assigned to relatively routine tasks. They are not generally tasked with high level design or highly complicated tasks that require an in-depth level of knowledge. They may be tasked with the design of a piece of the application though, especially as they are in the zone to become a senior developer.  They are good at maintenance tasks or tasks where they can focus on just their piece of the puzzle, but are not usually expected to consider the application as a whole unless working with senior developers or being prepped for promotion to senior. They can usually do a decent job of troubleshooting and debugging, but they have to really slog through to get the hard ones. They do not yet have enough experience to see the patterns in the problems that point them to the probable place they are occurring. But they are gaining those skills and rarely need to ask for debugging help. They have probably been through the whole development cycle at least once and seen the results of design problems and are learning how to avoid them in the future. Usually they tend to be more likely to take a requirement at face value and not push it back when it has obvious problems or gaps.  They have learned enough to know what they don't know and are starting to gain that knowledge. They are the workhorses of the programming world, they deliver probably 80-90% of the routine code and maybe 10% of the very difficult stuff.  No one who is senior level even needs to ask this question. They are experts in their chosen technology stacks. They are given the hard tasks (the ones nobody knows how to solve) and often get design responsibilties. They often work independently because they have a proven track record of delivering the goods. They are expected to mentor Junior and intermediate developers. Often they are amazing troubleshooters. They have run into those same problems before and have a very good idea of where to look first. Seniors often mentor outside the workplace as well. They generally have at least ten years of experience and have almost always been on at least one death march and know exactly why some things are to be avoided. They know how to deliver a working product and meet a deadline. They know what corners can be cut and what corners should never be cut. They know at least one and often several languages at the expert level. They have seen a lot of \"hot new technologies\" hit the workplace and disappear, so they tend to be a bit more conservative about jumping on the bandwagon for the next exciting new development tool (but not completely resistant to change - those would be the older Intermediate developers who never make the leap to Senior). They understand their job is to deliver working software that does what the users want, not to play with fun tools. They are often pickier about where they will work because they can be and because they have seen first hand how bad some places can be. They seek out the places that have the most interesting tasks to do. Often they know more about their company's products than anyone else even if they have been there only a few months. They know they need more than programming knowledge and are good at getting knowledge about the business domain they support as well. They are often aware of issues that juniors never consider and intermediates often don't think about such as regulatory and legal issues in the business domain they support. They can and will push back a requirement because they know what the problems with it will be and can explain the same to the laymen.  "
    },
    {
        "ID": "14956",
        "Question": "Often when I'm writing code to do a certain thing, I'm faced with either writing my own or using someone else's code. Assume here that this \"thing\" is something that I've never done before and am interested in learning how it's done. Which would you say is better from a learning perspective: try writing your own solution; or looking at code by someone else? I've always written my own code if I have an idea on how to do it, but resorted to looking at someone else's when I don't have a clue. I believe that the best is probably a combination of both: make your own attempt and then look at how someone else did. ",
        "Best answer": "First try to write your own.  Then look at someone else's solution. "
    },
    {
        "ID": "14968",
        "Question": "It's a fairly common adage that adding more programmers to a late project will make matters worse. Why is this? ",
        "Best answer": "Introduction overhead Each new developer has to be introduced to the code base and development process which takes not only the new person's time but also requires assistance from [a] senior developer[s] (guiding them through the build process, explain the testing process, help them with pitfalls in the code base, much more detailed code reviews, etc). Therefore, the more new developers you add to the project the more time has to be spent to bring them to a point where they can actually contribute to the project. "
    },
    {
        "ID": "14975",
        "Question": "I've gotten a bug report from one of my users in a section of the software. The scenario is basically a databinding scenario where the user enters info, and that info is printed to pdf. The problem is, that the functionality:  Is used frequently (about 40 times a week) Hasn't been updated/modified in months The area of code is relatively simple to walk through The validation appears fine (ie, if the information wasn't filled out in the app, validation fires indicating it with a msgbox before the pdf is generated)  But this one user claims that in the past 2 weeks it's happened about 3 times out of 50 and I just can't reproduce it. So what do you do in the case of a heisenbug? ",
        "Best answer": "Add some logging to this users code. "
    },
    {
        "ID": "15002",
        "Question": "This is in regards to Meta Programming System or MPS by JetBrains. Thus far, from my efforts to learn how to use MPS, I have only learned its basic purpose and that it is very complex. Is MPS worth learning? Is there anyone who already effectively uses MPS to create their own languages and editors for those languages and uses these created editors as their primary way of programming? If so, what types of programs have they made with this? What are the advantages and disadvantages of working with MPS? ",
        "Best answer": "While not on Java, I've been wanting to make my own meta-programming for a while on .NET (Irony(link 1) is a cool thing to look at), I think of using the M modelling language(link 2) once I learn it too.  I'd post this as a comment, but thanks to this rant I can't, so I'll try and throw a 2¢ while I'm at it).  If you think about it, a language is a tool to tell the computer what to do. Some languages are more specific than others, and let you express what you want in a much more concise way than others (for a particular subject). Others try to be broad and provide a consistent baseline from where to build upon. But these restricted, specific, and simplistic \"Domain Specific Languages(link 3)\" are great for the occasional cases where what you want is specific. In those cases, they save you a lot of typing and possible mistakes. For instance, think about how much time you'd waste if you had to use an Object-Oriented \"SQL API\" for Java (and how verbose that would be) instead of using SQL queries that are so short and convey the message so clearly. I'd love if more domain-specific languages were in use. I don't like XAML much (altough it is better than how UIs are coded in Winforms), but I see it as a step in the good direction. I'd like to, in a middle-term future, design DSLs for certain common tasks I do frequently, such as a simplified HTML annotation language that outputs hacked-for-IE code without me having to do the same things over and over (or maybe I don't do this, but that's not the point). tl;dr: I never used MPS, but I think if you're a Java guy by all means check it out, Java gets brains rusted after a while IMHO. How to learn it? Find something you'd like a language for (something repetitive and boring), and make a language for it! If it's useless, at least you'll have a new tool on the belt. LINKS (here due to rep)  http://irony.codeplex.com/ http://en.wikipedia.org/wiki/Oslo_(Microsoft_project)#Characteristics_of_the_.22M.22_Modeling_Language http://en.wikipedia.org/wiki/Domain-specific_language  "
    },
    {
        "ID": "15004",
        "Question": "I recently had a programmer in for an interview, who listed Python, PHP, Rails and ASP as a few of their skills. In the interview however, they interviewee didn't enough know what control structures and basic logic were, they had only followed a few demo tutorials. So my question is this: At which point can you add a technology to your resume accurately. Is it when you can demonstrate all basic concepts, write a useful program in it, or are just comfortable using it without having to refer to the documentation every 30 seconds. I don't believe this is overly subjective, a baseline should easily be established based on feedback. ",
        "Best answer": "You should be able to defend/explain each and every word you put in your resume. Kind of like you dissertation/thesis. I have seen many candidates rejected with the reason \"could not justify what he had put in his resume\". One approach is to follow Google's self questionnaire. Rate each skill on a scale of 10. That way we can project how relatively comfortable we are with various technologies.   1 means you can read others code with plenty of googling. 5 maybe for implementing modules in the technology. Etc.   8 for plenty of experience and comfortable with designing and implementing large projects in that technology.   9 for architectural knowledge with moderate understanding of what's under the hood.  10 means you have written a book on it or invented it.   I have seen resumes which have bar graphs indicating relative proficiency in various technology. Another option is to group skills as \"strong understanding\", \"moderate proficiency\" and \"familiar with\". Edit: I tried to put this as a comment, but didn't look due to lack of formatting. For a reference, here is what Google defines the rates in their Self Evaluation  0 – You have no experience 1 to 3 – You are familiar with this area but would not be comfortable implementing anything in it.- 4 to 6 – You are confident in this area and use it daily.- 7 – 9 You are extremely proficient to expert and have deep technical expertise in the subject and feel comfortable designing any project in it.- 10 – Reserved for those who are recognized industry experts, either you wrote a book in it or invented it.  "
    },
    {
        "ID": "15086",
        "Question": "This question asked how to auto update version number in VS 2010 :  https://stackoverflow.com/questions/4025704/publishing-in-net-produces-application-files-myapp-someversionnumber-how-to-ge The answer was,  You have to update both Assembly Version AND File version manually  Shouldn't the version number increment each time I publish?  Why should I do it manually?  Is this is a conscious decision by MS to do it this way? What's the thinking behind having people manually update their own version number? ",
        "Best answer": "You could always use the \"Build Version Increment\" open source add-on for visual studio to do it for you.  ...I've tested the addin with Visual Studio 2005/2008 on C#, VB.NET and C++.NET projects under Windows XP/Vista SP1. Functionality  Different auto increment styles can be set per major, minor, build or revision number. Supports C#, VB.NET and C++.NET projects. Not required to be installed by all project members. Configuration is shared via properties in the solution and project files. Developers who don't have the addin won't feel a thing. Automatically checks out required files if under source control. Can be configured per solution and/or per project. Can be configured to update only on certain configuration builds (debug, release, any or custom) Can update assembly attributes in an external source file instead of the default AssemblyInfo...   "
    },
    {
        "ID": "15096",
        "Question": "They both can be seen as centralized processing, but from a developer's perspective is it returning us to the days when access to computers you could program yourself were limited and expensive? To me that was the problem the PC solved. Heard this on http://thisdeveloperslife.com/ 1.0.6 Abstraction. ",
        "Best answer": "Computing is constantly swinging back and forth between centralized and decentralized architectures. I think it is simply a case of \"The grass is always greener\". However, I don't think that the centralized model (IaaS) is necessarily going to make access to computing power more expensive. That, in my opinion, was more a factor of how expensive the computing equipment was and not so much the purchasing model of computing power. "
    },
    {
        "ID": "15114",
        "Question": "I have a base class Shape, and drived classes Line Segment, Arc, B-spline,... I want to find the intersection point between these shapes. I would like: Shape* a = new LineSegment(); Shape* b = new Arc();  bool did_intersect = a->Intersect(b);  I don't like this design in Intersect: bool LineSegment(Shape* s) {   Arc* arc = dynamic_cast<Arc*>(s);   if (NULL != arc)     return LineSegmentArcIntersection(this, arc);    BSpline* bspline = dynamic_cast<BSpline*>(s);   if (NULL != bspline)     return LineSegmentBSplineIntersection(this, bspline);   ... }  So, What design pattern is best suitable with this context? ",
        "Best answer": "I think you are looking for double dispatch or multiple dispatch. Btw., this question is better suited for StackOverflow. "
    },
    {
        "ID": "15124",
        "Question": "Are there any services that 'for a reasonable price' will give and provide good and technical advice on applications. On a lot of projects, I'm usually the only developer, and sometimes, I think some of my work needs to be improved for efficiency, better MVC interactions, etc. It would be great if there was a professional service that actually can and will do such reviews ",
        "Best answer": "Find someone else who's an indie and needs their code reviewing, and buddy up with them. Sitting next to each other in an office or pub is best, but you can use online paste bin services and videoconferencing if there's no one in your town. "
    },
    {
        "ID": "15128",
        "Question": "One of the products I developed, is really a niche type application (2 or 3 other competitors in the market). Although for the most part, I'm the lone user of these applications, I'm thinking of selling the app right now. My question is:  Because of the small market share and that it's a niche project would you advise I open or close my application?  ",
        "Best answer": "An open source project without contributors is really bad. If the market is small, I doubt you will find any contributor. But I may be wrong. I don't know anything about your market.  You must do a market study.  If you can't afford it, try to release the software first. In free BETA to get more users, but you can do it as a paid edition too already. As soon as you get users, ask them questions directly (chat with them, don't waste your time with surveys). They will like that very much, and they will always help you to improve your solution. Ask them if they would contribute if you were open source. Analyze the results. Decide. "
    },
    {
        "ID": "15138",
        "Question": "I'm looking into Entity Framework for a new project that I'm going to be taking on and as part of my research into it I'm asking some industry professionals if it's stable and ready for 'real world' implementation. In the running is:  EF NHibernate DevExpress XPO  I already have a great deal of experience with XPO, but I'm not particularly happy with it.  ",
        "Best answer": "Yes, but with some caveats:  It's fully supported by Microsoft, and has a growing community--but being newer than its closest competetor, nHibernate, it still doesn't have quite as mature a community. Along with having a less mature community, there will be times where a feature is available with EF4 but barely documented; or EF4 will throw exceptions which Google can't help you with. It is full-featured when used as Microsoft intends, but in my experience it can be quite difficult to retrofit into an existing system.  Ideally you'll use it in a greenfield scenario with a 100% Microsoft stack.  It's certainly flexible enough to intermingle with other systems, but doing so increases the difficulty substantially.  However, to reiterate the main point, it is complete and stable enough for production use.  One key thing to point out, which seems obvious but is often overlooked until it causes pain, is that an ORM works to map from the relational paradigm to the OO-paradigm.  If either of these tiers doesn't follow the rules of its respective paradigm then you'll feel extra hurt. This can go both ways--if you're well versed in the relational/set-based paradigm of SQL and OOP then the ORM will let the two intermingle like butter.  If your database looks like it wants to be OO, and your OO-code looks like it wants to be record-based, then YMMV. "
    },
    {
        "ID": "15158",
        "Question": "While trying to answer questions on SO site (particularly C++ questions) I find that most of the times one person or the other includes a quote from the standard. I myself have hardly read the language spec. Even when I try to read it, I can't get my head around the language used in the specs. So, my question to become a good developer in a particular language is it essential to read its spec and know all its dusty corners which invoke undefined/implementation defined behavior?  ",
        "Best answer": "I would say that knowing the spec inside out is not a requirement to knowing how to program in C++ or even to become a excellent C++ programmers. I would say though that being able to find information from the spec is important, if there is a part of the language that you are using that isn't very well documented. It is also important knowing where your compiler doesn't meet the spec if you want you code to protable between compilers. "
    },
    {
        "ID": "15207",
        "Question": "Are there any recommendations for book(s) that are largely agnostic (with examples of MQ implementations is fine) on vendor but go to great details on the architecture, management, nomenclature, atomicity, durability, patterns, and logical deployments on Message Queue systems? Surely there are enough shared concepts between MQ Series, MSMQ, SysV IPC (OK, that might be stretching it), RabbitMQ, &c &c? ",
        "Best answer": "Have you tried Enterprise Integration Patterns? It's not exclusively on message queues, but on messaging systems.  The book Enterprise Integration Patterns provides a consistent vocabulary and visual notation to describe large-scale integration solutions across many implementation technologies. It also explores in detail the advantages and limitations of asynchronous messaging architectures. You will learn how to design code that connects an application to a messaging system, how to route messages to the proper destination and how to monitor the health of a messaging system. The patterns in the book are technology-agnostic and come to life with examples implemented in different messaging technologies, such as SOAP, JMS, MSMQ, .NET, TIBCO and other EAI Tools...  "
    },
    {
        "ID": "15241",
        "Question": "The saying \"It's easier to beg forgiveness than ask permission\" seems pretty popular among programmers and IIRC was attributed to Grace Hopper.  In what situations is this typically true, and why do you believe that such a counterintuitive proposition would hold? ",
        "Best answer": "I think that one important reason is responsibility. By asking for permission, you are transfering the responsibility to the person you are asking, so that person might be inclined to deny just to avoid being held responsible for the result, in case of failure. On the other hand, once it's done, that's no longer a problem. Even if the result was a failure, it's still your responsibility, no matter if you get forgiveness or not. "
    },
    {
        "ID": "15247",
        "Question": "What's next in the web industry, as social media reaches a plateau, what are the next milestones in this \"experiment\"? I guess I'm looking for in depth opinions on what's next in our beloved industry? What's beyond facebook, wikis, collaboration, shiny buttons, and interactive instantaneous communications?  What will drive the next dot com boom?  ",
        "Best answer": "Kevin Kelly gave a tedtalk on the subject, titled \"Predicting the next 5,000 days of the web.\" He has some interesting theories. (The projections start about eight minutes into the video.) "
    },
    {
        "ID": "15261",
        "Question": "I have been looking at RentACoder, Freelancer and eLance from last few years but I always hesitated to register at these site. Hesitation includes little worry too. These questions fill me: (1) Is there a legal contract between the bidder and the coder? Just in case my delivery date extends, what happens? (2) After delivery, payment reach without trouble or not. (3) In case the customer didn't find the project useful, then? Probably he might say that it is not useful to me and you return money. He might copy the source code and return another copy to me. (4) What are further recommendations? How should I start? ",
        "Best answer": "As with anything, you need to use common sense. The odds are somewhat stacked in favour of the buyer, but if you work with buyers that have a higher reputation, then you'll usually be fine. If the buyer has never awarded a job before, you need to be careful that the job they're posting is reasonable and they're not asking you to re-write Facebook for < $500 or something silly like that. You also need to remember that you're generally competing with teams of people in developing countries who will be happy working for 1/10 the rate you would expect. The regular buyers will have come to expect those rates and if your bids are significantly higher, your bids will be discarded out of hand. Having said that, these websites can be a good way to get your name out there on a couple of projects. But don't expect to be able to make a living exclusively from them... "
    },
    {
        "ID": "15269",
        "Question": "I feel that side effects are a natural phenomenon. But it is something like taboo in functional languages. What are the reasons? My question is specific to functional programming style. Not all programming languages/paradigms. ",
        "Best answer": "Writing your functions/methods without side effects - so they're pure functions - makes it easier to reason about the correctness of your program. It also makes it easy to compose those functions to create new behaviour. It also makes certain optimisations possible, where the compiler can for instance memoise the results of functions, or use Common Subexpression Elimination. Edit: at Benjol's request: Because a lot of your state's stored in the stack (data flow, not control flow, as Jonas has called it here), you can parallelise or otherwise reorder the execution of those parts of your computation that are independent of each other. You can easily find those independent parts because one part doesn't provide inputs to the other. In environments with debuggers that let you roll back the stack and resume computing (like Smalltalk), having pure functions means that you can very easily see how a value changes, because the previous states are available for inspection. In a mutation-heavy calculation, unless you explicitly add do/undo actions to your structure or algorithm, you cannot see the history of the computation. (This ties back to the first paragraph: writing pure functions makes it easier to inspect the correctness of your program.) "
    },
    {
        "ID": "15286",
        "Question": "I come from a scientific and embedded programming background, and I have had minimal experience with web programming.  What would be the best approach to take to get up to speed with web programming?  Tools and framework suggestions? One approach would be to dive into learning a framework, such as Rails. I started doing this with rails tutorial, but I find that the framework abstracts so many important concepts that I should be learning. To sum up, experienced programmer wants to learn web-app programming. ",
        "Best answer": "Few years back I asked this question to myself! This is what I find easy and organized way to start web programming, you can skip steps which you have already know    To learn web programming, first you have to know  What is a website What are the main role players [Webserver, Mark-up languages, Client side scripting, Server side scripting, Protocols (http), Browsers      Trace complete server round trip i.e. from typing google.com in browser and to loading the complete page.   Why http is stateless? Role of session to overcome this?   Start learning html & little JavaScript    Basic tags Marquee :-) Alert, change color page background color via javascript etc Have some fun playing around with html, javscript and css    Server side scripting   Start with php  Use all necessary input – type elements and create forms Validate form with plain javascript Retrieve submitted form data and display it via php   I think you can complete 1 to 5 quickly.  Its interesting part for all novice web programmers, because of the visual excitement they get while using html and css first time/ Then move to serious stuff!!!   At this time, you know fundamental things of web programming and working of website. Now, it’s your responsibility to choose most appropriate language, platform and framework. No one here can’t help you with this; You have to consider your personal interests and future plans to decide.   My recommendation is to go with php, since you learned it in initial stages.   Next, is databases a.  Learn how to connect database  b.  Basic sql queries. Select, insert, update and delete c.  Manipulate user inputs using database Now, start creating a personal website; or any simple website Download any open source website and learn from it.  Here are few references, which may help you  1. W3 Schools – for learning basics of html, css, JavaScript, asp, database queries  2. Php.net– for everything about php  3. For exploring open source projects   - http://bitbucket.org/   - http://github.com/   - http://www.codeplex.com/   - http://sourceforge.net/  Always remember that there are several peoples here for help you; if anything happen, post it in stackoverflow.   Find someone with some amount of web programming experience to guide you; it’s always easy to learn from experienced programmers.   Do not forget to find some books too… for a starter you can checkout dummies  All the best!!! "
    },
    {
        "ID": "15292",
        "Question": "For me I am C# and Java person. I find these 2 languages are quite similar so its easier for me to crossover each other. I'm not sure if its good idea to pick up other different languages. Any thoughts? ",
        "Best answer": "It's not about how many languages you learn. It's about learning new concepts. Some languages might be able to teach you many concepts, while others might be able to teach only one. If you know C#, you probably wouldn't learn anything terribly new by learning Java. But you would if you learned Haskell. So when you pick a new language to learn, pick something that will teach you concepts you don't already know. "
    },
    {
        "ID": "15317",
        "Question": "What are the risk factors that we need to consider while planning for a software project. ",
        "Best answer": " Is your team adequately trained? Is your team large enough?  Do you have contingency in case someone leaves the project, and how would it affect the schedule?  Is your team too large?  Do they have the resources they need? Might a competitor bring a product to market before your project completes? Can you deal with changed requirements?  Can you deal with the project becoming irrelevant?  Do you have buy-in from senior management?  Do you have any reliances on suppliers or contractors?  Are you doing anything in-house that your team isn't competent enough at?  Do you have budget big enough to meet the estimated project cost? Can you meet unforeseen project costs? And anything that's specific to your circumstances :-)  "
    },
    {
        "ID": "15321",
        "Question": "If you've just been introduced to a new project, what's the first thing you look for to get an idea of how it works? Do you look for the design first?  If there is a design, what do you look for in it? Class diagrams or deployment diagrams or sequence diagrams or something else? Or do you go straight for the code? If so, how do you understand how the different layers interact? ",
        "Best answer": "I start with code. Separate design documents, if there are any, are as likely to be wrong or misconceived as not. So, i start by trying to trace some simple flow through the code; if it's a webapp, it could be a request or a sequence of requests, for instance. Once i've done that, i have a sort of skeleton to hang more understanding on. Then, i might go back and read designs or other documentation, but at that point, i have something concrete to relate them to, and to validate them with, so i can detect duff information. Or i might just carry on reading code, or test cases, etc. "
    },
    {
        "ID": "15360",
        "Question": "I read this answer and found a comment insisting not to send password by email:  passwords should not be able to be retrieved by email, I hate that. It means my password is stored in plain text somewhere. it should be reset only.  This raises me the question of handling Forgot Password option? At any cost the raw password must be displayed in any UI so that user will be able to read it. So what would be the way to handle \"Forgot Password\" ",
        "Best answer": "A good application design will not be able to explicitly recover a users password.  This is because it is usually stored after it is run through some sort of hash which is a one way operation.  The best way to handle lost password is to perform a reset, email to the users account a link with a generated parameter tacked on that identifies this as a valid password reset for the account in question.  At this point they can set a new password. This does assume you have a users email address on file.   "
    },
    {
        "ID": "15379",
        "Question": "I have been developing on Android for a couple months now, and building upon one in house program I found the scalability to be lacking. I have that gut feeling that if I applied some OO Design Patterns I would get an overall better program in speed, scalability, and readability. Do any of you StackExchangers have ways you've applied a/many design pattern(s) that just seem to fit extremely well with the way Android works? ",
        "Best answer": "I use MVC pattern when developing for Android/Windows Phone 7.  Android Views lend themselves pretty well to it and is a great help when keeping the separate. Android code can get pretty complicated tossing views and intents around, so it helps to keep things as compartmentalized as possible. "
    },
    {
        "ID": "15397",
        "Question": "Am I wrong if I think that Python is all I need to master, in order to solve most of the common programming tasks? EDIT I'm not OK with learning new programming languages if they don't teach me new concepts of programming and problem solving; hence the idea behind mastering a modern, fast evolving, with a rich set of class libraries, widely used and documented, and of course has a \"friendly\" learning curve programming language. I think that in the fast evolving tech industry, specialization is key to success. ",
        "Best answer": "Yes You would be wrong to think that any single language can do everything without considering other languages.  I think that in the fast evolving tech industry, specialization is key to success.  I can't think of a better way to shoot yourself in the foot. Learning and mastering a language is great, but you mention you aren't comfortable with learning new languages if they \"don't teach me new concepts of programming and problem solving.\"  Languages don't teach you these concepts, thick dusty tomes and many years working closely with people better then yourself does. These basic patterns, concepts, and principals of design transcend languages and can be applied in many different scopes. "
    },
    {
        "ID": "15466",
        "Question": "This StackOverflow question asks \"where can I get Microsoft.Data.Objects\" It turns out the answer was probably that its in the CTP4 (code first) release of Entity Framework 4 However there where a lot of guesses.  Including   System.Data Entity Framework Microsoft.ApplicationBlocks.Data  Microsoft.Practices.EnterpriseLibrary.Data  10 years ago if someone asked a similar question for they might have gotten DAO, RDO, ADO. Is this just the nature of the beast or is it MS. Does this pattern happen with other vendors? Where the base data access strategy is either wrapped or changed? ",
        "Best answer": "To be fair all of the ones you mention are built on top of ADO.NET. Before that ADO was the favored route for a while but DAO just sort of hang around because it was native for Microsoft Access Databases. RDO was dead on arrival from what I can tell. With all the different frameworks you mention I think the problem is that they are trying to give a solution for everybody and to compete with every other platform. If you want a simple way to just use SQL in your code then go for System.Data. If you want an ORM using Entity Framework. For something in between then use Enterprise Library Data. Everyone wants something different. There is also the issue that MS is a very big company with different teams with different agendas. For example why do they also have 3 word processors (that I know of). "
    },
    {
        "ID": "15515",
        "Question": "Often when I write a functions I want to make sure the inputs to it are valid in order to detect such errors as early as possible (I believe these are called preconditions). When a precondition fails, I've always thrown an exception. But I'm beginning to doubt whether this is the best practice and if not assertions would be more appropriate. So when should I do which: when is it appropriate to use an assertion and when is it appropriate to throw an exception? ",
        "Best answer": "Assertions should only be used to verify conditions that should be logically impossible to be false (read: sanity checks). These conditions should only be based on inputs generated by your own code. Any checks based on external inputs should use exceptions. A simple rule that I tend to follow is verifying private functions' arguments with asserts, and using exceptions for public/protected functions' arguments. "
    },
    {
        "ID": "15527",
        "Question": "How often do you or have you seen meta-programming be used in projects? In uni I've never seen this be applied, but I've seen this on my previous job (when I saw it I was blown away of how effective it was). But how common is it? Is it used all the time, or just occasionally? ",
        "Best answer": "If you consider reflection a kind of meta-programming, it is relatively common. For some people even generic programming (templates and generics) is a form of meta-programming, so that's even more common. In my opinion, however, meta-programming is something more complex, that involves actual code generation, and therefore quite uncommon, even in scripting languages. "
    },
    {
        "ID": "15528",
        "Question": "I have a fully configured mantis bug tracker for tracking issues in apps that I create.  When an user is disciplined and goes straight to mantis to write a issue report, he/she will have fastest response and everything regarding the issue will be very easy to track. However, not everyone is keen to do so.  They report their problems via phone, e-mail, don't report them at all. What would be the best way to nudge them towards the using a bugtracker system?  Clearly, they HAVE to see some immediate benefits so they can return and look for more benefits. EDIT: I am talking about support for the products that I sell as an ISV. ",
        "Best answer": "Your bug tracker is for your convenience, not your customers'. If you can't be bothered to take their phone or email issue and enter it yourself, how do you think they feel? You need to be able to enter issues and assign them manually to a client. Then when they call in with an issue you can say, \"Thanks for reporting that! I'm going to enter it into our issue management system, and you'll start getting email (or whatever) as we deal with it. In the future, if it's easy for you, you can enter that sort of thing right there. Or feel free to just call me, that's fine too.\" One of the best such systems I've worked with as a customer is the one at the hosting provider I resell. Email to support@ gets parsed for a domain name in the subject line, assigned to a client account based on the from address, and auto-entered into their ticket system. Pretty slick. "
    },
    {
        "ID": "15556",
        "Question": "I'm kind of horrified that people actually suggest using UTF-8 inside a std::string and are OK with UTF-16 std::wstring. Do you actually do this? Do you think that it is OK? ",
        "Best answer": "Well, let's say that many programmers still don't know that UTF-16 is a multibyte encoding (they still think that 16-bit wide char is enough to represent all the Unicode characters, but actually they are stuck with the old UCS-2). However, there's no real drawback in using a wstring for storing UCS-16 text, but you should know that its length doesn't necessarily represent the number of text symbols that will be printed. "
    },
    {
        "ID": "15570",
        "Question": "I know it is expensive but (IMO) I believe it is a very good practice. I'm talking about rules like say, you can't save an Invoice if you are not a sales person... so in that case throwing an exception saying 'your are not authorized' or such... Another approach it would be having objects with a status or something like that  Is there any other approach? how do you feel about it? ",
        "Best answer": "If you mean representing individual business rule checks with exceptions, then I don't think it's a very good idea. Many times you have to report more than one failed condition, and not stop on the first one. On the other hand, I do believe that checking for all rules and then throwing an exception with the summary is a good practice. "
    },
    {
        "ID": "15610",
        "Question": "In the Java world, we often talk about the JVM, and when Java was new it had the supposedly killer feature of \"Write Once, Run Anywhere.\" From the way people talk and write, this seems different from the way that Python, for example, works.  Yet I've not been aware of any way that Python code that I've written would work differently on another machine.  (Though I haven't written that much Python.) So what am I missing?  How is the JVM different from a Python interpreter?  Is there a way that Python lacks Java's platform independence?  Or is this just a cultural difference? ",
        "Best answer": "Java does a very good job on isolating you from the underlying OS and gives you the same exact tools on most platforms it works on to talk to things in the underlying OS.   Python on the other hand does not do as good of a job in isolating you from the underlying OS, It does not have a standard way of handling between process communications (look at the differences in the sys module and os module between a windows and *nix implementation of Python for example.)   I have written code in python that would only work on a *NIX box or Windows box using just Python available API calls, where in Java it would be very difficult to write code that was just Java API that would not work the same on both a Windows box or *NIX box "
    },
    {
        "ID": "15623",
        "Question": "When it comes to \"interview test\" questions, the subject of FizzBuzz often comes up. There is also a Coding Horror post about it. Now, if you bother reading sites such as this, you are probably less likely to be in the demographic of programmers who would find FizzBuzz anything but trivial. But is it really true that 99% of programmers will struggle with it? Really? What is the evidence to back this up? Some real-life examples would be very helpful in answering this question. ",
        "Best answer": "99%? No. A significant percentage? Yes. From my own direct experience of interviewing people I can testify to this one. It might seem insignificant to you but there are a lot of people in the programming field who have more or less faked their way through for years and apply on non-entry level positions and fail this one. Even if you CAN easily solve it, but you give me huge static about being asked to do such a menial task will count against you. Being on a team means having to sometimes do things you might not enjoy but are necessary. If right off the bat, before we've even started to work together you think it would be best to try and assert your special status of being above doing something I've asked you to do then it will act as a mark against you. I don't care necessarily how elegant your solution is (although that would be nice) but seeing you take a stab at it on a whiteboard and talking your way through it shows me that you're at least willing to take a stab at it. If you get indignant and say something along the lines of \"I'm a problem solver, not a code monkey!\" then you will be knocked down a peg. I have had interviewees just flat out refuse to even begin to attempt it. Just simply refuse. No. Uh uh. Won't do it. I ask one or two more polite questions, thank them for their time and close the interview off. I say this as a manager and as a developer. "
    },
    {
        "ID": "15628",
        "Question": "How can I maintain proficiency (if not expertise) in multiple languages? In any large organization it seems necessary to know more than one language.  For example, this week I've looked at if not written code in the following languages.  VBA C C# Java  Thankfully they're all imperative languages, or I would have lost my mind.  I struggled for a day or so with the C as it's been a while (read: years) since I've used it seriously. What tips, habits, and training is there for keeping up your knowledge in so many languages? ",
        "Best answer": "Well, use them regulary. For example, I'm not doing much in C now, but whenever I need a little one-off program that reformats text files or whatever, I do it in C just to keep in touch with that language, even if the same program in Java would be more beautiful/shorter/easier to understand. "
    },
    {
        "ID": "15636",
        "Question": "What should be in a good (read:useful) coding standard?  Things the code should have. Things the code shouldn't have. Should the coding standard include definitions of things the language, compiler, or code formatter enforces? What about metrics like cyclomatic complexity, lines per file, etc?  ",
        "Best answer": "A reasons for every requirement.  This way, following the standard doesn't become some sort of cargo cult and people know that it's ok to change the standard if the reason for it no longer applies, or to violate the standard in specific cases where the reason clearly doesn't apply. "
    },
    {
        "ID": "15673",
        "Question": "What's so unique about it? Heard great things about it, used it, found that beside the Google Apps Integration there's nothing really special about it. Maybe I just don't get it?  So, what's your reason to use basecamp? ",
        "Best answer": "To me, the biggest draw has been the simplicity, which leads to flexibility. The concepts that Basecamp helps to manage are easily understandable, and there's very little configuration to worry about. (Have you ever tried to use a tool like JIRA? Even its configuration has configuration.) The simplicity starts to pay dividends when you are composing project teams of varying technical experience. The basic nature of to-do list items and milestones are easily understandable by most anybody. "
    },
    {
        "ID": "15681",
        "Question": "What tools are necessary when developing efficiently with Silverlight 4? VS 2010 is a gimme, but what version? Is Pro enough? Premium? What about Expression Blend/Web/Both?  When considering VS 2010, premium comes with an MSDN subscription but it's at the high end of the budget. It makes sense if Expression comes with it though. It's for a one man development show, making an LOB app that will integrate video/audo and mic/webcam equipment. ",
        "Best answer": "You can actually develop Silverlight applications with the Web Express edition of Visual Studio so the Pro version would work to. I'd also recommend getting Expression Blend - if only to get access to the options for copying and modifying style templates. "
    },
    {
        "ID": "15736",
        "Question": "A lot of people in the coding community talk about continuous improvement, deliberate practice and so-on - but when they talk about their current practices it's almost like they 'sprang fully formed from the loins of Zeus' because you don't hear about how their opinions changed over time or what they learnt recently. Every now and then though I go to a talk, or read a book, or talk to someone and they open up a bit more and I find that I learn a lot from these insights. So, if you had to pick one thing from the last 12 months that you learnt what would it be? ",
        "Best answer": "I learned that it takes only one rotten manager to spoil the whole project, but it takes lots of good programmers to clean up the mess afterwards. "
    },
    {
        "ID": "15742",
        "Question": "From my experience in the current working environment, one's professional development is definitely in the hands of the individual.  As I see it there are several routes to the same objective based on cost, time and availability   External Offsite training Online Training providers (itunes U,Pluralsight etc)  Books  Specialist /user groups   Specialist web sites (Channel9, stackoverflow,dnrtv,codeplex etc)  What would you consider to be the best approach (blend) to continued learning and maintaining a professional standard of work? ",
        "Best answer": "However you learn, you should always think about works for you. What works for me may not work for you, and vice versa. Here's what works for me:  Always learn from more than one source. Always put into practice what you're learning whilst you're learning (if that isn't possible then as soon as you can afterwards). Try not to learn about something in isolation. Sometimes you can't avoid it, but try to understand the context in which things are done and alternative approaches. In particular try to understand the consequences (good and bad) of doing something a certain way.  Here are the sources of information that I find valuable in order of usefulness to me.  Books. For me these work really well, you're presented with a coherent (hopefully!) block of knowledge. They're easy to use, cross-reference and if you're not intending to read it cover to cover you can often pick up extra knowledge by page flicking (something I find missing from electronic resources where I find that sometimes I fall into the trap of going looking for the answer I'm expecting). Colleagues. Probably the best way to learn about something is to work with someone who knows the topic well and is willing to share that knowledge. It can also be useful to learn with someone else who also doesn't know much - but beware of jointly coming to the wrong conclusions and developing bad practices. Online Resources. Books can never keep up with the speed of development and discussion in the world of software development. If you want to keep up to date you need to read blogs, mailing lists and so on. You'll also got detail on topics that either aren't mainstream enough or are too narrow to have had a book out yet. Real-world Meetups. These are things like conferences and user groups. I think these are often nice introductions to new topics and good for confirming what you already think/know about - but don't expect really in depth coverage unless the event is extremely focussed. Where the real value for me is that you often get a lot more honesty than you get from books and blogs, especially during the Q&A part. Formal Courses. A lot of people sneer at formal courses - I suspect that's because they've not been to really good ones. Formal courses are really good when you need to get up to speed with something really new really quickly. Providing the person delivering the course knows their stuff you will also get a good feel for the thinking behind something not just the technical part.  Self-taught people (and all good programmers will be mostly self taught as soon as they're more than a couple of years out of the last course they did) tend to pick up bad habits without realising it because what they're doing has worked in the context that they're in. That knowledge tends to not hold up when put into a different context. Sometimes it doesn't matter, you just recalibrate what you've learnt, but sometimes it does. Sadly good courses are rarely cheap and often aren't associated with any sort of certification - so picking a good course is both hard and (financially) dangerous so you could end up doing a rubbish course and having paid through the nose for it. Still, I recommend that people do a formal course every now and then to supplement self development.  "
    },
    {
        "ID": "15776",
        "Question": "I personally use CodeRush in Visual Studio 2010 to do refactoring, write code faster with templates and generally navigate my code 10 times faster than stock VS.  Recently, I've been working on another Android app and got to thinking...What are the top productivity plugins for Eclipse? Preferably free. I'm looking for plugins that help write in Java, not PHP or Rails or any of the other languages Eclipse supports. ",
        "Best answer": "Mylyn is a very widely appreciated plugin for Eclipse, and is available on the main Eclipse site now. It can watch the pieces of code that you work on together (for example, when changing \"tax calculation\" code, you tend to use the same five files) and then emphasize them the next time you work on the same task. It's a great way to undo the \"information overload\" you get when working on a large project. FindBugs for Eclipse will help you save time by analyzing your source code for potential Java bugs. It has a false positive rate, and you wouldn't want to run it each build, but it's a great process to go through. Eclipse's own refactoring and navigation features will save you time as well. My favorite feature of the JDT is the \"Quick Fix.\" When you have an error in your source code (you can use Control-Period to navigate to it), simply do a Control-1 for the Quick Fix operation. It will give you a list of ways to fix the error. For example, if you write a = foo(s), but a is not declared, one of the Quick Fix options is to \"declare a\". Eclipse will look at the return type from foo and use that for a, automatically adding any imports. With this style, you will find you write code with errors intentionally, because the Quick Fix route is faster! My other favorite Eclipse shortcut is \"Expand Selection To->Enclosing Element\" (Alt+Shift+Up). This takes where your cursor is and then selects the element of the parse tree you are on. When you do it again, you move further up the parse tree. This is great, because you can select an entire expression easily, not having to worry about selecting the code before or after it. That makes it much easier for you to have a valid expression in order to perform the \"Extract Local\" refactoring. JUnit is indispensible if you are writing unit tests, and it's well integrated with the environment and process. If you do any work with GWT, then Google's GWT Eclipse plug-in is nice. You can even use it for working with any Tomcat application, not just a GWT one. All of these tools are available free. "
    },
    {
        "ID": "15781",
        "Question": "For applications targeting multiple platforms, I see mainly two development approaches-  Go for some JAVA-like development platform. Have one code solution and let the intermediate runtime handle different platforms. If something goes wrong in any platform, tweak the code a bit. But keep it same for all. Make modular code separating core logic and UI. Develop separate UIs for respective platforms which will be calling same core libraries. Build the application separately for each of the target platforms.  So, which one to follow? I know, the answer will start with \"It depends\". But I want to hear your opinions on these approaches and the factors to be considered to choose any of them. ",
        "Best answer": "Current Oracle/Apache/Google squabbles aside, it's still hard to beat the JVM for this purpose. It's really very high quality on most platforms, universal, and you have a good number of languages to choose from (Java, Clojure, Scala etc.). It lets you target a single machine architecture (the VM), and not worry too much about the specific end-user hardware. That said, there are certain application types it may not be as suitable for: low-level networking comes to mind, as does heavy graphics/video processing.  "
    },
    {
        "ID": "15819",
        "Question": "What is the best way to choose colors for LoB app, i need to style TreeView, dropdown's, textboxes and a pivot grid, so i need a bunch of colors, tried Kuler but it gives only 6 colors.  So the question is how to do it without designer? PS. It's a silverlight app. ",
        "Best answer": "Talking as an ex-graphic designer - ask yourself carefully if you really need more than 6 colours. Using too many colours is likely to make things look like a mess. There is a reason why tools like Kuler don't suggest large numbers of colours. If you absolutely require (which IMHO is unlikely) more than 6 then use them as base colours and make lighter/darker variations (something like this or this online tool will do this - a google for colour scheme or colour wheel and you'll find plenty of tools like this out there). "
    },
    {
        "ID": "15839",
        "Question": "End-User Programming is where regular end users, i.e. non-programmers, are able to customize a program, or create a new one, with minimal training or instruction. App Inventor for Android is one recent example, but perhaps the most popular example of end-user programming is the spreadsheet. What examples of end-user programming have you seen used, or believe to be useful? What's the philosophy of the system? What challenges remain? ",
        "Best answer": "Any domain-specific language worth it's salt ought to be considered a viable example of end-user programming, since that is its primary objective...to be familiar enough to the user that they don't have to be a software engineer to use it. "
    },
    {
        "ID": "15842",
        "Question": "I am thinking about creating a silverlight application, and I lack the skills to create a good looking UI. Today's graphic designers usually know HTML and CSS and thus save me the trouble of doing something I am not very good with. Is this the same case with XAML? Do I have to hire two employees for this job? ",
        "Best answer": "Silverlight is a pretty cool technology, but I'm seriously concerned about its future. However, if you want a cool UI done in XAML... you have several options.   Hire a Silverlight/WPF dev and hope they also design / See #3 Hire a UX designer with XAML skillz Hire a great graphic designer and then hire #1 OR you can use the built-in Ai/PSD to XAML tools in Expression Studio (design).  Tons of options, if you are a small company you may even qualify for Bizspark ( a free version of Expression Studio).  Good luck. "
    },
    {
        "ID": "15874",
        "Question": "Let's say one works in a hypothetical company that has several developers that rarely worked together on projects and the Boss didn't believe that code reviews are worth the time and cost. What are various arguments that could be presented in this scenario that will portray the benefit of code review?  Furthermore, what are the potential arguments against code review here and how can these be countered? ",
        "Best answer": "If you have to justify yourself for such basic stuff, you have a bigger problem. You are the expert, your team should decide what practices you use. Maybe you should start to convince your boss of that very important principle.  Your boss is supposed to decide WHAT to do and more importantly WHY doing it. You should take care of the HOW build it  (that doesn't means you can't suggest what and why do things in your company of course). A great boss should encourage his employees to participate in enterprise strategy) However here is how I view peer code reviews: Because programming is a very intensive intellectual work, one person can't ensure everything is perfect. Therefore code review ensure that:  vulnerabilities or bugs are found before the app is shipped constant mutual education between developers (almost for free) is achieved code respect standard for easier app maintenance code match the requirements  Everyone is taking direct benefits of it:  the developer that increases his/her knowledge and can pass his own to his/her team mates the customer/user that has less bugs and spend less in maintenance the boss that has more happy customers/users and spend less in trainings  "
    },
    {
        "ID": "15925",
        "Question": "Usually when talking about coding standards we refer to the code of the program itself, but what about the unit tests? Are there certain coding standards guidelines that are unique to unit tests? What are they?  ",
        "Best answer": "Roy Osherove recommends the following pattern for naming your tests: NameOfMethodUnderTest_StateUnderTest_ExpectedBehavior()   See http://weblogs.asp.net/rosherove/archive/2005/04/03/TestNamingStandards.aspx "
    },
    {
        "ID": "15928",
        "Question": "Can any one explain about the agile methodology in simple sentences? ",
        "Best answer": "I think nothing puts it better than the Agile Manifesto itself: We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:   Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan That is, while there is value in the items on the right, we value the items on the left more. from http://agilemanifesto.org/ "
    },
    {
        "ID": "15930",
        "Question": "Where I work there are a few experienced software developers with a software background, but the majority of developers are physicists or chemists with excellent domain knowledge but limited experience when it comes to developing high quality, maintainable software.  To address this we have started running regular talks and workshops. What topics do you think we should discuss to help make these people more effective software developers? In particular we are struggling to gain enthusiasm for these talks as many developers do not see software as an interesting subject.  How could we make these more interesting to people without a software background? Thanks ",
        "Best answer": "If these chemists and physicists are not primarily profesional developers, and are not intended to become so, I would suggest thinking differently at the problem. The \"real\" developers should provide easy environments for them to develop into.  You should provide mentoring and you should provide peer review of their code with strong incentives to make the code good enough for passing in the first place. In other words, do not treat them as equals, but provide all you can for them to excel on what they actually do that is their strength. "
    },
    {
        "ID": "15949",
        "Question": "There are many software development methodologies - SCRUM, agile, XP, etc. - and they all have there advantages and disadvantages I suppose. But when do we really need to apply them? Surely they are not necessary for small 1-man projects, but for large 50+ teams you can most certainly not go about ad-hoc:ing the whole thing. So what's the line for when to use and when to not use such a methodology, if there indeed is one? ",
        "Best answer": "Even in a strictly 1-man project (you coding software for yourself with no schedule) you have to:  Figure out what you actually want/need. Figure out how it can be done (various approaches). Implement it. See what you got and go back to 1), refining the requirements.  You could do it informally (cowboy), but given that the 1-man project is a special edge case (usually there's at least you and someone else you're working for), doing it with some well founded light formalism is virtually always preferable. Keep in mind that the core of the Agile Manifesto is really just a few principles. The formal methodologies (e.g. Scrum) aimed to reach those principles can and should be tailored according to the team size etc. "
    },
    {
        "ID": "16016",
        "Question": "What is the difference between update and upgrade in the context of application software? ",
        "Best answer": "Depends entirely on the installation technology, company developing the software and the whim of the person using the terms.  Generally though, updates stay within a product version (for example, hotfixes), while if you want to move to a later version, you would upgrade. So you might install an update (hotfix) for Office 2007, or you might upgrade to Office 2010. This page gives the definition according to Windows Installer: http://msdn.microsoft.com/en-us/library/aa370579(v=VS.85).aspx "
    },
    {
        "ID": "16025",
        "Question": "When I started using an object-oriented language (Java), I pretty much just went \"Cool\" and started coding. I've never really thought about it until only recently after having read lots of questions about OOP. The general impression I get is that people struggle with it. Since I haven't thought of it as hard, and I wouldn't say I'm any genius, I'm thinking that I must have missed something or misunderstood it. Why is OOP difficult to understand? Is it difficult to understand? ",
        "Best answer": "I personally found the mechanics of OOP fairly easy to grasp.  The hard part for me was the \"why\" of it.  When I was first exposed to it, it seemed like a solution in search of a problem.  Here are a few reasons why I think most people find it hard:  IMHO teaching OO from the beginning is a terrible idea.  Procedural coding is not a \"bad habit\" and is the right tool for some jobs.  Individual methods in an OO program tend to be pretty procedural looking anyhow.  Furthermore, before learning procedural programming well enough for its limitations to become visible, OO doesn't seem very useful to the student. Before you can really grasp OO, you need to know the basics of data structures and late binding/higher order functions.  It's hard to grok polymorphism (which is basically passing around a pointer to data and a bunch of functions that operate on the data) if you don't even understand the concepts of structuring data instead of just using primitives and passing around higher order functions/pointers to functions. Design patterns should be taught as something fundamental to OO, not something more advanced.  Design patterns help you to see the forest through the trees and give relatively concrete examples of where OO can simplify real problems, and you're going to want to learn them eventually anyhow.  Furthermore, once you really get OO, most design patterns become obvious in hindsight.  "
    },
    {
        "ID": "16040",
        "Question": "Often in C code (and some other languages as well) I have seen a section in the top-level comments for mod lines. These include the date a modification to the file was made, the user who made the modification, and a brief description of what was changed. I have not yet seen an equivalent of this in Java. I think this information is useful, especially when trying to track down bugs in code I did not write. Is this because Java code is often under version control, or are there other reasons? Are there any suggestions as to how to include the information in the code file? ",
        "Best answer": "I see no reason to have those useless comments in any source file. Its just reinventing version control extremely poorly. "
    },
    {
        "ID": "16059",
        "Question": "I'd like to know what people think of ASP.NET Dynamic Data.  I see the project type in visual studio and have found very little about it mention on blogs and in books.   I created a site using it and it is great the way it wires up the CRUD pages, but i don't understand how to modify the the pages without modifying the template that all the pages use.  Do you think it will be abandoned in future versions of visual studio?  I'm wondering if it's worth the effort of learning this type of application. personally i think i'd like something simpler....create the CRUD for us, but make them on seperate pages, without the whole template system. ",
        "Best answer": "I've always viewed Dynamic Data as an excellent way to create administrative pages...pages that can be thrown up quickly to edit metadata, but which a normal user will never see, so customization becomes less important. If you are using ASP.NET MVC, Dynamic Data takes on far less importance, as the View builders do much of the scaffolding work for you already. "
    },
    {
        "ID": "16070",
        "Question": "Is there a generally agreed upon definition for what a programming abstraction is, as used by programmers? [Note, programming abstraction is not to be confused with dictionary definitions for the word \"abstraction.\"] Is there an unambiguous, or even mathematical definition? What are some clear examples of abstractions? ",
        "Best answer": "The answer to \"Can you define what a programming abstraction is more or less mathematically?\" is \"no.\" Abstraction is not a mathematical concept. It would be like asking someone to explain the color of a lemon mathematically. If you want a good definition though: abstraction is the process of moving from a specific idea to a more general one. For example, take a look at your mouse. Is it wireless? What kind of sensor does it have? How many buttons? Is it ergonomic? How big is it? The answers to all of these questions can precisely describe your mouse, but regardless of what the answers are, it's still a mouse, because it's a pointing device with buttons. That's all it takes to be a mouse. \"Silver Logitech MX518\" is a concrete, specific item, and \"mouse\" is an abstraction of that. An important thing to think about is that there's no such concrete object as a \"mouse\", it's just an idea. The mouse on your desk is always something more specific - it's an Apple Magic Mouse or a Dell Optical Mouse or a Microsoft IntelliMouse - \"mouse\" is just an abstract concept. Abstraction can be layered and as fine- or coarse-grained as you like (an MX518 is a mouse, which is a pointing object, which is a computer peripheral, which is an object powered by electricity), can go as far as you want, and in virtually any direction you want (my mouse has a wire, meaning I could categorize it as an objects with a wire. It's also flat on the bottom, so I could categorize it as a kind of objects that won't roll when placed upright on an inclined plane). Object oriented programming is built on the concept of abstractions and families or groups of them. Good OOP means choosing good abstractions at the appropriate level of detail that make sense in the domain of your program and don't \"leak\". The former means that classifying a mouse as an object that won't roll on an inclined plane doesn't make sense for an application that inventories computer equipment, but it might make sense for a physics simulator. The latter means that you should try to avoid \"boxing yourself in\" to a hierarchy that doesn't make sense for some kind of objects. For example, in my hierarchy above, are we sure that all computer peripherals are powered by electricity? What about a stylus? If we want to group a stylus into the \"peripheral\" category, we'd have a problem, because it doesn't use electricity, and we defined computer peripherals as objects that use electricity. The circle-ellipse problem is the best known example of this conundrum. "
    },
    {
        "ID": "16089",
        "Question": "I have coded like this many times, and I've never encountered an issue, but the compiler always warns when it expects a return and there is none. For instance, look at this: -(NSString *)outputStringForInteger:(NSInteger)int {     if (int == 0)     {         return @\"Number is Zero\";     }     else     {         return @\"Number is not Zero\";     }     //no \"failsafe\" or other explicit return }  If the function will never get to the last line, ever, is it important to still have a failsafe option, or do you guys just deal with compiler warnings? ",
        "Best answer": "Get a better compiler.  The ones I use complain that you have code which will never execute if you put the \"failsafe\" return in there.  That's a much better warning that the false one you're apparently seeing. "
    },
    {
        "ID": "16135",
        "Question": "Just a random observation, it seems that on StackOverflow.com, there are questions about if \"++i == i++\". That question gets asked all the time though, I think I saw it asked about 6 or 7 times in the past 2 months. I just wonder why C developers are so interested in it? The same concept/question exists for C# and Java devs as well, but I think I saw only one C# related question. Is it because so many examples use ++i? Is it because there is some popular book or tutorial? Is it because C developers just love to cram as much as possible into a single line for 'efficiency'/'performance' and therefore encounter 'weird' constructs using the ++ operator more often? ",
        "Best answer": "I suspect that at least part of it is a bit simpler: even now, we see a lot of questions like this starting around the beginning of the school year, and they gradually taper off throughout the year. As such, I think it's fair to guess that quite a few of them are simply a result of classes in which the teacher talks at least a little about it, but doesn't explain his point(s) very well (as often as not because he doesn't really understand them himself). Especially based on the people who seem to ask these questions, few are based on actual coding. "
    },
    {
        "ID": "16137",
        "Question": "After writing a few (relatively obscure) OSS frameworks, I've learned the hard way that writing a good framework isn't enough--there has to be some time spent marketing your framework as much as the time you spend coding it. So here's my question: What are some effective ways for making your OSS project become well-known? Aside from blogging, how do I gain the most market attention for my code with the least amount of effort as humanly possible? (EDIT: I'm a lazy programmer and I hate documentation, but I'm biting the bullet. I want to be famous) ",
        "Best answer": "Start making some friends with larger voices than yours.  Social networking is a great tool for this: influential people on Twitter, Facebook, Buzz, what-have-you love sharing new and interesting things their followers might enjoy. The novel link is like currency. So, think about people who are popular and have a large audience and would be interested in your work. Then, just let them know about it. To facilitate this, you should be treating your project just like you would a startup: come up with an elevator pitch that succinctly describes what it is your project does, what problem it solves, and why someone should care. A blog or some sort of record of progress over time is also valuable, as people who are interested in following a project generally want to see how it evolves just as much, if not moreso, than the project itself. 9 out of 10 times if you're not spammy about it, realize you're talking to a person who is just trying to find something cool, and your project is interesting in its own right, they're going to talk about it to others, or at least link to it. "
    },
    {
        "ID": "16179",
        "Question": "Do you think Object Oriented Programming is a solution to complexity. Why? This topic may be a bit controversial but my intentions to know the answer of Why from the experts here ! ",
        "Best answer": "There is no solution to complexity. In \"The Mythical Man-Month\", Fred Brooks discusses the difference between accidental and essential complexity in programming.  Accidental complexity is caused by our tools and methods, such as having to write and test additional code in a language because we can't express our ideas directly, and things like that.  New methods and techniques can reduce accidental complexity.  I can write programs faster and better than I could twenty-five years ago, because I have better languages and tools. Essential complexity comes from the fact that what we try to do with programming is inherently complicated, and that there is an irreducible complexity.  \"Essential\", in this context, means \"relating to the essence of the thing\" rather than \"very necessary\". Therefore, he claimed that there would be no silver bullet, that writing software would continue to be difficult. I strongly recommend that you read his book:  specifically, I recommend the Silver Anniversary edition, with an additional essay \"No Silver Bullet\".  In that, he reviews the proposed solutions to complexity and considers their impact.  (What he finds the most effective is shrink-wrap software - write something complex once, and sell thousands or millions of copies.) Now, object-oriented programming helps, when done right, by creating abstractions and hiding away complexity.  An object of a class has a certain defined behavior that we can reason from, without caring about the complexity of the implementation.  Properly written classes have low coupling with each other, and divide-and-conquer is an excellent way to deal with complexity if you can get away with it.  They also have high cohesion, in that they're a set of functions and data that relate very closely to each other. "
    },
    {
        "ID": "16189",
        "Question": "What is that feature according to you that has made object oriented programming so much successful ?  Message Passing Inheritance Polymorphism Encapsulation  Or some other feature that you may like to introduce. Also I would like to know that what is the connection between Abstract Data type and Object Oriented programming? ",
        "Best answer": "I'd suggest that the most important characteristic of object oriented programming is that of complexity management. The human brain can only hold so many concepts at one time - the oft quoted limit of remembering 7+/-2 independent items comes to mind. When I'm working on a 600kloc system at work, I can't hold the whole thing in my head at once. If I had to do that, I'd be limited to working on much smaller systems. Fortunately, I don't have to. The various design patterns and other structures that we've used on that project mean that I don't have to deal with the entire system at once - I can pick up individual pieces and work on them, knowing that they fit into the wider application in well defined ways. All of the important OO concepts provide ways to manage complexity. Encapsulation - let me deal with an external API that provides me with various services, without worrying how those services are implemented. Abstraction - let me concentrate on the essential characteristics and ignore what's not relevant. Composition - let me reuse components that have already been built in new combinations Polymorphism - let me ask for a service without worrying about how different objects might provide it in different ways. Inheritance - let me reuse an interface or an implementation, providing only the pieces that are different from what has gone before. Single Responsibility Principle - lets keep the purpose for each object clear and concise, so it's easy to reason about Liskov Substitution Prinicple - let's not lay traps for each other by introducing odd dependencies Open/Closed Principle - let's allow extension and modification in ways that don't require us to risk breaking existing code Dependency Injection - let's take composition to the next level and assemble the components together much later. Interface oriented development - let's take abstraction to the next level and only depend on the abstraction, never on a concrete implementation. "
    },
    {
        "ID": "16211",
        "Question": "I often struggle to see the advantages of pointers (except for low level programming). Why use of char* instead of a String or char[] or what advantages pointer arithmetic brings.  So what are the pros and use cases of pointers? ",
        "Best answer": "Complex data structures. You can't build something like a linked list or a binary tree without pointers. There are no \"pros\" and \"cons\" of pointers. They are just a tool, like a hammer.  "
    },
    {
        "ID": "16308",
        "Question": "I always start the day thinking \"I'll easily get this done by the end of the day\" and set what looks like a realistic target. So why do I never hit it? The task always ends up taking 3x longer due to unforeseen bugs, last minute changes etc. Is it just me?  I don't seem to be getting any better predicting what can be done in a day. ",
        "Best answer": "Hard to believe that nobody has mentioned Hofstadter's law yet. I think the real answer is that your planning always assumes a best-case scenario, as if everything worked immediately an no interruption ever occured. In real life, you start coding, then the telefone rings, you are distracted for 5 minutest, spend another 15 minutes on stackoverflow or programmers.stackexchange to calm down and refocus, do some coding, run into an unexpected behaviour of some API, do some googling, spend 2 hourse to test the possible solutions etc. In other word: \"best-case\" only happens in your dreams. "
    },
    {
        "ID": "16323",
        "Question": "After reading this post about ideal programming language learning sequence, I am wondering what would have been the answers if the question was performance -instead of learning- oriented ? Since there are many programming languages I chose to ask the question for OOL to be the least subjective. But any thought or comparison about no-OOL are appreciated :D If we omit the programming effort, time and costs. What is your ranking of the most powerful object oriented languages ? ",
        "Best answer": "Any time you care deeply about performance, you generally want to get as close to the metal as you can.  In most languages, you can write out performance critical segments in C code.  C programmers can drop down to assembly language for the really critical stuff.  So if I'm writing some C# code, but I really need a tight performance on an inner loop, I can write some C or C++ code and use interop to call that code.  If I need even more performance, I can write assembly in my C library.  Going lower than assembly is possible, but who wants to write machine code these days?   However, and this is the big consideration, dropping close to the metal is only high-performance for small, tight goals.  If I was writing a 3D renderer, I might do the floating point math and rendering in C, (using a library to execute it on the video card.)  But performance problems are also architectural, and performance issues from large-scale problems are often better solved in a high level language.   Look at Erlang: Ericsson needed a language to do massive parallel work easily, because doing parallel processing was going to get them way more performance than any tightly optimized C routines running on one CPU core.  Likewise, having the fastest code running in your loop is only performance enhancing if you can't remove the loop entirely by doing something better at the high level. You can do huge system, high level programming in C, but sometimes the greater expressiveness of a more powerful language will show opportunities for architectural optimizations that wouldn't be obvious otherwise. "
    },
    {
        "ID": "16326",
        "Question": "I suck at estimates. When someone asks me how long something will take, I don't even dare to make a guess since I will be completely off the mark. Usually I'm way too optimistic, and should probably multiply my guess with some large X factor... How can I learn to make better estimates? It's not taught at my uni, and even though we have deadlines for all laborations I never think about how long something will actually take. Which I should. For everyone's sake (especially mine). ",
        "Best answer": "I'm still not great at it, but I have found that tracking how long you estimate for tasks and how long you actually take can be a big help.  That way you can get a solid idea of how far off you usually are.  Issue management software with time tracking (Jira in my case) or a spread sheet can be a big help with this. I think more than anything it's an experience thing. "
    },
    {
        "ID": "16365",
        "Question": "Apart from the obvious questions relating to specific project work someone is working on are there any questions I should be asking a fellow dev who is leaving the company? So far I am thinking about;  Locations of things on the server he uses that not maybe everyone does. Credentials he has set up that we wouldn't have needed.  Client details he has not yet saved into our CRM system.  ",
        "Best answer": "A phone number and/or e-mail address. No matter what you ask him before he leaves, you will remember 10 more things to ask him as you see his car pulling out of the parking lot. Note: you are much more likely to get good information if he is leaving on good terms - try to make the transition as pleasant as possible (no matter why he is leaving). "
    },
    {
        "ID": "16390",
        "Question": "Many tend to write \"C/C++\", as if they were the same thing. Although they share many similarities, they are clearly not the same. But what are really the fundamental differences between C and C++? Is C++ an enhanced version of C, or are there features in C which do not exist in C++? ",
        "Best answer": "The following points relate to C++:  (user-defined) static type system: allows static checks about your data and their usage - points a lot of easily done errors in C. multi-\"paradigm\":  allows working like in C, with object-oriented paradigms, with generic paradigms etc. Constructor/Destructor: the only way to say once what to do when creating or destroying something and be sure the user will not have to find the right function and use it like in C. RAII (badly named): you don't have to always manage memory. Just keep things in scope and use smart pointers describing your objects lifetime. Still can use raw pointers. Templates: better than macro, a real language to manipulate and generate types before the final compilation. Only lacks a type system (see Concepts in future C++ standards). Operator overloads: allows to describe operations in a simple syntactic manner and even to define embedded domain-specific languages inside your C++ code. Scoped names: namespaces, classes/struct, functions, etc. have simple rules to make sure names don't clash.  Exception system: a way to propagate errors that is often better than return code. In fact, return code are good for domain-specific logical errors, because the application has to manage it. Exceptions are used for \"hard\" errors, things that make the following code just incorrect. It allows for catching errors higher in the call stack if possible, react to such exception (by logging or by fixing the state) and with RAII, if well used, it doesn't make the whole program wrong - if done well, again. The Standard Library: C has its own, but it's all \"dynamic\". The C++ standard library is almost (not IO streams) made of templates (containers and algorithms) that allows generating code only for what you use. Better: as the compiler has to generate code, it will know a lot about the context and will hapily apply a lot of optimizations without having to require the coder to obfuscate its code - thanks to templates and other things. const-correctness: The best way to make sure you don't change variables you shouldn't. Allows to specify read-only access to varaibles. And it is only checked at compile time so there is no runtime cost.   "
    },
    {
        "ID": "16436",
        "Question": "You know the prospective company fairly well but are asked \"Do you have any questions for us?\".  To show interest, what are some of your favorite questions to reply? ",
        "Best answer": "My standbys:  What languages/frameworks to you usually work with? What development tools- IDEs, Bug Trackers, Version Control, etc do you use? What's your development methodology like (Agile, Waterfall, Scrum, Crystal, etc)? Do you have dedicated QA personel?  Even if so, do developers do some amount of their own unit testing?  Also a good list to consider is the Joel Test:  Do you use source control? Can you make a build in one step? Do you make daily builds? Do you have a bug database? Do you fix bugs before writing new code? Do you have an up-to-date schedule? Do you have a spec? Do programmers have quiet working conditions? Do you use the best tools money can buy? Do you have testers? Do new candidates write code during their interview? Do you do hallway usability testing?  Some things to help you decide if you really want the job:  What's the best part of your job?  And the worst or most frustrating part? Why do people tend to leave the company? If I took this job, what would you most hope I could take care of for you? (Especially to your would-be manager, but also to peers, other managers; it's curiosity not sycophancy.) What are the goals for this project over the next year or two?  What changes would you like to see in the way the project runs?  How do the short and longer-term project goals get set and how is work distributed?  "
    },
    {
        "ID": "16445",
        "Question": "Where I work, employees use a third-party desktop program for their clients. This program saves data to a flat file. My colleague wants to write a Java program that uploads that flat file to a remote server, opens the desktop program when the flat file is downloaded from a Web site, and checks if the desktop program is running or not by looking at the Windows processes. He keeps calling this helper/utility program a \"wrapper.\" But it doesn't wrap anything! I tried to clear it up with him, but he said, \"Well, I call it a wrapper.\" He now has everyone in the company calling it a \"wrapper.\" What would you call it? I say that it's a helper program or utility program. ",
        "Best answer": "I don't think there's an official definition!  In my mind, a wrapper program is a program that, in general, starts before the wrapped program starts, and exits no earlier than after the wrapped program exits.  A wrapper program would probably also manage the lifetime of the wrapped program (start it for you, stop it for you.) I recommend against arguing too much over the names of things, though.  The important thing is to design software that works well.  Sometimes belaboring terminology can aid in this goal; sometimes it is a hobgoblin. "
    },
    {
        "ID": "16528",
        "Question": "Which is better/more generally accepted? This: if(condition) {   statement; }  Or: if(condition)   statement;  I tend to prefer the first one, because I think it makes it easier to tell what actually belongs in the if block, it saves others from adding the braces later (or creating a bug by forgetting to), and it makes all your if statements uniform instead of some with braces and some without.  The second one, however, is still syntactically correct and definitely more compact.  I'm curious to see which is more generally preferred by others though. ",
        "Best answer": "The first is better because the second is error-prone.  For example, let's say you are temporarily commenting out code to debug something: if(condition)  //      statement; otherStatement;  Or adding code in a hurry: if(condition)      statement;     otherStatement;  This is obviously bad.  On the other hand, the first one does feel too verbose at times.  Therefore, I prefer to just put everything on one line if it's sufficiently short and simple: if(condition) statement;  This cuts down on syntactic noise while making the construct look like it does what it actually does, making it less error-prone.  Provided that this syntax is only used for very simple, short conditions and statements, I find it perfectly readable. "
    },
    {
        "ID": "16571",
        "Question": "It seems like every .net book talks about value types vs reference types and makes it a point to (often incorrectly) state where each type is stored - the heap or the stack.  Usually it's in the first few chapters and presented as some all-important fact.  I think it's even covered on certification exams. Why does stack vs heap even matter to (beginner) .Net developers?  You allocate stuff and it just works, right? ",
        "Best answer": " It seems like every .NET book talks about value types vs reference types and makes it a point to (often incorrectly) state where each type is stored - the heap or the stack. Usually it's in the first few chapters and presented as some all-important fact.  I agree completely; I see this all the time.  Why do .NET books talk about stack vs heap memory allocation?  One part of the reason is because many people came to C# (or other .NET languages) from a C or C++ background. Since those languages do not enforce for you the rules about storage lifetime, you are required to know those rules and implement your program carefully to follow them.  Now, knowing those rules and following them in C does not require that you understand \"the heap\" and \"the stack\". But if you do understand how the data structures work then it is often easier to understand and follow the rules. When writing a beginner book it is natural for an author to explain the concepts in the same order that they learned them. That's not necessarily the order that makes sense for the user. I was recently technical editor for Scott Dorman's C# 4 beginner book, and one of the things I liked about it was that Scott chose a pretty sensible ordering for the topics, rather than starting in on what really are quite advanced topics in memory management. Another part of the reason is that some pages in the MSDN documentation strongly emphasize storage considerations. Particularly older MSDN documentation that is still hanging around from the early days. Much of that documentation has subtle errors that have never been excised, and you have to remember that it was written at particular time in history and for a particular audience.   Why does stack vs heap even matter to (beginner) .NET developers?   In my opinion, it doesn't. What is much more important to understand is stuff like:  What is the difference in copy semantics between a reference type and a value type? How does a \"ref int x\" parameter behave? Why should value types be immutable?  And so on.  You allocate stuff and it just works, right?  That's the ideal. Now, there are situations in which it does matter. Garbage collection is awesome and relatively inexpensive, but it is not free. Copying small structures around is relatively inexpensive, but is not free. There are realistic performance scenarios in which you have to balance the cost of collection pressure against the cost of excessive copying. In those cases it is very helpful to have a strong understanding of the size, location, and actual lifetime of all relevant memory.  Similarly, there are realistic interop scenarios in which it is necessary to know what is on the stack and what is on the heap, and what the garbage collector could be moving around. That's why C# has features like \"fixed\", \"stackalloc\" and so on.  But those are all advanced scenarios. Ideally a beginner programmer need worry about none of this stuff. "
    },
    {
        "ID": "16595",
        "Question": "I keep reading that it is recommended for a programmer to take frequent breaks while programming, and the usual recommendation I see is 5 minutes every half hour or 10 minutes every hour. I gave it a try, but quite often I find something interesting during those 5 minutes, and it takes me away from what I was working on for longer than I planned. Either that, or my mind gets focused on something else and I find it hard to get back into my work and don't focus very well. Is it really that beneficial to take frequent breaks while programming? Am I doing something wrong for it to be decreasing my productivity instead of increasing it? ",
        "Best answer": "I do take frequent breaks but they normally have a purpose (bathroom, food/coffee, etc). I tend to find that while I am away from my desk that I am still thinking of the problem at hand. However, this thinking is not distracted by the code in front of me and allows me to think more about the problem as a whole rather than nitpicking through details in front of me. Frequently when I return to my desk I have a new idea at how to approach the issue I am working on. "
    },
    {
        "ID": "16608",
        "Question": "I'm working as a developer in a small software company whose main job is interfacing between separate applications, like between a telephony system and an environment control system, between IP TVs and hospitality systems, etc...And it seems like I am the candidate for a new job title in the company, as the person who does the proof of concept of a new interfacing project and does some R&D for prototyping.  What do you think the pros and cons of such a job would be, considering mainly the individual progress/regress of a person as a software engineer? And what aspects would you consider essential in a person to put him/her in such a job position? ",
        "Best answer": "Personally I would worry about hiring someone who spent their time doing proof of concept stuff because it sounds like they are trained to get stuff working in a very beta/high level way but maybe couldn't deal with the real fiddly bits of actual implementation. That being said if the work was there it would be a wicked awesome job. "
    },
    {
        "ID": "16634",
        "Question": "There are many ways how to exit program in C. Some of them are returning zero or error code, using exit function with EXIT_SUCCESS, EXIT_FAILURE, exit(0), exit(1) or some other error code, abort and probably few more I can't think of now. My question is which option is the best and why. I'm asking here because every time I asked more experienced programmers this question, it ended up as a holy war. ",
        "Best answer": "I think the best thing is return EXIT_SUCCESS or return a non-zero number for which you have documented an error code, or EXIT_FAILURE if you can't define error codes for all the conditions that could cause you to fail. This would let a program using your program (in a pipe or batch function) actually use your error code to determine what to do next if applicable.  Personally I am not married to macros that always, on every single implementation on earth equal 0 but hey that sort of comment can get you downvoted. "
    },
    {
        "ID": "16654",
        "Question": "Have you ever let your coding standards slip to meet deadlines or save time? I'll start with a trivial example: We came up with a coding standard at work around which brackets/formatting to use and so on. I ignore it and use the auto-format tool in netbeans. ",
        "Best answer": "I know this is generally a big no-no in books on effective programming, but I often flip the bozo bit on some people. Didn't have negative effects so far. "
    },
    {
        "ID": "16701",
        "Question": "I work in the web development industry and we implement a time tracking system to log our time (Project/time/comment). In the beginning of a project, we create a contract, and decide upon a price based on our hourly rate x estimated hours.  We log out times to see if we go \"over budget\". Is time tracking in this industry the norm? Is it required? What are the pros and cons? ",
        "Best answer": "Time tracking is a wonderful tool for:  making your estimates more accurate managing the size of your team justifying invoices when a client takes issue with what they are being billed providing more data for performance bonuses (temporal efficiency is important, but only if it comes with quality) finding the drag in your workflow so that you can become more efficient over time choosing the types of work at which you can be more cost-effective/efficient than your competition scheduling projects more effectively  The problem is that when done wrong (which is easier than doing it right) time tracking itself can be the drag on your workflow.  I have a colleague who, in a very unscientific study of me sharing an office with him for two days during which I was curious enough to time him, spent 15% of his time documenting how he spends the other 85%! To my mind (though I admit I'm a better technician than business strategist) that is way too much overhead for time tracking.  In a small company, doing it this badly is, in my opinion, worse than not doing it at all. "
    },
    {
        "ID": "16708",
        "Question": "I started seriously programming as a hobbiest, student and then intern about 4 years ago and I've always done small projects on the side as a learning exercise.  Schools over now though, and I spend my days at work as a software developer. I would still love to do projects on the side to learn about areas in computer science that I'm not exposed to at work, but I've noticed that after 8 hours of starring at an IDE it's far to tempting to veg out.  Any time I do get up the gumption to work on something for a few hours lately it's gotten left by the wayside. Anyone have any advice for sticking with side projects when you spend most of your day coding? ",
        "Best answer": "One tip - make sure your hobby project has nothing to do whatsoever with your day job.  If you use C++ at work, use something else in your hobby projects.  This will help you avoid some of the burnout because you're at least switching to a different IDE and/or skill set. But, a hobby is a hobby...so don't fret it.  It's supposed to be relaxing, not stressful. "
    },
    {
        "ID": "16735",
        "Question": "I have no clue where to put the following class and I've noticed I just keep putting them in a helpers folder which isn't much helpfull towards reusability in future projects. Example:  I want to have the ability to use the DisplayNameAttribute together with resource files. In order for this to work I made a LocalizedDisplayNameAttribute which inherits the ´DisplayNameAttribute´ and adds the following properties DisplayNameResourceName and DisplayNameResourceType. In which namespace/location would you place such a class which is likely to be reused in future projects? ",
        "Best answer": "I think the best way to do this is putting all helper classes and methods in a separate project, and then reference this project in each solution you create. Much easier than copying folders from place x to place y. This project with all kinds of extensions, helpers etc, can be called something like \"Company.Core\", or similar. Example (your attributes): using Company.Core.Attributes;  // ....  [LocalizedDisplayNameAttribute.DisplayNameResourceName(\"ResourceName\"), ...] public string Example { get; set; }  "
    },
    {
        "ID": "16760",
        "Question": "I must do a quick assessment of dozens of fresh students very quickly. I have 30 minutes meeting with each of them (in two or three days). I want to use most of the time to discuss non technical aspects. So I plan to use 10 minutes for technical questions. The same questions for all of them. They are all hired already (they are students), what I need to know is the average level in order to prepare a training. Ideally, the difficulty must be progressive in order to set a level for each. Level I will average, and I'll do an average on each question too. If you had only 10 minutes and 10 questions to ask to a candidate, what would be your top 10 ? ",
        "Best answer": "Three questions, elaborating on the end of Eric Lippert's answer here: Question 1:  On a scale of 1 - 10, where do you   rate yourself in (name the skill   here)? They answer [n], a number   between 1 and 10.  Question 2:  What could you learn from someone at   level [n+1] ?  Question 3:  What could someone at level [n-1]   learn from you?  The most important factor in your decision is to determine where a student (realistically) places themselves, and those three questions will help you to determine that quickly. It also helps identify people that might be compromised by the Dunning-Kruger effect (on either end), but that is another topic. If anyone can find the reference to this method on SO and edit this post to include a link, I would really appreciate it. Anyway, that should fall well under ten minutes. "
    },
    {
        "ID": "16769",
        "Question": "As I get it, having an error (even a typo like or missing \";\") in your whiteboard code will often cost you some interview points. Avoiding that will inevitably make one proof-reading code again and again (losing time and possibly neural energy / concentration) or even using a simpler (and thus less effective) algorithm -- and both these ways are \"costly\" again! So, why not just fast write code as elegant and effective as you would having a (unit) testing framework at your disposal and then just normally test it it (just on the whiteboard)? Has anyone tried / seen this approach? Is the whole idea worthy? [this also applies to the pen-and-paper case of course] ",
        "Best answer": "I absolutely want you to test the whiteboard code I ask you to write. I want you to talk out loud while you write it, look it over, spot most of the syntax mistakes you made, and point out how it could be more efficient. In fact, that's kind of the point of doing it at the whiteboard. It's not a one-shot, write-it-all-out, uh-huh-you-get-70/100 kind of thing. It's a conversation, mediated by code and held at the whiteboard instead of across my desk. Here are some great ways to fail the \"Whiteboard coding\" test:  refuse it don't ask a single clarifying question (language, platform, something about the requirements) AND don't tell me your assumptions about any of it AND make assumptions that are way off what I would have answered  (eg: write it in Fortran, interpret \"display\" or \"print\" as \"write to the event log\", that sort of thing. I might allow it if you told me in advance those were your assumptions)  ask me what language I want it in, receive an answer that is in the job description, and then write it in a different language because you're not comfortable in the language I asked for.  (We're consultants here. I am testing for consultant behaviour as much as coding. Asking the client is only correct if the client actually has a choice. Controlling conversations with people who will pay you is hard. This is lesson 1. It's a mark against you on any topic, but for the specific \"you're hiring an X programmer but I don't want to write in X for you\" you now have two big black marks.)  show me what an architecture astronaut you are by filling two whiteboards with interfaces, factory patterns, abstractions, injections, and tests when I wanted you to \"print the numbers from one to 5\".  (you think I'm exaggerating but I had a guy who generalized my problem dramatically - sticking to the example above let's say instead of 1 to 5 his solution would do any arbitrary sequence of integers (got from where? I wondered) and was 5 times as long as anyone else's - and he forgot to actually call the function that did the work. Repeated prompting and suggesting that he walk through it as though he was the debugger did not lead to his noticing that the function was never called.) I always say \"do you like that?\" \"can you improve that?\" \"walk me through that\" and the like. Typically the missing semi colon gets spotted, or the off-by-one, in that conversation. If not, I usually mark it up to nerves. Other things you may not think matter at the whiteboard that matter to me:  when you're done, can I still read it? Have you smudged, scribbled over, switched colours, drawn arrows, crossed out and generally left a mess that can't now be used? Or are you aware that whiteboards are erasable, pointed to lines of code in the air instead of circling/arrowing them, and left me something I could take a picture of and keep in the design file? how much did you ask me as you did it? Do you like to be left alone and not discuss your code, or do you see code as a collaborative thing? How did you respond when I asked you things while you were still writing it? did you sneer at the \"easy\" task or faint at the \"hard\" one? Were you rude about being asked to show you can code? Are you easily intimidated by a technical problem, or arrogant about your ability to come up with a good algorithm? are you working it out in your head, or remembering a solution you read somewhere? I can usually tell for the hard problems. did you plan ahead about where you started writing? Folks who run out of whiteboard usually start too low or write too big - I can tell they didn't know this was going to be 20 lines of code and so only left room for 5 - believe it or not this tiny detail is mirrored in bigger estimating tasks as well. did you look it over before you said you were done? Did I see you pointing or tapping your way through it and testing it yourself before I asked you to? When I prompted you, or asked you specific questions about it, did you look at it again, or just go from memory? Are you willing to consider that your first draft might not be complete?  I strongly recommend practicing coding at the whiteboard. I always warn interviewees that they will be asked to do it. If you have access to an actual whiteboard then set yourself some simple problems and practice doing them there. It will help your performance and your confidence. Sorry I know I'm in TL;DR territory but here's the thing - coding at the whiteboard is about more than coding. It's a test of more than your grasp of syntax. There are a lot of behaviours of good programmers that are demonstrated in your response to this task. If you think it's only about coding you are missing the point. In other conversations about whiteboard testing, people tell me I may reject a good candidate with it. Honestly, that's a risk I'm willing to take. Every hiring round contains several people I could hire. Some people with great resumes, who are doing ok in the question-and-answer part of the interview, fall apart at the whiteboard and clearly cannot (with any amount of prompting) write simple code in the language they claim to know. I might have hired some of these. Any tool that prevents that is a tool I will continue to use. I have never ended up in a no-one to hire boat because all my candidates messed up at the whiteboard and I don't expect I ever will. "
    },
    {
        "ID": "16798",
        "Question": "In my web application, I give the user the option to import all of his/her contacts from their email account, and then send out invites to all of these accounts or map the user to the existing accounts based on emails.  Now the question, is once all of these contacts are imported, would it be right to save these contacts back for repeated reminders, etc.?  I am quite confused here because that is the way all of the sites operate, but would that not be violation of data privacy? Is there an algorithm for this? ",
        "Best answer": "I think it would only be valid to store those contacts for repeated reminders people if they explicitly opt in to do so.  Also very importantly, that reminder should not be sent unless the original user clicks on the magic button (s/he annoying their friends that's better than you annoying them). Contacts for a user change all of the time anyhow, so inviting them to go through the process from scratch is probably better idea anyhow. "
    },
    {
        "ID": "16807",
        "Question": "I thought about it and could not come up with an example. Why would somebody want to catch an exception and do nothing about it? Can you give an example? Maybe it is just something that should never be done. ",
        "Best answer": "I do it all the time with things like conversion errors in D: import std.conv, std.stdio, std.exception;  void main(string[] args) {       enforce(args.length > 1, \"Usage:  foo.exe filename\");      double[] nums;      // Process a text file with one number per line into an array of doubles,     // ignoring any malformed lines.     foreach(line; File(args[1]).byLine()) {         try {             nums ~= to!double(line);         } catch(ConvError) {             // Ignore malformed lines.         }     }      // Do stuff with nums. }  That said, I think that all catch blocks should have something in them, even if that something is just a comment explaining why you are ignoring the exception. Edit:  I also want to emphasize that, if you're going to do something like this, you should be careful to only catch the specific exception you want to ignore.  Doing a plain old catch {} is almost always a bad thing. "
    },
    {
        "ID": "16869",
        "Question": "When interviewing for a job, is there a delicate yet accurate way to find out if they have insane policies, like \"no open source\", or \"no installing software without permission\", \"not upgrading software until it's about to lose support\", or \"no visiting blogs\"? ",
        "Best answer": "I don't think you can be too subtle, you can perhaps ask: \"The last place I worked at utilised some open source libraries - I found that useful as I was able to delve in and fix problems directly instead of waiting for a Vendor to get back to me.  I can also understand that there can be legal concerns on using such libraries, do you have a  policy here?\" \"I assume that our machines are looked after by some sort of tech support?  What's the process I'd have to go through if I had to install (for example) a trusted free text editor that supports Hex encoding?\" \"I'm really passionate about my craft and I keep up to date with the luminaries in my field - what's the policy on visiting work related websites such as Oracle tech blogs etc?\" "
    },
    {
        "ID": "16905",
        "Question": "In work we have just started using the Scrum methodology, it is working well but I have a question on the daily sprint meetings. We set aside 15 minutes for the meeting (between the three devs and the scrum master if we think he is needed) but I have found that we have normally finished by 5 minutes. This might be because we have said all that needs to be said but I was wondering what people tend to talk about in them, in case we are missing something. For the record we normally update each other on current objective, troublesome previous objectives and plans for the rest of the day (including if we will not be available on that project). ",
        "Best answer": "The intent of the stand-up meeting is to keep all of the team members informed of recent progress and any impediments that may hinder progress. If it only takes your team 5 minutes to accomplish that goal, then 5 minutes is the proper time and you shouldn't worry too much about it. I tend to stick pretty much to the classic:  What I have accomplished since the last stand-up What I'll be doing until the next stand-up Any issues that I think might keep me from getting my tasks done  but if I need to I'll add  Anything new I've learned (about the system / environment) that I think the team needs to know Quick thank-you acknowledgements for team-members who have helped me out in a significant way  The important thing is to share important and timely information quickly and concisely. If that's being accomplished, then the exact time-frame is secondary. "
    },
    {
        "ID": "16936",
        "Question": "And why do most programmers quit coding and become managers so early compared to other areas? You can work as a construction worker, a lawyer, researcher, writer, architect, movie director for decades if not your whole life, without quitting the favorite thing you are doing. And yet most programmers think being a manager, or better, a non-coding manager is a cool thing and the ultimate goal of the software career. It's so rare to see an ordinary coder in his 40s or 50s (or 60s!) in a typical software company. Why? (And I am for being a coder for as long as you love doing it.) ",
        "Best answer": " And yet most programmers think being a manager, or better, a non-coding manager is a cool thing and the ultimate goal of the software career.  They do? I have yet to meet one of those types that was coding because they loved it. Most that think this way are programming because they think they are going to make a lot of money doing it and think moving to management they will make more money. They may be right.  I am 42 and I love coding. I am currently in a team lead position that has management overtones and I hate that aspect of it. I love being in charge, but management duties suck ass. Honestly, I don't want to work on motivating my team, performance reviews, code reviews, etc. I see this as baby-sitting stuff. I expect them to be as motivated as I am, and when they are not it is frustrating. Don't get me wrong, I like being a mentor and would actually love skipping middle management and go right to upper management. That appeals to me. I like thinking long term and making strategic decisions for the business. But middle managers are simply baby sitters who must watch and motivate their charges. I see this as a waste of my skills, to be honest.  "
    },
    {
        "ID": "16945",
        "Question": "From my understanding, Ruby is a language that can be used to creat desktop applications. How does this get converted into a 'Web App' - and is a 'Web App' really any different than a Web Site with interactivity?  ",
        "Best answer": "This may be completely offsides, since I primarily do ASP.NET in C#, but I think I have still have some insights to the difference between a \"web application\" and an interactive web site.  It is largely an issue of whether you view the website as the product or the interface to the product. With an interactive site, you're primarily selling the site itself.  I'd say this is becoming an unpopular model really but things like a particular company's website might still fall into this model.  There can be a database providing content, but generally each page carries a lot of it's content with it, and may require some custom code to achieve the pages goals. A web application, on the other hand, is really about the service.  The web site is a just a form factor for it.  Blogs, twitter, stack overflow, CMS, Facebook, and so on are all web applications.  Typically there are less pages, more templates.  Very little content is actually on a given page, because its all being pumped up from the database.   The tools used and how they are used can also make a big difference.  With Ruby or ASP.NET, you can almost pretend like the website isn't even there.  You're just processing and pumping data like any application, but the end result is pushed out to a website instead of a form or something.  Very often, web applications have APIs and other access points that further distance them from the web.  RSS feeds were early examples, but these days you can write a client app that connects directly to Twitter through their API so that the web site itself is never actually involved.   An Application can be further distinguished by its generality.  A blog engine is a general purpose web application, that can be applied with different designs and content to create many unique blog sites.  A lot of Python or Ruby web apps are like this, and are further distinct from web frameworks.  I'm somewhat familiar with Pylons in Python, which is a framework for building apps, which are themselves the frameworks for hosted sites.   I might use Pylons to write the front-end for a general purpose warehouse management system, then sell that application to clients of my own.  Their designers might further customize my web application to have some static reporting pages, or some general info pages, and now that's a web site. So the application really is distinct from the site and the web (and the framework, and the language).  It's not a straight division, lots of sites include a mix of static content and dynamic content.  And if its a web application then it needs a web site front end to support it (but it can support other front-ends as well, this is the key.) SHORT VERSION A web application has a web site (which is a front end for the application's functionality) but a web site need not have a full application stack to back it.  Its more a matter of intent than actual composition. "
    },
    {
        "ID": "17059",
        "Question": "While reading this wikipedia article, Brookes has told that there is a difference between \"good\" designers and \"great\" designers.  What is the difference between them? How can I decide if a designer is good or great? ",
        "Best answer": "A good designer can design for simplicity or flexibility or efficiency or robustness.  A great designer has a deep understanding of the tradeoffs involved and can effectively balance all of these and come up with a solution that satisfies all of them reasonably well. "
    },
    {
        "ID": "17099",
        "Question": "After viewing this video on InfoQ about functional design patterns I was wondering what resources are out there on design patterns for non-object orientated paradigms. There are plenty out there for the OO world (GOF, etc, etc) and for architecture (EoEAA, etc, etc) but I'm not aware of what's out there for functional, logic, or other programming paradigms. Is there anything? A comment during the video suggests possibly not - does anyone know better? (By the way, by design patterns I don't mean language features or data structures but higher level approaches to designing an application - as discussed in the linked video) ",
        "Best answer": "The best books I have come across that where not about OOP, but about programming in general is the Art of Computer Programming books (3 books when I bought them - now a 4th released) by Donald Knuth. http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming A few years back I used one of his well taught data processing algorithms to speed up an import/merge of 100,000+ records from about a 20 minutes process down to about 30 secs.... I was just not doing it the right way! And remember, before they were called Desigh Patterns, we called them Algorithms! "
    },
    {
        "ID": "17105",
        "Question": "Lets say you are somewhere where coding and getting online isn't possible (On a busy flight, for example)what do you do to stay productive? Things I would do are read whatever tech book I am currently slogging through and maybe doodle some UI stuff or workflows.  What else could I be doing? ",
        "Best answer": "Why not use that slack to relax and free your mind? If you are 100% of your time busy, thinking, working, inventing, etc, THAT will makes you less productive. EDIT: Even in World Of Warcraft you have to rest ;) (no I don't play wow, but I feel it's a good example) "
    },
    {
        "ID": "17120",
        "Question": "In Java, C and C++ I see people using intensively debugging strategies (because mostly they don't know about TDD). On the other hand, debugging too can help to understand software abstractions. So, when and how do you use debugging in Rails applications? ",
        "Best answer": "Its the same as everything else, you need a debugger when you don't really know where the problem is. This may be more rare if you are using TDD, but if your application is fairly complex you will never have 100% test coverage. I do find with Rails that I don't really need an integrated debugger, the console is enough. "
    },
    {
        "ID": "17136",
        "Question": "...in percentage. For example 60/40 or 90/10 or 100/0. My hypothesis is that the bigger the proportion of time you spend thinking the smaller your code can be as a result (and the less time will be needed to write it down). Think more, write less, in other words. Do you think it is true? As a side note, I think in typical software companies thinking is not part of the culture anyway: you are usually supposed to be sitting there at your computer typing something. You will almost definitely be noticed by your managers if you wander about with a blank look thinking over your next steps with your code. Too bad. ",
        "Best answer": "As with anything else, it depends At the beginning of something, the majority of time is spent thinking and planning how to code it. Once you have the plan in place, most of the time is spent coding. "
    },
    {
        "ID": "17177",
        "Question": "Just out of curiosity what's the difference between a small, medium and large size project? Is it measured by lines of code or complexity or what? Im building a bartering system and so far have about 1000 lines of code for login/registration. Even though there's lots of LOC i wouldnt consider it a big project because its not that complex though this is my first project so im not sure. How is it measured? ",
        "Best answer": "Complexity.  The more complexity, the harder it is to learn everything in the project. "
    },
    {
        "ID": "17254",
        "Question": "I'm reading Coders at Work by Peter Seibel, and many a time it has been mentioned that programmers who can't write generally make poor programmers - it's been claimed by Douglas Crockford, Joshua Bloch, Joe Armstrong, Dijkstra (and I've only read half the book). What's your view of this? Is an inability to express yourself in writing in a natural language such as English a hindrance of writing good code? ",
        "Best answer": "There's much more to programming than 'writing code'. A big part of being a successful programmer involves communication; Being able to connect with customers, understand their needs, translate them into the technical realm, express them in code, and then explain the result back to the customers. Programmers who have a hard time expressing themselves clearly in writing may not be able to communicate well in general, whereas those who have a good grasp of language and writing can generally translate those skills to the code they write. I think being unable to write well, and thus communicate well, will keep one from being a very good programmer. As Jason Fried and David Heinemeier Hansson (of 37signals) say in their book Rework:  If you're trying to decide among a few people to fill a position, hire the best writer. Being a good writer is about more than writing. Clear writing is a sign of clear thinking. Great writers know how to communicate.  "
    },
    {
        "ID": "17310",
        "Question": "sometimes a programmer comes up with a brilliant idea to protect his/her webservice created with Windows Communication Foundation. I would like to hear from you guys, which techniques do you use to protect your WCF service and avoid unauthorized users to consume it? For example, you would:  avoid Impersonate, use it only if necessary  publish metadata information to prevent tampering avoid memory consuption enforcing a maximum size quota  create a security context token to control number of sessions  ",
        "Best answer": "Yesterday, I found an article, a video and code about using API keys with WCF. I've got to lock down a publicly exposed web service as part of what we need to do in order to comply with PCI-DSS, and this looks like the right way to move forward. In the past, this app and webservice were used only by a VB4/5/6 (now .NET) desktop application, but the boss wants it opened up as a for-pay service to others. One financial client used a scheme with a security information element in the SOAP header. This element had 4 attributes, one was the name of the application, the timestamp and guid elements were used to prevent replay attacks and the 4th attribute was a hash based on the name of the app, the timestamp and guid, along with a \"secret\" (think of a password) stored in the registry (for windows servers, or a special locked down file for unix-based servers, with different \"passwords\" for different application names). The \"secret\" (or password) was intended to prevent situations where a trojan in the datacenter would be able to make inappropriate calls, or respond to them. This was obviously not WCF as it had to support unix, windows and other operating systems in the data centers, but the technique was fascinating and could be used elsewhere. Because they used url-rewriting, the security information element would not show up in WSDLs, you had to know about it from documentation that only authorised folks received; if you added ?WSDL to the end of a webservice endpoint, you got a lie. "
    },
    {
        "ID": "17315",
        "Question": "As stated by the title, what is the difference in years of experience in given language in terms of developers? For example, if one developer has had five years working with language A and the other developer has had two years working with language B followed by three years working with language A, would there be perceivable difference between them? ",
        "Best answer": "\"it depends\" Experience <> knowledge or understanding. Programmer 1 could be very good, a guru even, or they could be someone fumbling around with the language for the last 5 years. Programmer 2 could be someone who understands concepts independently of language they're using. Or someone who found language B too difficult and hopes A is easier. Coding Horror's \"The Years of Experience Myth\" is worth reading "
    },
    {
        "ID": "17341",
        "Question": "Many compilers have warning messages to warn the programmers about potential runtime, logic and performance errors, most times, you quickly fix them, but what about unfixable warnings? How do you deal with unfixable warnings? Do you re-write a portion of the code, or rewrite it in the \"long, hackless way\" or disable warnings all together? What should be the best practice? What if you are editing someone else's code and his code has warnings? Here is a good example: jQuery has a lots of JavaScript warnings as a Mozilla-class browser detected, why the jQ developers don't fix them? If you contribute to jQuery, are you going to fix them? ",
        "Best answer": "My opinion is that you should be strict with yourself. The compiler has been written by total experts in the language. If they are reporting that something is a bit wiffy (think code smell) then the code should be reviewed.  It is entirely possible to write code that compiles without errors and without warnings. "
    },
    {
        "ID": "17355",
        "Question": "I'm experimenting with a new platform and I'm trying to write a program that deals with strings that are no longer than 60 characters and I'd like to populate the data store with some famous or well-known small chunks of code and equations, since programming and math goes along with the theme of my software. The code can be in any language and the equations from any discipline of mathematics, just so long as they're less than a total of 60 characters in length. I suspect people are gonna break out some brainfuck for this one. For example,  #include<stdio.h> int main(){printf (\"Hi World\\n\");return 0;}  60 characters exactly! Thanks so much for your wisdom! ",
        "Best answer": "The classic C string copy routine is known by fewer and fewer people theses days: while (*d++ = *s++);  "
    },
    {
        "ID": "17427",
        "Question": "I am using ReSharper since version 3 and I always used InteliJ Idea/Resharper 2.x keymap schema. Recently I have learned Ilya's Visual Studio keymap. They each have advantages. What keymap do you use/prefer with ReSharper? ",
        "Best answer": "I think it's mostly going to depend on what background you're from. Personally I found going from Visual Studio to Visual Studio + Resharper very easy using the Visual Studio key map (not suprisingly). Someone coming from an IntelliJ background is going to find the IntelliJ mapping easier. If you're new to both Visual Studio and IntelliJ I would suggest going with the Visual Studio mappings in case you find yourself in the horrible, horrible situation of having to use VS without R#er in the future. Most people will supplement the default bindings with a few of their too - so if you're really missing something from one the just add it to the other. "
    },
    {
        "ID": "17442",
        "Question": "Clang is able to be used instead of gcc? What your experience on that? What disadvantages there are yet? Performance of compilation is very better than gcc but about performance of code generated when executing it? Are there good tools as front-end (IDE) to run on Linux or Windows? EDIT: I mean C compiler. C++ it's not so good yet. ",
        "Best answer": "Update: Now (2013) Clang is ready for prime time and used by some companies like Google. However it is not totally usable on Windows, work on this platform is a \"work in progress\". LLVM/Clang is currently the default compilator on MacOSX/XCode but it's not exactly the same releases than the LLVM ones so beware of the minor differences (mostly difference of version numbers).  Well following the clang dev mailing list, recently the trunk version have been successfully building :  the linux kernel (a recent revision) Qt (with it's special build process too, apparently) Chromium (a recent revision)  So, I would say that the coming version (2.9) might be a good \"ready for prime time\" compiler. However, if your project have a a planning and budget, maybe it's not a good idea to try a compiler that isn't heavily tested yet. If you're allowed to experiment and provide feedbacks to the Clang developers community, then go for it, it's win-win for everybody. If not, maybe you should use a mature-and-heavily-used compiler like gcc (in its recent versions) to have a \"stable ground\" to rely on while working on your project. "
    },
    {
        "ID": "17443",
        "Question": "In this question I asked whether being a bad writer hinders you from writing good code. Many of the answers started of with \"it depends on what you mean by good code\". It appears that the term \"good code\" and \"bad code\" are very subjective. Since I have one view, it may be very different from others' view of them. So what does it mean to write \"good code\"? What is \"good code\"? ",
        "Best answer": "A good coder is like a good pool player. When you see a professional pool player, you at first might not be impressed: \"Sure, they got all of the balls in, but they had only easy shots!\" This is because, when a pool player is making her shot, she doesn't think about what ball will go into which pocket, she's also thinking about where the cue ball will end up. Setting up for the next shot takes tremendous skill and practice, but it also means that it looks easy. Now, bringing this metaphor to code, a good coder writes code that looks like it was easy and straightforward to do. Many of the examples by Brian Kernighan in his books follow this pattern. Part of the \"trick\" is coming up with a proper conceptualization of the problem and its solution. When we don't understand a problem well enough, we're more likely to over-complicate our solutions, and we will fail to see unifying ideas. With a proper conceptualization of the problem, you get everything else: readability, maintainability, efficiency, and correctness. Because the solution seems so straightforward, there will likely be fewer comments, because extra explanation is unnecessary. A good coder can also see the long term vision of the product, and form their conceptualizations accordingly. "
    },
    {
        "ID": "17444",
        "Question": "In an answer that I posted to another question, I noted that there is likely to become more and more specialization in software development: there being experts or specialists for given type of programming.  I'm wondering, what specialties are known to exist in reasonable quantities in the industry now (not academia), that are not technology specific (eg. for this discussion winforms isn't a specialty)? I'm also interested in the industry that such a specialist would have to work in if necessary. ",
        "Best answer": "The way I see it there's two categories of specialisation:   specialisation in a particular programming field; or specialisation in a problem domain.  Examples of a programming topic would be:  Embedded GUI Visualisation Testing  Then there's specialisation in a particular problem domain such as  Financial Scientific Medical Almost any major industry can be considered a problem domain  I think it's an important distinction. "
    },
    {
        "ID": "17582",
        "Question": "What do you consider a good client web framework to use on interface business application? jQuery? YUI!? Another? Some plugins useful? The application will be use mostly internally replacing a desktop application for data entry and lots of queries/views. ",
        "Best answer": "For my money, you can't beat jQuery. It makes working with JavaScript just that much easier. The out-of-the-box GUI side of things is limited, but there are lots of 3rd party extensions for grids and such that may be of help for your views. "
    },
    {
        "ID": "17600",
        "Question": "What software do you use to track time spent on a project? ",
        "Best answer": "Toggl I use this all the time and I like it.    web based so it will stay synced between multiple computers decent interface nice api  integrates with GMail, iGoogle or anything you can put an iframe into.  integrates into Basecamp and FreshBooks  "
    },
    {
        "ID": "17606",
        "Question": "There are online services such as IKM that offer skills tests in many areas, including programming.  Would you use these kind of tests when hiring for a senior developer position?  What about just for objectively benchmarking candidates before calling them for an interview? Would you use it as a step after short-listing candidates after interviews? Is this approach more suitable in some situations compared to others? Have you personally used this kind of service or know someone who has? ",
        "Best answer": "To be blunt:  No, No, No, No and No! Get the candidate in to do some coding with you, it's the only way you'll know how they think their way through problems and how they might fit into your team. As an aside I'd try to avoid recruiting via the CV lottery technique :-), instead find good people through word of mouth, conferences, technical community meetups etc.  Avoids the sharky recruitment agents as well. "
    },
    {
        "ID": "17632",
        "Question": "I'm wondering if the term Hacker means different things to different people.  When most people hear the word hacker what are the first things that come to mind? ",
        "Best answer": "Someone who has the ability to change the functionality of a program, device, or methodology to perform a task or function that is different than it's original design in the effort to improve the program, device, or methodology, or to solve a problem with what is available. "
    },
    {
        "ID": "17639",
        "Question": "Theres talk about what syntax and feature you like in a programming language; i'll now ask what core principles or feature would you like in a library in your favorite (or any) language?  An example is having adding list += anotherList valid as oppose to only allowing list += listElement (although some may see this as a bad idea) ",
        "Best answer": "I would follow the best practices you can find in How to design a good API and why it matters by Joshua Blosh (Google). The PDF version can be found here. According to him, characteristics of a good API are :  Easy to learn   Easy to use, even without documentation   Hard to misuse   Easy to read and maintain code that uses it   Sufficiently powerful to satisfy requirements   Easy to extend   Appropriate to audience  "
    },
    {
        "ID": "17696",
        "Question": "I have a few questions about writing a specification and they are:  When we write a software specification, under the topic \"User requirements definition\" we have to specify the \"Functions\" and \"Constraints\" only ? Does \"User Interface\" fall into \"functions\" or \"constraints\" ? What are the major key areas (requirements) a software can be broken into (e.g. UI ) ?  ",
        "Best answer": "While I am not a big fan of gathering all requirements in detail up front (as they are subject to so much change over the course of a non trivial project), if you are writing requirements documents, the Volere requirements specification template is an excellent guide.  While it may be overkill for some projects, it provides a great checklist of things to think about, even if it's just to mentally check off the list that you don't need that item for this requirement. Here's a link to more information about the template: http://www.volere.co.uk/template.htm The template itself (and the book Mastering the Requirements Process - which is actually slightly less expensive than the template and contains the full template text) contains a lot of information, examples and advice within the various sections as to what should go in each section. Here's a summary of the sections in it (quoted from the above link):   The Purpose of the Project The Stakeholders Mandated Constraints Naming Conventions and Definitions Relevant Facts and Assumptions The Scope of the Work Business Data Model and Data Dictionary The Scope of the Product Functional and Data Requirements Look and Feel Requirements Usability and Humanity Requirements Performance Requirements Operational and Environmental Requirements Maintainability and Support Requirements Security Requirements Cultural and Political Requirements Legal Requirements Open Issues Off-the-Shelf Solutions New Problems Tasks Migration to the New Product Risks Costs User Documentation and Training Waiting Room Ideas for Solutions   "
    },
    {
        "ID": "17700",
        "Question": "Off the top of my head, I can think of a handful of large sites which utilize the Microsoft stack  Microsoft.com Dell MySpace PlentyOfFish StackOverflow  Hotmail, Bing, WindowsLive  However, based on observation, nearly all of the top 500 sites seem to be running other platforms.What are the main reasons there's so little market penetration?  Cost?  Technology Limitations? Does Microsoft cater to corporate / intranet environments more then public websites?  I'm not looking for market share, but rather large scale adoption of the MS stack. ",
        "Best answer": "I'll posit that it's because most of the \"big websites\" started out small.  Google, Youtube, Facebook et al. were all at one time single-server sites that someone built as a hobby.  They used LAMP-like stacks because: 1) they're cheap and the devs were poor and often 2) because they were at a university and university environments tend to favor OSS. After the sites started growing, the developers just stuck to what they knew.  In the early years, there wouldn't be enough time or money to do a big system rewrite.  When, and if, that ever became an option, why switch to an entirely different base? So I'm saying it's because that's just what they knew and had when they started.  SO isn't any different if I recall that story correctly.  The SO Founders knew MS stack, and had access to the tools/licenses/etc to start using it, and so that's what they used! (I've also heard that they also wanted to prove that MS stack was just as good as LAMP for big sites, but that may be apocryphal.) "
    },
    {
        "ID": "17710",
        "Question": "A question on software specialties inspired this question. How valuable is a software generalist compared to a specialist?  When I say generalist, I mean someone who can take a project from requirements to deployment, and is competent with all phases of the software development lifecycle. Someone who can put all the specialties together into a cohesive whole. An expert generalist knows his or her weaknesses and fills them by relying on specialists - example: Oracle specialists or UX specialists. What do you see as the ultimate career path of the software generalist? ",
        "Best answer": "Consultant I did this for a while, and being a generalist is the #1 skill that will make you a success.  When people have a problem, they typically have been solving that problem one way for a long time, and need consultants to get a fresh perspective.  As a consultant you need to know about ALL KINDS of products, open source, closed source, Oracle, Microsoft, Red Hat.  You need to know what's good, what's bad, and what's best for the client.  To be able to do that you need to be a generalist and know how to be an expert FAST.  To be an expert fast (without already being one), you need to know the core principles and practices of software development, without only knowing ONE implementation of them.  You need to be able to pick up things without having seen them before and within a short time be able to be proficient in it.  As a self-proclaimed generalist myself, consulting was the best career move, and the most fun I've had doing software development.  New experiences, new and diverse range of projects and technologies, good pay, and generalist traits help you succeed. "
    },
    {
        "ID": "17738",
        "Question": "If you are doing code reviews  How much time do you spend on code reviews, compared to implementation? How much of the changes undergo code review? you think it's to much / should be more?  Are there any studies about effectiveness? [edit] thank you all for the answers, it's hard to pick a \"winner\" for such a question, there is lots of valuable info in the other replies, too. ",
        "Best answer": "At my work we have the following procedure for code reviews.  It has worked well for us so far, and we found it to be very time-efficient, especially in terms of man-hours.  We do not really have any specific time allocated to the reviews.  Every commit or merge to the trunk must be reviewed, and it takes as long as it takes for the reviewer to ok it. Edit: The time it takes, of course, depends on the magnitude of the change.  Small features and bug fixes take minutes.  Large new features, refactorings, or changes that affect many parts of the system can take half a day to review and another day to address all the issues that come up as a result.   To make this system work it is crucial to commit to the trunk often, so that the changes are of manageable size.  You do not want to have the situation when you have to review a year's worth of somebody's code. "
    },
    {
        "ID": "17766",
        "Question": "I saw this asked in the SO Tavern, so I'm posting the question here. I thought it an interesting question. (Of course it doesn't belong on SO, but I think it's OK here.)  Do you add periods (or, as the OP wrote, \"full stops\") in your code comments? To keep it relevant, why? ",
        "Best answer": "Full stop is for ending sentences, but if a comment consists of just one sentence surrounded by code, then full stop is not necessary in my opinion. Sometimes I even don't capitalize the first letter. A detailed multiline comment, on the other hand, does need full punctuation. // This function returns an average of two integers. Note that it may // return an irrelevant result if the sum of a and b exceeds the int // boundaries.  int avg(int a, int b)   // make it static maybe? {     // A better algorithm is needed that never overflows     return (a + b) / 2;  }  "
    },
    {
        "ID": "17788",
        "Question": "Generally speaking, what type of optimizations do you typically slant yourself towards when designing software? Are you the type that prefers to optimize your design for  Development time (i.e., quick to write and/or easier to maintain)? Processing time  Storage (either RAM, DB, Disc, etc) space  Of course this is highly subjective to the type of problems being solved, and the deadlines involved, so I'd like to hear about the reasons that would make you choose one form of optimization over another. ",
        "Best answer": "Maintenance Then profiling if necessary and optimize for speed.  Rarely have I ever had a need for storage - at least not in the past 10 years.  Prior to that I did. "
    },
    {
        "ID": "17790",
        "Question": "I'm current the sole developer/architect of a fairly large web application (ASP.NET MVC stack, roughly 150K+ lines of code) and the end of development is on the horizon. As such, I'm starting to think about what needs to be done for the hand off of the project and I want to make sure I do the right thing for anyone that has to maintain the project in the future.  What are some things to be aware of when getting ready to hand a project off to another developer or team of developers of maintenance? ",
        "Best answer": "IMHO, if you could only do one thing before handing off your project (either directly or indirectly), I would recommend that you double and tripple check that it compiles as-is from source control. No laughing, but I cannot tell you how many times I've gotten \"latest\" from a source control and it failed to compile, only to find out later that I wasn't \"on Fred's old box\" because apparently the code \"only compiles on Fred's old box\". I even had a former employer promptly remove my desktop from my cube, and replace it with \"Fred's old box\" so I could work on the project I was suppose to. As an extension of the above recommendation, because sometimes getting latest isn't all that is necessary to compile an application, I recommend that you create a README.txt and place it in the root directory of your application and put that in source control. This README document should contain a list of external dependencies that could not be checked into source control (if any exist), how to setup the database, and any other oddities about the compilation, execution or deployment cycles of the application. Anything above and beyond the above two suggestions would just be gravy, but IMHO the above two are almost required on any project larger than \"Hello World\". EDIT: On the topic of documentation... Over the years I've both written and read my fair share of software documentation for the purpose of easing a developer's transition. I'd say that such documents are rarely worth the paper they are printed on. Developers (myself included) rarely think of the important parts of the application while writing such documents, we only tend to think about the most recent fires we've battled. Above and beyond the fact that these documents tend to not cover all the important aspects of the software, they also get outdated VERY quickly. Once the document is out of date a future developer is more than likely going to completely disregard it instead of bringing it back up to match reality (think changing requirements). Instead of documentation per se, I recommend unit tests. I know it probably sounds old at this point, but let the code do the documenting for you. Broken unit tests are hard to ignore (and easier to spot) than a Word document. Additionally, the English language is horribly imprecise for articulating the finner points of software design. There are simply too many ways to interpret the meaning of even the simplest of English sentences, and this just leads to confusion and/or bugs. "
    },
    {
        "ID": "17824",
        "Question": "What specific advantages and disadvantages of each way to working on a programming language grammar? Why/When should I roll my own? Why/When should I use a generator? ",
        "Best answer": "There are three options really, all three of them preferable in different situations. Option 1: parser generators, or 'you need to parse some language and you just want to get it working, dammit' Say, you're asked to build a parser for some ancient data format NOW. Or you need your parser to be fast. Or you need your parser to be easily maintainable. In these cases, you're probably best off using a parser generator. You don't have to fiddle around with the details, you don't have to get lots of complicated code to work properly, you just write out the grammar the input will adhere to, write some handling code and presto: instant parser. The advantages are clear:  It's (usually) quite easy to write a specification, in particular if the input format isn't too weird (option 2 would be better if it is). You end up with a very easily maintainable piece of work that is easily understood: a grammar definition usually flows a lot more natural than code. The parsers generated by good parser generators are usually a lot faster than hand-written code. Hand-written code can be faster, but only if you know your stuff - this is why most widely used compilers use a hand-written recursive-descent parser.  There's one thing you have to be careful of with parser-generators: the can sometimes reject your grammars. For an overview of the different types of parsers and how they can bite you, you may want to start here. Here you can find an overview of a lot of implementations and the types of grammars they accept. Option 2: hand-written parsers, or 'you want to build your own parser, and you care about being user-friendly' Parser generators are nice, but they aren't very user (the end-user, not you) friendly. You typically can't give good error messages, nor can you provide error recovery. Perhaps your language is very weird and parsers reject your grammar or you need more control than the generator gives you. In these cases, using a hand-written recursive-descent parser is probably the best. While getting it right may be complicated, you have complete control over your parser so you can do all kinds of nice stuff you can't do with parser generators, like error messages and even error recovery (try removing all the semicolons from a C# file: the C# compiler will complain, but will detect most other errors anyway regardless of the presence of semicolons). Hand-written parsers also usually perform better than generated ones, assuming the quality of the parser is high enough. On the other hand, if you don't manage to write a good parser - usually due to (a combination of) lack of experience, knowledge or design - then performance is usually slower. For lexers the opposite is true though: generally generated lexers use table lookups, making them faster than (most) hand-written ones. Education-wise, writing your own parser will teach you more than using a generator. You have to write more and more complicated code after all, plus you have to understand exactly how you parse a language. On the other hand, if you want to learn how to create your own language (so, get experience at language design), either option 1 or option 3 is preferable: if you're developing a language, it will probably change a lot, and option 1 and 3 give you an easier time with that. Option 3: hand written parser generators, or 'you're trying to learn a lot from this project and you wouldn't mind ending up with a nifty piece of code you can re-use a lot' This is the path I'm currently walking down: you write your own parser generator. While highly nontrivial, doing this will probably teach you the most. To give you an idea what doing a project like this involves I'll tell you about my own progress. The lexer generator I created my own lexer generator first. I usually design software starting with how the code will be used, so I thought about how I wanted to be able to use my code and wrote this piece of code (it's in C#): Lexer<CalculatorToken> calculatorLexer = new Lexer<CalculatorToken>(     new List<StringTokenPair>()     { // This is just like a lex specification:       //                    regex   token         new StringTokenPair(\"\\\\+\",  CalculatorToken.Plus),         new StringTokenPair(\"\\\\*\",  CalculatorToken.Times),         new StringTokenPair(\"(\",    CalculatorToken.LeftParenthesis),         new StringTokenPair(\")\",    CalculatorToken.RightParenthesis),         new StringTokenPair(\"\\\\d+\", CalculatorToken.Number),     });  foreach (CalculatorToken token in              calculatorLexer.GetLexer(new StringReader(\"15+4*10\"))) { // This will iterate over all tokens in the string.     Console.WriteLine(token.Value); }  // Prints: // 15 // + // 4 // * // 10  The input string-token pairs are converted into a corresponding recursive structure describing the regular expressions they represent using the ideas of an arithmetic stack. This is then converted into a NFA (nondeterministic finite automaton), which is in turn converted into a DFA (deterministic finite automaton). You can then match strings against the DFA. This way, you get a good idea how exactly lexers work. In addition, if you do it the right way the results from your lexer generator can be roughly as fast as professional implementations. You also don't lose any expressiveness compared to option 2, and not much expressiveness compared to option 1. I implemented my lexer generator in just over 1600 lines of code. This code makes the above work, but it still generates the lexer on the fly every time you start the program: I'm going to add code to write it to disk at some point. If you want to know how to write your own lexer, this is a good place to start. The parser generator You then write your parser generator. I refer to here again for an overview on the different kinds of parsers - as a rule of thumb, the more they can parse, the slower they are. Speed not being an issue for me, I chose to implement an Earley parser. Advanced implementations of an Earley parser have been shown to be about twice as slow as other parser types. In return for that speed hit, you get the ability to parse any kind of grammar, even ambiguous ones. This means you never need to worry about whether your parser has any left-recursion in it, or what a shift-reduce conflict is. You can also define grammars more easily using ambiguous grammars if it doesn't matter which parse tree is the result, such as that it doesn't matter whether you parse 1+2+3 as (1+2)+3 or as 1+(2+3). This is what a piece of code using my parser generator can look like: Lexer<CalculatorToken> calculatorLexer = new Lexer<CalculatorToken>(     new List<StringTokenPair>()     {         new StringTokenPair(\"\\\\+\",  CalculatorToken.Plus),         new StringTokenPair(\"\\\\*\",  CalculatorToken.Times),         new StringTokenPair(\"(\",    CalculatorToken.LeftParenthesis),         new StringTokenPair(\")\",    CalculatorToken.RightParenthesis),         new StringTokenPair(\"\\\\d+\", CalculatorToken.Number),     });  Grammar<IntWrapper, CalculatorToken> calculator     = new Grammar<IntWrapper, CalculatorToken>(calculatorLexer);  // Declaring the nonterminals. INonTerminal<IntWrapper> expr = calculator.AddNonTerminal<IntWrapper>(); INonTerminal<IntWrapper> term = calculator.AddNonTerminal<IntWrapper>(); INonTerminal<IntWrapper> factor = calculator.AddNonTerminal<IntWrapper>();  // expr will be our head nonterminal. calculator.SetAsMainNonTerminal(expr);  // expr: term | expr Plus term; calculator.AddProduction(expr, term.GetDefault()); calculator.AddProduction(expr,                          expr.GetDefault(),                          CalculatorToken.Plus.GetDefault(),                          term.AddCode(                          (x, r) => { x.Result.Value += r.Value; return x; }                          ));  // term: factor | term Times factor; calculator.AddProduction(term, factor.GetDefault()); calculator.AddProduction(term,                          term.GetDefault(),                          CalculatorToken.Times.GetDefault(),                          factor.AddCode                          (                          (x, r) => { x.Result.Value *= r.Value; return x; }                          ));  // factor: LeftParenthesis expr RightParenthesis //         | Number; calculator.AddProduction(factor,                          CalculatorToken.LeftParenthesis.GetDefault(),                          expr.GetDefault(),                          CalculatorToken.RightParenthesis.GetDefault()); calculator.AddProduction(factor,                          CalculatorToken.Number.AddCode                          (                          (x, s) => { x.Result = new IntWrapper(int.Parse(s));                                      return x; }                          ));  IntWrapper result = calculator.Parse(\"15+4*10\"); // result == 55  (Note that IntWrapper is simply an Int32, except that C# requires it to be a class, hence I had to introduce a wrapper class) I hope you see that the code above is very powerful: any grammar you can come up with can be parsed. You can add arbitrary bits of code in the grammar capable of performing lots of tasks. If you manage to get this all working, you can re-use the resulting code to do a lot of tasks very easily: just imagine building a command-line interpreter using this piece of code. "
    },
    {
        "ID": "17826",
        "Question": "I see that most of the good programmers have the habit of reading big books for learning about technology. What does it really take to read technical books, apart from the real interest on the technology? How can I improve my ability to read these books? ",
        "Best answer": "How to eat an elephant? One bite at a time. "
    },
    {
        "ID": "17830",
        "Question": "I have been involved in many development models, and have found XP to be the best for a new programmer from the aspect of learning, as the collaboration between the team members is very high and the opportunities to share knowledge are great. What are the expert opinions on this?   ",
        "Best answer": "I would say regardless of methodology, the group must have the right attitude when sharing their knowledge. Where participants don't worry about prestige, and dare to ask questions from each other. "
    },
    {
        "ID": "17843",
        "Question": "I read few articles on web to find out how Agile, XP, Scrum, pair programming are different from each other / related to each other and I derived the following line:  Scrum and XP are almost same. XP has shorter period of releases than Scrum Pair programming is employed in both Agile and XP methodologies  But I was unable to identify how Agile is different from XP. More than providing a URL, I would be happy to read your experience and thoughts on this. ",
        "Best answer": "You are confusing the issue. Being agile means that you are following a bunch of values and practices from the agile manifesto. Thats it.  XP and Scrum are development processes that follows those values. Both are \"just as agile\". The big difference between Scrum and XP is that Scrum does not contain practices specifically for programming, whereas XP has lots of them (TDD, continuous integration, pair programming).  "
    },
    {
        "ID": "17887",
        "Question": "Is a Program Manager a person who manages mutiple projects (under a single program) which are in turn managed by Project Managers? Or is a Program Manager a person as defined by Joel Spolsky here.  NOTE: I am not referring to this.  ",
        "Best answer": "I suppose the answer hinges on what the terms \"Program\" and \"Project\" mean. According to Project Management Institute (PMI), The Standard for Program Management, 2nd Ed., \"A Program is a group of related projects managed in a coordinated way to obtain benefits and control NOT available from managing them individually. Programs may include elements of related work outside of the scope of the discreet projects in the program... Some projects within a program can deliver useful incremental benefits to the organization before the program itself has completed.\" "
    },
    {
        "ID": "17912",
        "Question": "Although I like VS2010's multi monitor support, I seem to have too many accidental window drags, where I unsnap a window without intending to.  This behavior has made me still live in the mental model of vs200X's non-snappable windows. 1) Do you have a particular layout that works for general purpose work? 2) How have you optimised for specific scenarios (SQL, Debugging, Workflow, WPF) 3) Are you aware of any hotkey or utility that will jump to a particular window layout? 4) Are you aware of any way I can reduce the times I accidentialy undock a window? ",
        "Best answer": "I have 2 screens but I don't use VS on both of them. I prefer to keep the other screen for the application being debugged because I want to see the effects of each instruction. Sometimes I undock 2 windows code editor and put them side by side, on the same screen, for code comparison and similar operations. "
    },
    {
        "ID": "17929",
        "Question": "As a longtime desktop developer looking at doing our first large-scale web application, what are the pros and cons of doing web development? Is developing a web application much worse than developing a desktop app?  E.g., is it more tedious or annoying?  Is the time to market much worse?  Is the web platform excessively limiting?  If the answer to any of these is yes, then why? (And how does developing a Flash or Silverlight app compare?) ",
        "Best answer": "No It's painful if you don't know what you're doing or don't plan correctly, but that's true with any development. It is easier to bottle the application in a desktop application, but then you lose the accessibility that is provided by coding for a web application.  I would make the choice between desktop and web based on the desired use. I see many applications written web based that shouldn't be because they don't know how to code desktop applications. I don't see many desktop applications that should be web based though, and I think that's something to consider. If you need the centralized storage, remote accessibility, and UI traits then sure.  I can't comment of Flash or Silverlight because I use neither.  "
    },
    {
        "ID": "17947",
        "Question": "1) Up-time2) Latency3) Throughput4) Server software such as SQL, etc.5) $$$ -- transfer, overages, etc. What else? And is there a web site rating hosts by this criteria that is actually reliable and not simply a dishonest advertising site... or do you guys have any recommendations? (BTW, someone with 150+ rep should create \"hosting\" and \"web hosting\" tags.) ",
        "Best answer": "Comparing hosting is like comparing dentists: there are plenty to choose from, everyone has a favorite and everyone recommends his/her own. And unfortunately just like with dentists, you need to find yours via trial and error rather than others' recommendations. You can probably start with various reviews and ratings available on the Internet. One non-obvious thing that I learned recently though was that these so called \"cloud-based\" services where you pay per resource usage can turn into a disaster. I saw how one poor guy received a ridiculous bill from his hosting company just because he was running poorly optimized code (actually a third-party Wordpress plugin). I'd say stay away from this kind of hosting unless you absolutely know what you are doing. "
    },
    {
        "ID": "17976",
        "Question": "Basically, I want to learn lots of programming languages to become a great programmer.  I know only a handful to depth and I was hoping someone could elaborate on how many classes or types of programming languages there are.  Like how you would lump them together if you had to learn them in groups. Coming from a Java background, I'm familiar with static typing, but I know that in addition to dynamic typing there has to be such variety in available languages that I would love to see a categorical breakdown if possible. ",
        "Best answer": "It depends on how you want to classify languages. Fundamentally, languages can be broken down into two types: imperative languages in which you instruct the computer how to do a task, and declarative languages in which you tell the computer what to do. Declarative languages can further be broken down into functional languages, in which a program is constructed by composing functions, and logic programming languages, in which a program is constructed through a set of logical connections. Imperative languages read more like a list of steps for solving a problem, kind of like a recipe. Imperative languages include C, C++, and Java; functional languages include Haskell; logic programming languages include Prolog. Imperative languages are sometimes broken into two subgroups: procedural languages like C, and object-oriented languages. Object-oriented languages are a bit orthogonal to the groupings, though, as there are object-oriented functional languages (OCaml and Scala being examples). You can also group languages by typing: static and dynamic. Statically-typed languages are ones in which typing is checked (and usually enforced) prior to running the program (typically during a compile phase); dynamically-typed languages defer type checking to runtime. C, C++, and Java are statically-typed languages; Python, Ruby, JavaScript, and Objective-C are dynamically-typed languages. There are also untyped languages, which include the Forth programming language. You can also group languages by their typing discipline: weak typing, which supports implicit type conversions, and strong typing, which prohibits implicit type conversions. The lines between the two are a bit blurry: according to some definitions, C is a weakly-typed languages, while others consider it to be strongly-typed. Typing discipline isn't really a useful way to group languages, anyway. "
    },
    {
        "ID": "17984",
        "Question": "Even if it could be bad practices, I would say that there is time that it would fulfill its purpose. ",
        "Best answer": "https://stackoverflow.com/questions/995255/why-is-multiple-inheritance-not-allowed-in-java-or-c covers this question nicely. My take on it is this: The designers probably wanted to make a language that promoted good design principles. Ok, so there are times that multiple inheritance is perfect. Those are the exception, rather than the rule, though, and can be abused very easily. So, the designers decided to make it impossible to do. For those cases where it would be good, you need to use interfaces. Those work, albeit clumsily; but, you won't need them for that that much. "
    },
    {
        "ID": "18011",
        "Question": "Has anyone read all three of Mary Poppendieck's books on Lean Software Development?  She and her husband Tom wrote:  in 2003: Lean Software Development in 2006: Implementing Lean Software Development in 2009: Leading Lean Software Development  Mary and Tom Poppendieck are highly regarded lean experts, so I should have probably read one of their books, but I haven't read any of them. Which one should I start with? ",
        "Best answer": "I have all three and I find them complimentary so it's hard to say which one. If I had to pick a single one though then I'd go for the most recent. It covers pretty much all of the material in the first two but goes into more detail in how to implement lean methodology in practice. "
    },
    {
        "ID": "18029",
        "Question": "I have recently started a job that has me working on an existing system. It requires tweaks and updates as well as new code. I have done several maintainance/feature adding projects now, and several of them have ended up being significantly different from what was actually requested. So, I had to program the item several times to get it to where the requester wanted. Now, I don't mind reprogramming the feature if that's what needs to be done. However, I would like to decrease turnaround time on my projects. The bottleneck seems to be in the requester's perception of what needs to be done. Do you have any ideas on what I could do to figure out what the requester needs more quickly? ",
        "Best answer": "A few points of advice:  Listen to problems, Not Solutions. A lot of clients like to tell you how to solve their problems. Don't listen to them. You're the programmer, and it is your job to find solutions to problems. Instead listen to what problems clients are having, and figure out the best way to solve it. As others have said, clients don't really know what they want; sometimes you have to show it to them first.  Ask Questions. When you are done asking questions, ask some more. Clients are rarely forthcoming with details, as they don't really think about it. The only way you are going to get the information you need is to pry it out of them.  Get Things in Writing. Depending on the situation with the client, this can be really important later when they start complaining about how what you delivered \"isn't what they asked for\". And if nothing else, writing out detailed specifications can help you make sure you have all of the information you need, and help clear up ambiguities between you and the client.  Communication is key. Don't just talk to the client, get info, knock out some code and not talk to them until it's done. Always keep in touch with the client. Ask questions throughout the process. Show them the progress you've made and get feedback. It'll make everyone's life easier in the long run.   "
    },
    {
        "ID": "18059",
        "Question": "How do you go about explaining refactoring (and technical debt) to a non-technical person (typically a PHB or customer)? (\"What, it's going to cost me a month of your work with no visible difference?!\") UPDATE Thanks for all the answers so far, I think this list will provide several useful analogies to which we can point the appropriate people (though editing out references to PHBs may be wise!) ",
        "Best answer": "When you have a big home theater and you add things, slowly but surely a big rats nest forms in the back.  If you are often times replacing parts, sometimes its worth straightening all that stuff out. Sure, if you do that, it was working before, and it's not going to work better than when you started, but when you have to mess with it again, things will be a lot easier. In any case its probably best to make a similar comparison to some subject area the PHB or customer is already familiar with, ie car or construction or something... "
    },
    {
        "ID": "18074",
        "Question": "How do open source projects maintain quality? What keeps the projects from people with relatively little programming experience checking in poor quality codes? ",
        "Best answer": "How do closed source projects maintain quality? What keeps the projects from people with relatively little programming experience checking in poor quality codes? The same way:  code review testing static typing static analysis metrics consistent coding styles consistent guidelines shame (because everybody in the world can see who wrote that crappy code)  But really the answer is: most don't. In both cases, open and closed source. After all, programming is subject to the Pareto Principle and Sturgeon's Revelation (“Ninety percent of everything is crud”) just like everything else. In Linux, for example, in almost 100% of all cases where a company releases a previously closed source driver as open source, that driver needs a considerable amount of work before it can be merged into the kernel tree, simply because its quality is so bad. In some cases, it is so bad that the code is basically unsalvageable and has to be rewritten from scratch. So, in Linux, the way that quality gets maintained is that low quality contributions get rejected, but there is a mentoring process helping people to write better quality code. "
    },
    {
        "ID": "18086",
        "Question": "Is it possible to calculate shadow areas of buildings or simulate shadows of buildings in a city, using the heights of these buildings and the sun angle and azimuth? The basic light tracing concept using trigonometry is known. However, how would one come up with simulated shadow of group of buildings in one place? ",
        "Best answer": "Yes, this is a matter of projecting the geometry of the buildings onto a shadow plane along the sun angle, and then computing the union of the resulting polygons. This gives a single polygon that you can then get the area of by simple triangle subdivision. All of these features are actually built into your graphics card these days. If your buildings are non-polygonal and you need a precise answer you're out of luck unless you can model them mathematically, but really that's a rare case these days--everything's built of polygons, and/or you can get away with an approximated answer. For a specific solution, I'd recommend asking this question on Stack Overflow. "
    },
    {
        "ID": "18116",
        "Question": "I've always been curious about trying out online freelance sites... it would be nice to work from home, feel independent, get to choose what I want to work on, get to work on different technologies, lose the PHB, etc. However I never really gave them a chance because I'm used to American rates and assumed that I would be competing with people from India, Russia, China, etc. that would severely undercut me, and it wouldn't be viable for me. Am I correct in this assumption or should I give it a shot?  What kind of hourly rate would I be able to expect on short-term programming work? ",
        "Best answer": "When I set out to try working freelance, the first thing I did was to make a list of 100 small design and web companies in my area. I put together an informal email introducing myself, saying what I did and explaining that I was local, linking my portfolio site and suggesting that I might be a good person to pick up any spare capacity on a freelance basis. Before I had got a quarter of the way through my list I got enough work to fill in the gaps between other projects. People liked having someone who was competent and close enough that I could go and meet them in person to demonstrate stuff or talk through projects. You don't need a lot of hits, just a few that can consistently put some work your way. In the end I quit freelancing because I hated all the stuff that isn't about programming- all the invoicing, accountancy and other necessary administrative elements and I realised I'd be happier contracting or working full time, but if you're in an area where there are a fair number of businesses you can work with and you've got the skills ( and financial cushion ) to do the job, then it's quite feasible. "
    },
    {
        "ID": "18127",
        "Question": "I'm considering writing a low level driver or OS components/kernels. The osdev.org folks seem to think that the important bits are not meaningfully testable this way, but I have read some discussions where people thought differently. I've looked around, but have failed to find any real life examples of TDD on low-level components. Is this something people actually do, or just something that people talk about in theory because there is not a good way to do it in practice? ",
        "Best answer": "If you're interacting with or controlling hardware, then it's difficult to test without it.  You can try emulating the hardware, but that's often more difficult than writing the driver in the first place, so you wind up not knowing whether the bug's in your driver or your emulator. "
    },
    {
        "ID": "18137",
        "Question": "I am a web developer. Worked in many projects. Now I have changed my work place. It is big organisation. I and my group are thinking about new project. There is no difficulty to write project but our organisation is big and it's has little offices in many places (within the country). There is no any problem about internet in center. But there is problem with internet in remote areas sometimes. My question is: is there any solution to solve problem as it? If anybody has create project such this please tell little about what technologies you have used. Thanks ",
        "Best answer": "Distributed working is doable but most of the tools that make it easy are going to be internet based so you're going to struggle a bit. First thing is to look for tools that don't demand being on-line continually.  Look at distributed source control which can have mini-hubs in each office - something like Git.  This means that they can sync when their connection is active and it's not a problem when it's not. For communications, forums or something like 37signals Campfire, both of which keep a history of the discussions will be better than chat clients.  That way when someone's connection disappears they can go back in and review what people were saying once their connection is back. You also need to work on your processes.  Two things I'd look at: 1) Don't assume that people have seen changes or received e-mails.  Use read receipts and if you don't get a read receipt tell people they need to pick up the phone and make sure the person is aware.  Make it the responsibility of the person communicating the change to get acknowledgement. 2) Divide work into chunks that programmers can work on for a few days at a time so day to day communication isn't so critical.  That way if the connection does go down they're not going to be doing nothing as they'll always have things on their task list.  If you can put whole chunks of the project out to different offices even better as they'll be their own little unit. "
    },
    {
        "ID": "18141",
        "Question": "I know that it is possible to get into software development without a degree in computer science, but is it possible (or rather, common) to be able to get an embedded programming job without the computer science degree (or any engineering degree as well)? ",
        "Best answer": "Some background in electronics (or at least a desire to get into it) is pretty much needed to be successful in embedded programming.  I've been doing this for 30+ years, and did the formal way (BSEE + MSCS), but I've run across a lot of good self-taught embedded programmers along the way.  They all had one thing in common though -- somewhere in their background they had some electronics education (formal or otherwise). You're going to be doing a lot of bit twiddling and working with lots of low-level I/O registers, with so-called datasheets that might run several hundred pages, like this one (so if that document doesn't look interesting, forget about embedded programming).  You'll be using lots of serial busses like UART, SPI, I2C, and USB.  Eight and 16-bit timers.   Lots of interrupts. You don't have to be able to design a circuit, but it's best if you learn to read a schematic since you will probably be probing a circuit board with an oscilloscope or logic analyzer. Assuming you're working with medium-sized devices, that might have only 32K to 256K of program memory, and 4K-16K of RAM, you'll probably be doing most of your programming in C -- but C with proprietary extensions (different for each chip), because most of these processors are Harvard architecture and have separate address spaces for fast RAM, extended RAM, and code space and pointers can get really complicated. (I consider small micros to be ones with just a few K of program memory and a few hundred bytes of RAM.  Those are still mostly programmed in assembly.  Then there are larger microprocessors that can run Linux and have MB of flash and RAM, but that's not really where main-stream embedded programming is either.) "
    },
    {
        "ID": "18147",
        "Question": "As i try to program a solution to a request arise. I can't separate the difference between implementation problems and design problems.  How do you specifically express the design problem? ",
        "Best answer": "Design problems are a subset of implementation problems since technically everything is an implementation. Design problems have the distinct bonus of requiring you to change large sections of code in order to make it work.  If you're having to rewire half your program due to this request, it's a design problem.  Pray it isn't.   "
    },
    {
        "ID": "18161",
        "Question": "After taking a class about information systems, I want to find additional resources for learning the various patterns, concepts, and examples of their implementation. The course did focus on Martin Fowler's Principles of Enterprise Application Architecture and had a project that attempted to implement some of them. Basically, where can I learn more about Information System design and implementation? ",
        "Best answer": "Well, if you are looking for examples from a Coders point of view - this book - Core J2EE Patterns helped me a lot when I was first starting out developing Enterprise applications in Java.  It's examples are specific to Java, but they can easily be applied to most OOP languages.  Of course, there is always the Gang of Four book.  However, if you are looking for more high level concepts... go no further than Fowlers own Domain Specific Languages or something like this.  Also, it never hurts to read classics like Mythical Man-Month or CODE Complete to get overview of what it will take to design, develop and deliver enterprise solutions. (Even if those last two are not specific to enterprise development - but do apply). Also, one last point - everyone should read this Peopleware.  It really brings home how without the right people, not design will ever be successfully developed and delivered. "
    },
    {
        "ID": "18166",
        "Question": "We are a team of two people who need a versioning system for their university project. Due to administration issues, the graders need us to register on a site that happens to only support SVN; however, we cannot commit to SVN from inside the university as the required protocols are filtered out by the proxy (including the HTML extensions that would make SVN checkins over HTTP possible). Sigh. Is it possible to have a hg repo for ourselves that can then be converted to SVN for purposes of hosting on this site?  Please note that practically all students commute to university, so the \"we can't commit from university so we can't do this\" is not as strong of an argument as you'd think. ",
        "Best answer": "As someone who's been on the grader/ta end of the world, the appropriate action is to raise a stink with the professor and the grades. He needs to know. Also raise a stink with the IT department. Stink all around. This is unreasonable, and something needs to give, and it's not you - in this case. "
    },
    {
        "ID": "18180",
        "Question": "I saw this in a job posting for an ASP.net developer. \"An understanding of basic software development practices such as ...and Dependency Analysis.\" I read the wikipedia entry for Dependency Analysis and understand that it basically means one thing depends on the other so you can't reorder or parallelize them. What does this mean in practice?  Is there a tool that is used to do a Dependency Analysis?  What should I know about it for an interview and in practice if I get the job? ",
        "Best answer": "When you say Dependency Analysis, the first tool that comes to mind is NDepend. Here's a great blog post by Scott Hanselman explaining how you can use it to analyze dependencies in your code. You can use NDepend to create dependency graphs of your assemblies, use CQL (Code Query Language) to query for overly complicated classes and methods, and perform a number of other code analysis tasks. "
    },
    {
        "ID": "18181",
        "Question": "I am primarily a web developer and I have a couple of personal projects which I want to kick off.  One thing that is bugging me is database design. I have gone through in school db normalization and stuff like that but it has been a couple of years ago and I have never had experience with relational database design except for school. So how you do you approach database from a web app perspective? How do you begin and what do you look out for? What are flags for caution? ",
        "Best answer": "The best book I ever bought regarding database design was Database Design for Mere Mortals by Michael Hernandez ISBN: 0-201-69471-9. Amazon Listing I noticed he has a third edition. Link to third edition He walks you through the entire process of (from start to finish) of designing a database. I recommend you start with this book. You have to learn to look at things in groups or chunks. Database design has simple building blocks just like programming does. If you gain a thorough understanding of these simple building blocks you can tackle any database design. In programming you have:  If Constructs  If Else Constructs  Do While Loops  Do Until Loops  Case Constructs  With databases you have:  Data Tables Lookup Tables One to One relationships One to Many Relationships Many to Many relationships Primary keys Foreign keys  The simpler you make things the better. A database is nothing more than a place where you put data into cubbie holes. Start by identifying what these cubbie holes are and what kind of stuff you want in them. You are never going to create the perfect database design the first time you try. This is a fact. Your design will go through several refinements during the process. Sometimes things won't seem apparent until you start entering data, and then you have an ah ha moment. The web brings it's own sets of challenges. Bandwith issues. Statelessness. Erroneous data from processes that start but never get finished. "
    },
    {
        "ID": "18240",
        "Question": "We recently revived the lunch in learns for the programming department in the company I work for. We were all asked if we had any ideas for a session, and if we would be interested in doing a presentation. I've had a few ideas ranging from various topics such as: How to think like a user when designing UI or Differences in HTML5 A few coworkers I tossed these ideas around to seem to like them. However, I'd like some more ideas before I dig too far into creating a presentation. What are some great lunch and learn topics? ",
        "Best answer": "Some general ones:  Test Driven Development Debugging in [IDE of choice]  (you can throw in things like remote or virtualised debugging as well) What's new in the latest version of  (could be an IDE, an database system, whatever) Design patterns Security factors in [technology of choice] Performance factors in [technology of choice] Continuations & closures  (been reading Eric Lippert's fantastic series on this) Overview of [new language or technology of choice]  But remember you don't have to pick general topics, you can do L&L topics on your own work as well.  Arguably, this is even more valuable because the audience can get a feel for what you do (rather than assuming it all happens by magic).  For example, your install guy could do a topic on how the install works, your QA lead could do a topic on preparing test environments, your build guy could do a topic on the build process, and if your project has an interesting architecture that maybe not everyone's aware of, then do a topic on that. Also remember that your audience is not necessarily only composed of programmers.  You may have QA guys and project managers in there as well, so don't assume that \"Design patterns\" isn't a valid topic because everyone must know design patterns. Obviously you can't go into too much detail on some of these (for example, don't engage in a deep analysis of the pros and cons of every single pattern). "
    },
    {
        "ID": "18288",
        "Question": "Both asserts and unit tests serve as documentation for a codebase, and a means of discovering bugs.  The main differences are that asserts function as sanity checks and see real inputs, whereas unit tests run on specific simulated inputs and are tests against a single well-defined \"right answer\".  What are the relative merits of using asserts vs. unit tests as the main means of verifying correctness?  Which do you believe should be emphasized more heavily? ",
        "Best answer": "Asserts are useful for telling you about the internal state of the program. For example, that your data structures have a valid state, e.g., that a Time data structure won't hold the value of 25:61:61. The conditions checked by asserts are:  Preconditions, which assure that the caller keeps its contract, Postconditions, which assure that the callee keeps its contract, and Invariants, which assure that the data structure always holds some property after the function returns. An invariant is a condition that is a precondition and a postcondition.  Unit tests are useful for telling you about the external behavior of the module. Your Stack may have a consistent state after the push() method is called, but if the size of the stack doesn't increase by three after it is called three times, then that is an error. (For example, the trivial case where the incorrect push() implementation only checks the asserts and exits.) Strictly speaking, the major difference between asserts and unit tests is that unit tests have test data (values to get the program to run), while asserts do not. That is, you can execute your unit tests automatically, while you cannot say the same for assertions. For the sake of this discussion I've assumed that you are talking about executing the program in the context of higher-order function tests (which execute the whole program, and do not drive modules like unit tests). If you are not talking about automated function tests as the means to \"see real inputs\", then clearly the value lies in automation, and thus the unit tests would win. If you are talking about this in the context of (automated) function tests, then see below. There can be some overlap in what is being tested. For example, a Stack's postcondition may actually assert that the stack size increases by one. But there are limits to what can be performed in that assert: Should it also check that the top element is what was just added? For both, the goal is to increase quality. For unit testing, the goal is to find bugs. For assertions, the goal is to make debugging easier by observing invalid program states as soon as they occur. Note that neither technique verifies correctness. In fact, if you conduct unit testing with the goal to verify the program is correct, you will likely come up with uninteresting test that you know will work. It's a psychological effect: you'll do whatever it is to meet your goal. If your goal is to find bugs, your activities will reflect that. Both are important, and have their own purposes. [As a final note about assertions: To get the most value, you need to use them at all critical points in your program, and not a few key functions. Otherwise, the original source of the problem might have been masked and hard to detect without hours of debugging.] "
    },
    {
        "ID": "18303",
        "Question": "How important is it to learn XML when JSON is able to do almost all that I need? Having said that, I use JSON mainly for AJAX requests and obtaining data from various APIs. I am a total newbie to web development and the reason I am asking this is that I want to know whether I should go ahead and buy a book on XML or whether I can just give it a pass. ",
        "Best answer": "You'll need to learn XML to get anywhere in the web world. It's what drives of lot of B2B communications and there are many standard XML formats describing important. Just restricting yourself to JSON is hugely self-limiting. Yeah, you'll be chucking AJAX calls around but what happens when you need to communicate with a GeoServer? It'll be adhering to GIS standards and will be spurting out XML in WCS (Web Capabilities Service), WMS (Web Map Service) and WFS (Web Feature Service) formats among others. If you don't know how to handle XML, you'll have some trouble with that. Of course, any marshaller (domain object to text format) worth its salt will be able to convert their objects to and from XML/JSON/YAML so you could make the argument that so long as you can hide behind the marshaller you only have to deal with the domain objects. Web services provide WSDL exactly for this purpose. But sooner or later you'll need to read and understand the contents of your requests and responses and that will certainly require an understanding of XML. And let's not forget good ol' XHTML the old web standard for HTML pages. It's XML. So, in short, learn XML - and keep JSON wherever you can 'cos it's lovely. "
    },
    {
        "ID": "18304",
        "Question": "I would like to know what kind of jobs/roles one could expect on their career path if they start as a developer.  I also want to know how many years you'll be expected to stay in the same role until you progress to the next level. The career path is of course, based on how good you do your job, the company etc. but assume a normal person who balances life and family with career. Some examples of roles that I have in may head: developer, senior developer, architect, project manager etc. ",
        "Best answer": "Broadly speaking I've seen people take the following routes: 1) Stay as a developer.  There's no reason to actually move on at all, developing is just fine and there are a lot of people I know into their 30s and 40s who have no intention of moving from something they enjoy into something they won't just for the sake of \"progression\". 2) Technical Architect.  Potentially still hands on to a degree but also involving higher level technical design and analysis, platform selection and so on.  Generally speaking a Technical Architect will have spent 5 - 10 years as a developer before moving into this sort of role. 3) Project Management.  The first of the hands-off options (a proper Project Manager won't code except maybe for the odd little tool to make their own life easier).  A Project Manager runs the project as a whole from start to finish, liaising with the business and the developers, ensuring the business case is valid, planning and managing the plan, monitoring risks and so on.  In terms of when you can move into Project Management, it can happen at any point, though the earlier you do it the smaller the projects you're given to manage are likely to be. 4) Business Analysis / General Consultancy.  Writing specifications, discussing what's needed with users and clients, documenting it, working with the developers and testers to make sure it's understood.  Again, a move into this sort of role can happen at any time, though more experience as a developer will tend to afford you better opportunities as an analyst. 5) Development Management.  Distinct from Project Manager though in some instances (particularly in smaller organisations) they'll be rolled into one.  The simplest distinction is that a Project Manager is responsible for the project, the development manager is responsible for the team.  A Development Manager will almost always have a technical background, may still be hands on, and will have a good understanding of development process and the associated tools.  Most of their time will be spent keeping the team productive and keeping the development work moving forward.  Generally speaking someone will have worked for 5 - 10 years as a developer before moving into this sort of role. If you want to move beyond these into more senior management then Project Manager and Development Manager are the most likely routes out. Which is best for you is entirely down to what you want to do and what your skills are and none of them should be seen as right or better than the others.  It's entirely possible to try them out and move back or move on again.  From my experience the only thing that stops that sort of move being possible is when you become senior enough that you won't take the salary drop which comes as part of moving into an area where you're less experienced. "
    },
    {
        "ID": "18316",
        "Question": "We are part of a software company which was just acquired by a larger one. This company has a large development center in India; we are based in Europe. We don't yet know what will happen with our projects, maybe they will be outsourced, maybe not but I want to know if we can rival an indian programmer as what salary is concerned. I know there are a lot of factors involved here, not just the salary issue, but I just want to get an ideea of the difference. Can someone mention salaries (in euros or dollars) and associated years of experience. I found some info on the web but it is not that recent. Thanks in advance! ",
        "Best answer": "When well managed, an offshore team can be more efficient than a onsite team, and sometimes cost half the price. A typical freelance C# developer can earn up to $100 per hour in western countries (sometimes lot more). The same guy (same experience) in india is 5 times cheaper (sometimes lot less). A typical employed C# developer can earn up to $25 per hour in western countries (sometimes lot more). The same guy (same experience in india is 3 times cheaper (sometimes lot less). I also discovered recently that you can take developers on leasing (or renting), just like objects (cars, etc)! You tell the leasing/renting company you need 4 C# developers with knowledge of nHibernate. They will find them, they will hire them, they will pay their salaries, they will buy computers, they will put them in an office, they will add phones, emails, etc. You pay a monthly flat fee for the service. Want more ? Just tell the company they add more developers. Want less? No problem bob, they remove developers.  The world is changing... you have to do something.  I suggest you this great book: My Job Went To India  It will helps you improve yourself in order to avoid being replaced by a cheaper (and more competent) offshore developer. To summarize: offshoring will eventually become legion. And \"simple developers\" with no added value will be replaced.  Invest in yourself  "
    },
    {
        "ID": "18339",
        "Question": "I am using the following structure to save the images that users are uploading for their items:  Images  User_dir1  Item_dir1  Picture_file1 Picture_file2   User_dir2  Item_dir2  Picture_file3  Item_dir3  Picture_file4     Can I go on with this if I am going to have thousants of users and hundrets of items per user? Talking about performance will the Linux dedicated server have trouble in the future? Thanks! ",
        "Best answer": "Well, how much time does it take to create a thousand folders over the course of say a month?  I would argue not a lot.  It would take a lot of time if you were regularly removing and creating folders, but it seems to me the only files that will be changed regularly will be the picture files themselves (or at worst, the folders themselves initially as they won't exist at first, but that will diminish over time), which is the same as if you had lumped everything in a single folder to begin with. Obviously keep an eye on space and state of defragmentation of your disk, but with a little maintenance, I wouldn't expect a lot of difficulty in your idea. "
    },
    {
        "ID": "18392",
        "Question": "Background: I'm a microsoft DBA/IT type person by day, an iOS and Mac developer by night. I have one app in the App Store and another one in the works. I try to spend as much time as possible working on my two apps, but with a new daughter and other (non computer) hobbies, I find it hard to spend more than a few hours every week programming. I'm wondering how this compares to other developers. I see a lot of apps out there written by single developers who don't do iOS development as their main job. Are they spending 4 hours a night working on their projects, or do they just plug away for a year or two before releasing anything? ",
        "Best answer": "For me it really goes in streaks.  I have a lot of hobbies, one of which is extracurricular programming.  Like most hobbies, I get into it for a while, then lose interest for a while, then come back to it later when I'm sick of all my other hobbies.   When I'm on a hardcore hacking streak, I'll go a few weeks where I spend 4-5 hours a night and a good portion of my weekends programming just for fun, in addition to the programming I do at work.  During these times, programming is constantly on my mind.  Then I temporarily burn out and give it a rest for a few months.  During this time, I spend some time on another hobby and for a while, and do almost no recreational programming.  I maybe spend an hour or two a week on bug fixes for my existing projects. I find it very hard to just consistently spend a few hours a week because to write really good hobby project code, I need to be thinking about it all the time.  On the other hand, such effort is unsustainable in the long run.  The result is this extreme bimodality. "
    },
    {
        "ID": "18406",
        "Question": "It has been six years since I have been coding. Coding into all kinds of things like ActionScript, JavaScript, Java, PHP, Ajax, XML HTML, ASP, etc. I have used arrays, maps, linked lists, sets, etc and wherever I worked people like me. But whenever I am interviewed it is very likely that people ask me question on hashes, trees, stacks, and queues. Some questions are on juggling some sorting algorithms.  I don't know if I should really know them or should I stop calling myself a programmer. There is something in me which also tells me even if people who are asking all these questions select me, they will never be making me work of these things. Am I really required to know all this? ",
        "Best answer": "If all you know how to do is write glue code, you may call yourself a code monkey.  Lots of glue code needs to be written and you can make a decent living as a code monkey.  To call yourself a Real Programmer TM and be trusted when code needs to be written from scratch, you have to know algorithms, data structures, memory management, pointers, assembly language, etc. and understand how to use this knowledge to evaluate tradeoffs. "
    },
    {
        "ID": "18427",
        "Question": "I'm creating a CMS and have lots of legacy code from other applications. I'm wondering should I bother providing other syndication formats except rss 2? I'm inclined to say no but was wondering what other peoples opinions were. ",
        "Best answer": "I would say either RSS2 or Atom, pick one. Any modern reader should be able to handle either. "
    },
    {
        "ID": "18454",
        "Question": "I've often written this sort of function in both formats, and I was wondering if one format is preferred over another, and why. public void SomeFunction(bool someCondition) {     if (someCondition)     {         // Do Something     } }  or public void SomeFunction(bool someCondition) {     if (!someCondition)         return;      // Do Something }  I usually code with the first one since that is the way my brain works while coding, although I think I prefer the 2nd one since it takes care of any error handling right away and I find it easier to read ",
        "Best answer": "I prefer the second style.  Get invalid cases out of the way first, either simply exiting or raising exceptions as appropriate, put a blank line in there, then add the \"real\" body of the method.  I find it easier to read. "
    },
    {
        "ID": "18495",
        "Question": "Is it possible to have egoless programming or is it even desirable? As a profession we do seem to want to show off the latest gizmo, technique or say “look at this awesome piece of code I’ve written”. Yet we can get very defensive when asked to submit items of work for code reviews or get negative comments from other programmers (hearing the term WTF, has never been a good sign). Can we as a profession be able to sit down and analyse a piece of code, data or architecture for its merits or constraints and a calm and respectful manner, without causing offense or antagonising our colleagues, or are we just the archetypal Dilbert character, raging against the system? ",
        "Best answer": "What you should bear in mind at all times is that for most of your professional life you don't own the code that you write. If you are an employee the software is owned by the company. If you are a contractor the software is owned by the company. If you are a freelancer then the software is owned by the client. Only if you are the company/client is the software owned by you. Once you realise this then there's less of the \"this is my code\" thoughts and attitude. Yes, it still happens and it's only human nature to get upset if someone criticises what you've just produced, but by taking that step to try to remove the attachment it should be less of a blow. From the other side when you are reviewing the code produced by someone else concentrate on what the code does and how it does it and be constructive when suggesting improvements. "
    },
    {
        "ID": "18512",
        "Question": "Given a long method with Pac-Man ifs, would it be wise to:  Break down code blocks into regions. Then refactor code blocks into methods.  Or would it be best to leave it alone? Would it be risky? Would it be a waste of time? In the absence of automated unit-tests, I'm trying to understand the risk-reward relationship associated with this manuever. ",
        "Best answer": "It's risky in the absence of automated tests. Of course you'll test as you go along, but that's tedious, and error-prone. Ideally you'd write unit tests capturing the current behaviour, but that's not always feasible. Having been in this position before (with PHP), if the code really stinks, refactor it very slowly and very carefully. I think your plan's sound - take the long method, break it up with whitespace into paragraphs, and try Extract Method. Quite how you do that depends on how many variables are in play, and how they interact. Try trace the dependencies of a paragraph before you start. Is it worth it? If the code's really awful, and its awfulness is starting to spread, then it's time to get stuck in there. If it's been running fine for ages, then you risk introducing bugs into something that for now Just Works. "
    },
    {
        "ID": "18579",
        "Question": "Is there any reason to keep using Linq to SQL, or is it better to move to ORM techniques like EF, NHibernate etc. We are using Linq to SQL in a new large enterprise application that will exist for a long time. The motivation for this new enterprise application is that the application was ordinary written in Visual Basic and since Microsoft stopped the support we where forced to rewrite the application. It’s seems that we are already there but this time with our DAL (Data Access Layer). I have already read this article, but it only compare to EF's weakness. ",
        "Best answer": "It's not dead, but Microsoft is now focused on the Entity Framework. I've used LINQ to SQL on small projects, and it's quite nice as a lightweight data-layer and i'd consider using it again on similar sized projects. The LINQ implementation itself is really good and until recently much better than the NHibernate LINQ project. On the larger project I used L2S on, I found it hard to come up with a unit-of-work pattern that I was happy with, due to limitations with the L2S 'DataContext' class. Trying to implement something like 'Session per request' with L2S seems either very difficult or impossible. I also wouldn't really consider L2S as a true ORM, as it really doesn't give you many mapping options. Your class design really needs to follow your database schema (table-per-class) otherwise it will fight with you every step of the way. Another thing I don't like about L2S is the need to use specific types (EntitySet and EntityRef) to handle collections, references and lazy-loading. This means it's not possible to keep your domain model ORM agnostic without adding another layer of abstraction. My other issue with L2S is the sole reliance on LINQ to generate queries. The LINQ provider is very well written and generally creates decent SQL for the majority of queries but I have my concerns that there are more complex queries that can't be expressed well with LINQ. Using L2S you basically have to revert to calling stored procedures in these cases, whereas (for example) NHibernate has several API's (LINQ provider, QueryOver, HQL etc) that can be used when you want more control over the generated SQL. In L2S's defence over NHibernate, there is a lot less overhead in getting it up and running on a project. "
    },
    {
        "ID": "18679",
        "Question": "This is in continuation to my previous question where I asked is it necessary to learn algorithms and data structures. I feel yes it is. Now I work in an environment where I won't ever get the chance to learn it by experimenting or practically or in any assignment. What is the right approach like the right books, right kind of problems, right kind of resources that I can go through to give six months or a year or two to learn algorithms and data structures?  And also mold my mind in a way that it can relate problems to data structures and algorithms. ",
        "Best answer": "Read. No, really, read. Read everything about algorithm and design you can possible find.  There are phenomenal books out there.  The Sedgewick algorithm books are good.  The Algorithm Design Manual by Skiena is good as well.  Together these books follow me on every bookshelf at every job I go to, along with The Mythical Man-Month. Then ask. Talk to people you respect.  Ask them what decision points they had and why they made the decisions they did.  The good ones will always be able to tell you \"I chose to do X because it's better than A, B in these ways.  I could have gone with C, but I felt this was a better choice because of this\". Next, do. Build stuff.  Build stuff that you'll never use.  Build stuff that you'll never need.  Go write a program that solves a Sudoku puzzle.  Now go do it again.  And again.  Build it 5 completely different ways.  Build a program that generates Sudoku puzzles and feed it into the solvers.  Find which solver is fastest.  And then... Find out why. The \"what\" is almost never important.  I mean, yeah, it is critical to finishing the project at hand, but at the end if you know the \"what\" without knowing the \"why\", then you might as well never done it in the first place.  You got a bullet point on your resume.  Go get a cookie and congratulate yourself.  The \"why\" is so much more important than the \"what\". And for the record Sudoku was an example.  I spent a lot of free time going through that exercise with a ton of the logic puzzles on Kongregate and learned a lot on the way. http://www.amazon.com/Bundle-Algorithms-Parts-1-5-Fundamentals/dp/020172684X/ http://www.amazon.com/Algorithm-Design-Manual-Steven-Skiena/dp/1848000693/ http://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary/dp/0201835959/ "
    },
    {
        "ID": "18720",
        "Question": "I've recently had a discussion with a coworker about versioning web applications. I don't think you need it at all, and if you just want a sanity check to confirm your latest release is live, I think a date (YYMMDD) is probably good enough. Am I off base?  Am I missing the point?  Should I use web application version numbers ",
        "Best answer": "If you resell the same web application to multiple customers, you absolutely should version it.  If it's a website that's only ever installed in one location, then the need is less dire, but it still likely couldn't hurt. You'd be less susceptible to timezone problems and such, too. Diagnosing an issue in a library version 1.0.2.25 is a lot nicer than hunting down the library build on November 3, 2010 11:15 a.m. "
    },
    {
        "ID": "18813",
        "Question": "I am putting together a proposal for a local store with both a brick-n-mortar and web presence. It's an upscale shop that also happens to be one of my wife's favorites. I am considering offering the option of barter for part of the payment. I have two questions in this regard: What would be a reasonable “exchange rate” for such a transaction? That is, in lieu of $1000 cash money, I should ask for about how much in store credit? In your own experience, has using barter introduced any complications into the client relationship? Are there particular pitfalls to be concerned about? ",
        "Best answer": "In my experience, it was worth it, but it depends on how you think about it. I did some light web work for a friend of a friend who owned an electronics store. Instead of paying me the ~$200 cash I'd have charged, he got me a monitor. Like I said, I feel we both came out ahead, but it depends on how you break it down; I know that I'd have paid well over $200 for what I got out of the deal. On the other hand, he's getting supplier pricing so he probably paid a lot less than $200 for the same. That's pretty much the only time I've gone in for a barter, and I don't regret it. I'm still using the monitor. Not sure how the equation would have changed if he wanted to effectively pay me in gift cards. I certainly wouldn't have taken a 1:1 on dollars to store credit. Probably would have pushed into the 1:1.2 - 1:1.4 range. It still needs to be worth it for both parties. "
    },
    {
        "ID": "18838",
        "Question": "I currently code with C, C++, and Python. I'm wanting to pick up a functional programming language, and right now I'm leaning toward Haskell. I do NOT want to start a \"Haskell vs Lisp\" war here; what I want to know is this: if I learn Haskell primarily for exposure to functional programming, what benefits, if any, will I gain from later learning Lisp? ",
        "Best answer": "I suggest learning both, Haskell first, then Common Lisp. My experience with Haskell was that the static typing seemed to be a restricting annoyance at first, but once I got used to it, I noticed that most of my type errors had logic errors hiding behind them. When you get to this point, and the next milestone, which is learning to think in types and define your own types as a means of expressing your solution, you'll be ready for Common Lisp.  With Common Lisp, you can add monads, currying, and everything you liked from Haskell, but you also get multiple inheritance like Frank Shearar mentioned, and generic functions with multiple dispatch, and an advanced exception handling system.  So why not just learn Common Lisp first? Coming from a procedural and OOP background, my experience has been that I didn't really understand functional programming until I had to use it exclusively. Once functional programming is comfortable, you can add the rest of the tools that Common Lisp makes available, and use whatever tool is best at the task at hand.  "
    },
    {
        "ID": "18852",
        "Question": "I'm very good at programming in C++ but when it gets to linking and the other important stuffs I feel very ignorant. I want to learn allegro without wasting time. So please suggest a book or a resource to learn the concepts mentioned above. Thanks a lot in advance. ",
        "Best answer": "I wouldn't spend time on any book. There's really not much knowledge here. Use Wikipedia, Google, and unix man to  Understand what make is (file stamp driven dependency aware synthesizer of whatever). Use GNU Make manual as a reference as you go. The important flags of GNU Make to be aware of are '-p', '-n', and '-d' Read about gcc/g++ flags for compiling/linking shared/static debug/optimized 32/64-bit modes. Read what LD_LIBRARY_PATH, LD_PRELOAD, position independent code, and rpath are. Learn what nm, c++filt, strings, and ldd are. You will need to use them to debug build problems.  This is more or less it. "
    },
    {
        "ID": "18868",
        "Question": "Since I'm learning Java in this period and I just finished with the tutorials I think I'm ready to start contributing to a project (since I know from experience this is the best way to really learn).  I've seen GWT, looks interesting so I thought I should give it a try. I'm thinking however, since GWT deploys Java applications in JavaScript why should I learn GWT when I could learn the actual thing instead (this being JavaScript).  My question: is it worth it? Wouldn't someone be better of learning JavaScript if they want to build JS applications, instead of using Java and GWT? I realize that it might be easier to build certain things with GWT but in the end is it worth it?  Thanks.  ",
        "Best answer": "GWT is essentially a compiler to native code, much like Visual Studio compiles source to byte codes or machine language. This allows the programmer to abstract the differences in underlying architecture away, much like Visual Studio users not needing to worry about the differences in word length, the number of registers and exact conventions for calling the operating system when they code for 32-bit or 64-bit.    This is a good thing, as it allows you to move some of the maintainance burden over to others.  As this is Google you know they have more development resources than you have, so you are essentially bringing in extra manpower for free. "
    },
    {
        "ID": "18886",
        "Question": "Is Computer Science science, applied mathematics, engineering, art, philosophy? \"Other\"? To provide background, here is Steven Wartik's blog posting for Scientific American titled \"I'm not a real scientist, and that's okay.\" The article covers some good topics for this question, but it leaves open more than it answers. If you can think of the discipline, how would computer science fit into its definition? Should the discipline for Computer Science be based on what programmers do, or what academics do? What kind of answers do you get from people who've seemed to think deeply about this? What reasons do they give? ",
        "Best answer": "There are two distinct IT disciplines:  Computer Science - is the discipline study of computers and computation using the scientific method. Software Engineering - is the discipline of designing and implementing software following proper engineering principles.  The two overlap somewhat, but the distinction is really about desired outcomes of science versus engineering.  The desired outcome of a scientific discipline is knowledge.  The desired outcome of a engineering discipline is things that work. So to answer your question:  Is Computer Science science, applied mathematics, engineering, art, philosophy? \"Other\"?  Computer Science is Science ... when done properly.  However, like other disciplines CS has overlaps with Mathematics, Engineering, Physical Sciences, Social Sciences, Philosophy and so on. I would also add that what most programmers do is neither Computer Science or Software Engineering.  It is more like what a craftsman does. And sad to say, neither academic Computer Science or the Software Engineering profession are as rigorous as older science and engineering disciplines.  (There are fields of Computer Science that are traditionally rigorous; for example, the ones with a strong mathematical basis.  But for many fields, it is simply too hard / expensive to do proper scientific studies on the questions that really matter.) "
    },
    {
        "ID": "18895",
        "Question": "I've been spending the last week or so learning selenium and building a series of web tests for a website we're about to launch. it's been great to learn, and I've picked up some xpath and css location techniques.  the problem for me though, is seeing little changes break the tests - any change to a div, an id, or some autoid number that helps identify widgets breaks any number of tests - it just seems to be very brittle. so have you written selenium (or other similar) tests, and how do you deal with the brittle nature of the tests (or how do you stop them being brittle), and what sort of tests do you use selenium for? ",
        "Best answer": "The purpose of Selenium is to create UI-driven integration tests. Integration tests verify that all components of your system work correctly when deployed together.  Integration tests are not a sufficient test strategy and complement other test strategies having a different focus, for example unit-testing and acceptance testing. UI-driven tests are inherently brittle, although Selenium and Watir are a step up from the early days of record-and-playback tools.  There are several ways to deal with this problem - here's is a compilation of advice from some world-class experts: Don't try to get all your test coverage from this type of tests.  Robert C. Martin  argues that your code coverage by integration tests should be about 20%.  It is simply impractical to test all paths of execution when the input is several application layers away. Get most of the test coverage from unit- and acceptance tests.  Look for links to Gojko Adzic's articles in FinnNk's answer.  Adzic argued repeatedly about testing business logic through acceptance tests and bypassing UI. But some amount of UI-driven tests still needs to be written.  This is where you need some practical advice in addition to \"don't test your business logic via UI.\"  I'd recommend Patrick Wilson-Welsh's blog as the starting point. "
    },
    {
        "ID": "18952",
        "Question": "Are there any arguments out there against using the keyword substitution (i.e. replacing $Revision$ with $Revision: 5$) feature in many version control systems?  Are there any overall bad practices that using the feature encourages?  Are the any pervasive and hard-to-solve problems that it causes when you do use it? Here an argument against using it:  Keyword Substition: Why You Don't Need It  It's succinct, and I think the argument about the problems it causes for binary files is cogent, but I don't find it compelling. ",
        "Best answer": "This is an artifact from the old days, where files were versioned on an individual basis (think RCS, SCCS, and CVS) where modern version control systems think in atomic commits and not individual files.  This means that in the old days it made sense to keep track on a file level basis and the best way to do this was in a way that could make its way to binary code.  Hence expandable keywords which could go in strings that could go in the object file. These days, you have a single commit identifier which only need to be registered once for any binary and which can be explicitly scripted to go in a file instead of the versioning system expanding keywords. This is better, simply because it means that the sources only change when YOU edit them. "
    },
    {
        "ID": "18953",
        "Question": "I am new to Mac. If I want to start a iPhone development, what is the basic hardware I require? Would Mac Mini be good enough? ",
        "Best answer": "A Mac mini will serve you just fine. I taught a course last semester that did iPod touch programming (another college had iPod touches available for educational use, so I figured programming them would be educational). The only thing I needed was to buy four Mac minis to put in the lab. The Macs didn't need to be advanced, just new. One note: Prepare for some major headaches getting your first \"Hello, World!\" app over the wire and onto your iPhone. Doing so requires the proper public/private keys and certificates. I used a university license program, but you will probably need to spend the $100 on the single developer kit. The development tools themselves are free, and you can work on the simulator right away. "
    },
    {
        "ID": "18959",
        "Question": "What proportion of programming is done for embedded systems? More generally, please share any interesting statistics on programmers by domain, e.g. real-time/business/UI/gaming/… How you define the domain boundaries (e.g. what is an embedded system) is up to you, but please give a few words to explain your definition. ",
        "Best answer": "Well, there are many orders of magnitude more embedded processors than general purpose computers. About 250 million PC produced per year. Many billions of embedded processors produced per year. Global embedded software market is worth about 3.4 billion dollars per year. http://www.slideshare.net/pboulet/socdesign Embedded systems (with hardware added) worth 88 billion dollars per year in 2009. (Now, PC's are worth more than 350 each, so there's more money in PC hardware.) Lots of embedded processors are doing \"the same thing\", but embedded is hard, costly and cost constrained, so there is a lot of pressure in the \"consumer embedded\" space to reuse code. The mainstream software industry is worth about 300 billion per annum. (from wikipedia.) The bottom line About 100 times less money in embedded than \"normal\" software. But programmers for embedded are often Electronic engineers, getting paid less; or in Asia, where they are paid 5x less than in, the USA for example. Electronic engineers in Australia often get paid 20% less then software engineers. So probably about 1/30th as much programming work is embedded. "
    },
    {
        "ID": "18979",
        "Question": "As a freelance programmer:  What is your process for gathering requirements from a client? How much time does the requirement gathering process take you? I know this is not fixed, and there are variables such as how prompt the client is in responding and such. In general, accounting for the delay in responses and such, how long does it take to get to the final requirement? Which communication channel (email, phone, instant messenger, other) do you use to gather these requirements? Do you charge for the time spent in requirements gathering? Are there any deliverables in your requirements gathering process? If so what are they?  ",
        "Best answer": "1.What is your process for gathering requirements from a client? interview, whiteboard, conference call, shop tour, worker observation, staff interviews, meetings, etc. - whatever is appropriate, whatever it takes to understand the real problem, balanced with whatever they're amenable to and will make time for 2.How much time does the requirement gathering process take you? I know this is not fixed, and there are variables such as how prompt the client is in responding and such. In general, accounting for the delay in responses and such, how long does it take to get to the final requirement? obviously this depends on the size of the project. it's not unusual to spend 20 hours in requirements and modeling for a very small project (< 100 hours), because you have to understand the business context well enough to peel back the layers of the problems the customer is presenting in order to get to the real problem that you'll have to solve to make them happy whether that 20 hours is two calendar days or six weeks depends on customer responsiveness and availability, and how much thinking you have to do in between session (for hard problems) 3.Which communication channel (email, phone, instant messenger, other) do you use to gather these requirements? all of them 4.Do you charge for the time spent in requirements gathering? Hell, yes! You have to understand the client's business, comprehend and document their problems, and propose solutions which they could then take to someone else to implement. This is the consulting part of the process, and consultants don't work for free. 5.Are there any deliverables in your requirements gathering process? If so what are they? Typically, a draft list of features, user stories, test-case descriptions, an abbreviated work-breakdown structure (with project estimates), a highlighted list of unclear/unknown areas/items for further discussion/investigation, and a list of things (info, resources, tools, access, etc.) that you will require from the client with target dates. All of this is packaged as a proposal with some background info on the business, the methods used to identify the problems, constraints and caveats about the solution, notes about the expected timelines and ROI, and a request for follow-up by a specific date. "
    },
    {
        "ID": "19006",
        "Question": "In your resume, what would you classify as a Programming Language? For instance, under Programming Languages (or a similarly titled section of your resume), do you list C#, Java, C and leave it at that or do you go and add XML (because, well it is eXtensible Markup Language) and XSLT and jQuery and all that too? If you are not going to add jQuery under languages, where exactly would you put it? One resolution to this dilemma would be put in the technologies you have used under the project, but that way, you are forcing the recruiter to go through the projects that you have done rather than giving a highlight of the technologies and languages you are familiar in.  What are your thoughts? ",
        "Best answer": "My CV has a \"Languages and Frameworks\" section that looks something like this:  ECMAScript/Javascript (JQuery, Qooxdoo, YUI) C++ (Qt, STL, ATL, MFC) Python (Wx) XML, XSLT, XSD  This is for three reasons:  Although no one would expect you to have experience using JQuery or YUI in C++, this does help disambiguate your hypothetical experience of Qt in C++ from your lack of experience in Wx in C++.  Simply placing Wx or Qt in a later bundle of frameworks doesn't do this. It gives a concise headline for each category, so that a knowledgeable recruiter can scan it to find what they want, whilst still filling it with enough buzzword to get your CV to get past recruitment bots. By not calling this section \"programming languages\", I also get to avoid being roundfiled by someone who disagrees with my assertion about whether or not XSLT is a programming language.  Not to mention those who maintain an arbitrary distinction between Programming and Scripting languages.  "
    },
    {
        "ID": "19007",
        "Question": "A question that I have been asking myself and really confused which path to take.  So I need your guys help as to the pros and cons of these 2 professions in today's world. I love web applications development as the Web is the best thing to happen in this age and nearly everyone gets by on the World Wide Web. And also tend to keep learning about new technologies and about web services. On the other hand I like software engineering also for the desktop applications as I have had experience with development small scale software in VB.Net, Java, C++, etc. Which path has more scope and better future? What's your view? ",
        "Best answer": "Stop separating the two - Web development is a subset of software engineering, like a specialization. And there's nothing wrong with having specializations.  Do you use good software engineering practices like using version control, writing specs, code refactoring, writing unit tests wherever necessary, having a formal test phase with proper testers, etc? If yes then congratulations, you are already a software engineer. If no, then these are the things you can apply even in web development and become much more productive/professional as a developer.  "
    },
    {
        "ID": "19031",
        "Question": "We usually worry about coding standards, ethics, latest technology etc. But at times we tend to ignore our writing skills and communicating skills.  Do we need to pursue a communication skills courses? Which ones do you prefer? Any online courses available which you might have tried?  ",
        "Best answer": "Writing skills and communication skills are incredibly important.  There's a quote by Karl Fogel http://www.producingoss.org \"S/he who communicates the best, has the most influence in a project\". In other words, you might be a nova-hot programmer, but if you can't communicate your ideas and changes to the rest of your team, your contributions won't make it. As Jon stated you don't need to learn from a course, but I'd recommend at least learning from someone who is an expert in professional writing. They can help you shape your writing and once you get into a good habit you'll be fine for the rest of your career.  For example I'm currently writing a technical book and both my editor and an experienced author have helped me immensely in changing my written prose into something that readers can hopefully learn from. Don't neglect the other types of communication, there are still many people who prefer phone calls, in person meetings, IRC or other forms (we all absorb information differently).  Always try to find out the preferred method of the people you are working with. "
    },
    {
        "ID": "19071",
        "Question": "I am preparing a memoir about \"Why do (some) IT projects become too complex and how to avoid it ?\". If you have ever encountered projects having complex codes or were hard to maintain, how did you manage to go through it?  If you have to choose between several applications to use in your project, what would be your top priority in their functionality and why? ",
        "Best answer": "Document, Document, Document! I work with some pretty complex MVC code, and am only now actually understanding how it works, as opposed to accepting it on faith. Whenever I make a change that I have a hard time figuring out, I put a comment in the code at that section to help clarify what is happening. It might not have helped me since it wasn't there, but it'll help the next person around. I also like to ask other programmers around me to look at my changes to see if they are clear. My company has an internal wiki that we use for documenting a lot of the meta tasks we do, like virtual server problems, common queries, etc. Whenever I come across something I've not done before but will have to do often, I check to see if there's a wiki on it. If there isn't I'll make one. More people have gotten involved in it as well recently, so I hope it will grow faster. I think it really helps communication of the more mundane tasks we have. "
    },
    {
        "ID": "19104",
        "Question": "How would you interview someone you know well or someone you may even be friends with? What if you already know their strengths and weaknesses. I would prefer to avoid this situation by delegating this task to somebody else, but what if this is not an option. I feel like there is just too much personal feelings involved and it is almost impossible to be unbiased. If you have been in an similar situations, how did you handle them?  ",
        "Best answer": "Precisely as you described - by delegating to someone else. If you already know their strengths and weaknesses what are you hoping to learn from the interview? "
    },
    {
        "ID": "19174",
        "Question": "Is Project Titan (Facebook email app) going to be a game changer from a programmers perspective? Although some details are still scarce, the internet is slowly learning more about this new software. From the perspective of a programmer, what if any functionality do you think this will bring to the mass communications sector of internet use? Will it in anyway be a \"game changer\" from the existing layer of highly successful web based email clients (such as hotmail, gmail, yahoo mail, etc) I'm interested if anyone thinks email fundamentally needs to change other than to add more \"connectedness\" to other sources of data such as social media profiles. ",
        "Best answer": "More integration inside of Facebook may make Facebook users stay inside the Facebook site more (and more page views means more advertising dollars in Facebook's pockets).  People who are e-mailed from Facebook but who don't already have an account may be encouraged to get one by their peers.  However, there are many many people who use email but do not use Facebook and are not interested in social networking.  I don't see there being much of a change.  Google Wave attempted to extend email but failed (it looked like a great concept but was confusing for users, and ultimately abandoned by Google); Microsoft Lync and Google Buzz attempt to integrate social networking and email, but I only see that as being useful for people who really want to use it for social networking. "
    },
    {
        "ID": "19199",
        "Question": "I'll use C# as an example, but it should apply globally. Say I have a string value that should be one of a few constants, but I also want the client to set which string value to use so: private int foo; private string bar;  public int Foo {     get { return foo; }     set     {         foo = value;         bar = getStringValueFromDatabase(value);     } }  public string Bar { get { return bar; } }  I use this technique quite a lot and want to know if it's considered as any formal concept. ",
        "Best answer": "It's perfectly valid OO technique, though I think it can be made a bit more efficient if the lazy-loading (which is what this effectively is) is deferred until the Bar value is actually needed.  private int foo; private string bar;  public int Foo  {     get { return foo; }     set     {         foo = value;         bar = null;     } }  public string Bar {      get      {          if(bar == null)              bar = getStringValueFromDatabase(Foo);         return bar;      }  }  Edit: I think that's an improvement on the original code though as others have observed, even better would be to abstract the data access into a separate layer. "
    },
    {
        "ID": "19203",
        "Question": "I'm planning to do a talk on Dependency Injection and IoC Containers, and I'm looking for some good arguments for using it. What are the most important benefits of using this technique, and these tools? ",
        "Best answer": "Most important, for me, is making it easy to follow the Single Responsibility Principle. DI/IoC makes it simple for me to manage dependencies between objects.  In turn, that makes it easier for me to break coherent functionality off into it's own contract (interface).  As a result, my code has been far more modularized since I learned of DI/IoC. Another result of this is that I can much more easily see my way through to a design that supports the Open-Closed Principle. This is one of the most confidence inspiring techniques (second only to automated testing). I doubt I could espouse the virtues of Open-Closed Principle enough. DI/IoC is one of the few things in my programming career that has been a \"game changer.\"  There is a huge gap in quality between code I wrote before & after learning DI/IoC.  Let me emphasize that some more.  HUGE improvement in code quality. "
    },
    {
        "ID": "19225",
        "Question": "Java's checked exceptions have gotten some bad press over the years. A telling sign is that it's literally the only language in the world that has them (not even other JVM languages like Groovy and Scala). Prominent Java libraries like Spring and Hibernate also don't use them.  I personally have found one use for them (in business logic between layers), but otherwise I'm pretty anti-checked exceptions. Are there any other uses that I don't realize? ",
        "Best answer": "First of all, like any other programming paradigm you need to do it right for it to work well.   For me the advantage of checked exceptions is that the authors of the Java runtime library ALREADY have decided for me what common problems I might reasonably be expected to be able to handle at the calling point (as opposed to a top-level catch-print-die block) and consider as early as possible how to handle these problems. I like checked exceptions because they make my code more robust by forcing me to think about error recovery as early as possible. To be more precise, to me this makes my code more robust as it forces me to consider strange corner cases very early in the process as opposed to saying \"Oops, my code does not handle if the file doesn't exist yet\" based on an error in production, which you then have to rework your code to handle.  Adding error handling to existing code can be a non-trivial task - and hence expensive - when reaching maintenance as opposed to just doing it right from the start. It might be that the missing file is a fatal thing and should cause the program to crash in flames, but then you make that decision with } catch (FileNotFoundException e) {   throw new RuntimeException(\"Important file not present\", e); }  This also shows a very important side effect.  If you wrap an exception, you can add an explanation which goes in the stack-trace!  This is so extremely powerful because you can add information about e.g. the name of the file that was missing, or the parameters passed to this method or other diagnostic information, and that information is present right in the stack trace which frequently is the single thing you get when a program has crashed. People may say \"we can just run this in the debugger to reproduce\", but I have found that very frequently production errors cannot be reproduced later, and we cannot run debuggers in production except for very nasty cases where essentially your job is at stake. The more information in your stack trace, the better.  Checked exceptions help me get that information in there, and early.  EDIT:  This goes for library designers as well.  One library I use on a daily basis contains many, many checked exceptions which could have been designed much better making it less tedious to use. "
    },
    {
        "ID": "19267",
        "Question": "We all know what a software development manager does, but I'm afraid that we only know it vaguely. We think we know what he is doing, but to exactly list down what is the job scope is a bit hard. In your opinion, what are the roles of a software development manager? ",
        "Best answer": "Speaking as someone in the job (who has also been a developer), the key things I have to do are:  Keep the development team on track (and happy where possible) - move things out of their way that are stopping them work where possible, explain why it's not possible where they can't be moved to try and reduce any resulting stress (people are more likely to accept things if they at least understand them).  Ultimately if there is a conflict between the project and the team that can't be resolved, normally the project will win.  That's doesn't necessarily make you popular with the team but you're paid to deliver projects/products, not as a union leader.  The obvious skill is in minimising how often this happens.  Make sure that the team are communicating with the customer the right amount.  This tends to be equal parts keeping the customer away from the team, and making sure the team are asking the customer about things they don't understand fully (rather than just making assumptions which may be incorrect).  Developers are very big on making sure that the customer doesn't disturb them and occasionally forget that the customer might have something useful to add. Project planning and prioritisation of resource conflicts, customer demands, support issues and the like.  I tend to be the person who says this customer takes priority over that one, or that this bug has to be fixed before it ships but that one can go out as a known issue. Manage the commercial side of development - that is making sure that things that should be charged for and being charged for and that we're not trying to charge for things which should be covered under support. Be the voice of the team in the business and the business within the team - help everyone understand the other's position and help resolve differences where they arise.  This largely tends to cover cultural conflicts between the teams needs/wants and the larger organisations, and budget matters.  This is actually pretty shitty as it means when there are disagreements you're everyone's enemy. Work with the team to ensure sufficient processes and tools are in place to meet the requirements of the business and customers.  Make sure that these processes are being followed and adjusted as needed.  Some of this is making sure the team define processes (for instance for technical things they understand better than I do), some is defining them myself (for things I understand better than they do - planning, estimating and so on).  The important word here is sufficient - you don't want process for process sake but there are things that have to happen and process is the best way to achieve that consistently. Ensure that every member of the team is working to at least a reasonable level, and ideally beyond that.  Work with them to help resolve any issues that are preventing them reaching this level.  I'd love to say that my role is making them be the best they can be but while this is true to a degree other demands (project, budget, time) mean that this will almost always be compromised to a greater or lesser extent. Doing all the administration and stuff the organisation (and the law) demand  Overall it's part mentoring, part secretarial, part project management, part account management and part PR (for the team).  There's a lot of picking up things the developers don't need to think about or don't think about doing, and some making sure they do things they need to do but don't want to do. What it's not about is being the best developer (generally you're too hands off to remain current for long so you need to accept that people will know more than you - the skill is in knowing where your longer but outdated experience is more relevant than their shorter but more recent experience) or being some sort of dictator.  In that respect the best way to think about it is not that you're more senior, just that you have different responsibilities.  Sometimes this will involve making the final call on something (which may go against the views of the team) but more often it should be about consensus or compromise. "
    },
    {
        "ID": "19273",
        "Question": "Reading Niklaus Wirth, one can notice that despite some popularity of Pascal, he is not glad that Oberon (as a \"polished\" successor of Pascal and Modula) didn't get much popularity. I never did anything in Oberon, but reading the page Oberon For Pascal Developers I really did not like many of the changes as a Delphi/pascal developer, for example   forcing the reserved words to be always uppercase making the language case-sensitive getting rid of enumeration types   What do you think about Oberon, is it really \"a better Pascal\" from your point of view? ",
        "Best answer": "Yes, I would call Oberon a better Pascal. With Oberon, Professor Wirth got to the core of object oriented programming with type extension and procedure variables. I find it elegant that Oberon is a smaller language than Pascal with much more power.  Oberon 2 took the language a step further by binding methods to records. I do dislike the upper case reserved words. I find the syntax an improvement with the elimination of many begins and ends. Oberon was used to write a very interesting operating system described in Project Oberon: The Design of an Operating System and Compiler. "
    },
    {
        "ID": "19274",
        "Question": "When you encounter in your work some boring, repetitive (possibly not-programming) task how do you deal with it? Do you try to automate it immediately? Do you postpone automation till you encounter the task twice? Do you try to delegate it? Do you try to find something interesting in it? What do you do to make the work more joyful? Learn deeper to make it more interesting? Learn deeper immediately or when you need it regularly?  ",
        "Best answer": "If automating it will take longer than getting the task done, then I'll postpone automating it until I have to do it a second time.  But if I can automate the task faster than I can do it, then it's automated without hesitation. No sense wasting time. "
    },
    {
        "ID": "19359",
        "Question": "In every problem solving there's always people who think differently, who come with a 6th option when only 5 were suggested by others, who think \"out of the box\". Please tell how one can achieve such ability? And what it takes to achieve it? ",
        "Best answer": "There is no box! There is a specific problem to solve, and a set of constraints which might apply. Work out what the problem is (think abstractly and in real terms), defining it in both specific topic-based terms, and in more general terms. Examine each of the constraints (don't make assumptions) to see if, when, and to what extent they might apply. Look at the problem from the perspective of who it affects (don't forget the goal) as well as from behind the scenes. Don't make assumptions. If you assume certain things are true when they're not, you'll prevent yourself from examining different perspectives. Also challenge existing decisions/assumptions that others have made - there might be a good reason for it, or there might not, or there was one that no longer applies. Think abstractly. Learn to see things as patterns and in abstract terms. When you spot a pattern, consider similar things and see if you can apply actions from it to the current thing. If your subject area has named patterns, learn them - but don't treat them as cookie cutter solutions. Don't think abstractly. Always try to see things as they are too - remember that \"users\" are people, and they're not always logical or rational. Practise empathising with the people using what you create. Don't forget the goal. Sometimes it's easy to get bogged down with a particular target/implementation (e.g. \"how do we fit these X controls into the UI?\") instead of remembering the real goal (such as \"how do we allow the user to do Y?\") Never stop learning. General knowledge can be a great source of inspiration - a lot of problems have been solved by someone already - the more you know the more you might remember something applicable to the current situation. Be a good programmer, not just a good at [programming language]. Don't be scared to learn multiple technologies and techniques - even multiple \"overlapping\" languages can help you to see things in different terms, but a good variety of different ones may help more. Of course pick a few areas to specialise/master, but also make sure you have a decent grounding in general concepts, which you can gain by learning multiple different languages, Don't assume someone is too inexperienced to help. Sometimes people that appear not very knowledgeable, or that have never programmed, can appear to be useless for a programming problem - but that doesn't mean you should ignore them. Everyone has different perspectives and skill-sets, and might provide a unique insight that spring-boards you to a solution. Young kids can especially be a good source of an \"untainted\" perspective that can be inspirational. "
    },
    {
        "ID": "19397",
        "Question": "One day while trawling through the Java language documentation, as you do, I found this little beauty lurking within Double: 0.25 == 0x1.0p-2  Now, obviously (!) this means take the number hexadecimal 1 and right shift it decimal 2 times. The rule seems to be to use base 16 on the integer side and base 2 on the real side.  Has anyone out there actually used the right hand syntax in a necessary context, not just as a way getting beers out of your fellow developers? ",
        "Best answer": "Just a guess:  Some numbers that are rational in base10 are irrational in base2, and conversely some numbers that are rational in base2 are irrational in base10 (Please correct me if I'm wrong...I'm looking this up to confirm). EDIT: thanks to \"Note to self - think of a name\" for correcting me. I supposed that if you had a need to specify an exact binary value as a floating point (such as some epsilon value in graphical programming), then it might be more convenient to use this syntax.  For example, 1/1024 is 0.0009765625 in base 10, but using this syntax it can be written as 0x1.0p-10 "
    },
    {
        "ID": "19416",
        "Question": "Hopefully the topic is not too confusing, I am curious though as lately I have seen more and more use of the word parameter to denote a function's passed in values, whereas during my studies I feel as if argument was used almost exclusively.   Is parameter more appropriate as argument is more specific to a programs arguments specific at runtime whereas when a function is called, the values passed to it are parameters to the function itself?  From a curious programmer.  ",
        "Best answer": "See this article in wikipedia with a good explanation. The difference is that parameters appear in method/function definitions, while arguments are concrete values you pass to a method/function. "
    },
    {
        "ID": "19437",
        "Question": "I've been looking at the Ms-PL license that ASP.NET MVC and DotNetOpenAuth are published under and 3A says this: 3 Conditions and Limitations  (A) No Trademark License- This license does not grant you rights to use any contributors' name, logo, or trademarks.  Does this mean that I cannot name my project ASP.NET MVC DotNetOpenAuth Sample Project and publish it if I use these two technologies or does it just mean that I cannot use the author's name to promote this project? ",
        "Best answer": "Trademarks are a business thing, so a personal name isn't a trademark unless that person uses it as such in a business or similar enterprise.  Such things as Microsoft Windows and ASP.NET are trademarks.  I could trademark \"David Thornley\" software if I were to write and distribute it (remember Peter Norton and the Norton Utilities?), but since I'm not actually in that business my name isn't a trademark. Also, pay attention to the wording.  The license doesn't grant any rights to a trademark, but it doesn't restrict any ability you already have.  You can't call your project Microsoft software, but you can advertise that it runs on Microsoft Windows.  The guiding rule (at least in the US) is if you're trying to create any confusion about who produces the software, or if a reasonable person could be confused.  You'd probably be safe calling it \"Project Gligoran, using ASP.NET and running on Microsoft Windows\", but if you're worried consult a trademark lawyer. Also, in the US, trademarks work on a \"protect it or lose it\" basis.  If you use a trademark improperly, and the owner of the trademark finds out about it, the owner is legally required to take some sort of action against you or risk losing the trademark. "
    },
    {
        "ID": "19457",
        "Question": "Today I was looking at some blogs on Ruby and Python. A Python loyalist wrote a few lines of code with a loop to show how Python is faster than Ruby, with a similar code. I also found a blog of a Ruby disciple who says that it is wrong and he too submitted a code sample and benchmarking results. I am wondering whether background programs or background processes may sometime hinder the results and we may not get exact benchmarking scenario. Please correct me if I am wrong. ",
        "Best answer": "It's possible.   But if that's the case, running the benchmark again would almost certainly produce different results.  That's why benchmarks are always run multiple times, to make sure that there are no background processes that are skewing the results. "
    },
    {
        "ID": "19513",
        "Question": "I've created a distribution of my open source application framework in a working virtual appliance.  It includes everything to get started with the tutorial.  The distribution is Fedora 14 running Tomcat 5.5 and Oracle 10g Express Edition, plus my framework.  It is completely preconfigured and boots into a working running copy. Would this be something you might try? What assurances might you need to get you to try it? Edit: The VM is just over a 2Gb download.  Alternatively it is also available via 23Mb download for the source and a PDF detailing how to configure the Tomcat and Oracle dependencies. ",
        "Best answer": "Yeah, I think this is a terrific thing, especially for systems which require (potentially) complex configuration to get up and running.  If you can ship a working VM image that you just \"boot and go\" it makes it far easier to get a system up and running for evaluation / analysis ( at a minimum).  I believe that if you want people to evaluate your software and see the value in it, you need to make it as easy as possible for them to get it deployed and working so they can give it a spin. I wouldn't make it the only means of distribution, but it's something I think many software projects should make available.  I'm planning to (eventually, when I can find time) do a virtual appliance distribution of my own project.  So yeah, I'm definitely on-board with this concept. That said, I don't think having a VA distribution obviates the need to make it as easy as possible to build, deploy and configure the system via other methods.  Building from source, for example.  I'd still try to do as much as possible to make the source download, build, deploy, configure stuff as easy as possible. Edit:  also, just to be clear... I'm talking about this in general / conceptual terms only.  Since you didn't say much (if anything) about what your actual project is or what it does, then no, I'm not actually going to download it and try it out.  I'm assuming you were asking this in the context of \"Would you recommend that I make this available to people who are already interested in my project and visit the website looking for a download\" as opposed to \"would you, Stackexchange User $FOO, want to download this appliance and try it out?\" "
    },
    {
        "ID": "19541",
        "Question": "When building a parser to a programming language what I earn and what I lost choosing one or the other? ",
        "Best answer": "I'll contrast LL and LR parsing for a number of criteria: Complexity LL wins here, hands down. You can easily hand-write an LL parser. In fact, this is commonly done: the Microsoft C# compiler is a hand-written recursive descent parser (source here, look for a comment made by Patrick Kristiansen - the blog post is very interesting as well). LR parsing uses a rather counter-intuitive method to parse a text. It works, but it took me some time to wrap my head around how it works exactly. Writing such a parser by hand is therefore hard: you'd be more or less implementing an LR parser-generator. Generality LR wins here: all LL languages are LR languages, but there are more LR languages than LL languages (a language is an LL language if it can be parsed with an LL parser, and a language is an LR language if it can be parsed with an LR parser). LL has quite a few nuisances that will bother you when implementing just about any programming language. See here for an overview. There are unambiguous languages that are not LR languages, but those are pretty rare. You almost never encounter such languages. However, LALR does have a few issues. LALR is more or less a hack for LR parsers to make the tables smaller. The tables for an LR parser can typically grow enormous. LALR parsers give up the ability to parse all LR languages in exchange for smaller tables. Most LR parsers actually use LALR (not secretively though, you can usually find exactly what it implements). LALR can complain about shift-reduce and reduce-reduce conflicts. This is caused by the table hack: it 'folds' similar entries together, which works because most entries are empty, but when they are not empty it generates a conflict. These kinds of errors are not natural, hard to understand and the fixes are usually fairly weird. Compiler errors and error recovery LL wins here. In an LL parse, it's usually pretty easy to emit useful compiler errors, in particular in hand-written parsers. You know what you're expecting next, so if it doesn't turn up, you usually know what went wrong and what the most sensible error would be. Also, in LL parsing, error recovery is a lot easier. If an input doesn't parse correctly, you can try to skip ahead a bit and figure out if the rest of the input does parse correctly. If for instance some programming statement is malformed, you can skip ahead and parse the next statement, so you can catch more than one error. Using an LR parser this is a lot more difficult. You can try to augment your grammar so that it accepts erroneous input and prints errors in the areas where things went wrong, but this is usually pretty hard to do. The chance you end up with a non-LR (or non-LALR) grammar also goes up. Speed Speed is not really an issue with the manner in which you parse your input (LL or LR), but rather the quality of the resulting code and the use of tables (you can use tables for both LL and LR). LL and LR are therefore comparable in this respect. Links Here is a link to a site also contrasting LL and LR. Look for the section near the bottom. Here you can find a conversation regarding the differences. It's not a bad idea to critically look at the opinions voiced there though, there is a bit of a holy war going on there. For more info, here and here are two of my own posts about parsers, though they are not strictly about the contrast between LL and LR. "
    },
    {
        "ID": "19573",
        "Question": "What are the worst false economies (that is ways of saving money that ultimately cost more than they save) prevalent in the software industry and how do you combat them? ",
        "Best answer": "Technical Debt ie \"Just do it quickly, we'll refactor later\". Firstly because I have yet to see someone engaging in this behaviour actually refactor later. Secondly because doing things the quick way, instead of the good way makes it harder to add future features or resolve future bugs so you end up losing time in the long run. Sadly, many still think it saves developer cycles to have them do something fast. I guess it's possible, but I have yet to see it in practice. "
    },
    {
        "ID": "19592",
        "Question": "Im really not getting  the idea:  What is Detailed Design. Why use Detailed Design. Advantages/Disadvantage of using Detailed Design. Any alternative methods other than using Detailed Design.  Could some one please guide/explain me ? ",
        "Best answer": "1) When most people talk about detailed design, they are referring to a process known as top-down design.  In short, when you think about the problem you are trying to solve, you start at the highest level and then work yourself into the details.  This approach works very well when you have an overall structure you want your application to live within.  At the macro level you are considering how many machines will be needed to host your application, which existing services you will need to use, etc.  As you dive deeper, you are looking at use cases (or user stories if you prefer that terminology), and error handling (use cases have both normal and error condition paths to worry about).  As you go even further into the details, you are looking at your algorithm, state transitions, logical sequence, and how internal parts of the code work together. 2) The classic top-down approach to detailed design is what is taught with the \"waterfall\" methodology, IEEE process guides, UML vendors, universities, and CMMI among others.  In many of these heavy processes they have you writing two design documents.  One is the overall architectural diagram (the top level design).  The other is the detailed design where you go further down the rabit hole.  In many cases it is the only approach to design many people know.  It is a very logical and methodical approach to breaking down a software problem. 3) The main advantage is that you have identified what the critical sections are likely going to be.  If you will need to start working on how your software will be using another existing service, you have compiled your list of integration points.  You would be able to start talks with the owners of those services to plan your integration along with how to handle unexpected events. The main disadvantage is that many times people go too far down the rabit hole, and the design document takes a life of its own.  While it is advantageous to have an overall vision and architecture for how an application will work, you will invariably find your initial thoughts on the core details were flat wrong.  When that happens, either the design document is neglected or you have whole teams maintaining the paper and slowing progress on work. 4) I've already mentioned top-down design, so it follows there must be a \"bottom-up\" approach, right?  As it turns out, there is.  Essentially, the \"bottom-up\" approach is the core thought process behind Test Driven Development and Continuous Design methodologies.  Essentially, the details of working code start driving the design of how larger chunks of working code cooperate and interface with each other.  With TDD, you have unit tests to ensure that the details behave properly and keep validating your design.  Continous design teaches us that we will never truly know the details until the software is done.  Therefore, why try to fight the fact?  Instead of a big up-front design stage, the design is built in increments over several iterations of design/code and testing.  It is actually a very liberating concept that lets you concentrate on solving problems. Bottom line, there is no perfect answer.  For me, I find a healthy balance between top-down and bottom-up design works well.  Essentially, I still take the time to think about the big picture.  I even map out a plan for the user interface idioms and my best ideas for it's design.  However, I don't go into great detail because I know the details will change.  Basically, you need to stop the top-down process when you are getting to areas you are not sure about.  Once you get to those areas, start with the bottom-up approach.  This lets you try different things, verify with the customer if it makes sense to them, and continue to improve over time. "
    },
    {
        "ID": "19627",
        "Question": "As I have had it explained, the open/closed principle states that once written code should not be modified (aside from bug fixes). But if my business rules change shouldn't I modify the code implementing those changes? I suspect I'm not understanding something about how the principle because it doesn't make sense to me. ",
        "Best answer": "This is probably the hardest of the solid principles to explain. Let me try. Imagine you wrote an Invoice class that works perfectly and has no bugs. It makes a PDF of an invoice. Then someone says they want an HTML invoice with links in it. You don't change any code in Invoice to satisfy this request. Instead, you make another class, HTMLInvoice, that does what they now want. You leverage inheritance so that you don't have to write a lot of duplicate code in HTMLInvoice. Old code that was using the old Invoice isn't broken or really affected in any way. The new code can use HTMLInvoice. (If you also do Liskov Substitutability, the L of solid, you can give HTMLInvoice instances to existing code that's expecting Invoice instances.) Everyone lives happily ever after. Invoice is closed to modification, open to extension. And you have to write Invoice properly in advance for this to work, btw. "
    },
    {
        "ID": "19631",
        "Question": "I think it's safe to assume that for most programmers, producing documentation is not as fun as actually coding. I think it's also safe to assume that most good programmers recognize the need for useful documentation, and the code that they write is not an exception by any stretch of the imagination. So, I'd like to know: what's the best documentation you produce (and source code doesn't count). Answer can be anything from comments to unit tests. The bigger question is why is that the best documentation you produce? ",
        "Best answer": "Our Wiki. Because people actually use it and update it. I think the nature of a Wiki lends itself to the way developers want to work which is just get the facts down and move on.   It's quick and easy, searching makes it simple to find what you're looking for which minimises the chance of duplication (and the subsequent \"so which is right\") and you need minimal technical skill to read or change stuff (as opposed to, say, tests).   Versioning is automatic, formatting is basic but effective which means the content tends to be prioritised over prettiness, and there's no pressure to have cover pages, change tables, summaries and so on all of which add work for little benefit.  Because it doesn't have a specific structure, people just worry about putting down what's important, not what fits. That's all my guess as to why it gets used but as a development manager the main thing I like is that it does get used and remains broadly up to date. "
    },
    {
        "ID": "19649",
        "Question": "It's a common practice to place copyright notices, various legal disclaimers and sometimes even full license agreements in each source file of an open-source project. Is this really necessary for a (1) open-source project and (2) closed-source project? What are you trying to achieve or prevent by putting these notices in source files? I understand it's a legal question and I doubt we can get a fully competent answer here at programmers.SO (it's for programmers, isn't it?) What would also be interesting to hear is, when you put legal stuff in your source files, is it because \"everyone does it\" or you got legal advice? What was the reasoning? ",
        "Best answer": " Is this really necessary  No. It's not legally required. (I am not a lawyer, but I've seen this stated by one.)  If you've got a project where individual files might be taken out of context, it may be sensible - but it only requires a couple of lines, to say something like:  This file is part of <project> which is released under <license>.   See file <filename> or go to <url> for full license details.   For anything else, you can simply put a LICENSE text file in the project root, and any relevant details/credits/etc in the README file - it's still copyrighted (automatically), so it's just a question of being clear license-wise in the readme file. "
    },
    {
        "ID": "19673",
        "Question": "I'm a fairly new convert to Emacs and I really love it as an editor, primarily because I keep finding new and super-useful commands. Are there any other programmer 'must known' commands missing from my list? M-x replace-string     - Find and replace a given string. M-x goto-line          - Goto a specific line M-x column-number-mode - Show the current column number in text bar  ",
        "Best answer": "Well, First You need to know some of the basics of text editing: C-w : Cut  M-w : Copy C-y : Paste C-x s : save C-x c : save all and close  Then, it's handy to learn how to move around the file: M-b : back one word M-f : foward one word C-a : beginning of line C-e : end of line C-n : next line C-p : previous line M-< : beginning of buffer M-> : end of buffer     Then, It's good to start learning how to navigate with multiple files/buffers and windows C-x C-f : find file C-x b : switch buffer C-x k : kill buffer C-x 2 : split-window-vertically C-x 3 : split-window-horizontally C-x o : switch window C-x 0 : kill this window C-x 1 : kill all other windows  After that, here are a few other misc. commands that can come in handy: C-s : search C-r : search backward M-/ : autocomplete word (based on previous words in the file) M-x : align-regexp M-( : start keyboard macro M-) : end keyboard macro C-x e: execute keyboard macro.  For a complete reference: link "
    },
    {
        "ID": "19701",
        "Question": "I am being asked to define several of my algorithms in mathematical terms to describe my work to a customer.  I am trying to determine if anybody knows whether common operators for collections like sequences, lists, tuples, etc have been defined. If so, is there a good reference that I could be pointed to. Thanks. I am interested in the actual symbols used.  I am wondering if the following would make sense or be appropiate to anybody. Given two sequences (or strings): S = (A, B, C) and T = (A, D, H) In my mind, the intersection of these sequences would look like S ∩ T = (A) and the union of these sequences would be S ∪ T = (A, B, C, A, D, H) ",
        "Best answer": "Sequences or lists in which there is an implied ordering of the elements are commonly delimited between ⟨ and ⟩. For example: S = ⟨A, B, C⟩         (They look like <A, B, C>, but taller. HTML entities are &lang; and &rang; ) If there is no order implied, use set notation. S = {A, B, C} Tuples, such as rows from a table or ordered pairs/triples, use parentheses: car = (Toyota, Camry, 2010) coordinates = (10, 45) The union and intersection of sets are represented with the ∪ and ∩ symbols, as usual. For lists, the operations are different. You concatenate lists rather than finding their union. This can be represented as S+T or simply ST (depending on who's watching). As tuples are indivisible, the union or intersection of two of them is nonsensical. You might want to ask this on https://cstheory.stackexchange.com/ "
    },
    {
        "ID": "19718",
        "Question": "\"Google Docs\" allows for real-time, collaborative document editing. Multiple document viewers are able to simultaneously read and change a document's content so that one user's updates are instantly applied and visible to all other viewers.  Are there any software/web-app IDEs that facilitate this type of collaboration for writing code? Which processes (ie pair programming) stand to benefit from a system like this and which don't?  Does this model for collaborative editing present a viable alternative to the current source control systems generally used by teams of software developers working on a single code base?  Related but less general ",
        "Best answer": "Collaborative code writing tools Visual Studio.Net has a plugin called wave-vs.net that allow real-time collaborative editing to support pair programming features. Eclipse has two plugins called DocShare and Saros, that allow real-time collaborative editing of documents (DocShare) or projects (Saros). (the above quoted from wikipedia) Which processes benefit? Obviously pair programming. Off the top of my head I can't think of any immediate show-stoppers when considering the improvement the GDocs experience brings, and extending that improvement to collaborative coding. I think the collaborative approach would work best if multiple people are working on sections of code that can be considered 'close'. It can also improve round-trip time for changes, as you don't have to wait for a commit-update cycle to have the latest version. Are they an alternative to VCS? They are complementary. They both serve the same purpose of streamlining contributions from team members. But there is a fundamental difference in their approach: Current VCS'es mostly document snapshots of a project, while collaborative editing documents fine-grained changes, capturing the thought process of a developer.  Furthermore, commits in a VCS are neatly organised as a change set. In a collaborative environment, on the other hand, you can't even ensure the code will compile at any moment before completion of the whole project, because someone else might always be in the middle of writing a line of code. Change is continuous. Therefore it might not be possible to unravel the contributions into change sets.  "
    },
    {
        "ID": "19720",
        "Question": "I try to find an answer of a very difficult question. The question is:  How many developers and how many software companies are there in the world?  For example if I check the Statistic Austria page I can see that in Austria there are 8383 companies with 35522 employees defined as \"Computer programming, consultancy and related activities\" and another 4102 companies with 15171 employees defined as \"Information service activities\". This statistic is good, but it is only for Austria. Is there a similar for Europe, USA, Worldwide? ",
        "Best answer": "Most developed countries keep statistics, although they all use slightly different standards for what they measure, so it's hard to compare. For the United States, there are 1,336,300 programmers, according to the Bureau of Labor Statistics. The United Kingdom has 333,000 \"software professionals,\" according to the Office for National Statistics. In Canada there are 387,000 people working in IT according to Statistics Canada. Japan has 1,016,929 people working in \"information services\" according to Official Statistics of Japan The BLS maintains a detailed list of online statistics agencies With a bit more research you can probably gather data for the rest of the world; if you do, please edit this answer (I will make it community wiki) so that we can develop a single source of information. "
    },
    {
        "ID": "19807",
        "Question": "I'll be forming a remote team, and I want to be able to use scripts (or something) to make sure that people's code is checked in every night.  I currently use some BASH scripts that are kindof wonky and aren't customizable easily. Does anyone have any useful tools, tips, or resources for this? Are there any pitfalls to doing this that I'm not seeing, or can I do it better another way? (cross-posted to super-user) ",
        "Best answer": "Why do you want to force people to check in code ? Doesn't seem like a good idea to me; if the developer is not ready to commit changes, there is probably a good reason.   What is the benefit to be gained from being forced to check in everyday ?   I have half built stuff that I wouldn't wish on anyone.   Every place I have worked the checked in code is assumed to work, or has passed tests and compiles correctly etc. I'd set up a continuous integration server, hudson, in conjunction with subversion or cvs. (backing-up should be different to checking in, imho) "
    },
    {
        "ID": "19845",
        "Question": "Over the years I have developed several style based techniques which I use to keep my from make Error if(const == lvalue) rather then if(lvalue == const) since the first can't fall victim to the classic accidental assignment goof. I recently  worked on a project with very unusual style standards and I found that I had much greater difficulty reading code.  Has anyone seen any statistics on a particular coding style and its defect levels or have any experience with changing style alone to improve defect rates. ",
        "Best answer": "If you haven't read it yet, you should definitely take a look at Code Complete, 2nd Edition by Steve McConnell. Almost the entire book is devoted to this type of discussion along with actual studies to back up his ideas. "
    },
    {
        "ID": "19851",
        "Question": "This is happening in every team. For some reasons, conflicts arise in the team and they affect the overall motivation and productivity. What is your recommended approach to solve that common problem? Examples:  one part of the team wants to implement dependency injection, the other part think it's a waste of time. some developers think the rest of the team is slowing down development (which explains why they are late on the schedule) personal incompatibilities between one or more developers one developer refuse to talk to another one (without apparent reason)  ",
        "Best answer": "I have had a team of 10 people for two years without a conflict(touch wood) I could be lucky or may be doing something right. The best way to handle conflict is never to let one exist for a longer time. There are several core values that you can preach.   Team Spirit Fairness in everything (compensation/rewards) Being appreciative Give recognition, responsibility Give freedom Let people know they are not greater than the team Personal success means nothing if the team fails attach personally to people  never show a carrot you dont intend to give  never hire(no matter how good) who could ruin the team communicate more often etc etc.  Appreciate whenever someone goes beyond the job Give regular feedback on performance and set expectations preferrably monthly. Let people know when they behave like children.  All these take guarded effort from some one. Software is pretty much a team game individual brilliance is generally short lived. If I go by your examples :  We have decided decided to go with dependency injection. Period. We will see if it is the best way or not. If it is not, you get a chocolate :-) till than cooperate and let's make this thing happen If the rest of the team is slowing you down you help them to make it faster they are your teammates you are the elder guy, help them. I know you are good. Talk to both of them tell them they are spoiling the environment. If nothing works get rid of one of them or both of them.  One thing I find very effective is to repeat \"we are a good team\" and repeat \"we are a team to the lonely one's\". "
    },
    {
        "ID": "19856",
        "Question": "The Joel Test includes the question:  Can you make a build in one step?  Which is important, but I'm wondering, how streamlined have some of the deployment processes been? All software from shrink-wrap to web applies: what's the best deployment environment you've worked with, and what steps were involved? ",
        "Best answer": "It's the environment I have set up in my company, and am working with right now. Description of the environment We are a team of 4 developpers, working on a Java desktop project. The source code is under Mercurial, with the main repo hosted on our development server. We mostly use TortoiseHg to work with Mercurial. The projects that we open sourced are on BitBucket. The project is built with Maven. The IDE we use is Netbeans, which works impressively well with Maven (it works ok with Mercurial, too). Our dev server runs Archiva, which is a proxy Maven repository. We use maven to build the project, but we use it also to execute it (mvn exec), to deploy the generated artifacts to Archiva (mvn release), and to generate an assembly from the artifacts hosted by Archiva (mvn assembly). We have a Redmine bugtracker too, and it is aware of the Mercurial repos. We use a RSS client to be informed of the project activity (from Redmine and Mercurial). We also have a Jabber server to send messages and files to each other. We set up an Hudson server (continuous integration) and a Sonar server (code metrics). But in practice we don't really use it. We have the choice of using Windows or Linux Steps to make a release Example to release a version 1.1.3 # tags the VCS, updates all the version numbers in the maven config file mvn --batch-mode release:prepare -DreleaseVersion=1.1.3 -DdevelopmentVersion=1.1.4-SNAPSHOT # performs a clean build, runs all tests, deploys to the server mvn release:perform # creates a unique jar (the final product) from the previously deployed artifacts (no recomilation involved) <update the version number in a config file to 1.1.3> mvn assembly:assembly  "
    },
    {
        "ID": "19896",
        "Question": "For the last couple of years, all of the serious projects I have worked on have been either web based, or had a non graphical user interface (services, command line scripts etc...).  I can throw together a WinForms app or do some simple WPF when needed, but I've never really delved into some of the lower level API's like MFC or QT. I understand that this depends on the situation but in general is it still worth taking the time to learn desktop development well or are applications moving to the web and mobile devices at a pace which makes this knowledge less relevant?  Also, do you expect developers you work with to have desktop gui expertise? ",
        "Best answer": "I'd say yes, it is.  There's sort of a pendulum effect in program development.  First everything ran directly on the computer.  Then when the computer became powerful enough to run multiple programs, they got mainframes with dumb terminals.  But dumb terminals really suck in terms of usability, so as soon as computers got powerful enough to put reasonable amounts of hardware inside a terminal-sized system, we got personal computers, and everything ran directly on the computer. Then they invented the World Wide Web, and we're back to a mainframe (server) and a dumb terminal (browser.)  But dumb terminals still really suck in terms of usability, and people are starting to relearn the lessons of 30 years ago, and we're trending away from that again.  A lot of the really hot development these days is for desktop (or mobile) apps that run locally, but are able to connect to the Internet for specific purposes to enhance their functionality. "
    },
    {
        "ID": "19966",
        "Question": "I have been tasked with evaluating a new PC for our dev team that is spec'd only slightly better than our current models.  Could anyone recommend appropriate benchmarking software that would be useful to emulate typical development tasks? ",
        "Best answer": "I'd use a benchmark consisting of builds, integration and performance tests, and other development activities most frequently run on machine's of developers working in your organization. Build time is important in day-to-day software development activities and if it's too long, build may become a bottleneck.  Suppose the build runs 15 minutes and occupies all of some machine's resources.  With a better machine, it takes only 5 minutes.  The better machine thus creates some value added and I want the benchmark to measure this value. The same reasoning can be applied to integration/performance tests.  If they run too slowly, developers tend to run them less frequently, lengthening feedback loops.  Again, value can be added by running them faster.  A benchmark consisting of a sample of such tests would measure the value. If your faster machine uses SSD while the slower machine uses HDD, such benchmarks should let you measure how much faster your faster machine is at your usual tasks.  Simply saying \"SSD is faster than HDD\" doesn't answer the question. "
    },
    {
        "ID": "19974",
        "Question": "Many IDEs have the ability to spit out automatically written pieces of code. For example, it might generate a getter/setter pair for you. I wonder whether it isn't an attempt to work around some flaws in the language. If your IDE can figure out what the code should be without your help, why can't the compiler? Are there cases where this is a really good idea? What kind of code generation does your IDE provide? Would the same thing be better provided as a feature of the language? ",
        "Best answer": "The difference between the IDE auto-generating some code and building that directly into the language is very simple: I can edit the code after the IDE generates it. The IDE can generate some code template for a getter/setter, and then I am free to add additional functionality to that getter/setter as my requirements dictate. If the getter/setter was an integrated part of the language, then if I ever wanted to customize the behaviour, I'd have to write the getter/setter myself anyway... "
    },
    {
        "ID": "19987",
        "Question": " Possible Duplicate: Is it better to specialize in a single field I like, or expand into other fields to broaden my horizons?   Recently, I don’t know from where I got a thought in my mind that, “is knowing .NET development environment enough for a successful career in IT industry”. Should I be learning more languages too or will .NET suffice me for next 10-15 years. By successful career I mean earning decent living and having good growth opportunities. ",
        "Best answer": "If you want to be stuck in the same place forever, then know .NET and forget everything else. Because technology moves, as do development environments and languages, the developers need to move as well. Example (though I am a bit of an oddball), in 25 years I've actively used APL, BASIC (more varieties than I can remember), Fortran (several variants), C, C++, Pascal, Delphi, Ada, and a bunch more I can't even remember. These have all been for large and complex projects. Then some of these were on operating systems (Windows, unix[es], linux, other strange proprietary OS's, IBM MVS/TSO, etc). And some have been on the bare-metal hardware (lots of embedded stuff), some had RTOS environment, some didn't. The point I'm making here is - if you learn new things and move in new directions as the needs arise you will be more knowledgeable and more employable. The danger of learning one thing only is that obsolescence creeps up on you. There are not so many jobs these days for Fortran programmers - or buggy-whip makers :) "
    },
    {
        "ID": "20036",
        "Question": "I've been hearing a lot of enthusiasm about functional programming languages lately, with regards to Scala, Clojure, and F#.  I've recently started studying Haskell, to learn the FP paradigm. I love it, it's really fun, and fits my math background. But will it ever really matter?  Obviously, it's hardly a new idea. Here's my questions:  What has contributed to the recent FP enthusiasm?  Is is merely boredom with OO, or has something changed to make FP more needed than before? Is this indicative of a FP future?  Or is this a fad, like object orient databases?  In other words, can anyone help me understand where this comes from and where it might be going? ",
        "Best answer": "One of the major innovations in FP that has resulted in the \"explosion\" of interest is monads.   In January of 1992, Philip Wadler wrote a paper called The Essence of Functional Programming which introduced monads into functional programming as a way to deal with IO.   The major problem with pure, lazy, functional programming languages was utility in dealing with IO.  It's one of member of the \"Awkward Squad\" in programming, because \"laziness and side effects are, from a practical point of view, incompatible.  If you want to use a lazy language, it pretty much has to be a purely functional language; if you want to use side effects, you had better use a strict language.\" Reference  The issue with IO before monads was that maintaining purity was not possible for programs that were actually useful for anything.  By IO, we mean anything that deals with changing state, including getting input and output from the user or environment.  In pure functional programming, everything is immutable, to allow laziness and purity (free from side effects). How do monads solve the problem of IO?  Well, without discussing monads too much, they basically take the \"World\" (the runtime environment) as input to the monad, and produce a new \"World\" as output, and the result:  type IO a = World -> (a, World).  FP has therefore entered more and more into the mainstream, because the biggest problem, IO (and others) has been solved.  Integration into existing OO languages has also been happening, as you know.  LINQ is monads, for example, through and through. For more information, I recommend reading about monads and the papers referenced in my answer.   "
    },
    {
        "ID": "20040",
        "Question": "For blackberry GUI developement.  I am using the name convention like backButtonField and some of my colleagues are using btnBack. But sometimes I stuck with the names like loginVerticalFieldManager its a very long name.  At that time vfmLogin looks better So for GUI components names what convention should I follow? ",
        "Best answer": "I would say use btnBack instead of backButtonField. Even I at the other side of the world recognize that type of notation.  Update: However there is no right or wrong as long as all members in the team agree to write the same notation. "
    },
    {
        "ID": "20116",
        "Question": "I'm considering using Grails for a new website, but am open to other/new programming languages and frameworks.  I have done development using J2EE/JSF2, ASP.NET, and PHP.  Is Grails or Ruby on Rails pretty much the best way to get functionality up and running quickly? Some initial thoughts:  DJango looks similar to RoR/Grails and I'd consider it GWT is an interesting concept but it doesn't seem like turnaround time is quite as fast  Thanks, -Jon ",
        "Best answer": "As with all questions of this sort, the answer is \"it depends\". Factors to consider include how comfortable you are with the language/framework and what features the project requires. Having said that, I've built sites using a number of frameworks, each with its own strengths and weaknesses: Grails If I need to get a site up quickly for anything professional, Grails would probably be my first choice. GORM is far and away the simplest and most intuitive ORM that I've tried, the MVC paradigm is very well executed, there is 0 configuration to start coding (no worrying about URLs, DB patches, anything), very rapid iteration (just refresh the page), seamless Java integration (a good thing in the business world), and some wonderful plugins (e.g. Searchable is a thing of beauty). Biggest downside is that hosting can be tough to find (and expensive). Ruby on Rails My experience with RoR is very similar to that of Grails: MVC well executed. On the positive side, it has a bigger community, so the online resources (documentation, FAQ, code samples, etc) are very plentiful, there are TONS of plugins, Ruby is a bit more flexible/expressive/\"funky\", and it's much easier to find hosting (esp. for personal projects); on the negative side, the dependency management/setup sucks (I've used RubyGems on Windows, Ubuntu, Fedora and OSX and ran into non-trivial problems on each one), there is slightly more configuration/overhead than Grails (in particular, dealing with routes.rb and tons of db migration files), and apparently, RoR has some serious scalability issues. PHP (including the CakePHP framework) If I need to quickly hack something out or build a site for personal uses, then I'd probably go with PHP. PHP is far and away the easiest language to learn and deploy: download any of the convenient LAMP packages out there, click a couple times, and begin hacking away. The community is bigger than that of RoR, so documentation is plentiful and there are countless plugins (easily \"installed\" by just dropping in the php file and refreshing the page). The language is simple to learn, but some things in PHP are just downright strange and it takes a lot of discipline to avoid ugly code. The CakePHP framework enforces a nice MVC paradigm to help keep things orderly, and for the most part is on par with RoR, although I personally found it slightly more unintuitive. Java (servlets, JSPs, JSTL, struts, Velocity) I've built many sites using the Java servlet technologies, and I can honestly say that there is no good reason to pick them for any new site nowadays. They have a steep learning curve, tons of configuration to fight with (XML hell), slow iteration due to the need to redeploy stuff all the time (unless you use JRebel), verbose code, and no \"freebies\" in terms of functionality. I've played around with the Play! Framework and Spring Roo a little bit and both are doing some very cool stuff with more or less \"pure\" Java code and are worth looking into further.   "
    },
    {
        "ID": "20128",
        "Question": "Which world most important algorithms have contributed most to humankind in past decades?  I thought this is a good general knowledge for a developer to know about. Update: If possible, please keep the answer to a specific programming algorithm. I would like to get a list of the most important ones, only one algorithm per answer. Please consider to state why the algorithm is significant and important... ",
        "Best answer": "Public/private key encryption is pretty darn important. Internet commerce would be nowhere as ubiquitous without it. "
    },
    {
        "ID": "20206",
        "Question": "So I was currently in the middle of coding, unfortunately for me, I was in \"The Zone\" then I thought to myself, are my method/variable names to long?  POP out of the Zone I go!   So I came here to ask, are my method/variable names too long? You be the Judge! Bonus points to anyone who can figure out what I'm writing, although I'm sure a guru will figure it out fast! Anyway, here are some of my method and variable names. Methods :   searchBlockedListForBlockedSquares(),  isCurrentSquareNextToAtLeastOneBlockedSquare(), searchBlockedListForBlockedSquares()  Variables:    isNextToBlockedSquares   I guess there was only one variable that seemed too long. ",
        "Best answer": "Your names seem okay to me in terms of length. However, the way they are named suggests that maybe some new classes are in order? For example, instead of searchBlockedListForBlockedSquares() you could have blockedList.getBlockedSquares(). Similarly, isCurrentSquareNextToAtLeastOneBlockedSquare() becomes currentSquare.isAdjacentToABlockedSquare(). "
    },
    {
        "ID": "20233",
        "Question": "My question is: How can you verify the users' requirements early in the software creation process?  I show the user specifications, prototypes, demos... but still users forget to share some \"insignificant details\" about the process or business rules and data. Which pops up after the final test as some \"really small and rare exceptions\" - which are turned into change request and accumulate a lot of work. So how do you prototype (or verify) users requirements early in the project? ",
        "Best answer": "Start by not getting rid of intermediates like business analysts, because they are actually trained in this. If you don't have such people, accept that at least one of your developers is going to need to develop this skill. Next, over time, accumulate a set of instincts about the sorts of things people often ask for late in projects, and bring them up in conversation early in the project. \"Will the discount always be the same for all orders, or does it vary by customer?\" \"Can every user see every report, or are some just for supervisors?\" \"Is the sales tax always the same, or does it depend where the customer is?\" and so on. In addition, try to write your code so it's insulated from these kinds of changes coming late, because some will still come in late no matter what. If you have business logic in the click handlers for your buttons, these changes really hurt. If you have business logic in a specific class whose name makes it easy for you to find it, and you can leverage polymorphism to, say, have regular orders and rush orders and each order calculates its own shipping charge, then the change they've asked for is much less painful. You cannot prevent late changes. Laws change, business drivers (customers, what sales is promising, the great idea the CEO has while reading something on the plane) change. Prevent what you can and embrace the rest by designing your code to be changeable. "
    },
    {
        "ID": "20240",
        "Question": "I want to build a PHP web application, but always faces the problem that I need to connect to database and deal with related objects. So I tried codeigniter and it uses a special way to connect to database named Active Record and I've known that its an ORM technique and there are lot of ORM libraries out there. What is the best, the easiest and the fastest of them? ",
        "Best answer": "Doctrine and Propel are two popular ORMs.  Personally I only have experience with Doctrine and have no complaints. You should also read through the answers to this question on StackOverflow. "
    },
    {
        "ID": "20293",
        "Question": "Is there any compelling reason a programmer would host on a shared platform versus just having a VPS account somewhere? ",
        "Best answer": "Advantage Shared Hosting  Generally cheap so ideally for doing development or running smaller sites with low traffic A lot of features are pre setup among other a basic security model (hopefully) In general you get a lot of extra goodies like one-click installs that have been tested in the environment you are running it on and sometimes even configured When there is more traffic in general peaks are handled: with VPS your server is dead e.g. on mediatemple there is a Grid based model which can scale . Even if there is a peak it just runs along though you get a mail that you will have to pay extra It is save you TIME investing in learning e.g. security (firewall on linux, ssh security, how to read the performance indicators e.g. what do these sockets indicators mean) And therefore saves TIME each month in maintenance (and this INCLUDES: issues means just mailing the helpdesk, outtages just waiting and shouting at support) Diskspace is often a lot more e.g. look at Dreamhost  Disadvantages Shared Hosting  Security is often less. E.g. a lot of users are running around in that same environment and when you have your files wrongly CHMOD'ded they can read them. If ever there is a general security breach... Often Peaks lead to scaling your performance down after that which is never really clear At some point with enough steady traffic / cpu usage / socket usage you run against the boundaries no matter what you simply have to switch You have not complete control over your server e.g. you can not really install another operation system (or something on top of that that is out of line) e.g. running some kind of server.  Disadvantage VPS  Unmanaged AND managed VPS requires you to have the knowledge to secure the thing this costs you TIME (all knowledge is out there) but YOU are responsible for  it. You need to learn about linux firewalls for example (and a lot more). Maybe you do not want to spend your time on that. VPS requires also enough knowledge on OS level, maintain it, patching software you specifically installed and learning how to patch the specific software e.g. upgrade Apache with dependencies. This costs TIME learning but also TIME doing it each month ISSUES on VPS are YOUR responsibility (well ... for the most part) e.g. if your server is slow you need to go out there and check not only the software part but also the OS part and the Database performance part. E.g. if both CPU and Memory are ok... what the heck can it be. This also cost TIME. (also think of user management, email setup, etc...) Also it more expensive. And this is really a lot more expensive than shared hosting because the upper limit of what you can do on shared hosting is not the bottom limit of what VPS can offer. E.g. if your WordPress Multisite is on its max on your shared hosting account you can not get that 1mb cheap slice. You need to possible get immediately that 4 GIG memory thing. (ok this probably means go dedicated) (since database performance plays and issue here) OR if your facebook alike ajax chat application ran against its limit on your shared hosting account... go get something much bigger because probably the max sockets are your pain point. I just wanted to summarize ALL of this as : more TIME needed from yourself and possible A LOT if you still need to learn al lot and on the second place MONEY (since I assume that the revenue behind it would cover that larger need for infra). If you VPS runs against its limit e.g. 512mb memory (...) your site is DEAD (ok depending on how you set it up but lets assume worst case you need to go to the panel you put on it and restart the whole lot) as said on most shared hosting they will scale it and bill you afterwards unless your page hit the front page of digg then they will think some kind of ddos attack is taking place.  Advantage VPS  More secure: YOU are on top of it ;) from a certain level MORE memory, MORE cpu, MORE sockets, MORE everything (except possibly disk space) so at some stage you HAVE to You learn a lot of stuff which you need to learn anyway (i think) You can run anything you want as long as you have the resources for it  Personally I think: run on a good shared account as long as possible, then switch to dedicated if it no longer goes on the shared platform unless the thing you are runnning can be confined on VPS. (such as : development environment that needs own server stuff). "
    },
    {
        "ID": "20363",
        "Question": "Please put a direct link to it. Required:   Still available online (please put a direct link to it) Must be a speech from a current or former developer Speaker's popularity doesn't matter Target audience should be other developers It is not required that the topic is programming (the task). I'm more looking for subject generally developing here within P.SE rather than SO. Explain why it was inspiring  (please, native english speaker, edit my question) ",
        "Best answer": "Alan Kay's OOPSLA 1997 speech: The Computer Revolution hasn't happened yet. "
    },
    {
        "ID": "20371",
        "Question": "What is your experience when developing web applications using one of these two languages/frameworks? Which have you found to be more convenient and faster to develop with? ",
        "Best answer": "Either would be perfectly fine for developing web applications. If your stronger in PHP than Ruby, then likely Cake would be \"faster\" because you wouldn't have to spend the extra time familiarising yourself with Ruby syntax. Obviously the converse is true. I don't have a huge amount of experience of either but I'd say I prefer Ruby because I find the libraries easier to use and install (rvm/gems etc) and I like having a local development server rather than using Apache and the large and vocal open-source community on github/blogs/twitter is mostly a good thing. Really, who cares, they're basically the same: Dynamic scripting language, MVC framework... If you're looking to expand your knowledge you might as well look at something very different like node.js or Haskell and Snap. "
    },
    {
        "ID": "20385",
        "Question": "I am currently working as an intern at a consulting firm. I am soon to move to a full time employee once I graduate next semester, and I love working there. However, as a student, I lack money and I have met a business owner outside of work who has offered to hire me for some freelance web development.  Because I met this individual outside of work, I feel it would not be a conflict of interest to freelance for him. However, the work his is wanting me to do is very similar to what I already do for my current boss.  Should I speak with my boss before considering the offer?  Edit: I understand that I cannot take IP from work, and I am not in a contract, I'm employed at will. ",
        "Best answer": "No Unless your contract specifically forbids freelancing, what you do on your own time is none of his business. "
    },
    {
        "ID": "20406",
        "Question": "I've just started making some simple but non trivial(i think) programs in Ubuntu -- as of now have made a small xkcd scraper, which i plan to develop into a multi webcomic downloader+viewer At this point, would it be a good idea to start publishing the code on a site like Github or launchpad? (currently I'm not really worried about people copying my code/licensing) Or is it better to publish the code only after the program is completed? Also, can you suggest some such sites where I can post such codes and get suggestions for improvements/discuss the code with others? ",
        "Best answer": "It's a good idea to push it to a public repository for multiple reasons -  Motivates you to keep it up, maintain it You might get unexpected feedback from others Looks good on your resume/profile/blog  So even if it's not complete yet, put it up somewhere. Don't be worried about incompleteness, because people who visit will know it's a work in progress. Update : As Skilldrick mentions, www.github.com is a great repos. "
    },
    {
        "ID": "20407",
        "Question": "In a post, Joel Spolsky mentioned that 5 digit Stack Overflow reputation can help you to earn a job paying $100k+. How much of that is real? Would anyone like to share their success in getting a high paid job by virtue of their reputations on Stack Exchange sites? I read somewhere that, a person got an interview offer from Google because a recruiter found his Stack Overflow reputation to be impressive. Anyone else with similar stories? ",
        "Best answer": "No  the real answer: spend a few months earning a five digit Stack Overflow reputation, and you'll be getting job offers in the $100K+ range without an interview.  There is no reason why a high reputation (or \"score\") on any site will get you a job at all. I have pointed this out before, you are more likely to get a job by maintaining open source projects, writing proficiently, leaving good impressions, and making personal connections within the community. Are these people good programers? Undoubtedly yes! Does that mean they are a good fit for your team? Absolutely not. Calling these people \"superstars\" may be completely correct, but that doesn't make them perfect. 1 What determines if you are a good fit? Interviews and connections. You can't replace meeting people face to face with a number.  Having a high reputation can't hurt, but it isn't a magic bullet.  1: In no way do I mean to imply these people are bad programmers, I mean to emphasize the inability to instantly and wholly judge someone based on a number. "
    },
    {
        "ID": "20466",
        "Question": "Should I break SQL queries in different lines? For example in the project I am working on, we have a query that is taking 1600 columns! 1600 + tab chars. I wrote queries like this:    \"SELECT bla , bla2 , bla FROM bla \" .       \"WHERE bla=333 AND bla=2\" .        \"ORDER BY nfdfsd ...\";  But they demanded me to put them in one line and said that my style is bad formatting. Why it is bad practice? ",
        "Best answer": "For source control reasons, we have linebreaks after every where clause, or comma. So your above turns into   SELECT bla       , bla2       , bla  FROM   bla  WHERE  bla=333    AND  bla=2 ORDER  BY nfdfsd         , asdlfk;  (tabbing and alignment has no standard here, but commas are usually leading) Still, makes no performance difference. "
    },
    {
        "ID": "20472",
        "Question": "I'm considering using Google App Engine for an online matchmaking (and possibly ranking, not sure) of users in a (currently hypothetical) RTS game. However, I just read this bad review, which has made me nervous. Which problems have they solved (or are planning to solve), which might be an issue for me, and are there any alternatives? ",
        "Best answer": "Google App Engine uses a NO-SQL data store---you should read up on that concept, and decide if it sounds like a good fit for your needs.  From that review, it sounds like that is the main issue the reviewer had with GAE. There is a certain amount of vendor-lock-in with GAE.  You end up writing code to target a specific environment.  You could certainly migrate off of GAE in the future, but it would likely require at least some code change.   You should also plan to code import/export features so that you can get your data out of GAE, and into a new system. Once you understand how GAE differs from a standard environment that you might set up if you were to get a stock linux server or VM, you will be able to decide if it fits your needs.  There are some definite advantages to GAE, especially in the cost arenas (you don't need to pay for hardware or infrastructure management until your site get popular, when hopefully it will be able to pay for those costs).  Just be sure you do the due diligence and research before jumping in. "
    },
    {
        "ID": "20536",
        "Question": "I was just reading https://stackoverflow.com/questions/155609/what-is-the-difference-between-a-method-and-a-function and all of a sudden, the thing came to my mind was the static methods.  As static methods are no way associated with an object how can it be called as static methods (almost all developers do)? Is my understanding about the methods/functions is correct?  ",
        "Best answer": "(Using Java terminology): Static methods can be associated with static members (members of the class object). If the methods don't access static members (nor IO resources or anything else that could change state), of if those members are final primitives or immutable objects (essentially constants), then such static methods could be called functions, but in general, because they potentially could access the class object's members, they're usually called static methods. In order to count as a function, a method must be independent of any state; it must not cause any side effects nor be affected by any side effects. When called with specific parameters, it must always return the same result. Otherwise it's not a pure function. That is, the following area is a function: class MyClass {     static final double pi = 3.14;     static double area(double r) {         return pi * r * r;     } }  while the following getCallCount is not a function: class MyClass {     static int callCount = 0;     static int getCallCount {         return ++callCount;     } }  In general, mutable (non-final) static members should be used with caution - someone would say that they should not be used at all - because they make a global state to the program, which likely turns out to be a bad design choice in the long run. There are exceptions, but be careful... You don't even need static members to make static non-functions: System.nanoTime() is definitely not a pure function (because it returns different value on successive calls), even though it only accesses the computer's clock, not any static members.  Confusingly enough, you could also make non-static pure functions: class MyClass {     final double pi = 3.14;     double area(double r) {         return pi * r * r;     } }  Now, although not static any more, area is still a pure function: it doesn't touch anything that could change. The fact that you would have to create an instance of MyClass to access area doesn't reduce its \"functionness\". Indeed, one could argue that this kind of pure functions should always be made static. "
    },
    {
        "ID": "20542",
        "Question": "Getting into zone is a pleasurable and fruitful process. We produce good source code and we get lots of satisfaction from our work done while being in the zone. But, how does one get into the 'zone'? Do you follow a specific process? Apart from switching of email system, mobiles and other mundane non-productive applications, is there anything else that can be done? ",
        "Best answer": "Concentrate on what you need to do. Make the effort to actually start doing it. This can be one of the hardest things - to actively stop fluffing about. Don't have email open. Don't have Fakebook in another window. Don't have any StackExchange going. No forums. Only quiet. And then get on with it. It generally takes me (and pretty much everyone else I know) about 15-20 mins to get there. You can generally sustain \"the zone\" for about 2 hours, and generally only once per day - its mentally pretty tiring. If you are super-duper you might manage it twice in a day. After \"the zone\" the rest of your day is pretty much lightweight by comparison, you get things done but the burst of huge productivity is over. Oh - and getting out of the zone takes about 3 seconds - eg a phone call, or somebody sticking their head and saying: \"Can I bother you for a moment\" - to which the answer is: \"yes, you already did\". Bang. The zone is gone. Another 15-20 to get back. Amazing how many stupid s/w defects get introduced by getting knocked out of the zone. Amazing also how many people (esp managers) think that open plan is a really good way to develop quality software (where nobody EVER gets into the zone let alone stays there). "
    },
    {
        "ID": "20596",
        "Question": "I am currently facing a situation where I will need to obfuscate data for legal reasons (these are countrywide rules and not specific to me) I will still need to work with teams outside this area who will still need to access applications/databases for testing/deployment and so on, however the data must be fully obfuscated. What are your experiences of this scenario? e.g. Who owns what stage of the process, what access do the various parties have, how strong is the encryption, how do updates occur ",
        "Best answer": "One large financial client we do business with has a standardized automated process for obfuscating data. We don't, so I have a few scripts where I do this by hand. The point is to leave reasonably realistic data (lengths of names, postal codes) while rendering the personally identifiable data irretrievably scrambled. Their system is far more complicated than this, but basically when production data gets copied to development and QA environments, it will be scrambled automatically. This way there is no potential for \"forgetting\" to do some of the scrambling.   Passwords: Set them all to something test accounts use: like Password1 or 1234567.  Tax ID numbers, Social Insurance Numbers, Social Security Numbers: Take the first 3 digits and generate random numbers for the remainder. In the US, the first 3 digits are generally assigned based on where you lived when the SSN was issued, so not all combinations of first 3 digits are valid. For EINs, take the first 2 digits, as not all combinations of first 2 digits are valid. Adjust which digits get left alone if your country uses different rules.  Names: Hash and base64 the first and last names separately. Take the first letter of unhashed name append the hash afterwards and truncate the result to original name's length  Example:  Name = \"John Doe\"  (I am using SHA384) So John Doe gets turned into Jnbn Dnh. It helps to keep the names the same length as that may help to point out usability issues.    If you have rules such as \"names cannot have digits\" then you need to remove out the base 64 values that aren't valid, also lowercasing the subsequent letters (done in sample code below).  Addresses:  Street names and city names get hashed as names above do. Numbers stay the same. State and zip stays the same.  So 1313 Mockingbird Lane becomes 1313 Mvtqiwtuqrd Lzzx Phone numbers: Leave area code the same, generate random digits for the remaining digits.    Credit Card Numbers: You should not be storing these at all.    Here is some sample & crude C# code for hashing and truncating (simple to display the concept)       using System.Security.Cryptography;       using System.Text.RegularExpressions;         public string ScrambleInput(string sInput)     {         string sReturn = sInput.Substring(0,1);         string sTemp = string.Empty;         System.Security.Cryptography.SHA384Managed Hasher = new SHA384Managed();         System.Text.ASCIIEncoding enc = new System.Text.ASCIIEncoding();         byte[] buff = new byte[sInput.Length];         buff = enc.GetBytes(sInput);         Hasher.ComputeHash(buff);         sTemp = Convert.ToBase64String(Hasher.Hash, 0, Hasher.Hash.Length, System.Base64FormattingOptions.None);         sTemp = sTemp.ToLower().Replace(\"+\", \"\").Replace(\"/\", \"\");         sReturn += Regex.Replace(sTemp, @\"\\d\", \"\");         sReturn = sReturn.Substring(0, sInput.Length );         return sReturn;     }  "
    },
    {
        "ID": "20607",
        "Question": "I was wondering if there are obvious advantages and disadvantages to using Ruby on Rails to develop a desktop application. RoR has great infrastructure for rapid development, proper implementation of specs and automated acceptance tests, an immense number of popular libraries and the promise of being actively developed and maintained in the future. The downsides I can see are mostly about usability - installation of a Rails app as a local service and launching of a browser when it needs to be active may not come naturally to many users... or be technically easy to implement and support for different platforms. ",
        "Best answer": "No, for many reasons:  Sub-par UI. Either you will be limited to traditional web forms based UI, or you will die trying to emulate rich UI interface with lots of JS/HTML5. No direct access to hardware. If you would like finer control of your print output, access to the scanner or smartcard reader or audio headset, you're doomed. Difficult deployment. While it's easy to set up your web server, Ruby interpreter and set of gems on a single web server, making a self-installing executable out of it is impossible, or nearly impossible.  Probably a lots of other issues I forgot. And this stands for any web platform, not only for RoR. "
    },
    {
        "ID": "20624",
        "Question": "I'm planning to develop a web based ERP, which should be full-ajax and with desktop-like UI. It will be a data-entry & data-report application. For developing it I'm considering all technologies. GWT: I saw that with GWT Designer you could create cool UIs, but databinding seems to be too complex JSF: Netbeans no longer supports the visual web editor ZK: supports databinding in a relatively easy way, and has got an Eclipse-based visual editor Some people talk about REST + javascript as a winning choice I'd like to have your opinion about what could be the right choice. Thank you very much in advance! ",
        "Best answer": "You are comparing a language (js), with a language framework(zk), with an architectural design style (rest), with a development toolkit (gwt). Have you experience with any of them ?  I'd start with the core basics.  I've been building websites and enterprise scale systems for ten years and have never used a visual designer, and neither has anyone I worked with, so I wouldn't worry about that too much. Related: GWT or Vaadin "
    },
    {
        "ID": "20628",
        "Question": "The canonical books on software development is fairly well established. However, after reading through a dreadful book full of bad advice on managing programming teams this weekend I am looking for recommendations for really good books that focus on the management side of programming (recruiting, performance measurement/management, motivation, best practices, organizational structure,  etc.) and not as much on the construction of software itself. Any suggestions? ",
        "Best answer": "Peopleware: Productive Projects and Teams . If you read one book make it this one.  It covers how to set about making your developers productive and backs up why these things are important with hard numbers. The chances are that you won't get all of them implemented (too many companies have ingrained cultures which will prevent them) but it's worth knowing what the ideal is and why, and having the ammunition to get what you can.  "
    },
    {
        "ID": "20684",
        "Question": "Recently an architect described our company as offering a Rolls-Royce solution (MVC) when all he needed was a Toyota (Web Forms). I'm curious to find out what you think about web forms vs MVC as an architectural choice. ",
        "Best answer": "The Rolls-Royce/Toyota analogy is terribly flawed and misleading. One (ASP.NET MVC) is not simply a fancier or more expensive version of the other (ASP.NET WebForms). They are vastly different approaches to building web applications using ASP.NET. To me, the biggest architectural difference between MVC and WebForms is how they work with the stateless environment of the web: WebForms works hard to build a set of abstractions that hide the stateless nature of web programming, whereas MVC embraces the stateless environment and works with it. Each approach has its benefits and detriments, but I really enjoy how building web sites with MVC feels much more natural than WebForms (with its layer upon layer of leaky abstractions). "
    },
    {
        "ID": "20729",
        "Question": "I want to know what aspects in VPS hosting packages are related to performance for the website which will make it faster in loading for the end useres. And does RAM really matter? ",
        "Best answer": "Here are the main ones in order of best improvement for time spent. Some of these will be more of a bottleneck depending on what data you're dealing with. Front End: There are easy wins on the front end. Cut down the number of requests with image sprites and aggregating css/js files. Make sure gzip and cache headers are being used. Hardware: Hardware can be cheap compared to your time. Throw more ram at the problem. Perhaps put your database on another server to cut down disk io. Static Files: Put static files on another domain and don't use cookies. You could use static.mydomain.com. It helps to serve these with a light weight server like nginx, this will cut RAM usage a lot. A CDN is an easy option for this. 0.1 seconds is considered to be fast in usability. I'm in Australia so you've probably lost me no matter how fast you serve up the files. Server side caching: Setup a reverse proxy, it's much more efficient to have Nginx serving pages to slow mobile connections than have a heavy apache process waiting idle for the response to finish.  Memcached is an easy way of caching things in memory for fast retrieval. Database Tuning: Keep track of what queries are running slow. Add indexes on the columns you are using. Cut down the number of joins by denormalising where appropriate. You'll need to put the database on a second server at one stage. A lot of RAM can help as the indexes can then be kept entirely in RAM. If you're interested in clustered hosting I would look at some of the presentations from facebook and youtube, wikipedia engineers on how they've done it. edit some videos: Youtube scaling: http://video.google.com/videoplay?docid=-6304964351441328559# Django/General Scaling workshop: http://python.mirocommunity.org/video/1689/pycon-2010-django-deployment-w Also one thing I may have forgot was if you have search a dedicated search engine like Solr/Sphinx can help. "
    },
    {
        "ID": "20764",
        "Question": "In the last 3-5 months I was thinking about which web programming language I should use. PHP? Ruby? C#? Python? Perl? And you know what? Every time I keep changing my mind like crazy! I learn some things about each one of them and then move to another.  Now I'm totally lost, totally lost guys. :) What I want form you is to help me to decide, not to decide for me, and give me some recommendations. It would be more than helpful if you can tell me a short story about yourself, how you got started, where you are now and what web programming technologies you are using. ",
        "Best answer": "Reverse the focus of your thinking - if you are still contemplating what language to learn, stop contemplating languages and think about what you want to build or what problem(s) you have to solve. Once you have some idea of what your end product is going to be, start thinking about what of these three major factors are important in the project: -budget/cost -quality -time Revisit your language list again. Which one do you feel might fit the measures of these factors that are required for this project the best? Try to eliminate one at least. Now think about the individual requirements for the project. Are there any items which one language lends itself to more? Revisit your language list again. Try to get it down to one or two. If you are still on the fence...think about other items. Do you know anyone who's an expert in one of those languages or will someone be working with you who prefers one to the other? Does one language have a better community than the other?  Its not about picking a language itself, its about picking the right language for the job. My story short and sweet: Sometimes I have projects which lend themselves to C#, and sometimes I have projects which lend themselves to PHP. I pick the one that gets it done the way I need it to. "
    },
    {
        "ID": "20790",
        "Question": "A previous question about why people hate Microsoft was closed. This is an attempt at a somewhat more constructive question along the same general line. This one is both broader and narrower though. It's more general by being about software vendors in general, not just Microsoft. It's narrower by dealing only with management of software products. So, what steps should be taken (and/or avoided) in managing individual software products to assure that not only the individual products, but the company as a whole is respected/liked/seen in a positive light? ",
        "Best answer": "Most important is obviously to deliver a quality product. Other imporant topics:  Honesty. Don't lie when the truth will come out sooner or later anyway. Reliabilty. Adhere to deadlines. Availability. Reply emails, pick up the phone. Willingness to cooperate. If it means to work with your closest competitor to make what the customer needs, do it and do it professinally. No dirty tricks that hurt the customer first.  The last item on my list is probably what got MS such a bad reputation (though I think they are much better now in that respect); and it's even worse when small companies do that. "
    },
    {
        "ID": "20800",
        "Question": "I just want to hear some pro and con's - it's obvious that there is no right answer So would Java be a better choice for a long term career? Or should I opt for .Net since it could be the platform having all the latest advances?  I just want to hear others point of view, so please don't close this question just because it's being subjective  ",
        "Best answer": "I think you should consider being proficient in both, since I believe it will make you a better programmer than one just knowing one. For where to start?  Well, where you can pick up a job.  Then learn what they need you to know, and then start picking up what you lack. Programming is a never ending process of learning, because there is so much to learn that you cannot keep up.   "
    },
    {
        "ID": "20801",
        "Question": "what kind of insights or questions would lead you to determine a person's OOAD skills. ",
        "Best answer": "You could show some half-assed OO design of a simple problem, and discuss what it does, what's good and bad about it, whether it's flexible enough, what could be improved, and how. If you need to get the discussion going, ask what the person thinks about some aspect of the code, but not with a leading question. Important is to remember that the discussion is important, not that you knew the answers beforehand. Any decent developer should be able to point out something about the code that you didn't even think of before. "
    },
    {
        "ID": "20832",
        "Question": "In every interview I have been in, I have been quizzed on mathematical analysis of complexity, including big-O notation. How relevant is big-O analysis to development in industry? How often do you really use it, and how necessary is it to have a honed mindset for the problem? ",
        "Best answer": " My question is, how relevant is this test to development in industry?   A solid understanding of computational complexity theory (e.g. big O notation) is essential to design scalable algorithms, applications and systems.  Since scalability is highly relevant to computing in industry, big O notation is too.  How often do you reeeally use it, and how necessary is it to have a honed mindset for the problem?  Depends what you mean by \"reeeally use it\".  On the one hand, I never do formal proofs of computational complexity for the software I write.  On the other hand, most days I have to deal with applications where scalability is a potential concern, and design decisions include selection of (for example) appropriate collection types based on their complexity characteristics. (I don't know whether it is possible to consistently implement scalable systems without a solid understanding of complexity theory.  I would be inclined to think that it is not.) "
    },
    {
        "ID": "20896",
        "Question": "Been calling them \"implementers\", but it seems weird to me. /// <summary> /// Implementers should implement this.  Derp /// </summary> protected abstract void InternalExecute();  A point of clarification, I'm interested in what to call the people who create child classes, not the child classes themselves. \"Hey, you there\" not \"that thing there.\" ",
        "Best answer": "MSDN class documentation often uses \"you\" to refer to developers.  When you inherit from WebRequest, you must override the following members... You do not typically inherit from ButtonBase. To create your own button class, inherit from the Button, CheckBox, or RadioButton class....  You can also simply state what the requirements are for descendent classes. It is implied that developers are your audience.  Classes that inherit IDbConnection must implement all inherited members, and typically define additional members to add provider-specific functionality.  In your example, instead of \"Implementers should implement this\", write \"Descendent classes must override InternalExecute to...\" or \"In descendent classes, you must override InternalExecute to...\". "
    },
    {
        "ID": "20909",
        "Question": "Simple question, but I often hear these three terms defined with such ferocity, but which have been known to me to mean different things over the years. What are the \"correct\" definitions of \"Procedures\", \"Methods\", \"Function\", \"Subroutines\", etc? ",
        "Best answer": "I'm going with a different answer here: practically speaking, there's really no difference, with the slight exception that \"method\" usually refers to a subroutine associated with an object in OO languages. The terms \"procedure, function, subroutine, subprogram, and method\" all really mean the same thing: a callable sub-program within a larger program.  But it's difficult to come up with a definition that captures all variant usages of these terms, because they are not used consistently across programming languages or paradigms. You might say a function returns a value.  Well, the following C function doesn't return a value: void f() { return; }  ...but I doubt you'd find anyone who would call it a procedure.   Sure, in Pascal, procedures don't return values and functions return values, but that's merely a reflection of how Pascal was designed.  In Fortran, a function returns a value, and a subroutine returns multiple values.  Yet none of this really allows us to come up with a \"universal\" definition for these terms. In fact, the term \"procedural programming\" refers to a whole class of languages, including C, Fortran and Pascal, only one of which actually uses the term \"procedure\" to mean anything. So none of this is really consistent.  The only exception is probably \"method\", which seems to be used almost entirely with OO languages, referring to a function that is associated with an object.  Although, even this is not always consistent.  C++, for example, usually uses the term \"member function\" rather than method, (even though the term \"method\" has crept into the C++ vernacular among programmers.) The point is, none of this is really consistent.  It simply reflects the terminology employed by whatever languages are en vogue at the time. "
    },
    {
        "ID": "20927",
        "Question": "Just as the title says, what is your favorite whiteboard interview problem, and why has it proven effective for you? Junior, senior, Java, C, Javascript, PHP, SQL, pseudo-code, etc. ",
        "Best answer": "I've found this one extremely illuminating when interviewing candidates and filtering out those with no business being there. It is similar in complexity to Fizz Buzz, but focuses on database skills. Assuming the following basic  table structure Documents (DocID, DocDate) Keywords (KeyWordID, KeyWord) DocumentKeywords (DocID,KeywordID)  Write a query to return the following: Part 1: Documents with a DocDate after 4/1/1995   Part 2: Documents that contain the keyword \"Blue\"   Part 3: Documents that contain the either the keyword \"Blue\" or \"Yellow\" Part 4: Documents that contain the both the keywords \"Blue\" and \"Yellow\"  I let them write it in any SQL variant they want, and am not too picky on minor syntax problems. I mainly want to know that they understand basic relational DB concepts. Most candidates can get through part 3 with no issues at all. You'd be amazed how many think that the answer to part 4 is just to change the operator from OR to AND in the where clause. "
    },
    {
        "ID": "20932",
        "Question": "What percentage of time is saved and costed doing TDD. I assume this percentage of cost and reward changes during a projects life-cycle. I'd imagine the initial phase has a lot more cost but little rewards attached. Further on (during re-factoring) you get the benefit of your tests. I've heard anywhere from 30-50% of your time is writing unit tests. However that doesn't take into account the time saved from writing those tests. Whats the time saved as well as the time cost? In bug fixing and refactorablity? ",
        "Best answer": "Each time you run your unit tests, you save yourself the amount of time it would have taken to manually test your code. The 30% to 50% of time you quote as being required to write your tests is also offset a great deal by the benefits of having a better (testable) software design.  Let's say it takes four times as long to write an automated test as it does to manually perform the test.  That means that the fourth time you run your automated test, it pays for itself.  Every time you run the automated test after that, it's free.   This holds true whether the test is an automated unit test, or an automated functional test.  Not all functional tests can be automated, but many of them can.  Plus, the automated test is more reliable than a person; it will run the test in exactly the same way, every time. Having unit tests means that you can refactor the underlying implementation of a method (for performance or other reasons), and the unit tests will verify that the functionality of the method has not changed.  This is especially true of TDD, where the unit test specifies the functionality of the method. "
    },
    {
        "ID": "20940",
        "Question": "Rails, to me, seems like a perfect level of abstraction for most types of web development. However, having watched some seasoned Rails consultants build an application, I'm finding that they use Gems to an extent that it's often pretty difficult to figure out what's going on in the code. As an intermediate-level Rails developer, I've had a pretty hard time digging through their code to figure out how things work, and I'm really struggling to see the payoff of this approach. So is it me, or does Rails, by dint of the fact that it has such a vibrant community of contributors, kind of encourage over-abstraction? ",
        "Best answer": "Your question reminded me of a point Paul Graham makes in On Lisp: If people complain that using utilities makes your code hard to read, they probably don’t realize what the code would look like if you hadn’t used them. Bottom-up programming makes what would otherwise be a large program look like a small, simple one. This can give the impression that the program doesn’t do much, and should therefore be easy to read. When inexperienced readers look closer and find that this isn’t so, they react with dismay. Also, this is a minor nitpick, but most people are referring to something else when they use the word \"abstraction.\" "
    },
    {
        "ID": "20950",
        "Question": "I find myself using my text editor of choice (vim, nano, gedit, pick your poison) much more often than any IDE as of late.   After noticing my ide shortcuts getting dusty I started to think about this and wonder: what justifies use of an IDE for you opposed to a text editor? For that matter what rationale would you have for not using an IDE and merely relying on an editor?  ",
        "Best answer": "The I: integration.  A good text editor may be nice for writing code, but most of your programming isn't spent writing; it's spent testing and debugging, and for that you want your text editor to integrate with your compiler and your debugger.  That's the greatest strength of an IDE. "
    },
    {
        "ID": "20988",
        "Question": "In Python's tutorial one can read that Python's original implementation is in C;  On the other hand, the Python implementation, written in C, (...)  I'm very curious why was Python written in C and not C++?  I'd like to know the reasoning behind this decision and the answer should be supported by historical references (and not opinion based). ",
        "Best answer": "From everything I've seen, it's a combination of practical and historical reasons. The (mostly) historical reason is that CPython 1.0 was released in 1989. At that time, C was just recently standardized. C++ was almost unknown and decidedly non-portable, because almost nobody had a C++ compiler. Although C++ is much more widespread and easily available today, it would still take a fair amount of work to rewrite CPython into the subset of C that's compatible with C++. By itself, that work would provide little or no real benefit. It's a bit like Joel's blog post about starting over and doing a complete rewrite being the worst mistake a software company can make. I'd counter that by pointing to Microsoft's conversion from the Windows 3.0 core to the Windows NT core, and Apple's conversion from MacOS 9 to Mac OS/X. Neither one killed the company -- but both were definitely large, expensive, long-term projects. Both also point to something that's crucial to success: maintaining both code bases for long enough that (most) users can switch to the new code base at their leisure, based on (at least perceived) benefits. For a development team the size of Python's, however, that kind of change is much more difficult. Even the change from Python 2 to 3 has taken quite a bit of work, and required a similar overlap. At least in that case, however, there are direct benefits to the changes, which rewriting into C++ (by itself) wouldn't (at least immediately) provide. Linus Torvalds's rant against C++ was brought up, so I'll mention that as well. Nothing I've seen from Guido indicates that he has that sort of strong, negative feelings toward C++. About the worst I've seen him say is that teaching C++ is often a disaster -- but he immediately went on to say that this is largely because the teachers didn't/don't know C++.  I also think that while it's possible to convert a lot of C code to C++ with relative ease, that getting much real advantage from C++ requires not only quite a bit more rewriting than that, but also requires substantial re-education of most developers involved. Most well-written C++ is substantially different from well-written C to do the same things. It's not just a matter of changing malloc to new and printf to cout, by any stretch of the imagination. "
    },
    {
        "ID": "21082",
        "Question": "I just read about cyclesort via a sortvis.org blog post. This is probably the most obscure one I have heard of so far, since it uses maths that I am not familiar with (detecting cycles in permutations of integer sets). What is the most obscure one you know? ",
        "Best answer": "Slowsort works by multiply and surrender (as opposed to divide and conquer). It is interesting because it is provably the least efficient sorting algorithm that can be built (asymptotically, and with the restriction that such an algorithm, while being slow, must still all the time be working towards a result). This offsets it from bogosort because in the best case, bogosort is quite efficient – namely, when the array is already sorted. Slowsort doesn’t “suffer” from such a best-case behaviour. Even in its best case, it still has runtime   for ϵ > 0. Here is its pseudocode, adapted from the German Wikipedia article: function slowsort(A, i, j):   if i >= j: return    m = (i + j) / 2   slowsort(A, i, m)   slowsort(A, m + 1, j)    if A[j] < A[m]:     swap(A[j], A[m])    slowsort(A, i, j - 1)  "
    },
    {
        "ID": "21118",
        "Question": "A link and a quick explanation why would be very helpful. General tech news, I mean not like InfoQ which is too technical. Probably much more like P.SE than SO. EDIT: site should help developers gain general knowledge about the industry, technologies, inventions, etc. Much like when we listen to the news every morning in our car to know what is happening in our world. But target audience must be either programmmers, geeks or any other persons interested in technology. ",
        "Best answer": "Hacker News It comes from a broadly tech entrepreneur position so combines interesting tech stories with relevant business and general stories. Because the stories that make the front page are submitted and voted on by the community they tend to be more relevant and interesting than those which are run by many news sites who aren't necessarily as well placed to judge these things. The comments/discussions can be entertaining too. "
    },
    {
        "ID": "21128",
        "Question": "First off, I apologize cause I don't know how to make a community thread; so someone help me out please. As a developer, across many platforms, technologies and even on an infrastructure level; I always find myself asking, when am I doing TOO much?!? It's been a never ending learning process, since I started. One (1) thing I learned is that requirements are barely valid for an extended period of time, and as such a little foresight may go a long way. But where is the balance, and how do you know when you're losing time, not gaining it?! ",
        "Best answer": "When the processes overtake the results. Too many times have we seen that if the developers are focusing more on the process rather than the results (as in quality, making the deadline, etc.) bad things commence. This is why it should be never forgotten that the purpose of code reviews, design patterns, etc. is to make the code better, but they are not the target themselves. "
    },
    {
        "ID": "21142",
        "Question": "In several projects we have been using the following layers:  Action (has 1 or more managers) Manager(has 1 or more Daos) Dao  But most of the time the manager only calls the dao. We do use the manager class to prepare the date to be sent to the db or to prepare the data to be sent to the action(is this a good practice?).  What are the responsibilities of a Manager and of the Dao classes? Do we have to have a manager for every dao or can we just use the dao directly? How do I handle situations where I need to use the functionality implemented in several Managers?  PS: I know that there is no golden hammer so I'll narrow it to the types of apps we do, generally webapps for banks. Thank you ",
        "Best answer": "  What are the responsibilities of a Manager and of the Dao classes?   The responsibility of the DAO is the abstraction over data access.  The responsiblity of the manager it to organize and execute the business logic of the application -- the manager uses the DAOs to accomplish this.   Do we have to have a manager for every dao or can we just use the dao   directly?   I would think you should be using the DAOs directly. Again, for the manager to accomplish its tasks, it might need to make use of several DAOs to manipulate data -- I don't see a problem with this.   How do I handle situations where I need to use the functionality   implemented in several Managers?   This could be a code-smell that you need to group together (refactor) certain logic or functionality into a new manager, or (although it might be extreme) another service layer on top of that to organize action among the managers. "
    },
    {
        "ID": "21186",
        "Question": "I have always heard that SDE interviews are much harder to crack than SDET. Is it really true?  I have also heard that if candidate doesnt do well in SDE interview he is also sometimes offered SDET position. How much truth is there into these talks?  I would highly appreciate if someone would put good resources and guidelines for how to prepare for Microsoft interviews..which books to read, which notes, online programming questions websites, etc. Give as much info as possible. Thanks in advance to everyone for your valuable help and contribution. ",
        "Best answer": "Unfortunately, those are both myths. SDEs and SDETs get the same starting salary, so it would be silly to have a lower bar for one of the positions. You'll have to code at the whiteboard in both interviews and show that you can deconstruct complex problems. Depending on the group, the SDE loop may have more emphasis on algorithms, and on the SDET side, you'll find more emphasis on systems thinking - but the emphasis can vary slightly from group to group. To prepare, study algorithms and implementation examples, but more importantly practice solving random programming problems on paper (or on a whiteboard if you have one). Be able to think out loud while you code (the answer is often not as important as the thought process). Also have examples to share on how you learn, communication, and dealing with conflict. For an SDET interview, you can read my book (more importantly, the book will help you evaluate if you want to be an SDET. On the dev side, you can goo...bing \"Microsoft Interview Questions\" and probably get some good practice material - but skip the \"why are manhole covers round\" type questions, as those are rarely used anymore. "
    },
    {
        "ID": "21230",
        "Question": "The Joel Test question #11 is: \"Do new candidates write code during their interview?\". What are arguments for and against to ask new candidates to write code during the interview and to make a decision upon it? ",
        "Best answer": "With apologies to Scott Whitlock: Cons:  none  Pros:  Saves a LOT of time and heartache down the road if you prevent hiring someone who can't program Requires you to have a technical person in the interview  "
    },
    {
        "ID": "21256",
        "Question": "Java is often found in academia. What is the reason behind that?  ",
        "Best answer": "A few Universities have somebody who's sufficiently well known that many (if not most) decisions revolve around that person's likes, dislikes, opinions, taste, etc. Just for example, Texas A&M has Bjarne Stroustrup on staff; it probably comes as little surprise to anybody that their curriculum tends to emphasize C++. Most universities are a bit different though. First, decisions are often made much more for the benefit of the faculty than the students. The single biggest criterion in many cases is \"which language requires the least effort on our part?\" Most of them are also careful in their laziness -- they want not only the language with the fewest advanced concepts to learn, but also one that is (for example) the slowest to innovate, update, or embrace anything new. Second, most decisions are made by committee. This means the final decision is rarely (if ever) what anybody actually wanted -- it's just what the fewest members of the committee (especially those with the most influence) found particularly objectionable. It's like picking a flavor of ice cream. One really likes strawberry, but another is allergic to strawberries. Another really loves chocolate, but somebody else can't stand it.  Yet another thinks rum raisin is great, but the other two worry that mentioning \"rum\" would be interpreted as encouraging alcohol abuse -- so they end up with vanilla, even though it's not what anybody really wanted. Finally, even though it usually runs directly contrary to most of what the previous two criteria would produce in isolation, they generally need (or at least want) to be seen as responsive to the needs of industry. Java is the intersection of these three:  Every concept it embodies was well known by 1980. There's not much to know beyond the very most basic notion of what OOP is. It's the tasteless, odorless, non-toxic, biodegradable, politically correct choice. Nearly the only other language in history to have existed as long and (probably) innovated less is SQL. Even though they're hardly what you'd call fast-moving targets, COBOL and Fortran have still both innovated more than Java. It is widely used. When you get down to it, profs and PHBs have similar criteria.  Note that I'm not really saying (for example) that there's nothing more to know about Java than the most basic notion of what OOP is -- only that that's all that's needed to do what passes for an acceptable job of teaching it. "
    },
    {
        "ID": "21260",
        "Question": "I've found some videos that explain C# but it back to C#.NET when it is first appeared, if I learn from these videos will I learn something that has been removed from the language? will I miss alot?  ",
        "Best answer": "Whatever you find, it should start at C#/.NET 2.0.   Much of .NET 1.0 was refined in 2.0 with generics, and the 2.0 framework is still widely used today; the 3.0 and 3.5 frameworks are enhancements to 2.0, not rewrites.  If you're looking for a beginner book, Head First C# is pretty good. "
    },
    {
        "ID": "21291",
        "Question": "As I understand it, Test-Driven Development is about writing tests to define the program specifications (you can correct me if I'm wrong). If there is someone responsible for writing the specifications (including a public API) for the software (let's call him the Software Architect), does that mean that the Software Architect must write all of the tests? Or does the Software Architect write the specifications, and then hand them over to the developers to write tests against? Or do you allow the specifications to grow organically by allowing all of the developers to write their own tests, and forget about having a Software Architect? ",
        "Best answer": "Test-Driven Development is about writing tests to define the program specifications You don't write tests to define the specification, the test descriptions, user stories, and feature descriptions are the specification, in the 'dead trees' sense. To review, the TDD process in a nutshell is:  define a project in terms of features describe the stakeholder, behavior, and goal of each feature using user stories specify the expected givens, triggering events/conditions, and behaviors/outcomes associated with a user story using test descriptions [and this completes the 'specification'] pick a set of features for each iteration; iterations should be short [i'm omitting the planning and estimation steps for brevity]  code a test for a feature (it will fail, but you had to make API decisions to code the test) implement enough of the feature so that the test passes refactor the code if necessary repeat with the next test until the feature is completed repeat with the next feature until the iteration is completed  repeat with the next iteration until the project is completed  how much design, architecture, supporting documentation, et al you choose to do is not part of TDD. There are some practical 'best practices' you can read about, but keep in mind that those are the 'best' practices in someone else's workshop, not yours.  note that the point is for the customer and the developer to come up with the features and write the stories and test descriptions together, for mutual understanding so, with that out of the way, the original question was: what is the role of a software architect in TDD? And the short answer is: Same as it ever was, same as it ever was.   --David Byrne  EDIT: The long answer is: the architect plays the usual visionary/investigator/irritant/support/backstop roles during the entire process, as necessary. EDIT 2: sorry I missed the point of the sub-questions! Everyone is responsible for writing the specifications; all of the developers including the architect if/when appropriate plus the customer. The developers also code the tests. "
    },
    {
        "ID": "21300",
        "Question": "To my knowledge, all modern imperative programming languages support recursion in the sense that a procedure can call itself. This was not always the case, but I cannot find any hard facts with a quick Google search. So my question is: Which languages did not support recursion right from the start and when was that support added? ",
        "Best answer": "I'm not sure COBOL does (it certainly didn't at one time), but I can't quite imagine anybody caring much either.  Fortran has since Fortran 90, but requires that you use the recursive keyword to tell it that a subroutine is recursive. PL/I was pretty much the same -- recursion was supported, but you had to explicitly tell it what procedures were recursive. I doubt there are many more than that though. When you get down to it, prohibiting recursion was mostly something IBM did in their language designs, for the simple reason that IBM (360/370/3090/...) mainframes don't support a stack in hardware. When most languages came from IBM, they mostly prohibited recursion. Now that they all come from other places, recursion is always allowed (though I should add that a few other machines, notably the original Cray 1, didn't have hardware support for a stack either). "
    },
    {
        "ID": "21336",
        "Question": " Possible Duplicate: How do you organize your projects folders?   I'm interested to learn how everyone keeps their local development projects organized in a logical manner. What do you do to keep your dev projects organized in a manner that allows you to easily find things? ",
        "Best answer": "At any given time, I normally have several different clients, each of whom has one or more projects. I also use a mainline-with-branching development model, which works well with many version control systems like git and Perforce. So on any one of my development machines, above the individual project level, things look like this: Client 1 Directory     Project Foo Directory         Main Branch Directory         All Branches In One Directory (useful for Perforce branching/merging)         Simple Fixes Branch Directory         Big Ongoing Feature 1 Directory         Big Ongoing Feature 2 Directory         (etc.)     Project Bar Directory         (similar branch directories) Client 2 Directory     Project Baz Directory  and so on. I don't typically pull all branches onto any given machine, or pull Mac projects onto a Windows or Linux box. Individual branches usually look something like this: Build Instructions Document/Checklist Top-Level Makefile/Project Source (Directory)    Main Source Files    Component 1 Source Directory    Component 2 Source Directory    (etc.) Include (Directory for headers exposed to the outside world, if suitable) ThirdParty (Libraries, source code, etc. from elsewhere) Unit Tests  Of course, there's a lot of variation. And if I take over a project somebody else started, I usually retain their existing hierarchy. I typically keep documentation and test files either attached to a wiki in my issue tracking system (Redmine), or if they're bigger than Redmine's default 5 MB limit, on an FTP server, so I don't have to keep local copies on all my development machines, but can retrieve them as I need them. "
    },
    {
        "ID": "21387",
        "Question": "Previously I was searching for a good TimeLine control for a WPF project. I found an answer in Here which direct me to this CodePlex project. Now I want to change code to feed my culture needs. But there are some mismatches! My question is: How you interact with such thousands lines of code? EDIT: Any shortcut will be great! ",
        "Best answer": "You add comments to the source code when you have understood it enough to be able to do so.  Refactor these comments vigorously as you understand more and more. "
    },
    {
        "ID": "21397",
        "Question": "When working with WPF or Silverlight, how should one use control naming conventions? Do you name the controls in XAML markup? I have seen samples of projects at codeplex with control names such  as \"selectButton\" or \"btnSelect\". What would you recommend?  ",
        "Best answer": "Microsoft has guidelines published here on their web site.  The bottom line is that Hungarian naming conventions are out. EDIT To make this more clear, Microsoft has dropped Hungarian notation from all of their naming conventions, including UI elements.  HOWEVER, MS has not documented any recommendations for UI elements.  There are lots of links out there that note this and offer their suggestions but the bottom line is that with UI elements, you're on your own.  Example link. In our standard we've dropped Hungarian notation and are using explicit naming, meaning a button called OK would be named ButtonOK, a textblock called Comments would be TextblockComments.  The downside is that names can get kind of long, the positive is that EVERYONE knows exactly what the element is. As long as you establish what works for you and use that standard consistently, you can't go wrong. "
    },
    {
        "ID": "21403",
        "Question": "I always read about large scale transformation or integration project that are total or almost total disaster. Even if they somehow manage to succeed  the cost and schedule blow out is enormous. What is the real reason behind large projects being more prone to failure. Can agile be used in these sort of projects or traditional approach is still the best.   One example from Australia is the Queensland Payroll project where they changed test success criteria to deliver the project. See some more failed projects in this SO question (on Wayback Machine) Have you got any personal experience to share? ",
        "Best answer": "The main reason is an increase in scope, which the book \"The Pragmatic Programmer\" describes as:  feature bloats creeping featurism requirement creep   It is an aspect of the boiled-frog syndrome.  The idea of the various \"agile\" method is to accelerate feedback and - hopefully - correct the evolution of the project in time. But the other reason is release management: if you aren't geared toward releasing the project (however imperfect it may be), chances are it will fail (because released too late, with too many buggy features, and harder to fix/update/upgrade). That does not mean you have to have a fixed release date, but that means you must be able at all time to build a running version of your program, in order to test/evaluate/release it.  The blog post \"Late projects are late one day at a time\" contains many more examples:   I know the ‘Getting Real’ thing to do would be to Flex the scope and keep the launch date fixed, but that doesn’t work if there is agreed upon functionality that cannot be completed in time.  That’s why we don’t advocate specs or “agreed upon functionality.” That’s the root of the problem — saying you know everything about what you need and how its going to be implemented even before the first pixel is painted or line of code is written. When you predict a rigid future on a flexible present you’re in trouble. Rigid futures are among the most dangerous things. They don’t leave room for discovery, emergence, and mistakes that open new doors.  "
    },
    {
        "ID": "21412",
        "Question": "I'm working in development for 4 years, and 3.5 in PHP - why I don't seem to be able to be selected in an interview. I want to know what special things the interviewer wants to see in candidates - for senior PHP developer roles. Interviewer asks me 10 questions and I'm able to answer only 5. Does selection depend on these things? It doesn't mean that I can't solve the problem, I can google the question, I can ask on forums. Why don't they understand that a man can't remember all the answers for each and every question? Especially programming ones. Please advise. ",
        "Best answer": "\"Interviewer asks me 10 questions and I'm able to answer only 5. Does selection depend on these things? It doesn't mean that I can't solve the problem, I can google the question, I can ask on forums. Why don't they understand that a man can't remember all the answers for each and every question? Expecially programming ones.\" These things are very significant and will be a very significant part of the reason. Interviewers do understand that you can't know everything and generally tailor the questions to suit.   Generally most questions an interviewer will ask will be the sorts of things they expect a candidate to be able to answer without access to the internet. Why do they expect this standard?  A few reasons come to mind:  You indicate that you're looking at senior developer roles.  Senior developers are by definition those who have a good level of knowledge already and can help others out, not those who are dependent on Google. A programmer who knows this stuff - as opposed to having to post it on forums - is going to be far more productive that one who relies on the internet.  They're not having to wait for replies, understand what's been posted and adapt it to their purpose, they're just getting on and coding. They're obviously finding candidates who can answer these questions and in that instance wouldn't you hire the guy who got 9 out of 10 over the guy who got 5 out of 10. If they were happy with someone bright who understands the basics and Googles the rest, you can hire a junior developer for a lot less money.  Personally out of 10 questions for an intermediate or senior role normally I'd expect a candidate to be answering perhaps 8 well and having a fair idea at least one of the others. If you're not hitting that level then I suggest that you're probably applying for jobs a little above your current level and should adjust your expectations. "
    },
    {
        "ID": "21436",
        "Question": "The question of managing bugs in production has been a large feature in my mind of late. Sprint's are not meant to have any items added into them, but for critical bugs, this is simply unavoidable. How does one go about managing this break in the sprint? Do you simply give a sprint a percentage \"allowance\" of time, thus only filling say 80% of the schedule with sprint items \"just in case\"? ",
        "Best answer": "If this is critical, you must handle it. To measure its impact on the sprint, you must log it.  Look at this information radiator:  There is a part called \"Unplanned items\". Put your critical bug there. As you see there is the inverse with \"Next\" part where you put more user stories than planned in case you complete the sprint faster. You will talk about it in the sprint review and/or the retrospective. The objective is to find how to limit them, and also adjust your velocity accordingly. "
    },
    {
        "ID": "21534",
        "Question": "I think every single time I've used Math.Ceiling or its language variant, I've always had to cast it to an integer. I mean... that's the whole point, isn't it? To get a whole number. So why doesn't it just return an int? ",
        "Best answer": "For one, REALs/FLOATs usually have a much greater range. The result might not fit into an INTEGER. "
    },
    {
        "ID": "21535",
        "Question": "Almost all developers who work for a large corporation find themselves on the wrong side of site blocking software. It can be both frustrating (\"Just let me read that damn blog!\") and helpful (\"Woah! Dodged a bullet with that site\"). In your opinion, what is the right level of blocking to apply to developers and why? ",
        "Best answer": "No site blocking. If my projects are delivered on time and my productivity is not suffering, I don't see any reason to block anything (except - if you really must block something - well known spyware/malware sites). I don't really have anything else to add except that.  We are professionals, not children.  "
    },
    {
        "ID": "21538",
        "Question": "Currently I am working on a code best described as C code living in C++ body. However I haven't been able to convince power that be to re-factor on ground of ease of maintenance.  What in your opinion is the best argument for refactoring the code.  ",
        "Best answer": "The only way to get a business to agree to a refactor is to show them it will save them money. I dont mean just tell them, you need to be able to say we will save x days on bug type y saving us z pounds.Or in terms of savings when it comes to adding features. It's all about the money. Edit: I'm assuming this code is now live or late stage development. Refactoring during dev is a whole different question. "
    },
    {
        "ID": "21562",
        "Question": "One problem that I have faced over the years is that when I stop using a technology (COM,QT)/language (VBScript)/feature (Templates) for development over the time I lose skill in that. What in your opinion is the \"easiest\" way to retain familiarity so that when I come back to any of them, effort for relearning is minimal. ",
        "Best answer": "Keep hold of your own reference books. That way, you can go back and refresh your memory.  Personally, if you are not regularly using some memory allowing it to fade and to use that capacity for current stuff seems to be a more efficient use of the grey matter.  Rereading the same books that you originally used allows old memories to be refreshed surprisingly quickly. I am looking forward to 10 years time when COM programmers make £2000 a day.   :) "
    },
    {
        "ID": "21571",
        "Question": "I have previously worked on COM, however I have observed for quite sometime that hardly any company asks for COM exp. Is COM dead or reports of its demise are highly exaggerated? ",
        "Best answer": "COM is still widely used within Windows. Many of the new features within Windows are built upon a COM base as it is the main native code component model and Windows is primarily native code. But most non-system application development on Windows is now done in managed code so COM is less relevant (although accessible) as there are .NET framework wrappers. So it is definitely not dead but not that relevant for most people day to day. "
    },
    {
        "ID": "21575",
        "Question": "So your client asks you to write some code, so you do. He then changes the specs on you, as expected, and you diligently implement his new features like a good little lad. Except... the new features kind of conflict with the old features, so now your code is a mess. You really want to go back and fix it, but he keeps requesting new things and every time you finish cleaning something, it winds up a mess again. What do you do? Stop being an OCD maniac and just accept that your code is going to wind up a mess no matter what you do, and just keep tacking on features to this monstrosity? Save the cleaning for version 2? ",
        "Best answer": "Deliberately overestimate the time needed for your next features. Use that extra time to clean up. You'll never be able to justify maintenance, and the client needs it regardless, so give them the bitter medicine (slightly increased costs for the next features) so they can get better. "
    },
    {
        "ID": "21623",
        "Question": "What is the best way to improve your skills in software architecture? We were taught design patterns in university, and I see plenty of books with simple and straight forward examples, but aside from them, how can you learn good architecture? In other words, how does one evolve into a great architect? What are the prerequisites? ",
        "Best answer": " Network. Get to know some good architects. Talk to them. Learn from them, and bounce ideas off of them. Read voraciously. There are hundreds of books on software architecture (several good ones even). Read on topic, and read related topics (building architecture, management structures, electrical engineering, operational architecture, etc.). Study architecture.  There are thousands of platforms and products out there. Look at them. Dissect their parts. Some of my best learning early on was from looking at great architectures (look at the little languages, packed files, and simple structures in the original Doom source, for example). Think, sketch, try. Practice your design skills on imaginary problems. Fun problems. Interesting problems. Do. Work on real systems. Ship them. Feel the pain caused by your poor decisions, the pain you cause your developers, and learn from it.  Good architecture is in balancing what you get, how you get there, and how much it costs.  "
    },
    {
        "ID": "21631",
        "Question": "So that's my story: one of my colleagues uses to review all the code, hosted to revision system. I'm not speaking about adequate review of changes in parts that he belongs to. He watches the code file to file, line to line. Every new file and every modified. I feel just like being spied on! My guess is that if code was already hosted to control system, you should trust it as workable at least. My question is, maybe I'm just too paranoiac and practice of reviewing each others code is good? P.S: We're team of only three developers, and I fear that if there will be more of us, colleague just won't have time to review all the the code we'll write. ",
        "Best answer": "I would say YES! Two quick reasons for it: 1) If code is in production, you cannot assume that it is correct. Any change elsewhere in the system can introduce bugs. I think it is very important that code be checked regularly. This way, refactoring is done on a regular basis, keeping the code neat and \"more\" correct (up to date is probably bettter). 2) Being able to read code is a very important skill if you are going to be a programmer. And a skill it is, something you need to work on. For any programmer starting work on an existing code base, if he is not used to reading other people's code, there is a steep learning curve trying to get up to date with what is going on. I do not think you should feel spied on. Accept any critique someone gives you (if it is valid of course). And feel free to give other people VALID critique. It is the way we learn. Once we stop learning (or want to stop), then there are big problems. "
    },
    {
        "ID": "21697",
        "Question": "I am new to web programming and at this time I am learning about PHP. I would like to know when do I need to use a PHP framework such as CakePHP? What are things that this and other similar PHP frameworks offer for me?  And is it really important to use a framework to be a professional?  And can I create my own framework to provide the features I like into it?    ",
        "Best answer": "Build a few pages without a framework... you will essentially begin writing your own framework by trial and error. After you do that, move to a framework and enjoy how much time you save. Trying to build your own framework can teach you some things, and will help you appreciate the frameworks that are already out there. When I first got into PHP, I thought frameworks were a complicated waste of time. Now, I use CodeIgniter for even simple projects. It takes about 1 minute to get up and running you already have a ton of great libraries available to you. And, as a previous poster mentioned, most frameworks are extensible, so you can always add whatever functionality you want. "
    },
    {
        "ID": "21702",
        "Question": "I have no knowledge of Entity Framework and NHibernate but definitely want to learn. I want to know how to start. Whether to first read documentation of NHibernate or the default Visual Studio Entity Framework? Is Microsoft's Entity Framework a variant of NHibernate? ",
        "Best answer": "This is personal opinion, and I must tell you I'm a huge fan of nHibernate. I'm interested in all Microsoft's initiatives to standardize things. This include in ORM. When they released the first version of Entity Framework, they were critized enough to call what happened The ADO.NET Entity Framework Saga. Read that blog post and all the link it points to. Soon after that, the team leader of the project agreed that it got it wrong (I can't find his original blog post, maybe it has been removed). Today EF framework is very close to nHibernate and I will consider it again in the future because:  Just like Unit Testing, MVC, Ajax, ... EF will eventually become the standard. EF is commercially supported nHibernate is powerful, but has only a few contributors.   While I'm too familiar with nHibernate to switch today, I would recommend you to go for Entity Framework without any hesitation. "
    },
    {
        "ID": "21730",
        "Question": "How can I move a library inside a project's source tree and compiling static binaries? I want to use a particular tool that utilizes ANCIENT binaries without upgrading it's API usage. This way I can use the old libraries inside the single binary without wrecking the local host environment. I am on nix with netbeans/eclipse/code::blocks.  I don't have a problem reading, just looking for a starting point. Any thoughts? ",
        "Best answer": "You could make a script to compile the lib and modify your project file to use that library's path. for qmake it would be : DEPENDPATH += . oldlib/ INCLUDEPATH += . oldlib/ LIBS += -Wl,-rpath=oldlib-build-path-L. -L../ -Loldlib-build-path -loldlib  Note that the rpath part is strictly for gcc, could be different on different compilers. "
    },
    {
        "ID": "21791",
        "Question": "What would be a good example of the use of TDD in large, real-life, complex, projects?  All the examples I've seen so far are toy projects for the purpose of a book or a paper...     Can you name an open-source project that heavily uses TDD? Preferably in C++ but I can read Java and C# or other similar languages. ",
        "Best answer": " JUnit was developed 100% test-driven. In fact, it was developed 100% test-driven in JUnit, which as Kent Beck has said a couple of times was a truly mindbending exercise. I believe Sun's ZFS filesystem was developed test-driven. The ikj interpreter for the Ioke programming language (JVM), the ikc interpreter for the Ioke programming language (CLI), the entire Ioke core and standard library, and in fact the language itself was developed 100% test-driven (actually behavior-driven).  "
    },
    {
        "ID": "21821",
        "Question": "I started learning Java a couple of months ago. I also wanted to become more exposed to linux so I installed ubuntu and started working with Java since then. In the meanwhile however I became more interested in C# and in learning the Windows platform. At this point I'm asking myself if I should stop learning Java and move to the .NET world. I don't hate or dislike Java by the way, I am just more curious about windows/.NET at this point in time.  My question is, should I remain with Java or go with C#/.NET? What is your advice?   ",
        "Best answer": "If it will help your decision, Java is a relatively stable language; it's speed of evolution is much slower than that of C#.  Consequently, C# is getting all of the cool new features more rapidly than Java.  Whether this is a good thing or not depends on your own point of view. "
    },
    {
        "ID": "21843",
        "Question": "Not talking about the recent Java/Oracle debacle here, but something I've been wondering recently. When I first started programming, Java was the big thing client-side, however these days you hardly ever see a Java applet whereas Flash and Javascript are ubiquitous. Where did Java go wrong? Did Sun decide to concentrate on the server-side and let client-side stagnate? How did Flash become dominant over Java? Flashy (no pun intended) graphics? Marketing? Never having been a Java developer, I've not followed this closely so I'm interested to hear your thoughts. ",
        "Best answer": " Firewalls would sometimes block java applets, so developers couldn't be sure if they were going to work or not. They required a browser plug-in, which led many developers to favour javascript over java applets, so that they didn't require the user to download and install a plug-in.  The Macromedia Flash plug-in had a simpler, more seamless, and less noisy means for downloading, installing and updating. The API wasn't so standardized then, so applets could fail to work if the user didn't have the right version of java installed. They had a slow loading time and used too much memory.  They would often crash the browser. Java applets died before Swing came out, so GUIs were difficult and ugly. IE wasn't so good at supporting applets fully, so compatibility was difficult to sort out. Flash and javascript are much easier to use for non-programmers.  "
    },
    {
        "ID": "21845",
        "Question": "I have an idea for a program that I think is a good one (Don't we all?).  I am an amateur programmer and would like to recruit some more advanced programmers onto the project since there are some aspects that I am not skilled enough to do myself.  Is it better to  a) Develop the software on my own as much as I can, even though it will be amateurish, and then show a demo to potential recruits  or b) Try and recruit some more advanced programmers and seek their advice and guidance before I get started so that it is done correctly from the beginning? I would especially love it if someone has any experience of being in a similar position, but sage speculation is welcome too. ",
        "Best answer": " Nobody should start to undertake a large project. You start with a small trivial project,   and you should never expect it to get large. If you do, you'll just overdesign and generally   think it's more important than it is likely at that stage. Or worse - you might be scared   away by the sheer size of the work you envision.   So start small, and think about the details. Don't think about some big picture and fancy   design. It if doesn't solve some fairly immediate need, it's almost certantly over designed.   And don't expect people to jump in and help you. That's not how it works. You need to get   something half way first, and only then others will say \"hey, that almost works for me\",   and get involved in the project.  — Linus Torvalds pretty much sums it... "
    },
    {
        "ID": "21869",
        "Question": "We had a guest lecturer at our school he went on a tangent regarding those graduating and not being upto the skill level he expects ie. knowledge of one big-time compiled languague like Java, C++ or C#, one scripting language like Python or Ruby, one web framework. What are some other skills that you guys think should be absolutely critical to graduating student? ",
        "Best answer": "By far, I would say an eagerness to learn.  College isn't really intended to give you the skills you need to become a programmer.  It's intended to teach you the skills you need to learn those skills. "
    },
    {
        "ID": "21870",
        "Question": "Is is possible to correctly call yourself (or your team) \"Agile\" if you don't do TDD (Test-Driven Development)? ",
        "Best answer": "Yes, yes, yes, a million times yes. Agile is a philosophy, TDD is a specific methodology. If I wanted to be really picky I could simply point out that there are quite a few variations of xDD - which their advocates will explain in depth are not TDD - but those are still substantially bound up with test first so that would be cheating. So lets says this - you can be agile without doing \"test first\" development (look at the way scrum works - nowhere in there are there specifics about how you write code). Look at a kanban board, look at all sorts of agile methodologies. Do you want unit tests? Of course you do, for all kinds of reasons - and you might well make an argument that you can't be agile without unit tests (although I suspect that you can be) - but you don't have to write them first to be agile. And finally, its equally true that you could do Test First without being Agile and strong arguments for doing test first regardless of your overall dev philosophy.  It seems that others (with a more SOLID rep) have a similar opinion... http://www.twitter.com/unclebobmartin/status/208621409663070208  @unclebobmartin: http://t.co/huxAP5cS Though it's not impossible to do   Agile without TDD and OOD, it is difficult.  Without TDD the iteration   rate of...  (The link in the tweet is to the full answer on LinkedIn) "
    },
    {
        "ID": "21896",
        "Question": "I frequently encounter a situation where I need to write a program which reads data-objects from a file or system, does a transformation on the data, convert the data to another datamodel and then push the data in another file, or system. Do you know of a object-model which facilitates these type of programs in such a way that it is pluggable and scalable?   ",
        "Best answer": "here's a link to an article on an alleged Extract/Transform/Load (ETL) design pattern but it sounds like what you want is a framework "
    },
    {
        "ID": "21911",
        "Question": "I have a client who wants a web app that will let him do the following (as he told me word for word):  User registration Recurring payments for users Online conference room reservation  I am supposed to give him a cost estimate very fast and I don't have any time to do any deeper requirements analysis! How would you deal with that kind of situation? Should I just give a very broad estimate and work out the exact requirements once he has accepted the estimate? At which point do you usually gather requirements, after or before getting a contract? ",
        "Best answer": "Here is what I usually do to limit problems: Define the information yourself, by describing what you are going to do in details.  Bid on that, and only that.  Refer to that document in your official purchase order form you will ask your customer to sign. As an altenative, I suggest you to sell your customer with iterations instead of fixed features, with the flexibility to stop or continue development at the end of each iteration.  If you don't know how to estimate your own document, try to do some collective estimation using planning poker. You can also split each functionnality into tasks and estimate each. Sum and multiplicate the result by two if you are pretty confident, or three if you are not sure. If you not confortable with your estimations, it's a pretty good indication that you are not experienced enough to accept the job in a fixed price scheme. "
    },
    {
        "ID": "21917",
        "Question": "I am going to interview within two weeks for an internship that would involve Python programming. Can anyone suggest what possible areas should I polish? I am looking for commonly asked stuff in interviews for Python openings. Apart from the fact that I have already been doing the language for over a year now, I fail to perceive what they can ask me.  Like for a C or C++ interview, there are lots of questions ranging from reversing of strings to building linked lists, but for a Python interview, I am clueless. Personal experiences and/ or suggestions are welcomed. ",
        "Best answer": "How about something involving Python list comprehensions? To me, those were one of the big selling points over C++, after I read about them in Dive into Python. \"Write a list comprehension that builds a list of the even numbers from 1 to 10 (inclusive)\". Where the answer is anything like this (note the range values avoid a fencepost error): foo = [x for x in range(1, 11) if (x % 2) == 0] print foo [2, 4, 6, 8, 10]  For that matter, if you understand all the concepts listed in Dive into Python, that should cover most of the important features. "
    },
    {
        "ID": "21926",
        "Question": "The stereotypical view of a programmer can't do graphics very well, from what I've read and seen. However, I love programming (preferably OOP, PHP, C++, Objective-C) and can't deny the fact I have a unique taste in web design and others have said I am doing well at it (CSS). I thought to myself \"Hey, wait, I'm a programmer - how can I design well?\". Question is: is it possible to be good at programming and designing? Does anyone here feel the same? For the record: actual images I have created have been called programmer art several times before by friends ",
        "Best answer": "Well, why not?  Lots of people have multiple talents. But the amount of time that you devote to a particular skill does make a difference.  Spending more time one one skill means you have to spend less time on another, and spending less time means being less competent. For my part, I have spent the vast majority of my time on coding, not design.  As such, I am a pretty good programmer, but have stick-figure design skills (although I do believe I know good design when I see it). Good design means more than just looking pretty; it also means making an application that is intuitive and easy to use. "
    },
    {
        "ID": "21950",
        "Question": "The last week I saw at least three questions where people were confused about the differences between char * and std::string or made some related mistakes. Don't get me wrong, I don't have a problem with this, I just don't get the exact reason for this. Maybe it was just a coincidence. ",
        "Best answer": "Because they're both commonly known as \"strings\" in a language that has no real string type.  It's very understandable that inexperienced coders would be confused by that... "
    },
    {
        "ID": "22050",
        "Question": "I'm interested to hear what characters people permit in the various common fields of input forms (barring e-mail...) For example: Textarea/Messages:  do you allow completely free text, or restrict to alphanumeric and punctuation? I've tried to go the second route, but you always find someone out there that likes using tilde's... Full name: do you allow anything, or do you use a regex? Telephone numbers: numbers only Addresses: alphanumeric, whitespace, hyphens City/Town names: alphabet, hyphens, whitespace [NB: edited as requested] ",
        "Best answer": "no restrictions, provided that:  the text is not published back to a web page in a way that would permit hijacking/hacking the text is not included in a SQL statement in a way that would make it vulnerable to sql injection you're reasonably sure that a person is responding and not a bot (captcha, for example)  "
    },
    {
        "ID": "22079",
        "Question": "High performance computing means different things to different people. From a pure programming perspective what I gather is that these are loads of individual machines connected by extremely fast interconnect not necessarily based on usual TCP/IP.  My question is on the interconnect side of things: How do you make interconnects faster without changing your socket based code much? ",
        "Best answer": "You can use TCP over just about any high-performance network. OpenFabrics (a software stack for RDMA networks like InfiniBand and iWARP) has the \"sockets direct protocol\". Further, 10 GigE in all its forms was built specifically to provide compatibility with legacy socket code. So feel free to use TCP on your high-performance network. It won't have the absolute best latency advantages compared to, say, using the underlying verbs. But it should be fast enough for most customers' needs. "
    },
    {
        "ID": "22106",
        "Question": "Other than using Emacs / Vim with ctags or cscope or both, are there alternatives to C++ code navigation of an existing code base? I used to use Source Insight on Windows and it worked for me fairly well. The main functionality I am looking for is quickly find definition of macros, variables and functions and places a function is referenced. ",
        "Best answer": "In XCode, which comes with the OS, you have the \"Jump to Definition\" function, which I usually assign to Cmd-F1 (VC has some similar shortcut if I remember correctly). You will have to create an XCode project for your code base though. I don't know if there are any automatic converters, say, from Makefile to XCode project. Most of the time Jump to Definition works fine, except when the name of your .cpp module differs from the .h header name; also doesn't work for overloaded operators. There are some intermittent failures that I can't explain, but overall, on simple and straightforward C++ code it can do the job OK. P.S. there is no such thing like find references in XCode, but you have a project-wide Find function with an option to look for \"Symbols\", which I think is almost exact equivalent of finding references. "
    },
    {
        "ID": "22146",
        "Question": "I have a couple of developers at my company who wish to move from programming into architecture. What are the best books out there on the theory and practice of software architecture? Include a cover picture if you can.  Feel free to include general books, and also books that relate to a specific technology. ",
        "Best answer": " But, I would encourage these developers not to move into architecture and totally forget about programming. If they do, they won't have any respect from programmers and they won't be properly equipped to make architectural decisions. "
    },
    {
        "ID": "22219",
        "Question": "We will be engaging large corporations to help them run various types of analysis, mostly code analysis of their applications.  The storage requirements will likely be modest, hundreds of MB per client. My question is mostly aimed at finding the stack with existing frameworks, best practices and a community which will allow a rapid development process. I am comfortable with the .Net environment and am leaning towards Azure. Any thoughts? ",
        "Best answer": "We've been running a SaaS for enterprise customers on .NET and our own servers. Everybody seems to be happy. If you communicate the idea of SaaS to the customer properly, they won't care what it's running on and where. "
    },
    {
        "ID": "22314",
        "Question": "I've heard this term bandied about, especially in reference to web development, but I'm having trouble getting a clear understanding of what an Information Architect actually does. Any ideas what their role or deliverables would be? ",
        "Best answer": "As I understand it, an Information Architect decides how information will be organized and presented on a web site.  This would include navigation, aggregation, presentation (more what is presented than how) and access control (including security and filtering).  Think of them as responsible for \"content design\", as opposed to \"site design\". "
    },
    {
        "ID": "22352",
        "Question": "Consider a system that uses DDD (as well: any system that uses an ORM). The point of any system realistically, in nearly every use case, will be to manipulate those domain objects. Otherwise there's no real effect or purpose. Modifying an immutable object will cause it to generate a new record after the object is persisted which creates massive bloat in the datasource (unless you delete previous records after modifications). I can see the benefit of using immutable objects, but in this sense, I can't ever see a useful case for using immutable objects. Is this wrong? ",
        "Best answer": "Computation using immutable objects (as in functional programming) does not necessarily imply persisting every object that is generated! "
    },
    {
        "ID": "22411",
        "Question": "Which if the SOLID principles introduced by \"Uncle Bob\" Martin had the largest effect on your day to day coding? Please give details! ",
        "Best answer": "All of them, and none of them per se - I started with OO programming about ten years before Uncle Bob's first book came out. ;-) My favorite principle is DRY = Don't Repeat Yourself. Also called 'normalization'! "
    },
    {
        "ID": "22468",
        "Question": "What are some data structures that should be known by somebody involved in bioinformatics? I guess that everyone is expected to know about lists, hashes, balanced trees, etc, but I expect that there are domain specific data structures. Is there any book devoted to this subject? Thanks, Lucian ",
        "Best answer": "If you are to do any work on large datasets you need to learn how to handle those efficiently. Just think if you are going to do image analysis on a 2 Gb TIFF image.  Or sequence matching on a 150 Gb genome dataset. When you move from a \"nothing is slow\" to a \"this operation is fast and THIS is very, very slow but then THESE operations are fast\" mindset, your algorithms tend to change.  It is relatively cheap to manipulate data in memory as opposed to having to fetch them from disk all the time, and it is relatively cheap to manipulate data in the cpu cache as opposed to having to fetch them from memory all the time.  In other words, algorithms that take that in consideration will be faster resulting in faster results, more publications and fame and Nobel prizes. "
    },
    {
        "ID": "22504",
        "Question": "I'm a big fan of checklists. There is Travel Checklist, Moving Checklist and even a Scrum Checklist. Context: you have been hired by a large corporation and given the mission to setup the whole software development environment, processes, team, etc. You have \"carte blanche\". You will be responsible for the creation of the working increments of the software. Project size: 2000 man/days. What items would you add to the following (intentionally small and incomplete) checklist:  Install a Continuous Integration Server Write a DoD Write a one page coding guidelines Create a product backlog Install a Bug Tracking System Schedule Regular Face Time  ",
        "Best answer": "* 1.)  Talk to the developers to see what they really need! * 2.)  Investigate a solution for bringing up multiple environments really rapidly (think public or private cloud instances or old fashioned virtual machines if you'e not buzzword compliant) 3.)  Source/version control 4.)  Code review system (Crucbile/Fisheye as an example) 5.)  Kanban wall (or something similar) 6.)  Communication protocols (real time chat is a big plus), wikis also encourage collaboration.  This also covers public relations internally - how are you going to engage with your business owners, tech support staff and other groups? 7.)  Electronic whiteboards 8.)  Comfortable environment for developers (couches, tables, chillout areas, good WiFi etc) 9.)  Great coffee!!! "
    },
    {
        "ID": "22516",
        "Question": "I know this is a very very basic question. For some software applications there are a large almost infinitely high number of test cases for an application. It is not practical to test all those test cases. How do we decide when to stop testing? (other than \"when money runs out\").  ",
        "Best answer": "The simple answer is it depends on the system.  If you're writing embedded software for a heart monitor or safety monitoring tools for a nuclear reactor then the standard is far higher than if you're writing a blogging platform. This is really a question for a good system tester (and I'm not one) but I'll give it a shot. Your basic measure is going to be test coverage: How much of the application has actually been tested (both by unit test and functionally).   You need to assess each potential use case (and parameters for that use case) for likelihood of it actually being used (so you may drop edge cases), complexity (simpler things being less likely to contain bugs, or rather less likely to contain hard to find bugs), cost to test (in terms of time) and potential impact of a defect if discovered in that area (this is where the nuclear reactor vs. blogging platform comes in).   Based on that assessment you need to work out which of those are going to be tested and in how much detail.  Once you have a list like that the team (including a product manager / project manager / user representative) can go through that list and prioritise based on the constraints you have. One useful technique to think about is that you may also vary the use cases that are tested with each release.  For instance you might have a list of non-critical test cases and test half of them with one release and half with the next (then alternate).  This way you're increasing the total test coverage you get for the effort (though at the risk of regression bugs being introduced).   This could also extend to platform testing - if you support two database back ends (or multiple browsers) test half the app on one, the other half on the other and then swap next release.   (I think this is referred to as striping but don't quote me on that.) And then the final thing to think about is not what you test but what you actually fix when issues are discovered.  It's common to say \"fix all bugs\" but the reality is that there are time pressures and not all bugs are equal.  Again, regular bug scrubs with all the relevant parties are the best way forward.  This is particularly relevant where a bug fix may be particularly intrusive as the additional work in retesting and regression testing it generates may outweigh the benefit of the fix. "
    },
    {
        "ID": "22526",
        "Question": "I'm used to program with Python and GTK using pygtk library. I feel like my programs are not well separated according to MVC model. I think following a framework it would help me to design better desktop applications. I'd like to know which Python MVC framework for desktop apps you're using and why. ",
        "Best answer": "first, I think Qt, and even it's Python bindings are way better structured than GTK; so you could first try it. second, it seems you're slightly confused by the MVC buzzword.  It was originally defined as a modular architecture for GUI applications; unfortunately, there's a completely different layered architecture for Web apps also called MVC. Maybe you feel your desktop apps are not very MVC because there not very web MVC, but they might be very GUI MVC nonetheless. "
    },
    {
        "ID": "22552",
        "Question": "We've all (almost all) have heard about the horror stories as well as perhaps studied about them. Its easy to find stories of software that is over budget and late. I wanted to hear from developers the opposite story: Question:  Do you know, or have worked on a project that was on budget and on time? What is the most valuable lesson you learned from it?  ",
        "Best answer": "Yep, I've seen it happen. Key elements: 1) Well defined requirements, clearly agreed, with a solid change control process. 2) Developers involved in the estimates, with no pressure on them to produce estimates which were what the client wanted to hear, just what they really thought would be needed to complete the work properly 3) Estimates that also took account of risks and uncertainties 4) Facilitate early feedback from the client - we've provided videos, demos (hands on and hands off depending on stability) as early as possible 5) A stable team whose availability has been realistically figured into the schedule (for instance if they spend a day a week doing support and admin, then they're only expected to complete 4 days a week work on the project)   It's not rocket science but removing the commercial pressures and, critically, getting the requirements clear and controlling them is challenging (and where things normally fall down). "
    },
    {
        "ID": "22559",
        "Question": "As stated in the title, are optional parameters, such as those used in C# helpful or are they a hindrance to application maintenance and should be avoided as they can make the code harder to understand? ",
        "Best answer": "Optional parameters have, in my experience, been a good thing.  I have never found them confusing (at least no more confusing than the actual function), since I can always get the default values and know what's being called. One use for them is when I have to add another parameter to a function that's already in general use, or at least in the public interface.  Without them, I'd have to redo the original function to call one that's got another argument, and that can get really old if I wind up adding arguments more than once. "
    },
    {
        "ID": "22583",
        "Question": "I'm talking about something like this:  echo $form->input('general_addresss', array(             'label' => 'Where will you go today?'             'format' => array('before', 'input', 'after', 'label', 'after', 'error')             ));  Do you start with one array parameter, then break a line? If it can't fit on a line, do you immediately break a line?  After of which, do you do a set number of tabs over? What happens if an array within an array has lots of properties?  Is there any particular guide you follow?  ",
        "Best answer": "I do it like this: echo $form->input     (         'general_addresss',         array         (             'label' => 'Where will you go today?'             'format' => array             (                 'before',                 'input',                 'after',                 'label',                 'after',                 'error'             )         )     );  It may seem like a massive overkill of whitespace, but I find it quite readable and obvious - none of my brackets are hidden and everything is nicely indented visually. YMMV ;) Edit: Regarding the \"particular guide\", my particular guide are my eyes. If I can't quickly see where I left my keys in the source code, it's probably badly formatted. p.s. You must be coming from CakePHP :) "
    },
    {
        "ID": "22623",
        "Question": "I have recently been assigned to port an old COBOL program. I am used to GUIs, and I can't understand how a TUI works. I searched a lot through Google but couldn't find something. I knew that console applications could output line by line, but how are colors etc. drawn to the terminal screen? How all this stuff is being drawn? Do terminals support it somehow? Is there a standard? I am really confused. ",
        "Best answer": "Pretty sure your answer is going to vary based on platform here.  There are many different types of terminal/console and for many more than one standard for writing to it.  Most of them I have seen use some sort of escape sequence (ex: ANSI char escapes for dos, YMMV) where you send a special escape char to interrupt the writing to the screen then you can move the cursor, change colors, etc. If you are writing something literally for dumb terminals then probably going to be using something like https://secure.wikimedia.org/wikipedia/en/wiki/Ncurses "
    },
    {
        "ID": "22642",
        "Question": "I have seen several times on this site posts that decry Java's implementation of generics. Now, I can honestly say that I have not had any issues with using them. However, I have not attempted to make a generic class myself. So, what are your issues with Java's generic support? ",
        "Best answer": "Java's generic implementation uses type erasure. This means that your strongly typed generic collections are actually of type Object at runtime. This has some performance considerations as it means primitive types must be boxed when added to a generic collection. Of course the benefits of compile time type correctness outweigh the general silliness of type erasure and obsessive focus on backwards compatibility. "
    },
    {
        "ID": "22685",
        "Question": "In all projects that I have been involved with that have had input from an outside consultant the question has been asked about what sort of Configuration Management we were using. In none of these cases has the consultant been able to define Configuration Management. So what is it? ",
        "Best answer": "Configuration Management encompasses \"everything else\" that's not directly related to writing code. So it's how you manage releases, how you manage and triage bug reports and feature suggestions, how you perform your builds and so on. It often also includes how your source code is managed (e.g. what version control you use, branching strategies, how you [possibly] manage multiple concurrent \"releases\" etc). "
    },
    {
        "ID": "22742",
        "Question": "I am usually fascinated by articles defending PHP and the arguments being used. Quite often authors of these types of articles don't mention what experience they have with other languages. Personally I find it hard to imagine that somebody would know either Python or Ruby, and PHP at a high level and still prefer PHP. So if anyone fits this description, I would be interested in hearing why you prefer PHP to either of these languages. ",
        "Best answer": "Yes, I flat out prefer the C-style syntax in PHP over Python or Ruby (perhaps over Ruby less so).  I could do without the sigils ($) for variables in PHP, though. I've never had the problems with PHP that a lot of people seem to have.  If you develop your applications as applications, and not individual peieces that get inserted into web pages by intermingling HTML and PHP, PHP is not that much different from other langauges.  And the library support and available online resources are huge.  People moan about 'inconsistent' function parameters and such, but thats never been a huge hinderance to me. PHP does give you a lot of rope to hang yourself, though, if you arent a disciplined programmer.  You can get yourself into a lot of trouble if you start using little PHP tricks all over the place (variable variables, using include() as function calls, etc). "
    },
    {
        "ID": "22749",
        "Question": "If you're doing a solo project - would you use CI tools to build from a repository? I've used Hudson and Cruise Control in a team environment, where it's essential to build as soon as anyone checks anything in. I think the value of version control is still obvious, but do I need to build after every commit, seeing as I would just have built on my local machine, and no-one else is committing? ",
        "Best answer": "After a brief contemplation I would suggest that it might even be more important for a solo developer than for a team. At the most basic level a CI server demonstrates that you can build your application from scratch from committed source - combined with a decent set of tests it should demonstrate that you can build and run from scratch. Since one of the things I try to do is ensure that my build includes a deployable package you also know that you can go get something to deploy (clean and from a known state/version). In fact now, when you do File|New Project you should probably include creating or adding to your repository and setting up your CI build script and deployment setup (even if that's only to zip a pile of stuff for xcopy deployment)  Addendum (2016) - nowadays my CI system will also be an integral part of my deployment process, so its value has increased and I absolutely won't run any deliverable project without it. Automated push button deployment takes a lot of stress out of the process and in some way, shape or form a build server is integral to that. "
    },
    {
        "ID": "22753",
        "Question": "What C# project(s) would you consider contributing to if you were a beginner trying to sharpen your skills in C# and .NET framework ? The project should be (besides all) active and not something less active and/or stagnant.  ",
        "Best answer": "I'd advise you to get a couple projects under your belt first.  Think of a simple program idea which might help you in some small way, such as a calendar or organizer.  You can decide all the functionality you'd like, but have a basic basic functionality program that you can aim for first before adding new features.  Trying to do too much at once can easily overwhelm you.   Once you feel comfortable with the general feel for C#, you can hope to help others on sourceforge.net et al.  Most people don't have problems helping you understand the project, though they do expect you to know to do basic things in C#, meaning you can't ask them for help in that regard.  That's why I would say that it would be better to have some basic experience first. "
    },
    {
        "ID": "22809",
        "Question": "Why would someone use his own time to develop an open-source project for free and without compensation? ",
        "Best answer": "For small projects, reasons might be \"hobby\", \"getting some experience\", \"fame\", \"joy\" etc. but that's not how the big open source projects like Mozilla, OpenOffice, Linux work. Why did Sun buy StarDivision and made StarOffice an open source program (called OpenOffice.org)? Why does Mozilla create a top-notch browser and give it away as open source? Why are there people creating Linux, writing drivers and whatnot, and make it available to everyone for free? Why does Microsoft create opensource drivers for Linux so it can run better in MS's virtualisation? Because it makes some business sense for them. They make money that way, or at least plan to. In some cases, the dominance of MS's products, i.e. Windows, Office, Internet Explorer, was the reason the create a competing product, so it would be harder for MS to use their desktop dominance to conquer other domains, i.e. servers, internet services, too. This explains, to some extend, OpenOffice.org and Mozilla. In other cases, open source software is meant to drive sales of hardware, other software or services. Open Source drivers obviously help to sell hardware components to Linux users. RedHat sells support for their Linux distro, and they sell the fact that their Linux is genuine RedHat. Other products, e.g. Oracle, are certified for use on Redhat, but not on CentOS, even though it probably runs just-as-well. Server hardware is certified for Redhat, even though other linux distros probably run just-as-well. Big-money-clients don't care about the price, they want the certificate. Some companies, e.g. Google, sponsor many open source projects, because it helps their business. They don't do it for altruism. They want a free internet, a pervasive internet, a widespread internet, where people use Google's services so Google generates revenue. "
    },
    {
        "ID": "22854",
        "Question": "If given too little time, quality suffers. If given too much time, developers will find a way to use all alloted time, no matter what. What would be a good way with dealing with estimates? Are they a necessary evil? ",
        "Best answer": "Of course they're necessary but they're not intrinsically evil, they're just done badly. The basics of a good estimate: 1) Developers, ideally the ones who will do the work, have to be involved in generating them.   2) An estimate is a range (usually best case, worst case, most likely case), not a single value.  You may use the range to approximate a single value ((best case + 4* most likely case + 2 * worst case) / 7 being a fair formula for doing so) but never forget that that's all it is - an approximation.   3) In my experience the most common reason for bad estimates is wishful thinking, either on the part of the developer (who is optimistic it will all go fine) or the manager (who is desparate for it all to go quickly).  This needs to be fought - any estimate based on everything going well is pretty much a cert to be massively under.  Most likely case should probably assume that a certain amount of stuff isn't going to go the way you want it to. 4) In my experience the second most common reason for bad estimates is not thinking through everything that has to be done.  It's not just the coding, it's the unit testing, the admin, the user guide, the back end maintenance screens relating to the functionality.  Break down the task into individual components each taking between quarter of a day and five days - that sort of size can be estimated with a reasonable level of accuracy. 5) Measure the actual time things took and review afterwards and use that to improve estimates.  Refer to actuals for similar previous tasks as a bench mark and to justify why things are higher than might be expected. 6) Estimate both at the micro level (each individual task) and the macro level (gut feel on how long the whole thing should take).  Usually the macro level estimate will be lower - this is an indication that you might want to review (but not necessarily change) the micro level estimates.  The sweet spot in my experience is when they either agree or, more likely, the macro level estimate is still lower but no one is willing to change any of the individual task estimates.  At that point you use the individual task estimates. 7) Don't confuse the estimate and the commercial aspects.  If you estimate 100 days and the usual charge out rate is $1,000 per day that should obviously be $100,000.  If they say they can't charge that and win the business that's fine but (unless the scope changes) the estimate doesn't change, the charge out rate does. 8) Developers should be challenged on their estimates (as in asked to explain why something takes that long) but not pressured to lower them.  Certainly not just because the numbers aren't what someone wants in any case - if they can't justify them then they usually need to be pushed for a better breakdown and potentially a second set of eyes but in a non-confrontational manner - remind the PM (or if you are the PM remember) that they're going to look bad if the project isn't delivered on time too so they should be keen that the estimates are solid. 9) Contingency should be added both shit happens contingency (usually 10 - 20%) and to cover specific risks.  Contingency should be planned in but is \"owned\" by the project manager - that is if a developer needs to go into it he has to speak to the PM and say why he's over run his estimate.  This usually helps stop the just filling time and keeps contingency for genuine stuff.  If anyone objects to your shit happens contingency budget then explain it's just the reality of IT projects - stuff happens you can't predict.  If they still don't like it then remember to hide it inside the estimates next time. 10) (Edit) Don't confuse effort and elapsed time.  Effort is the total time that will need to be spent doing something, elapsed is the difference between when it starts and when it ends.  They're normally different - admin, support, mentoring and life mean that the chances are that you won't get 5 days effort completed in a working week.  Take 4 as a rough guide but adjust according to the company and the other commitments of the team members. 11) (Edit - from Martin Wickman) Other work you've done or are planning can be a valuable source of information when estimating.  In the comments below Martin refers to Relative Estimating which involves comparing like tasks with each other to ensure that the estimates assigned seem in proportion given the work involved.  It's surprising how often you'll happily assign 1 week to one task and 1 week to another but when you look at the two things one is clearly more complex than the other - that's a good indication that one (or both) figures are wrong.  Taking this further you get into evidence based scheduling which uses completed tasks and projects as a key input into estimates for future work - at a very basic level \"how long did it take last time we did something like this?\" Steve McConnell wrote about all of this in Software Estimation.  It's a bit dry but it's very thorough on the subject. "
    },
    {
        "ID": "22885",
        "Question": "If I see something I like in another application, and create something similar from the ground up in a commercial product, could I get sued over it?  Of course it would be styled to look differently, however most of the functionality would be the same since the functionality is what I liked about it. It also would not be the entire system, just a part of one. I live in the US. ",
        "Best answer": "From everything anyone (including lawyers) have told me, you can get sued for anything.  The real question is whether or not you will, and whether or not they'll win. One lawyer told me, \"if anyone thinks you're big enough to take a run at, then that means you're successful.\"  In other words, suing someone is a big undertaking.  It has to be worth someone's time and money. That doesn't mean you won't get a strongly worded cease and desist letter. (Of course, IANAL). Assuming you didn't directly copy any code or screenshots or whatever, then you're probably not breaking copyright rules.  Patents, on the other hand, are overly broad, and if someone big enough to sue doesn't like you, they likely have a patent that covers anything you might be writing. Further reading: it would be worth looking at the \"look and feel\" part of this case. "
    },
    {
        "ID": "22933",
        "Question": "My (non-technical) colleague has threatened me with a Gantt chart for a new project that we are currently planning.  What is this likely to provide for us, and will it be a helpful tool? ",
        "Best answer": "As Wikipedia says it Gannt chart is a type of a bar chart (more often a \"line type\") that helps with project planning. It is often drawn manually on the wall on a big (really big) piece of paper, since it is easily modified in that format.  It is a very simple type of planning tool; you can produce it in Excel or some equivalent; and rather effective, as long as the time required for certain phases of project can be roughly estimated. If there is a delay - no problem - one line get lengtened, the others stay the same and you have a new end-of-project-date. Overlapping phases (time-wise) are easily seen on it, as easily as dependencies of starting one phase relying on the end of another. 'tis all there is to it really. Of course, the problem with the Gannt (or the \"time chart\" as it is usually called in my part of the world) is that, that at the beginning of a project you have it all drawn up nicely on the wall, feeling enthusiastic and happy, ... then one delay occurs, and you change it on the chart, and you're still felling happy ... then another delay occurs, you draw it up again, and you're still feeling pretty good ... 100x delays occur ... you're feeling like _______ (censored). Meaning, it is only a good project planning tool if you're actually sticking to those little deadlines. So stop wasting time in here and get to work! "
    },
    {
        "ID": "22993",
        "Question": "Next semester (starts in just under 5 weeks), I'm taking System Programming and OOD concurrently.  Apparently SP centers on working in C on Linux, and I only have Java and Python experience, so I am a little nervous about this. For those of you who've taken these at an university, what should I look out for?  I've already taken Computer Organization which included an overview of Assembly.  With the exception of implementing loops in assembly, I ranged from B+ to A- in the assembly projects.  Building circuitry was another story, however. ",
        "Best answer": "Well, exactly what you're going to need wil depend on the class, though some generalities will likely hold. I'd suggest getting a C book intended for Java programmers. It's likely to point out the major pitfalls and help you transition. The biggest items that you're going to worry in C that you don't really worry about in Java are pointers and memory management. Java references are technically pointers, but they're seriously dumbed-down pointers, and from what I've seen, Java programmers don't really see them as the pointers that they are and often have some trouble dealing with pointers initially. Going hand in hand with that is the fact that you won't have a garbage collector, so anything that gets allocated on the heap will not only have to be manually allocated, but you'll have to manually de-allocate it when you're done with it. So, you're going to have to become familiar with malloc and free. You also aren't going to have classes, and structs in C can't have functions in them, so you don't generally program in C in an object-oriented manner. That may or may not be a big deal for you. It does mean, however, that what you'll be learning in your systems programming course and your object-oriented design course are likely to be very different. OOD should be more of an extension of what you know, teaching you how to better program in an object-oriented manner. Systems programming, on the other hand is much more about getting down and dirty and will be much more low-level in what it deals with. Both are really important to being a good programmer though. Without knowing your exact skill set and experiences and without knowing the exact courses, giving more detailed advice would be difficult, but primarily, the issue is likely to be in dealing with how C doesn't doesn't manage things for you and doesn't try and keep you safe from yourself. You will have to be more careful programming in C than you would be in Java or Python. There's plenty of stuff that's perfectly legal C which would be illegal many other languages and could surprise you. The main things to be concerned about though are pointers and manual memory management. "
    },
    {
        "ID": "23019",
        "Question": "I wonder how do you decide on the version number of your apps, do you really record down all changes made (for a change log)? Then how will you decide how to increment the version number. eg. +0.0.1 for each bug fix or something, +0.1 for each new feature release? or maybe you bundle many changes into +0.1 of your version number? ",
        "Best answer": "Major.Minor.Revision  Major. Big change, new features, breaks compatibility, stuff you want to charge extra for. Minor. New features that you can give away. Doesn't break stuff. Revision. Bug fixes.  "
    },
    {
        "ID": "23021",
        "Question": "Would you refactor your app as you go or focus on completing app first? Refactoring will mean progress of app app will slow down. Completing app will mean you get a possibly very hard to maintain app later on? The app is a personal project. I don't really know how to answer \"What drives the functionality and design\", but I guess it's to solve inefficiencies in current software out there. I like minimal easy to use software too. So I am removing some features and add some that I feel will help. ",
        "Best answer": "Make it work.  Then make it fast. Finally, make it beautiful. If you have good separation between your code (presentation, business, and data layers) using interfaces, and it's not a monolithic design, then refactoring should not be that difficult. If you're having that much difficulty refactoring, that's probably a code-smell --  I suggest you look at Solid Principles "
    },
    {
        "ID": "23064",
        "Question": "As most people agree, encouraging developers to make  fast code by giving them slow machines is not a good idea. But there's a point in that question. My dev machine is fast, and so I  occasionally write code that's disturbingly inefficient, but that only becomes apparent when running it on other people's machines. What are some good ways to temporarily slow down a turbocharged dev machine? The notion of \"speed\" includes several factors, for example:  CPU clock frequency. Amount of CPU cores. Amount of memory and processor cache. Speed of various buses. Disk I/O. GPU. etc.  ",
        "Best answer": "Run your tests in a virtual machine with limited memory and only one core. The old machines people still may have now are mostly Pentium 4 era things. That's not that unrealistic - I'm using one myself right now. Single core performance on many current PCs normally isn't that much better, and can be worse. RAM performance is more important than CPU performance for many things anyway, and by limiting a little more harshly than for an old 1GB P4, you compensate for that a bit. Failing that, if you're willing to spend a bit, buy a netbook. Run the tests on that. "
    },
    {
        "ID": "23075",
        "Question": "We've all been there:   Your project failed or got cancelled.  The code you spent days working on got rejected by your team.  The design pattern you introduced to the team created chaos.  Everyone ignores your ideas.  My question is, what is the most productive way for a programmer to handle development-related failures such as these? ",
        "Best answer": " Your project failed.  Software development is highly prone to project failures, and depending on the severity, this is best handled by management.  Many projects have failed and many more will fail, so take notes! Learn why your project failed so you don't make the same mistakes next time. You learn much more from your failures than from your successes.   What you have spend coding days on was rejected by your team.  Save your work (for later). There are two possibilities: (a) It sucks, and the fact multiple people responded the same way is indication of this (b) It's truly genius work, but far ahead of what people are used to or can understand. People generally do not like what they do not understand. Perhaps  its better to show it when the time is right OR in a different place with a different \"Culture\"  Nobody listen to your ideas in your company.  Its probably a bad idea, OR the culture is not aligned with your thinking. Either move to a place that supports your culture or critically evaluate your idea again (objectively without your own bias) -> is my idea really that good? <- Kill thy ego  The design pattern you introduced with force in your team created a mess.  Be honest, you tried your best but it did not turn out how you planned it. It may be better to start again or learn from the mistakes made in the design as a team and move forward. "
    },
    {
        "ID": "23098",
        "Question": "I'm creating a RESTfull service for a tree-like resource structure and was wondering what the best URL structure would be? I have three requirements:  be able to get a collection of root resources  be able to get an individual resource  be able to get a collection of child resources  My current thought is: /rest/documents /rest/documents/{id} /rest/documents/{id}/documents  I was also thinking of going the singular / plural routes to denote lists or individual elements, but I know I'm going to have a resource that is the same plural as it is singular, so decided against this. Does anybody have any thoughts on the above? or have a different / better way of structuring this? ",
        "Best answer": "Something like this perhaps: /rest/{rootEntity}/Item/{leafEntity}/{id} /rest/{entity}/ItemList /rest/{entity}/ItemList/{leafEntity}  where {rootEntity} is the starting point of your collection, {leafEntity} is any named leaf node within your tree. You could append a few parameters any of the above to select, say, Latest or All or something. "
    },
    {
        "ID": "23120",
        "Question": "If you were looking into a job applicant's background and discovered that he or she has 1200 followers on Twitter and averages 50 tweets per day (more than half of which are during business hours), would it affect your hiring decision? How and why? Personally, I'd be a little worried about the candidate's ability to focus on the job at hand if they're constantly checking in with their 'tweeps' thoughout the day. In non-tech jobs, a lot of companies simply block Twitter as an irrelevant distraction.  But it can be a useful resource---to programmers in particular. I just wonder how much is too much. At what point does it become a red flag? ",
        "Best answer": "There's a good chance you shouldn't hire this person.  If you've got to the stage of considering whether their Twitter profile is good enough reason to reject them, then I suspect you have some fairly serious doubts. "
    },
    {
        "ID": "23182",
        "Question": "What are the common bugs introduced by refactoring, carelessness, and newbies? I would like to request the experienced programmers here to share their experience and list the bugs they used to introduce when they were inexperienced. In your response, please write a headline mentioning the kind of bug in bold text, followed by few linebreaks, and then an explanation, cause of the bug, and finally the fix. ",
        "Best answer": "Manually written values instead of constants Example: public District GetDefaultDistrict() {   return GetById(1); }  public IList<Revenue> GetRevenues() {   return GetByCodes(new [] { 8010, 8011, 8096 }); }  and thousands of use of 1, 8010, 8011 and 8096 in other places. Try to image if the default district now is 2 and 8011 moved to 8012. Fix: public District GetDefaultDistrict() {   return GetById(Consts.DEFAULT_DISTRICT_ID); }  public IList<Revenue> GetRevenues() {   return GetByCodes(Consts.REVENUE_CODES); }  and use this constants everywhere where you need to determine default district id and/or other static values. Or even: public IList<Revenue> GetRevenues() {   var codes = GetRevenueCodes(); // get from db   return GetByCodes(codes); }  to get actual values from db. But this is just an example. "
    },
    {
        "ID": "23234",
        "Question": "Just wondered what are the features of the three main programming languages which show you are an 'expert'? Please exclude 'practical' skills such as indenting. Am I right in saying for C++ the most difficult aspect to master is STL/generics? Java seems much easier as memory is handled for you. I'm not entirely sure on C# either? I'm trying to use this to guage my current level of ability and what i wish to aim for. ps this was posted on stackoverflow but got binned due to arguing, please do try to keep it civil as I am really interested in the answers from everyone :) ",
        "Best answer": "Honestly, I don't think that you can really group those three languages together for such a question. Java and C# might be close enough to work together - though it wouldn't suprise me if some of C#'s more interesting features were more difficult and therefore more in the \"expert\" realm than the features that C# and Java have in common - but C++ is in a different league entirely. It just has too many things in it that do crazy stuff if you don't use them right. And some features - like templates - are just way more powerful and complicated in C++ than in Java or C#. I would rate very few features that these three languages have in common as being of \"expert\" level. In general, the stuff that makes you an expert at a language is the esoteric stuff that other languages don't necessarily have and which are more likely to trip you up. Measuring your knowledge as a programmer without regard to programming language means measuring stuff more along the lines of algorithms and data structures. How well you understand design patterns and good software engineering principles matters far more in general than how you indent. The types of skills that make you an expert programmer in general are not the same types of skills which make you an expert at a particular programming language. It's good to have both (and if you're not at least reasonably competent in the language that you use, your non-language specific knowledge doesn't do you much good), but you're talking about two very different things when you're talking about what makes someone an expert at a particular programming language and what makes them an expert programmer in general. And since even relatively similar languages such as C++, C#, and Java differ quite a bit in the details - especially the ones which would tend to qualify you as an expert in the language itself - trying to group them together when trying to determine what makes you an expert at them really doesn't work. "
    },
    {
        "ID": "23270",
        "Question": "I'm a PHP/Python programmer learning Java and C#(.NET). My main development OS is windows 7 but I've used Linux and I currently dual-boot with Ubuntu. My Linux knowledge however, is pretty limited. I can work with the command line on simple tasks but that's pretty much it. I don't do any shell scripting and I don't know many important commands. My interests are web development, mobile apps and maybe some embedded stuff in the future.  Will shell scripting skills be useful for these interests and if so, how? ",
        "Best answer": "If I am coding a windows app (or a mac app) why should I need to know Linux? Should a Ford mechanic know John Deer tractors?  It all depends on what you are doing, professionally or on your own \"hobby\" time. Now, it doesn't hurt to know it.  In fact, I am a firm believer in \"the more you know, the better off you are\" "
    },
    {
        "ID": "23276",
        "Question": "What skills, knowledge and talents should a junior programmer aim to obtain in order to be qualified to become a team leader?  ",
        "Best answer": "The best team leaders I've seen have all been dynamite programmers. But they've all had several other qualities, which are harder to define: wisdom, good judgment, good people skills (friendly and pleasant but not a pushover), dedication, commitment, and — most important of all — knowing how to give credit to others. Such people are natural leaders.  The worst qualities you could cultivate are arrogance, always having to be right, always having to have the final say, being a glory hound, one-upmanship, having a huge, bruiseable ego, never admitting you're wrong, deflecting blame onto others while hogging all the credit for yourself, and, worst of all, competing with the people who work for you. "
    },
    {
        "ID": "23280",
        "Question": "Do most programmers specialize on a single stack, leaving other things be, or are they expert at multiple languages at the same time? If it's more than one, how many is standard? By expert, I mean more than simply knowing the syntax of a language - an expert knows enough of standard libraries, tools, environments and syntax to be able to write non-trivial programs without having to constantly look things up or read books/tutorials. ",
        "Best answer": "The main benefit of knowing multiple languages isn't in writing them directly. All other things being equal, I'd rather work with a C# programmer who also knows C, Python and Lisp (for example) than one who's only ever hacked in C#. It's not that knowing more languages is better, it's that being able to think about problems at multiple levels and from multiple perspectives is really helpful.  A programming language that doesn't change the way you think about programming is not worth knowing. -Alan Perlis  It's not about checking off one more language, or putting it on your resume; you just need to understand its underlying concepts well enough to program in it to get the full benefit. You won't get that from having a basic understanding of the syntax. The more direct answer is \"it depends\". At larger companies you're expected/allowed to specialize, but as I said above, I believe there's still benefit to understanding things beyond your one favorite tool. At smaller places, you really can't get away with that. If nothing else, you typically need to maintain your app as well as build it, and you probably souldn't use the same languages for running through logs/data munging as you do to actually build your app. I guess you could technically get away with knowing a single language, but the benefit of having a well-performing, strongly-typed (or at least assertion-capable), probably compiled language do the heavy lifting, and a scripting language for maintenance/setup/scripting tasks seems pretty big. I wouldn't want to do without it, certainly. "
    },
    {
        "ID": "23295",
        "Question": "At work we have a few older projects that are stuck on .NET 1.1 and VS 2003. While these are probably to much work now to move forward I'm wondering if the effort to keep our newer projects up to date will be worth it. Specifically we would be looking at moving about 30 projects from .NET 3.5 to .NET 4.0 and VS2008 to VS2010. My Questions for the community are: Do you move your projects along as new tools and libraries become available or just start the newest stuff in the newest versions?  If you do move forward have you found the benefits out weigh the cost of the upgrade? ",
        "Best answer": "We have about 30 small and medium sized projects, and generally take the approach that we migrate projects when we do non-trivial work on them. Occasionally, we undertake a consolidation exercise to move the oldest projects to the latest versions. Of course, this means that our dev environments have multiple versions of Visual Studio, the .Net framework, SQL Server, IIS, Source Control, browser, etc.  Despite having to keep all these different tools up to date, this ad hoc approach actually works quite well. Migrations that usually go very wel:  Visual Studio Versions (but see below) Core SQL Server functions .Net Versions Source control (from VSS to Vault)  We don't usually migrate between languages unless the application is trivial. Migrations that have been problematic:  DTS to SSIS Reporting Servives. Different versions of SSRS are supported by different versions of Visual Studio.  "
    },
    {
        "ID": "23303",
        "Question": "Are there any programming languages that have a built-in database as a first-class language feature rather than connecting to an external SQL (or other) database? What would be the drawbacks and benefits to such a feature? What would such a feature look like and how would it change the way we program? ",
        "Best answer": "Languages are \"small\" and databases are \"big\"; so whenever the two are combined, it's not a language with the database as a features, but a database with the language as a feature. Many databases have some proprietary languages tacked to them, e.g. PL/SQL, T-SQL. "
    },
    {
        "ID": "23309",
        "Question": "Which (if any) extreme-programming techniques would be appropriate to use in a research environment - where the goal is to produce prototypes, patentable, and/or publishable work? ",
        "Best answer": "Speaking from a background of algorithm research:  Keep a long backlog of ideas Re-prioritize aggressively and frequently (e.g. every day)  Mark down backlog items that are no longer viable  Maintain an up-to-date picture of inter-dependencies between backlog items  Unlike regular software development, there is a lot more dependencies in research work.  Always measure, visualize and track algorithm performance (accuracy, etc) Don't work alone. Discuss, collaborate and share frequently. Keep a wiki, and spend lots of effort to extract \"wisdom\" from your work. Use version control. However, keep good algorithm candidates in the current system, even if they are not actively used.  It allows you to tinker with an older algorithm at the spur of the moment.  Stale performance data could be error-prone.  For example, the old data may be based on a less accurate metric To get fresh performance data, re-run the algorithm(s).  Prefer dynamic typing and flexibility. Use the right language.  If almost all successful researchers in the field use one particular language, then use it. Don't fight the wisdom of the crowd.  Instead, find ways to integrate smaller components into that language, if the smaller components can be developed in a language suitable for computation such as C/C++, or if existing open source code is available.   Ask fellow researchers for their source code.   Many researcher are actually quite friendly to such requests with proper credits and data sharing.  This will save a lot of trouble because their published papers will only cover the high level picture, yet the devil is in the details.  Always push yourself, but don't timebox.  Timebox don't work because of unpredictability in research work.   An example of how to use backlog in research: Suppose in the beginning there are items A, B, C, ..., X, Y, Z.  A B C ...  Over time, you worked on a number of items, and you have a sense of how promising each item is, not just the items you have worked but also those you don't. The updated backlog becomes:  A (promising: 90, progress: 70% done) B (promising: 70, progress: 60% done) Z (promising: 65, not started) ... C (seems it won't work, don't bother)  Notice how item C sinked to the bottom because of research insights gained from working on A and B. Also notice how Z floats to the top. Learning about what other researchers are doing will also help floating items to the top. At the end of one semester, do a backlog cleanup.  A (done, working) B (done, working) Z (done, some bugs) ----- Y (50% coded, kept in the system, not actively used) X (10% coded, removed from the system in revision 123) ----- C (dropped)  The ones that are working will be the result you publish. "
    },
    {
        "ID": "23313",
        "Question": "Do good programmers need to have syntax at the tip of their tongue when writing code? What do you make of them if they google for simple stuff online?  Are they good or bad(maybe they know where to look for)? Should programmers have a good memory? Is this a trait for a good programmer? ",
        "Best answer": "It depends on what you mean, but the short answer is yes. Are programmers going to forget a semi-colon or a brace here and there? Sure. Are they going to look up the syntax of a switch statement because they probably rarely use it? Well, I do. Will I be working with ActionScript but write it like Java instead? Definitely. It can take some time getting used to typing in a new language, especially if you've worked with a different one for years. The real concern should be if a programmer cannot make heads or tails of the syntax even with an IDE helping them out. If the person doesn't know what a semi-colon is for, or what those fancy curly braces are, then they have more studying to do. "
    },
    {
        "ID": "23327",
        "Question": "We stick to standard usages and best practices because they work and communicate intent well, but without trying new things we can't really grow. Have you ever tried something that seemed absurd but worked out really well? When it works, how do you avoid confusing other developers with non-standard usages? And what do you do when things go horribly wrong - refactor, start over, or just go with it? ",
        "Best answer": "Yes, but with simplicity wrappers. Most users will only need to access through the simplicity wrappers. The system would be very familiar to them, because the simplicity wrappers are designed to be very similar to the older systems. However, advanced users who need extra power, and developers on the new system team, will need to become fluent in the new pattern. If things go horribly wrong, the entire project has to be thrown away. This is why early feedback (integration) is important, so that you can stop loss earlier. Also, put some effort on educating your fellows about the inside, and any special precautions needed. Put a lot of effort on testing. "
    },
    {
        "ID": "23351",
        "Question": "This is a fairly general question. I know a bit of Perl and Python and I am looking to learn programming in more depth so that once I get the hang of it I can start developing applications and then websites.  I would like to know of an algorithm (sequence of steps :)) that could describe my approach towards learning programming in general. I have posted small questions on Perl/Python and I have recieved great help from everyone.  Note:- I am not in a hurry to learn. I know it takes time and that's fine. Please give any suggestions you think are valid. Also, please don't push me to learn Lisp, Haskell etc - I am a beginner. ",
        "Best answer": "The 11 step algorithm for learning a new programming language I'm currently in the process of learning Lisp, and I'd recommend the following algorithm:  Ask around if the language is worth learning and where good resources can be found. If positive responses to the language are given by experts then proceed to step 2. Create an initial programming environment. Keep it simple: text editor and compiler/interpreter. The bare minimum. Consider a specific user account on your machine with a special colour scheme to cue the change of mindset. Create the \"Hello, World!\" application.  Learn general syntax and control statements (if-then-else, repeat-until etc). Create sandbox to verify simple control cases (true/false evaluations etc). Try out every primitive type (int, double, string etc). Perform currency calculations. The number guessing game (as suggested by @Jeremy) is good for this. Create class (if applicable) with several methods/functions. Make calls between functions. Apply control statements. Learn arrays and collections. Create suitably complex examples that create arrays and collections of each of the classes/functions/primitives that are available to you Learn file IO. Create examples of reading, manipulating and writing binary and character based files. Ask more questions about idiomatic programming within the language (pointers, macros, monads, closures, support frameworks, build environments etc).  Choose (or adapt your existing) IDE to work in the recomended idiom. Write a variety of applications that please you (or your boss).  After 1 year return to step 1 for another language while maintaining your interest in the one you've just been learning.  "
    },
    {
        "ID": "23455",
        "Question": "It seems like most of the jobs I'm receiving, and most of the Internet, is still using standard HTML (HTML 4, let's say) + CSS + JS. Does anyone have any vision on where HTML5 is as a standard, particularly regarding acceptance and diffusion? It's easy to find information about inconsistencies between implementations of HTML5 and so forth. What I want to know about is relevance of HTML5. ",
        "Best answer": "I'd say definitely get in there and start learning some of the technologies involved.  Just be aware that 'HTML 5' right now is actually really a marketing term! HTML 5 has not been ratified as a standard yet and although all of the major players are throwing their support behind 'HTML 5' they're all actually just implementing bits a pieces of various specs including ECMA script (Java script), CSS, HTML and a whole lot more that I'm not qualified to talk about. MS for example released IE9 beta to great fanfare in San Fran, highlighting their awesome HTML 5 support.  Naturally their IE/GPU enhanced lovely demos didn't quite work so nice on other 'HTML 5' browsers such as the latest Chrome of FF Beta or Opera. So, um yeah, don't delay in starting to investigate - MS is banking on it heavily (even though they have their Silverlight platform) and I bet you'll see Adobe hedging their bets in the next few years as well. For production websites that users will use tomorrow?  Um don't use 'HTML 5' yet. "
    },
    {
        "ID": "23507",
        "Question": "I know how we start with JavaScript, we cut-and-paste a snippit to gain a little client-side functionality or validation. But if you follow this path in trying to implement rich interactive behavior, it doesn't take long before you realize that you are creating a Big Ball Of Mud. So what is the path towards expertise in programming the interaction layer?  What books, tutorials, exercises, and processes contribute towards the ability to program robust, maintainable JavaScript? We all know that practice is important in any endeavor, but I'm looking for a path similar to the answer here: https://stackoverflow.com/questions/2573135/ ",
        "Best answer": "read other peoples code (A LOT) and try to get an understanding of what they are doing, and why they are doing it. Write your own code (A LOT) try out new concepts and ideas, above all play and have fun. The best way to get better at something is to practice, practice, and then practice some more.  Read books, tutorials, articles, blogs, RSS feeds, etc. Create some project to work on in your spare time.  If you can't think of something, find an open source project you might be interested in working on.   also, don't get discouraged and above all, remember \"mastery\" is not a destination, but a journey--take it at your own pace, have fun, and enjoy the scenery. If you take it too seriously, you will get burnt out and that does nobody any good. "
    },
    {
        "ID": "23523",
        "Question": "I am currently a 3rd year computing science student at a Canadian University. I find it quite challenging, and extremely interesting. One thing  that i have noticed is that each person i speak to that took a class 3 or so years ago learned much less than i did in the same class, and people who are taking 1st year classes are doing more than i remember doing in my 1st year classes. Is this just me remembering stuff bad, or do you believe that getting a computing science degree was easier 10 (or even 5) years ago?  ",
        "Best answer": "I don't know if it's harder or easier, but definitely different.  I'm only a couple years out, but I've noticed that current students appear to me to have a more abstracted knowledge base, but lack fundamentals which cause them to falter on simple tasks.  It's the whole 'calculator debate': \"They can do higher order math using a calculator, but can barely multiply without one.\"1 1 I'm not advocating the truth of that particular debate, just using it as a reference point in understanding my point. "
    },
    {
        "ID": "23604",
        "Question": "In my experience, it is useful to spend a little while sketching plans for a project before getting into code. Such planning usually includes choosing frameworks/tools, writing requirements and expectations, and doing mockups. I usually only do this for serious projects though, not so much for one-off or short-lived attempts. I'd be interested to hear how much time you spend on planning/designing projects before starting to do the coding. Do you do it for every project, or just the \"serious\" ones? ",
        "Best answer": "It depends greatly on the project. If the project is based on a specification that has been already written (e.g. a binary data file specification), then there may not be much design involved. If the project is highly speculative or research oriented, I might spend more time writing throwaway code to formulate a design than I will on creating a design up-front. If the project is a large one, it will need more up-front thinking about high-level architecture. "
    },
    {
        "ID": "23652",
        "Question": "Take a large company, formed through many iterations of mergers and reorganizations. This company produces lots of different products that are all targeted to a specific industry. Most products share similar characteristics, such as communication standards and certification and safety requirements. But there is no unified development platform or model for the company. Each product has its own distinct hardware, and software is created in any number of languages. Most new products will be updates to older versions or target the same industry. What strategies would you use to promote software reuse? ",
        "Best answer": "Given your constraints, I'm not sure software reuse is even a good idea.  The teams for your individual products are already intimately familiar with the particular tools and platforms for each specific product, so reinventing them to promote code reuse may be a waste of time and money. Any reworking of this kind would most certainly involve corporate strategy.  You would have to make a compelling case that the restructuring would save time and resources, improve efficiencies, and make the company more profitable. A simpler approach: If you come out with any new products, you could try reusing pieces of your most successful platform. "
    },
    {
        "ID": "23691",
        "Question": "Leaving the whole pie to only a few of them, amplifying the huge differences between the two status. Pay is a (huge) one, not having to do overtime is another. I leave the question open to hopefully get many great answers on all the different subjects that affects that feeling and decision not to go. While this question is really global, I'll be interested in any studies, facts, articles, opinions regarding local markets such as US, India and even Australia in which I'm in love with. ",
        "Best answer": "Don't know about others, but thinking about myself:  I have a job that I'm currently happy with. I work regularly and get paid regularly. Of course there's always too much things to do, but still, the work is mostly interesting and the workload is approximately constant and predictable. Hardly so with freelancing (think of work requests as a Poisson process, and how the stability of frequency depends on the average frequency; a cafeteria with 10 customers and 1 toilet is not linearly proportional to a cafeteria with 100 customers and 10 toilets, i.e. the queue is not similar). Going freelance would require me to do all the marketing, selling, bureaucracy etc. boring and scary (but admittedly, important) stuff. Actually I don't think I could do it successfully. At least I would hate it.  "
    },
    {
        "ID": "23718",
        "Question": "I believe a lot of Fortran is used in HPC, but not sure if that's only for legacy reasons.  Features of modern programming languages like garbage collection or run-time polymorphism are not suitable for HPC since speed matters so not sure where C# or Java or C++ come in.  Any thoughts? ",
        "Best answer": "In my years of experience, up to 5 years ago, it has always been Fortran and C. Which one depended mostly on whether the people came more from engineering or more from CS school of thought (I don't know how to put this better, okey? :-)  In what we were doing Fortran was almost exclusively used. From what I read around nowadays, with the new updates to the Standard F2003/08 and with the introduction of Co-Arrays, it seems to be gaining momentum again. Also, one, if not somewhat biased article - The Ideal HPC Programming Language "
    },
    {
        "ID": "23838",
        "Question": "Clearly optimizing cache usages is bound to improve my program efficiency. Surprisingly, I don't see too many programming languages actually having this sort of a feature. So here's my question:   What kind of language constructs have you seen that help improving cache usage? How to innovate on cache usage since most systems won't easily reveal stuff like their L1 cache size easily? (Windows does have API or maybe /proc/cpuinfo on Linux, but I am looking for something simpler for the intermediate developer)  ",
        "Best answer": "This is just an off-the-head list. Concepts  Spatial locality  Temporal locality. Easily-predictable memory access patterns.   For example, reading/writing megabytes of data sequentially is not at all inefficient, because the CPU can predict the next address and automatically read ahead.   Programming language constructs.  Data alignment directives.  Switching between array of structure and structure of array. Vectorized data containers provided by high-performance libraries.  The compiler has already done some of the work for you. For example, groups of functions that are closely related (for example, likely to be called in successive sequence) will be compiled into binary instructions and then stored close to each other, so that they reside within the same 4KB page block. There is simply no way for a developer to cater to such details manually. For native compiled languages, local variables are stored on the stack, and the area closest to the current stack position is likely to be cached. If a large variable (several KBs or more) is allocated from the stack, the CPU may have to evict something else from the cache to make room. On the other hand, if the large variable is used very frequently, then allocating from stack can be justified. "
    },
    {
        "ID": "23848",
        "Question": "The complement of the Being a good mentor question. I work with many very senior people that have vast amounts of knowledge and wisdom in Software, Engineering and our business domain.   What are some tips for gaining as much knowledge from them?  I don't want to take up too much of their time, but also want to take full advantage of this since it really could help develop my skills. What are some good questions to get the conversation rolling in a sit down mentor/mentee session?  Some of the people providing mentorship have little experience in this area, so it would help to be able to lead some of these discussions. ",
        "Best answer": "ask smart questions where you can. read before asking. see how much you can figure out for yourself. resort to spoonfeeding only when everything else fails. "
    },
    {
        "ID": "23852",
        "Question": "I've seen others use Bitwise-OR to combine flags before: #define RUN 0x01 #define JUMP 0x02 #define SHOOT 0x04  const byte madPerson = RUN | JUMP | SHOOT;  That's also the way I do it. But I've also seen some (not as many) combine flags using addition: #define RUN 0x01 #define JUMP 0x02 #define SHOOT 0x04  const byte madPerson = RUN + JUMP + SHOOT;  Which one is more \"readable\"? (Which one do you think more people will recognize?) What is the \"standard\" way to do it? Which one do you prefer? ",
        "Best answer": "Bitwise-OR. Addition is dangerous. Consider an example where a bandit is a person, and an angry bandit is a bandit that speaks and shoots. Later, you decide all bandits should shoot, but you've forgotten about the angry bandit definition and don't remove its shooting flag. #define PERSON 1 << 0 #define SPEAKS 1 << 1 #define SHOOTS 1 << 2 #define INVINCIBLE 1 << 3 const byte bandit = PERSON | SHOOTS;                    // 00000101 const byte angryBandit_add = bandit + SPEAKS + SHOOTS;  // 00001011 error const byte angryBandit_or = bandit | SPEAKS | SHOOTS;   // 00000111 ok  If you used angryBandit_add your game would now have the perplexing logic error of having angry bandits that can't shoot or be killed. If you used angryBandit_or the worst you'd have is a redundant | SHOOTS. For similar reasons, bitwise NOT is safer than subtraction for removing flags. "
    },
    {
        "ID": "23863",
        "Question": "I keep hearing from people that if I don't become a programmer soon after graduation my degree will be a useless piece of paper, they claim it will become outdated.  I kind of find this hard to believe considering what I was taught at a fairly good school has been standard practice for some time.   So is this a true belief in industry?  That somehow I become less desirable after a year or two, even though the graduates who comes after me pretty much get the same identical education. ",
        "Best answer": "I don't think so. A degree will last as long as it exists. It is well known that people forget quickly [almost] everything they have studied. Jumping into working environment straight after the graduation won't change much. You'll still forget everything soon afterward since you won't be using most of your knowledge anyway. As you know most of the programming jobs it's just simple code&maintenance stuff with little to none CS application. In that regard, if you were going to devalue a degree after one year, you could it right away. A year in the industry instead of a year off won't help a programmer to hold on to the knowledge obtained. An entirely different matter is however what value is a degree for a programmer. I think most employers would put value on your practical experience not on your degree, so it comes as irrelevant when exactly you got one. "
    },
    {
        "ID": "23885",
        "Question": "I didn't ask how the quality of the software is important to the product itself, the customers/users, the manager or the company. I want to know how it is important to the programmer that build it. I'll be interested in any books (please specify the chapter), articles, blog post, and of course your personal opinion on the subject regardless your experience. ",
        "Best answer": "If your question mean that \"Should quality be important to programmer as a professional\", the answer is, \"Unequivocally yes\". However if your question is \"do programmers care about quality personally\", answer is, \"It depends\".  On the one hand there are programmers who are not satisfied until they have perfected the code, but on the other hand there are programmers whose main interest is problem solving, once they have the solution, quality seems like irksome details to them. If you fall in the latter kind (as I do, I am not proud to admit), you must try to fight and overcome this tendency. Of course sometimes perfect can be enemy of good too. Update: Now why are some programmers perfectionists and other analytical. I believe the answer to that lies in psychology and genetics. Investigating that is both outside the limit of this answer and my knowledge as well. That said, I will note following  Although these are not necessarily mutually exclusive, really good programmers are more comfortable with problem solving than quality, as I believe programming at its core is about problem solving.  doesn't however mean that if you are good at problem solving you are necessarily a good programmer. To use mathematical terms problem solving is a \"necessary\" condition whereas aspiring for quality is a \"sufficient\" one.   "
    },
    {
        "ID": "23942",
        "Question": "All programming languages I know of are written - i.e. typed out as lengths of text one way or another. But I wonder if there's any programming language where you can just drag-n-drop the whole program; to get a loop, you select this box over here and drag it to that section of the \"code\" over there, and so on. And if there isn't one like this, would it fly if one was invented? Personally I don't believe it would be such a good idea, but I'd like to hear what you think. ",
        "Best answer": "Lots of outfits have done drag-and-drop programming systems. National Instruments \"Labview\" is probably the best-known, and the best. The fundamental problem they all encounter is that there is no known way to convert a Flying Code Monkey into an expert programmer and engineer.  As ONE example, there is no difference to a Flying Code Monkey between an O(N^2) or O(N^3) process and an O(N log N) process, which means that they must be supplied with canned routines for the O(N log N) algorithms, that can be custom-fit into the quickie graphic kludges they will build. The second problem they all encounter is that, when you supply the special-purpose blocks required by the first problem, overhead imposed by moving the data between the blocks starts to get expensive.  I worked with one very nice such system called Rippen.  When I profiled, to see where we were hurting on a HIGH!-required-performance sensor processing application, I was rather disturbed to see that some 20% of my CPU time was going to data-moving.  (Since I was doing LADAR image processing, doing a fair chunk of floating-point processing on every pixel of an input image, 20% of CPU was a LOT of data-moving overhead.) You could probably get around part 2 by going to a compiler-based system: you feed it your picture, and it compiles to a heavily-optimized executable program, but I'm not certain that would really fix the issues, and it might hurt the interactive nature of the tool. "
    },
    {
        "ID": "23974",
        "Question": "For my final year project, I'm working on a script that will run your python program and provide a time line of events(so far working on assignments) and create a story of what happened(the changes in the variables). Based on what Exception is thrown, you can choose to use the library made available by me to help figure out what happened and how it happened. (Hopefully I explained this much better than I did on stack overflow)  My question to you, besides the variables, when an error occurs/exception you cant handle occurs, what would you look through in order to try and fix the problem? Many thanks. ",
        "Best answer": "The Call Stack With Arguments This might be a little obvious, but a lot of my error logs just consist of functions announcing they're beginning with a listing of the args passed. As you're creating your \"story,\" make it obvious every time a function is called and display the args passed to the function. "
    },
    {
        "ID": "24005",
        "Question": "Does everyone track time spent on personal projects? I feel that it is valuable to know how much time (and potential money) has been put into a hobby. I am currently using harvest to track time. ",
        "Best answer": "If you're idea of a good time is counting and tracking things, then go for it. I'd rather get lost/zoned out/lose track of time in a hobby project. Chances are you would have to code completely different, track time, bill, take requests, if you wanted to sell it. Sounds too much like work.  "
    },
    {
        "ID": "24036",
        "Question": "Is it must that every developer should know XML? How important is XML for a developer? Any suggestion... ",
        "Best answer": "it's only important to know it when you need it and when you need it, you can learn it in an hour. or less "
    },
    {
        "ID": "24147",
        "Question": "If you've found agile and walk into a workplace that doesn't particularly follow any methodology and they are resistant to change (as most people usually are), how would you introduce an agile methodology like scrum? NOTE:  Well, I've phrased it as a hypothetical question, but it isn't. I'm not very confident about Agile myself  ",
        "Best answer": "By making the case, just like anything else. You start by asking yourself this: What are the problems that our current methodology has, and how would agile help fix those problems? It may not be an easy sale.  Going Agile completely requires a different mindset, a different way of doing things, and a different corporate culture.  Specifically, the Agile Manifesto list these characteristics of Agile development:  Early and continuous software delivery Changing requirements Deliver working software frequently Customers and developers working together daily Autonomous, highly-motivated developers with the tools they need to succeed Face-to-face conversation Sustainable development Continuous attention to technical excellence  and good design Simplicity Self-organizing teams Tuning and adjustment of the development process  Most companies don't have all of these characteristics already; if they did, they'd already be agile.  But chances are good that your company already has some.  Evaluate each bullet based on your company's relative strengths, and start a conversation how you can begin incorporating some of these principles into the development process. "
    },
    {
        "ID": "24190",
        "Question": "This might be a stupid question, but it's been in the back of my head for a while and I can't find a decent answer anywhere else. I have a teacher who says we should explicitly list each parameter with a description, even if there's only one.  This leads to a lot of repetition: double MyFunction(const int MyParam); // Function: MyFunction // Summary: Does stuff with MyParam. // Input: int MyParam - The number to do stuff with. // Output: MyParam with stuff done to it.  When writing in-code documentation, how detailed are you? ",
        "Best answer": "For starters, I agree that the \"Function:\" line in your example is completely redundant. It's also been my experience that people taught in school to add that type of comment continue adding that type of comment in their production code.  Good comments don't repeat what's in the code. They answer the question \"Why?\" instead of \"What?\" or \"How?\" They cover expectations about the inputs, as well as how the code will behave under certain conditions. They cover why algorithm X was chosen instead of algorithm Y. In short, exactly the things that wouldn't be obvious to someone else1 from reading the code.   1: Someone else who is familiar with the language the code is written in. Don't write comments to teach, comment to supplement information.  "
    },
    {
        "ID": "24212",
        "Question": "If yes, can you provide an example? Is it due to learning new tricks that you wouldn't learn otherwise by reading other's code? Or any other reasons as well? ",
        "Best answer": "Yes, working on any kind of projects outside of work help expand your knowledge on a particular subject, allow you to network with other developers, and help you build your portfolio. As a developer, there is no reason not to have something working on the side, whether it is an open source or commercial project or even a technical blog. Also, I've found side projects a way to stay passionate about programming.  On your own projects you get to do whatever you want which can be exciting and a nice change from the day-to-day corporate programmer life. The more things you work on, the more you will learn and the more marketable you become. "
    },
    {
        "ID": "24221",
        "Question": "Has it ever happened to you that you are a good developer but suddenly you need to lead a team or are responsible for some PM activities as well? Did you find that it affected your productivity? How did you handle it? I love my job, but I sometimes feel I was much happier as a programmer and the additional burden of being a Project Manager is currently affecting my productivity as a developer. What do you guys suggest as remedies to this?  I do not have an alternative currently to quit from my job - basically because Im working for a startup that I co-founded. ",
        "Best answer": "I think that it's impossible to quickly shift from PM mode to Dev mode. I'm teamleader in my normal job and programmer for a website during the nights.  When I become teamleader I tried to keep contributing with the code, but I soon discovered that it was impossible.  The new responsibilities required me to speak with people or check emails several times in a hour, impossible to write code in that condition. So now when I have some spare minutes at work I started to seat with someone of the team, especially with junior, and try to help him/her. I saw that this is increasing team productivity overall probably more than if I could spend some time writing code. "
    },
    {
        "ID": "24302",
        "Question": "I am looking for open source projects being done in C# that are actively looking for developers and does not mind the person coming in from a C++ background. Any pointers appreciated. ",
        "Best answer": "There are plenty on codeplex.  http://www.codeplex.com/site/search/openings?query=&sortBy=DownloadCount&tagName=%2cC%23%2c&licenses=|&refinedSearch=true "
    },
    {
        "ID": "24310",
        "Question": "Consider a drop down \"Duration\" which has 3 options  1. 30 days 2. 60 days 3. 90 days  and by default 30 days will be the selected value. Does it make sense to make this dropdown mandatory? Do we really need to put a * (Mandatory mark) for this field? ",
        "Best answer": "No it does not make sense. The only context in which it could be argued to make sense is if there was a blank selection present. But having a blank selection be available for a required field with a default value doesn't make any sense.  If there were no default value, then one would want a blank field and required status. As it stands, I think that that would only add confusion. "
    },
    {
        "ID": "24342",
        "Question": "I really enjoy programming games and puzzle creators/games. I find myself engineering a lot of these problems the same way and ultimately using similar technique to program them that I'm really comfortable with. To give you brief insight, I like to create graphs where nodes are represented with objects. These object hold data such as coordinates, positions and of course references to other neighboring objects. I'll place them all in a data structure and make decisions on this information in a \"game loop\". While this is a brief example, its not exact in all situations. It's just one way I feel really comfortable with. Is this bad? ",
        "Best answer": "No, it's fine. The point of practical programming is to find solutions that will possibly be useful in many similar developments. You just found one. You can't and shouldn't be creating different solutions just for the sake of them being different. But you definitely should have a critical look at your solutions each time and ask yourself if they're still good or maybe the industry has progressed since and you need to align with it accordingly. "
    },
    {
        "ID": "24343",
        "Question": "Speaking as someone with an Electronic Engineering rather than Computer Science degree, what is the one bit of computer science I should know to make me a better real world programmer? (By real world I mean something I'm going to use and benefit from in my day to day job as a programmer - for instance I'd suggest understanding database normalisation is of more practical use than understanding a quick sort for which there are lots of libraries). ",
        "Best answer": "If I have to choose just one bit, which is a difficult decision, I'd say go for the Big O notation. Understanding the implications of O(n), O(ln n), O(n²), O(2^n), O(n!) helps you to avoid a lot of expensive mistakes, the kind of which work well in the test environment but disastrously fail in production. "
    },
    {
        "ID": "24361",
        "Question": "Companies acquire other companies that use different version control systems. Is there a common wisdom on how to integrate such systems together, for example using a Subverson-GIT bridge or even deciding on using just one tool over another - and how to migrate between systems? Do people use a set of criteria for such decision making, for example an equivalent to the \"Joel\" test on software development? ",
        "Best answer": "To answer the migration question from personal experience of several migrations: Don't be afraid to just put the current version of the software into the new source control system as the base line and work from there. The vast majority of the time you won't need the history. This means it's one less task to perform during the integration and one less thing to go wrong. Files/projects that are being actively developed will soon generate a new history. So when you need to find out why a change was made the chances are that the history will be in the current repository as it will be a recent change. Files/projects that were stable before the migration should (all things being equal) remain stable after the migration so you won't need to refer to the history. We found that if we had to investigate a bug in such an old file/project having the history wasn't really of any benefit. As long as you keep the old repository available for 6 months/a year you'll have the reference in such cases. "
    },
    {
        "ID": "24460",
        "Question": "I'm totally new to the Ruby world, and I'm a bit confused with the concept of Symbols. What's the difference between Symbols and Variables? Why not just using variables? Thanks. ",
        "Best answer": "Variables and symbols are different things. A variable points to different kinds of data. In Ruby, a symbol is more like a string than a variable. In Ruby, a string is mutable, whereas a symbol is immutable. That means that only one copy of a symbol needs to be created. Thus, if you have x = :my_str y = :my_str  :my_str will only be created once, and x and y point to the same area of memory. On the other hand, if you have x = \"my_str\" y = \"my_str\"  a string containing my_str will be created twice, and x and y will point to different instances. As a result, symbols are often used as the equivalent to enums in Ruby, as well as keys to a dictionary (hash). "
    },
    {
        "ID": "24464",
        "Question": "My question here is relative to jobs for programmers. Why does employer still contract programmers if today we have a lot of good commercial system avaliable on the market? I want to be very clear at this point, this question is relative only to a system, more specially to ERP systems.  Maybe this question looks a little bit useless, but right now I'm working and this doubt arose to me and my boss. I have a few answers to my own question, but I really would like to speculate this subject a bit more. Also I know, every good company needs a customized system, but ... the question is here. :-) Any answer will make me feel better. Thanks. ",
        "Best answer": "Do you realize how expensive some of those higher-end commercial systems can cost to buy in the first place?  If companies are spending hundreds of thousands if not millions of dollars, don't you think they would want someone to help them get the most out of this big purchase? Paying for the customization is why the contractors come in and don't forget that the big guys like SAP and Oracle may well have other companies that work as \"System Integrators\" that help companies implement the system correctly at a rather high cost.  Think of this as the difference between having a personal chef cook your meals and getting them in bulk from McDonald's.  That personal chef can put in so many touches that while yes it is expensive, some people tend to believe you pay for what you get and want lots of little things that they do pay but they may be happy in the end. "
    },
    {
        "ID": "24485",
        "Question": "A lot of web frameworks have a MVC-style layout to code and approaching problems. What are some good similar paradigms for JavaScript? I'm already using a framework (jQuery) and unobtrusive js, but that still doesn't address the problem I run into when I have more complex web apps that require a lot of javascript, I tend to just end up with a bunch of functions in a single, or possibly a few files. What's a better way to approach this? ",
        "Best answer": "Namespacing: Don't forget to use objects as a pseudo-package (most frameworks do this, you should also): var YourCompany = {};  YourCompany.util = {};  YourCompany.util.myFunction = function() { /* ...  */ }  You don't necessarily need to set it up that way, and use whatever naming conventions you prefer, but the namespacing will help you out a lot (makes it easier on refactoring search-and-replace too, when you decide to move something). "
    },
    {
        "ID": "24526",
        "Question": "Are there any patterns which seem sensible when designing an object oriented domain, but do not translate nicely onto a relational database schema?  If so, are there standard patterns that can be used instead? ",
        "Best answer": "Domains which have entities where the number of attributes (properties, parameters) that can be used to describe them is potentially vast, but the number that will actually apply to a given entity is relatively modest. An example of such a domain would be a medical practice, where there are a vast number of possible symptoms, but the number of symptoms that any patient might have at any given time is comparatively small. These kinds of domains are typically represented using an Entity-Attribute-Value (EAV) model.  This data representation is analogous to space-efficient methods of storing a sparse matrix, where only non-empty values are stored. In the case of a medical domain, the problem space is complicated by the fact that any given symptom or medical test can have its own set of custom attributes, just as products sold in an online store can have custom specifications.   In fact, online stores have to deal with this problem also.  A book has a \"number of pages\" specification, while a memory module has a \"number of bytes\" specification, and the two attributes are not related at all. So a set of attributes appropriate for each product is chosen from an attributes table. The Attributes table might look like this: AttributeID AttributeDescription UnitsID --> FK to Units table  The ProductAttributes table might look like this: ProductAttributeID ProductID AttributeID --> FK to Attributes table Value  Notice that Number of Bytes and Number of Pages aren't features of the database schema. Instead, they are soft-coded into the tables.  So there is no way to represent these features as part of the domain design. "
    }
]