[
    {
        "ID": "16",
        "Question": "I have read a few articles on Internet about programming language choice in the enterprise. Recently many dynamic typed languages have been popular, i.e. Ruby, Python, PHP and Erlang. But many enterprises still stay with static typed languages like C, C++, C# and Java. And yes, one of the benefits of static typed languages is that programming errors are caught earlier, at compile time, rather than at run time. But there are also advantages with dynamic typed languages. (more on Wikipedia) The main reason why enterprises don't start to use languages like Erlang, Ruby and Python, seem to be the fact that they are dynamic typed. That also seem to be the main reason why people on StackOverflow decide against Erlang. See Why did you decide \"against\" Erlang. However, there seem to be a strong criticism against dynamic typing in the enterprises, but I don't really get it why it is that strong. Really, why is there so much criticism against dynamic typing in the enterprises? Does it really affect the cost of projects that much, or what? But maybe I'm wrong. ",
        "Best answer": " The main reason why enterprises don't start to use languages like Erlang, Ruby and Python, seem to be the fact that they are dynamic typed.  I think this is only their primary excuse. The real reason is that businesses don’t really take them all that seriously and feel that they are perhaps a bit too amateur. Java and .NET are “big business names”, have good commercial marketing, commercial customer support, and are thus widely taken very seriously indeed. It is unfortunate that there is practically no statically-typed language that is anywhere near as popular as the big business names. Why are open-source/free-software programming environments almost always dynamically typed? This might indicate that a statically-typed language is actually not that easy to make, and that dynamic typing is a “lazy man’s hack”. If that is the case, the businesses who decide against dynamically-typed languages might actually have a point. "
    },
    {
        "ID": "49",
        "Question": "During my four years at university we have been using much functional programming in several functional programming languages. But I have also used much object oriented programming to, and in fact I use object oriented languages more when doing my own small project to prepare for my first job. But I often wish that I was coding in a functional programming language when doing these projects. However, when looking for a job, it is very rare to see a job where knowledge of a functional programming language is required. Why isn't functional programming languages used more in the industry? There is quite much news about functional programming languages these days, so I wonder if functional programming is catching on in the industry now? ",
        "Best answer": "I would say that one of the reasons that functional programming is not more prevalent is the lack of knowledge base.  My experience is that corporations are very risk averse in terms of implementing technologies that are not main stream and would rather invest in tried and true frameworks (java, c++, c#).  It's only when there is a business need (like in Ericsson) that new paradigms are considered.  But even in Ericsson's case I heard that management demanded that c++ be used and Joe Armstrong was compelled to code erlang calls in c++!! This should show how reluctant corporations are to implement new technologies! "
    },
    {
        "ID": "188",
        "Question": "For the longest time in places like Java's IRC channel, SO, and other places I've been told something along the lines of \"Worry about how the code looks and its readability/understandability now, and performance later if absolutely necessary\". So for the longest time, I haven't really been OCD about performance for my small desktop or web apps, just removing the obviously inefficient. Most responses are \"What about scalability?\". Thats a legitimate point, but if my app was only built to parse, say, files 10,000 lines long, should I make my code a mess for the small percentage of people that are going to shove in a 1,000,000 line file? My main question is when should I trade the easy but somewhat inefficient ways of doing tasks for big giant complicated beasts that do things extremely quickly but destroy any possible ways of upgrading and make the code excessively difficult and prone to rewriting anyway by the next developer?  ",
        "Best answer": "Worry about performance when it becomes a problem. If you write a small app to process 10,000 line files and you get a 1,000,000 line file every 100th file, it probably doesn't matter that it takes longer to process that one file. However, if you are regularly getting files that are 5-10 times larger than initially and your application is taking too long to do its job, then you start profiling and optimizing. Now, I said \"too long to do its job\". That is up to the user or sponsoring organization to decide. If I'm doing a task and it takes me 5 minutes to do something when it took me 3 without the software or with a different tool, I'd probably file a bug report or maintenance request to have that improved. If you are the user, how long you want your software to take to do its job is up to you - only you can decide if you want it done faster or if you are willing to wait longer to have more readable code. "
    },
    {
        "ID": "370",
        "Question": "I've been told that to be taken seriously as a job applicant, I should drop years of relevant experience off my résumé, remove the year I got my degree, or both. Or not even bother applying, because no one wants to hire programmers older than them.1 Or that I should found a company, not because I want to, or because I have a product I care about, but because that way I can get a job if/when my company is acquired. Or that I should focus more on management jobs (which I've successfully done in the past) because… well, they couldn't really explain this one, except the implication was that over a certain age you're a loser if you're still writing code. But I like writing code. Have you seen this? Is this only a local (Northern California) issue? If you've ever hired programmers:2  Of the résumés you've received, how old was the eldest applicant? What was the age of the oldest person you've interviewed? How old (when hired) was the oldest person you hired?  How old is \"too old\" to employed as a programmer? 1 I'm assuming all applicants have equivalent applicable experience. This isn't about someone with three decades of COBOL applying for a Java guru job. 2 Yes, I know that (at least in the US) you aren't supposed to ask how old an applicant is. In my experience, though, you can get a general idea from a résumé. ",
        "Best answer": "Having just got a new job at nearly 50 in the UK I can say that it's possible and you're never too old. There are two approaches - both rely on your skills being relevant to the job.  Stick with what you know and become a guru. This is risky as the number of jobs requiring \"old\" technologies are becoming fewer and further between as each year passes. However, as people retire from such jobs there will be openings. Keep refreshing your skills. I moved into Silverlight last year, which is what got me this job. That and my previous team leadership roles which my new employer saw as relevant.  "
    },
    {
        "ID": "408",
        "Question": "\"Regular\" golf vs. code golf: Both are competitions.  Both have a well-defined set of rules, which I'll leave out for simplicity.  Both have well-defined goals; in short, \"use fewer hits/characters than your competitors.\" To win matches, athletic golfers rely on  equipment  Some situations call for a sand wedge; others, a 9-iron.   techniques  The drive works better when your feet are about shoulder width apart and your arms are relaxed.   and strategies  Sure, you could take that direct shortcut to the hole... but do you really want to risk the water hazard or sand bunker when those trees are in the way and the wind is so strong?  It might be better to go around the long way.    What do code golfers have that's analagous to athletic golfers' equipment, techniques and strategies? Sample answer to get this started: use the right club!  Choose GolfScript instead of C#. ",
        "Best answer": "I'd say that thorough knowledge of the syntactical oddities of your language help. Here is one I found in Ruby when doing a bit of code golf: Instead of require \"sequel\" require \"nokogiri\" require \"chronic\"  You can do something like this:  body_of_your_program if %w{sequel nokogiri chronic}.each{|i| require i}  With this kind of thing, you too can write incredibly elaborate Ruby one-liners! In Ruby and Perl, you also get the magic variables like \"$_\" which can be used to do all sorts of magic with strings and regexes. Is your data not strings? Well, you might want to turn it into strings. Obviously, in C, the preprocessor is your friend. "
    },
    {
        "ID": "568",
        "Question": "Managed OSes like Microsoft Singularity and JNode are quite an interesting concept. Essentially, the OS is bootstrapped with code written in a low-level language (C/C++/Assembly), which essentially implements a virtual machine. The rest of the OS (and all userland apps) run on the virtual machine. There are some great things about this. For example, you suddenly make arbitrary pointers obsolete. And if well written, you get rid of a ton of legacy crud that most modern OSes currently have.  However, as a disadvantage, you're that much farther away from the hardware, and as a developer, you lose the ability to drop down to a lower level of abstraction and get your hands dirty.  What are your opinions on this? ",
        "Best answer": "I think that this is another case where \"it depends\". If you're writing applications such as web browsers, word processors etc. where lightning fast performance is not necessarily an issue then this approach has it's merits. By using this approach you can offer your customers a safer, more controlled experience. Not only are you limiting the damage that can be done by malware, but you are also running in a more consistent environment. It's like the difference between console games and PC games. The former know exactly what hardware they need to work with so can make use of that knowledge whereas the latter have to be able to cope with a wider variety of graphics cards, sound cards, hard disk speeds etc. However, there will be applications (such as games!) that require the low level access and will still need to be run \"natively\". Like managed languages you will have to use the appropriate tool for the job. "
    },
    {
        "ID": "648",
        "Question": "We, as programmers, are constantly being asked 'How long will it take'? And you know, the situation is almost always like this:  The requirements are unclear. Nobody has done an in depth analysis of all the implications. The new feature will probably break some assumptions you made in your code and you start thinking immediately of all the things you might have to refactor.  You have other things to do from past assignments and you will have to come up with an estimate that takes that other work into account. The 'done' definition is probably unclear: When will it be done? 'Done' as in just finished coding it, or 'done' as in \"the users are using it\"? No matter how conscious you are of all these things, sometimes your \"programmer's pride\" makes you give/accept shorter times than you originally suppose it might take. Specially when you feel the pressure of deadlines and management expectations.  Many of these are organizational or cultural issues that are not simple and easy to solve, but in the end the reality is that you are being asked for an estimate and they expect you to give a reasonable answer. It's part of your job. You cannot simply say: I don't know.  As a result, I always end up giving estimates that I later realize I cannot fulfill. It has happened countless of times, and I always promise it won't happen again. But it does. What is your personal process for deciding and delivering an estimate? What techniques have you found useful? ",
        "Best answer": "From The Pragmatic Programmer: From Journeyman to Master:  What to Say When Asked for an Estimate You say \"I'll get back to you.\" You almost always get better results if you slow the process down and spend some time going through the steps we describe in this section. Estimates given at the coffee machine will (like the coffee) come back to haunt you.  In the section, the authors recommend the following process:  Determine the accuracy that you need. Based on the duration, you can quote the estimate in different precision. Saying \"5 to 6 months\" is different than saying \"150 days\". If you slip a little into the 7th month, you're still pretty accurate. But if you slip into the 180th or 210th day, not so much. Make sure you understand what is being asked. Determine the scope of the problem. Model the system. A model might be a mental model, diagrams, or existing data records. Decompose this model and build estimates from the components. Assign values and error ranges (+/-) to each value. Calculate the estimate based on your model. Track your estimates. Record information about the problem you are estimating, your estimate, and the actual values. Other things to include in your estimate are developing and documenting requirements or changes to requirements specifications, creating or updating design documents and specifications, testing (unit, integration, and acceptance), creating or updating user's manuals or READMEs with the changes. If 2 or more people working together, there's overhead of communication (phone calls, emails, meetings) and merging source code. If it's a long task, account for things like other work, time off (holidays, vacation, sick time), meetings, and other overhead tasks when picking a delivery date.  "
    },
    {
        "ID": "724",
        "Question": "When learning a new programming language you sometimes come across a language feature which  makes you wish you had it in your other programming languages that you know. What are some language feature which were at the time of learning very new to you and that you wish your other programming languages had.   An example of this is generators in Python or C#. Other examples may include list comprehensions in Python, template in C++ or LINQ in .NET or lazy evaluation in Haskell. What other semi-unique language features have you come across which were completely new and enlightening to you?  Are there other features of older programming languages which were unique and have fallen out of fashion? ",
        "Best answer": "Practically anything in Haskell  Monads. Yes - the big scary word that makes increadibly easy parsers, IO, operations on Lists and other things so easy (once you notice common pattern) Arrows. The same for advanced users ;) Standard stuff like lambdas etc.  Currying functions Algebraic data types Pattern matching  And many more. PS. Yes. I am Haskell fanboy if anyone asked. "
    },
    {
        "ID": "775",
        "Question": "On any team, you are going to have the need for more grizzled and grey developers and some young pups. Some reasons include:  Money. There are often tasks that don't require the same level of experience to deliver, so it makes sense not to pay top dollar to have those tasks fulfilled. Energy. There's an energy and enthusiasm that new people can bring to a team that stops it from getting too stale and set in its ways. There's also calmness and wisdom that the more senior people can bring. Knowledge transfer and career growth. Both in terms of the project and skills, it's useful and often fun to teach people and to learn new stuff. It's satisfying to help \"bring on\" new team members.  I realise there are some cutting edge projects where it may be important for there to be more senior people than junior, but in general, is there an ideal mix of experiences on a team, or is it totally dependent on the project? ",
        "Best answer": "I really like what Eric Brechner has to say on this subject  Think of your team as a river instead of a lake. A lake stagnates. There’s no energy or impetus to change. The same is true of groups that stagnate. They cultivate mediocrity and complacency; they abhor risk. A river is always running and changing with lots of great energy. You want a river. A river depends on the flow of water, and your team depends on the flow of people and information. You can think of the people divided into three groups: new blood, new leaders, and elders ready for a new challenge. Here’s how those groups should balance and flow:  The largest group should be the new blood. Not all of them will become technical or organizational leaders.  Sometimes you’ll have more new leaders than elders, sometimes the reverse, but ideally you should maintain a balance.  For flow, you want a steady stream of new blood becoming your new leaders, and new leaders becoming elders.  The key to flow is new blood coming in and elders moving out. For this to work, you WANT your elders to transfer before they clog the stream and disrupt the flow of opportunitiesfor others.   Not all technologies flow at the same rate. Central engines, like the Windows kernel, flow slowly, while web-based services, like MSN Search, flow quickly. You need to adjust for your situation, but even the most conservative technologies do change and flow. How do you successfully encourage and maintain a healthy flow?  Keep a constant supply of new people.  Instill information sharing as a way of life.  Shape the organization and roles to create growth opportunities.  Find new challenges for your elders.    "
    },
    {
        "ID": "812",
        "Question": "I originally tried asking this on StackOverflow, but it was too subjective :-(. I am interested in methods of defining the power of programming languages. Turing completeness is one, but it is almost universally satisfied. What would be nice is to define a measure of power that discriminates among programming languages that are actually in used. For example, can anyone propose a non-subjective method that would discriminate between assembly and Java? Turing completeness means that a language is maximally powerful in what it can output (which pretty much means it can do anything non-time based in the real world). So if we want to define a stronger measure of power, we need to take another approach. Shortness was suggested in the original question, but this is not easy to define at all. Does anyone have any other suggestions? ",
        "Best answer": "If I understand your question correctly, you are nonetheless looking for something that is relatively measurable and not just a subjective judgement call. If so, I would personally favour the amount of time taken to solve any particular problem (averaged over all problems and all programmers). In this measure, you might need to consider not just the language itself but also the framework/API used with it. Succinct syntax is a very small factor: a much more important one is that the most commonly required functionality is easily accessible. If you are looking for something more subjective, I’d say how fun it is. Programmers tend to be people who want problems solved, so a programming language that is fun for programmers to use is inevitably going to be the one that will solve the most problems. This measure takes into account that different people have different preferences on how to use things, so the “best” programming language will be the one that is most appealing to the widest range of programmers. However, you might need to consider not just the programming language and API here, but also the environment (IDE), which is of course what the programmer actually interacts with. "
    },
    {
        "ID": "827",
        "Question": "For decades, the accepted degree to get to become a software developer was \"Compter Science.\" We've had a few questions already about whether that degree really prepares students to develop software. Some schools have, in the last 8 years or so, started offering multiple distinct majors in programming.  Using the curriculum from my school:  Computer Science, which starts out with some intro programming courses in the first year, and then focuses on theoretical computer science, algorithms, and a bit of OS stuff.  Most classes involve several smaller projects and homeworks, done solo or in pairs. Software Engineering, which starts out with the same intro programming courses, does a few classes of theory, and then goes into software development practices (testing, process methodologies, sofware metrics, requirements gathering) and software design (distributed system design, info system design, real-time/embedded design, subsystem design, etc)  Different schools do it differently, so the above is just a real-world example I'm familiar with.  What I ask is: Is there a need for distinct majors in programming? ",
        "Best answer": "Yes, they should be. The relationship between computer science and software engineering is the same as the relationship between physics and mechanical engineering. One provides the theoretical background while the other takes those theories, along with good engineering principles, and applies them to the design and implementation of software. You need both in order to produce new and better software. A good computer science education trains people to produce new and better algorithms, data structures, programming languages and paradigms, compilers, and a number of other things that can be used to enhance software systems. A good software engineering education, on the other hand, trains you to take these tools and knowledge obtained through a scientific study of computation, along with a knowledge of the software development lifecycle and process models to actually build the system that a customer wants and needs. "
    },
    {
        "ID": "874",
        "Question": "In my current job it feels like we have a lot requirement changes. We are an \"Agile\" shop, so I get that we are supposed to adjust and what not, but sometime the change is large and nothing trivial.  My question is, how do you effectively communicate the cost of the change? Because of being agile, if a change is big enough something will get dropped from the current sprint, but it usually just get added next time around. Since our model is SaaS, the end customer is effectively the business itself, and they know they will get the cut feature n weeks later. I guess what I am trying to get at is the removal of a feature really isn't anything to use for communication as it was only delayed by n weeks. What other ways do you have to get the business to understand what a change costs? ",
        "Best answer": "You could try setting a minimum age of a new addition / change (not applicable to bug fixes).  For example no new changes can be worked on until it is 3 weeks old.   Having a minimum age of a task is nice because at the start, every task looks like it's extremely important, but if you wait some time then it's importance will often drop significantly.  Depending on your time interval it will give you at least that amount of time of stability in the tasks you're working on. To track the age you would allow the tasks to be added to some list, but they wouldn't be considered as tasks to work on until that period has expired. "
    },
    {
        "ID": "1070",
        "Question": "Dcpromo.exe is famous among MCSEs for being they only way to create a Windows Domain Controller (in other words an Active Directory Domain) ... which in turn is often used by the ASP.NET Membership system. I'm trying to determine if I should put content on \"www.dcpromo.com\" geared for developers or more towards sysadmins. To me, a logical extension of this \"domain controller on the internet\" is to use WIF for the ASP.NET Membership system.  We'd then have a website that will serve the developer's interests in the SSO, SAML, user managment and identity areas we all struggle with. So my question is:  Q:  Do programmers see a connection between the utility dcpromo.exe and ASP.NET Membership?  If so does it make sense to have a purpose of http://www.dcpromo.com  help developers in the areas of membership and cloud computing?   ",
        "Best answer": "I've been a Microsoft developer for 11 years, mostly as a web developer.  Been coding since 1.1 and I've used .NET membership in many of my applications, and I've never heard of dcpromo. "
    },
    {
        "ID": "1106",
        "Question": "This goes back to a conversation I've had with my girlfriend. I tried to tell her that I simply don't feel adequate enough in my programming language (C++) to call myself good. She then asked me, \"Well, when do you consider yourself good enough?\" That's an interesting question. I didn't know what to tell her. So I'm asking you. For any programming language, framework or the like, when do you reach a point were you sit back, look at what you've done and say, \"Hey, I'm actually pretty good at this.\"? How do you define \"good\" so that you can tell others, honestly, \"Yeah, I'm good at X\". Additionally, do you reach these conclusions by comparing what others can do? Additional Info I have read the canonical paper on how it takes ten-thousand hours before you are an expert on the field. (Props to anybody that knows what this paper is called again) I have also read various articles from Coding Horror about interviewing people. Some people, it was said, \"Cannot function outside of a framework.\" So they may be \"good\" for that framework, but not otherwise in the language. Is this true?  ",
        "Best answer": "You can't call yourself good at X.  Only other people can. "
    },
    {
        "ID": "1135",
        "Question": "Planning Poker Summary, in case you don't want to read the wiki article:  Get a list of tasks you want to do for the upcoming iteration For each task: 2.1 Discuss with the group what it entails 2.2 Everyone writes down / selects an estimation of how much effort is required for the task 2.3 Everyone reveals their estimation 2.4 The highest and lowest outliers explain their reasoning 2.5 Repeat until a consensus is reached  Usually something similar to numbers from the Fibonacci sequence like 0, ½, 1, 2, 3, 5, 8, 13, 20, 40, 100 are the allowed values, so you don't get long arguments over close values like 23 vs 27. Further, the numbers represent a unit-less value of effort, whose value is determined by a baseline task that everyone agrees equals about a 1, and all else is relative to that. Ultimately, the goal is to get a good feel for a given team's \"velocity\", which is the number of these points that can be completed in a given iteration.  With that, it's possible to make reasonably accurate estimates of how long any given feature will take.  We did this at iteration planning meetings at one company I worked at, and I thought it was one of the few good things about that particular company.  So, what I'm wondering is, has anyone used this?  Do you think it's a useful tool for estimation?  Does it work in all situations, or does it lend itself to certain teams, projects, etc? ",
        "Best answer": "We use it in our company for the project I'm involved in.  Some notes about planning poker are expressed in my recent blog post, and here's a bigger list of why it's cool:  It makes everyone agree.  People are not forced to accept any result; instead they're forced to make their own estimate!  The time to defend their own estimates is also allocated, if it's necessary. It keeps everyone busy.  You can't slack during the meeting, while trying to show that you're so involved. Also, necessity of moving your hands constitutes a good physical exercise to keep you off of sleeping. However, a downside of this is that sometimes you do need to do something else (for example, take some notes and write down the details of the agreement you've just reached).   It keeps meetings faster.  There's no need for a constant involvement of a meeting leader to keep everything on pace.  The game with clear rules is way better for that.  Yes, you need to make some extra moves to put cards on, reveal them, et cetera, but these pay their way. A lot of people just like to play cards, especially poker :-)  This increases motivation.  A company that sells decks of such cards accompanied their site with an article about Planning Poker, which is also worth reading. "
    },
    {
        "ID": "1371",
        "Question": "Having worked on a failed project is one of the few things that most programmers have in common, regardless of language used, industry or experience. These projects can be great learning experiences, soul-crushing disasters (or both!), and can occur for a multitude of reasons:  upper management change of heart under-skilled / under-resourced team emergence of superior competitor during dev cycle over/under management  Once you've worked on a couple of such projects, is it possible to recognise at an early stage exactly when a project is doomed to fail?  For me, a big sign is having a hard & fast external deadline combined with feature creep. I've seen projects which were well planned out and proceeding right on schedule go horribly off the rails once the late feature requests started to roll in and get added to the final \"deliverable\". The proposers of these requests earned the nickname of Columbo, due to rarely leaving the room without asking for \"just one more thing\". What are the warning signs you look out for that set off the alarm bells of impending doom in your head? ",
        "Best answer": "Heroic Coding Coding late into the night, working long hours, and clocking lots of overtime are a sure sign that something went wrong.  Further, my experience is that if you see someone working late at any point in the project, it only ever gets worse.  He might be doing it just to get his one feature back on schedule, and he might succeed; however, cowboy coding like that is almost always the result of a planning failure that will inevitably cause more of it soon.  So, the earlier in the project you see it, the worse it will eventually become. "
    },
    {
        "ID": "1386",
        "Question": "Sometimes I feel like a musician who can't play live shows. Programming is a pretty cool skill, and a very broad world, but a lot of it happens \"off camera\"- in your head, in your office, away from spectators.  You can of course talk about programming with other programmers, and there is peer programming, and you do get to create something that you can show to people, but when it comes to explaining to non programmers what is it that you do, or how was your day at work, it's sort of tricky.  How do you get the non programmers in your life to understand what is it that you do?  NOTE: this is not a repeat of Getting non-programmers to understand the development process, because that question was about managing client expectations.  ",
        "Best answer": "I don't even try. If they aren't tech oriented enough to have at least a basic understanding of programming, I am only going to bore them with the details. Usually I just go with something very high level like \"I create web sites\" or \"I write computer programs to do X\" "
    },
    {
        "ID": "1516",
        "Question": "A few years ago I considered myself somewhat of a web developer, knowing the basic 3 languages (HTML, CSS, JS) and lots of PHP. Moving on from simple text to actual websites was a pain because of the so called \"standards\" out there, which at the time were ridiculously complicated for me. It pretty much boiled down to this (minus the IE related stuff):  Standards are there to replace old ways of doing things in a simpler way. However when trying to actually implement some of the stuff (Entirely CSS based layout for example), it took me 10x longer to actually do it then if I did the simpler and still working solution. If it rendered the same, then why should I use the more complicated example that takes 10x longer and breaks once you change browsers? This sparked many long religious debates in ##php, ##css, and ##js in Freenode IRC and actually got me banned from ##css because I messed with their little world over there. My question: Should I follow every single standard and coding conventions even if they take me 10x longer but get me the same result as the simple one?  For the poll tag, those of you who have websites of any size (huge or small), do you follow all the standards?  ",
        "Best answer": "Product first, then polish. Get your site/application/game doing what it's supposed to do. Get it up and running, and get people interested. Then, when you have the time, go back and polish it up. But only because you care, not because anybody else does. Of course, if the non-compliance issues mean people can't view it, or it's unreadably ugly, or it takes a month to load, or it's hard to maintain, or it crashes the browser, this is a major problem. But it would still be a major problem even if you were standards-compliant. Ordinary users do not look at the source for a website that isn't loading and go, \"Well, it's not displaying the pictures, but it's completely W3C-compliant\". They simply browse to another website and never return. Bottom line, standards are there to make writing browsers easier, and to close up potential security holes. Amazon, Penny-Arcade and Stack Overflow do not make their money from running a standard-compliant website. And unless you're in a website-writing competition, neither will you. "
    },
    {
        "ID": "1674",
        "Question": "i wonder apart from those very simple hello world apps, what other programs should i try developing for learning. i am entering uni next year and wonder what kind of programs will be developed in that environment.  this is not really a language specific thing, but if you want to know what i use currently mainly  windows: C#/WPF - i studied java/vb etc in poly but i think C# is the way to go, maybe even F# web: PHP/MySQL, Zend Framework/Doctrine, + HTML/CSS/JS(jQuery) of course. looking to try google app engine with python too.   for now, i am thinking of   todo lists apps that integrate with api eg. twitter/tumblr which i use text editor - i am currently trying to develop a text editor that uses markdown and generates html files for viewing (link to blog entry). not very pratical after i started developing it. cos when editing theres no formatting and in HTML format, i cannot edit directly  blog software (for web)  ",
        "Best answer": "Have a look at Project Euler.  There's nothing else like it for sharpening your foundational programming skills. "
    },
    {
        "ID": "1719",
        "Question": "In chapter one of \"The Pragmatic Programmer\" the first learning goal is:  Learn at least one new language every year. Different languages solve the same problems in different ways. By learning several different approaches, you can help broaden your thinking and avoid getter struck in a rut. [...]  To achieve this over a career, the list of languages is likely to get quite long (particularly if you do not want to \"progress\" into management). Clearly the education of a programmer (or whatever form) is going to get you started with a core of commercially useful languages (the usual list from job posting: C, C++, Ruby, Python, JavaScript, C#, Java, VB, ...). Additionally a formal or informal learning programme is likely to have covered functional approaches (via something like Haskell, LISP or an ML derived language) But once a reasonable subset of that list is learned- what's next, and why? ",
        "Best answer": "Make it interesting and spend each year writing an interpreter or compiler for your own programming language that fills up a niche you've never used a programming language for. Each year, write your next compiler/interpreter using the language you wrote the previous year. "
    },
    {
        "ID": "1750",
        "Question": "I work for a company that supports several languages: COBOL, VB6, C#, and Java. I use those languages for my primary work, but I often find myself coding some minor programs (e.g. scripts) in Python because I find it to be the best tool for that type of task. For example: an analyst gives me a complex CSV file to populate some DB tables, so I use Python to parse it and create a DB script. What's the problem? The main problem I see is that a few parts of these quick and dirty scripts are slowly gaining importance and:  My company does not support Python They're not version controlled (I back them up in another way) My coworkers do not know Python  The analysts have even started to reference them in emails (\"launch the script that exports...\"), so they are needed more often than I initially thought. I should add that these scripts are just utilities that are not part of the main project; they simply help to get trivial tasks done in less time. For my own small tasks they help a lot. In short, if I were a lottery winner to be in a accident, my coworkers would need to keep the project alive without those scripts; they would spend more time fixing CSV errors by hand, for example. Is this a common scenario? Am I doing something wrong? What should I do? ",
        "Best answer": "You need to get the situation formalised as it shouldn't really have got to this point. However, these things happen so you need to explain to your boss that you created these scripts for personal use, but they've \"escaped\" into wider circulation. Admit (if necessary) that you were at fault for not bringing this to his attention sooner. At the very least the scripts should be put under source control \"just in case\" - then at least if you aren't available (for what ever reason) your co-workers will have access to the scripts. Then you either need to convince your boss that Python is the way to go for these or accept that you are going to have to re-write them in a supported language. If the cost of documenting the scripts and educating your co-workers in Python is lower than that of the re-write you might even win the argument. "
    },
    {
        "ID": "1969",
        "Question": "I'm posting this here since programmers write viruses, and AV software.  They also have the best knowledge of heuristics and how AV systems work (cloaking etc). The EICAR test file was used to functionally test an antivirus system.  As it stands today almost every AV system will flag EICAR as being a \"test\" virus.  For more information on this historic test virus please click here. Currently the EICAR test file is only good for testing the presence of an AV solution, but it doesn't check for engine file or DAT file up-to-dateness. In other words, why do a functional test of a system that could have definition files that are more than 10 years old.  With the increase of zero day threats it doesn't make much sense to functionally test your system using EICAR. That being said, I think EICAR needs to be updated/modified to be effective test that works in conjunction with an AV management solution. This question is about real world testing, without using live viruses... which is the intent of the original EICAR. That being said I'm proposing a new EICAR file format with the appendage of an XML blob that will conditionally cause the Antivirus engine to respond. X5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-EXTENDED-ANTIVIRUS-TEST-FILE!$H+H* <?xml version=\"1.0\"?> <engine-valid-from>2010-1-1Z</engine-valid-from> <signature-valid-from>2010-1-1Z</signature-valid-from> <authkey>MyTestKeyHere</authkey>   In this sample, the antivirus engine would only alert on the EICAR file if both the signature  or engine file is equal to or newer than the valid-from date. Also there is a passcode that will protect the usage of EICAR to the system administrator. If you have a backgound in \"Test Driven Design\" TDD for software you may get that all I'm doing is applying the principals of TDD to my infrastructure.   Based on your experience and contacts how can I make this idea happen? ",
        "Best answer": "As you said in the question, it would have to work in conjunction with an AV solution.  In order for that to happen you would either need to write an AV engine, or become involved with an existing AV vendor. If such a thing did exist... Where does the benefit come in?  Just thinking devil's advocate here..  Couldn't the AV engine just report when it's database was updated? "
    },
    {
        "ID": "2042",
        "Question": "I've been working in the enterprise space for the past 4½ years and have noticed that generally speaking, enterprises are not conducive environments for the test-first style of development. Projects are usually fixed-cost, fixed-timeline and waterfall style. Any unit testing, if done at all, usually comes after development in the QA phase and done by another team. Prior to working for an enterprise, I consulted for many small to medium sized companies, and none of them were willing to pay for a test-first style of development project. They usually wanted development started immediately, or after a short design stint: i.e., something more akin to Agile, though some clients wanted everything mapped out similar to waterfall. With what types of shops, companies, and clients does test-driven development work best? What types of projects tend to be conducive to TDD? ",
        "Best answer": "Every line of code I write is using test driven development.  If management isn't on board with writing tests first then I don't tell management about it.  I feel that strongly that test driven development is a better process. "
    },
    {
        "ID": "2164",
        "Question": "So I'm sure everyone has run into this person at one point or another, someone catches wind of your project or idea and initially shows some interest. You get to talking about some of your methods and usually around this time they interject stating how you should use method X instead, or just use library Y. But not as a friendly suggestion, but bordering on a commandment. Often repeating the same advice over and over like a overzealous parrot. Personally, I like to reinvent the wheel when I'm learning, or even just for fun, even if it turns out worse than what's been done before. But this person apparently cannot fathom recreating ANY utility for such purposes, or possibly try something that doesn't strictly follow traditional OOP practices, and will settle for nothing except their sense of perfection, and thus naturally heave their criticism sludge down my ears full force. To top it off, they eventually start justifying their advice (retardation) by listing all the incredibly complex things they've coded single-handedly (usually along the lines of \"trust me, I've made/used program X for a long time, blah blah blah\"). Now, I'm far from being a programming master, I'm probably not even that good, and as such I value advice and critique, but I think advice/critique has a time and place. There is also a big difference between being helpful and being narcissistic. In the past I probably would have used a somewhat stronger George Carlin style dismissal, but I don't think burning bridges is the best approach anymore. Do you have any advice on how to deal with this kind of verbal flogging? ",
        "Best answer": "Don't just let them talk. Get them in front of a keyboard. The phrase \"ok, show me\" should do it. My experience is most blow hards aren't that great, and when they actually try to do what they say it doesn't work and things get real quiet. "
    },
    {
        "ID": "2252",
        "Question": "I've been asked this in a few interviews. And it always catches me off guard.My professional and academic background are already in the resumé, which the interviewer has obviously looked at. What more to tell him/her? Should I start with my hobbies? I like gardening, or looking at NSFW pictures on reddit in my free time? What and how do you answer to this specific question? Do you have a prepared answer for it? Am I wrong if I think this question is a bit silly? UPDATE There have been a lot of great answers to this question. I'm in pickle which to choose as the 'correct' answer, because most of them are very insightful. I found a great writing on this subject matter. It's a bit crazy for my taste, but it's interesting: How To Introduce Yourself… I Mean Practically ",
        "Best answer": "Don't assume the interviewer knows your resumé inside out. Often, they'll be interviewing several people for the position and may have just had a cursory glance over your resumé before starting the interview. With that in mind, and assuming this question comes early on in the interview, use this question as an opportunity to give a brief history of your career and why you are applying for the job, as well as what your stand-out skills or attributes are.  Your answer can effectively steer the course of the interview, giving the interviewer some \"jumping off\" points that could change what questions you get asked next. Focusing on your strengths with this answer means that it will be more natural to talk about what makes you great in answers to subsequent questions and not as something you have to try to shoehorn in to some other answer. "
    },
    {
        "ID": "2410",
        "Question": "I am referring to explaining to the non-programmer what programming is. I made sure to search for similar questions before creating this one, but the few ones I did find seemed to dodge the question, and I specifically would like to see some metaphors or analogies. I personally find it easier to explain something technical to someone through the use of metaphors or analogies. The reason I'm interested in this is because many people encounter the work of a programmer on a daily basis, but if you ask the average person what a programmer is or does, they don't really know. This leads to certain situations of misunderstanding (ex. \"[...] but I thought you were good with computers!\") I really would like to find the best one out there. I would like to be able to easily explain to someone what my career choice is about. Of course, at least the general idea. I personally don't have a solid one, but I have long thought about it and I have usually gravitated towards the 'language' metaphor, where we happen to know a language that computers understand, and therefore we are able to tell computers what to do, or \"teach\" them, to solve our problems. For example:  Imagine that in an alternate reality, humanoid robots with artificial intelligence exist, and some people are able to communicate with them through a common language, which is a variation of English. These people who can communicate with the robots are able to teach them how to solve certain problems or do certain tasks, like doing our chores. Well, although robots like that don't exist yet, programmers of our time are like those people, but instead of communicating with the robots, they communicate with computers. Programmers \"teach\" the computers how to perform certain tasks or solve certain problems by means of software which they create by using this \"common language\". Programmers and this \"common language\" are what give us things like email, websites, video games, word processors, smart phones (to put it simply), and many other things which we use on a daily basis.  I don't mean to put programming on the throne or anything, it's just the best metaphor I could come up with. I'm sure someone will find some issue with this one, it's probably a bit contrived, but then again that's why I'm asking this question. ",
        "Best answer": "I write very, very detailed instructions for a very, very dumb machine.  "
    },
    {
        "ID": "2682",
        "Question": "We’re working on a .Net framework which ultimately amounts to a single DLL.  We intend to charge for commercial use of the framework, but make it free for open source/non-commercial use.  The rough plan at the moment is to administer this through some form of fairly simple licence which will be issued whether you’re using it for free or paying. We’re debating whether to make the source code available.  It’s our perception (and our own preference) that it’s far more appealing to use something where you have access to the source code. I’m interested in whether people think making the source code available will damage our ability to make money from the framework, or whether it will encourage more usage and enough “good” people will arrange to pay for the correct licence if using it commercially. My feeling is that, generally, commercial operations don’t mess about on the licencing front and so making the source code available will only encourage usage and therefore ultimately generate more revenue, but I’d be interested in others views/experience. ",
        "Best answer": "If it is a managed .NET framework, people can use Reflector to look at the source code anyway. I've seen a number of companies that appear to work this dual-licensing model successfully.  Having access to the source code doesn't necessarily mean that people will steal the code. Personally, I would rather be properly licensed, as has any company I have ever worked for.  But having access to the source code can encourage people to try your product out.   There's something about having access to the source that feels like a security blanket; you know that you will probably never need it, but if you spend many man-hours worth of effort committing to a third-party library, it's good to know you can dig into the code and fix something if you ever get into a bind, or the original author of the code gets thrown under a bus. "
    },
    {
        "ID": "2699",
        "Question": "This is a \"Share the Knowledge\" question. I am interested in learning from your successes and/or failures. Information that might be helpful... Background:  Context: Language, Application, Environment, etc. How was the bug identified ? Who or what identified the bug ? How complex was reproducing the bug ?   The Hunting.  What was your plan  ? What difficulties did you encounter ? How was the offending code finally found ?  The Killing.  How complex was the fix ? How did you determine the scope of the fix ? How much code was involved in the fix ?  Postmortem.  What was the root cause technically ? buffer overrun, etc. What was the root cause from 30,000 ft ? How long did the process ultimately take ? Were there any features adversely effected by the fix ? What methods, tools, motivations did you find particularly helpful ? ...horribly useless ? If you could do it all again ?............  These examples are general, not applicable in every situation and possibly useless. Please season as needed. ",
        "Best answer": "It was actually in a 3rd party image viewer sub-component of our application.  We found that there were 2-3 of the users of our application would frequently have the image viewer component throw an exception and die horribly. However, we had dozens of other users who never saw the issue despite using the application for the same task for most of the work day. Also there was one user in particular who got it a lot more frequently than the rest of them. We tried the usual steps:  (1) Had them switch computers with another user who never had the problem to rule out the computer/configuration. - The problem followed them. (2) Had them log into the application and work as a user that never saw the problem. - The problem STILL followed them. (3) Had the user report which image they were viewing and set up a test harness to repeat viewing that image thousands of times in quick succession. The problem did not present itself in the harness.   (4) Had a developer sit with the users and watch them all day. They saw the errors, but didn't notice them doing anything out of the ordinary to cause them. We struggled with this for weeks trying to figure out what the \"Error Users\" had in common that the other users didn't. I have no idea how, but the developer in step (4) had a eureka moment on the drive in to work one day worthy of Encyclopedia Brown. He realized that all the \"Error Users\" were left handed, and confirmed this fact. Only left-handed users got the errors, never Righties. But how could being left handed cause a bug? We had him sit down and watch the left-handers again specifically paying attention to anything they might be doing differently, and that's how we found it. It turned out that the bug only happened if you moved the mouse to rightmost column of pixels in the image viewer while it was loading a new image (overflow error because the vendor had a 1-off calculation for mouseover event).  Apparently, while waiting for the next image to load, the users all naturally moved their hand (and thus the mouse)  towards the keyboard.  The one user who happened to get the error most frequently was one of those ADD types that compulsively moved her mouse around a lot impatiently while waiting for the next page to load, thus she was moving the mouse to the right much more quickly and hitting the timing just right so she did it when the load event happened. Until we got a fix from the vendor, we told her just to let go of the mouse after clicking (next document) and not touch it until it loaded.  It was henceforth known in legend on the dev team as \"The Left Handed Bug\" "
    },
    {
        "ID": "2806",
        "Question": "There's around a zillion \"PHP frameworks\". And most of them bill themselves as following the MVC pattern. While it's welcome to overcome osCommerce coding style (processing logic heavily intermixed with SQL and HTML), there are certainly simpler and easier to follow approaches to get a maintainable application design. The original MVC concept was targetted at GUI applications. And for Gtk/Python it seems feasible to follow it accordingly. But PHP web apps don't operate on live Views (GUI elements) and a persistent Controller runtime. It's quite certainly a misnomer if it just describes the used code + directory grouping or class naming. \"MVC\" seems to be used like a buzzword for PHP frameworks. And I've actually seen one or two mature PHP frameworks admit it, but redefining the phrase anyway to match interna. So is it generally snake oil? Why isn't better terminology used, and a more sensible concept for maintainable PHP propagated? Some elaborative reasoning Why I suspect that PHP implementations don't follow the real MVC pattern: Models: in theory, Models should be fat and contain business logic, and controllers should be thin handlers (input->output). In reality the PHP frameworks advocate shallow Models. CI and Symfony for example equate Model == ORM. Even HTTP input is handled by the controller, isn't treated as model. Views: workarounds with AJAX discounted, there can't be Views on web pages. PHP frameworks still pump out pages. The interface still effectively follows the ordinary HTTP model, there's no advantage over non-MVC applications. (And lastly, none of the widespread php frameworks can factually output to GUI Views instead of HTML. I've seen a PHP library that can operate Gtk/Console/Web, but the frameworks don't.) Controller: I'm unsure. Controllers probably don't need to be long-running and persistently active in the MVC model. In PHP framework context, they're however mostly request handlers. Not really something to get argumentative about, but it just feels slightly buzzwordish. Would there be better descriptors? I've seen acronyms like PMVC or HMVC thrown around. Though descriptions get more ambigous there, maybe these would describe the current web frameworks less hokey? ",
        "Best answer": " But PHP web apps don't operate on live Views (GUI elements) and a persistent Controller runtime.   No, they sure do! Think of AJAX applications, then the view asks something to the controller and gets a partial view back, this view or data is then filled in somewhere in the page and thus live updated. The Controller is also persistent because you can use cookies/sessions.  \"MVC\" seems to be used like a buzzword for PHP frameworks.  MVC is a Software Architecture, some frameworks might use it as a buzz, but others do it properly... See a list of some frameworks on Wikipedia.  is MVC just the SEO of php programming?  MVC and SEO are two things apart, but yes... MVC is getting more popular. "
    }
]