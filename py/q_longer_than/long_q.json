[
    {
        "ID": "16",
        "Question": "I have read a few articles on Internet about programming language choice in the enterprise. Recently many dynamic typed languages have been popular, i.e. Ruby, Python, PHP and Erlang. But many enterprises still stay with static typed languages like C, C++, C# and Java. And yes, one of the benefits of static typed languages is that programming errors are caught earlier, at compile time, rather than at run time. But there are also advantages with dynamic typed languages. (more on Wikipedia) The main reason why enterprises don't start to use languages like Erlang, Ruby and Python, seem to be the fact that they are dynamic typed. That also seem to be the main reason why people on StackOverflow decide against Erlang. See Why did you decide \"against\" Erlang. However, there seem to be a strong criticism against dynamic typing in the enterprises, but I don't really get it why it is that strong. Really, why is there so much criticism against dynamic typing in the enterprises? Does it really affect the cost of projects that much, or what? But maybe I'm wrong. ",
        "Best answer": "Yes, I believe that they do. There are a few reasons that need to be considered in the selection of a language for a new project:  Run-time speed. Compared to C/C++/Fortran, Perl and Python are so slow it's funny. Initialization speed. Compared to the above fast languages, Java falls over and cries as the JVM keeps loading and loading and...while(1).... Prototype-ability. Exhaustively going through and doing the declaration/definition work required for C++ or Java increases the LOC, which is the only known metric that reliably correlates with bugcounts. It also takes a lot of time. It also requires a bit more thinking about types and connections. Internal fiddlability. Dynamically messing around with your internals is great until you begin to debug your self-modifying code. (Python, Lisp, Perl) Correctness verification. A compiler can provide a quick once-over pass of semi-correctness of your code in C++, and this can be really nice.  Static analysis details. C and Java have pretty good static analysis. Perl is not completely statically analyzable at a theoretical level (Possibly Python too). I'm reasonably sure Lisp isn't either.  Weird platforms only take C, in general.  Support chain. If you can have a contract that you will get your bugs looked at and worked on, that's huge.   If you can presume that the organization you are working with has a principle of \"Going forward\"(There's an accounting term for this), and won't just randomly decide to not work on the software, then you have a much better case for using the software. Since there's no Major Business selling (carrying implication of taking responsibility of maintaining it) Python/Perl/$dynamic_language, it considerably reduces risk.  In my experience, open source maintainers often have an issue with fully taking responsibility for bugfixes and releasing updates. \"It's free, YOU work on it!\" is not an answer that is acceptable to most businesses (not their core compentencies, among other things).  Of course, I'm not talking about the webapp/startup world, which tends to play by high risk/high reward rules and be very open to staying on the frothing edge of tech. "
    },
    {
        "ID": "49",
        "Question": "During my four years at university we have been using much functional programming in several functional programming languages. But I have also used much object oriented programming to, and in fact I use object oriented languages more when doing my own small project to prepare for my first job. But I often wish that I was coding in a functional programming language when doing these projects. However, when looking for a job, it is very rare to see a job where knowledge of a functional programming language is required. Why isn't functional programming languages used more in the industry? There is quite much news about functional programming languages these days, so I wonder if functional programming is catching on in the industry now? ",
        "Best answer": "I was a professor and, just like programmers, professors are always looking for the Next Big Thing. When they think they've found one, they make it a bandwagon, and everyone piles on. Since they are preaching to students who think professors must be really smart, else why would they be professors, they get no resistance. Functional programming is such a bandwagon. Sure it's got lots of nice interesting questions to investigate, and lots of sort-of-interesting conference articles to write. It's not a particularly new idea, and you can do it in just about any modern language, and ideas don't have to be new to be interesting. It's also a good skill to have. Given that, functional programming is just one arrow to have in your quiver, not the only one, just as OOP is not the only one. My beef with computer science academia is lack of practical interplay with industry to determine what actually makes real-world sense, i.e. quality control. If that quality control were there, there might be a different emphasis, on classifying problems and the ranges of solutions to them, with tradeoffs, rather than just the latest bandwagons. "
    },
    {
        "ID": "188",
        "Question": "For the longest time in places like Java's IRC channel, SO, and other places I've been told something along the lines of \"Worry about how the code looks and its readability/understandability now, and performance later if absolutely necessary\". So for the longest time, I haven't really been OCD about performance for my small desktop or web apps, just removing the obviously inefficient. Most responses are \"What about scalability?\". Thats a legitimate point, but if my app was only built to parse, say, files 10,000 lines long, should I make my code a mess for the small percentage of people that are going to shove in a 1,000,000 line file? My main question is when should I trade the easy but somewhat inefficient ways of doing tasks for big giant complicated beasts that do things extremely quickly but destroy any possible ways of upgrading and make the code excessively difficult and prone to rewriting anyway by the next developer?  ",
        "Best answer": "Worry about performance when it becomes a problem. If you write a small app to process 10,000 line files and you get a 1,000,000 line file every 100th file, it probably doesn't matter that it takes longer to process that one file. However, if you are regularly getting files that are 5-10 times larger than initially and your application is taking too long to do its job, then you start profiling and optimizing. Now, I said \"too long to do its job\". That is up to the user or sponsoring organization to decide. If I'm doing a task and it takes me 5 minutes to do something when it took me 3 without the software or with a different tool, I'd probably file a bug report or maintenance request to have that improved. If you are the user, how long you want your software to take to do its job is up to you - only you can decide if you want it done faster or if you are willing to wait longer to have more readable code. "
    },
    {
        "ID": "370",
        "Question": "I've been told that to be taken seriously as a job applicant, I should drop years of relevant experience off my résumé, remove the year I got my degree, or both. Or not even bother applying, because no one wants to hire programmers older than them.1 Or that I should found a company, not because I want to, or because I have a product I care about, but because that way I can get a job if/when my company is acquired. Or that I should focus more on management jobs (which I've successfully done in the past) because… well, they couldn't really explain this one, except the implication was that over a certain age you're a loser if you're still writing code. But I like writing code. Have you seen this? Is this only a local (Northern California) issue? If you've ever hired programmers:2  Of the résumés you've received, how old was the eldest applicant? What was the age of the oldest person you've interviewed? How old (when hired) was the oldest person you hired?  How old is \"too old\" to employed as a programmer? 1 I'm assuming all applicants have equivalent applicable experience. This isn't about someone with three decades of COBOL applying for a Java guru job. 2 Yes, I know that (at least in the US) you aren't supposed to ask how old an applicant is. In my experience, though, you can get a general idea from a résumé. ",
        "Best answer": "I'm 52, and Technology Director of a company I co-founded 15 years ago, and this is a question close to my heart. I spend about 40% of my time coding, mainly developing existing and new products and I truly hope to be doing the same thing in 10 years time.  I'm intrigued by the notion that older programmers are uniquely hampered by irrelevant skillsets. I find that this is the problem with younger developers - if I want an Flash Programmer, or a Flex Programmer, that's easy. If I want one with proven enterprise database or network skills, or with a track record of commercial product development, that's much more difficult to find. Older programmers can talk more articulately about design choices and software lifecycle issues simply because they've had a lifetime of experience of successes - and failures. The problem for older programmers is not that they are losing their intellectual capacity, but that they've been seduced by the notion that they should become 'managers'. In my opinion a good programmer with decades of experience can earn more developing software than by climbing some ill-defined management ladder,  provided they find (or start) an organisation which rewards innovation and ability.  In a world where millions of developers with the same skillsets are available via the internet, the idea that youth alone has value is simply dumb. "
    },
    {
        "ID": "408",
        "Question": "\"Regular\" golf vs. code golf: Both are competitions.  Both have a well-defined set of rules, which I'll leave out for simplicity.  Both have well-defined goals; in short, \"use fewer hits/characters than your competitors.\" To win matches, athletic golfers rely on  equipment  Some situations call for a sand wedge; others, a 9-iron.   techniques  The drive works better when your feet are about shoulder width apart and your arms are relaxed.   and strategies  Sure, you could take that direct shortcut to the hole... but do you really want to risk the water hazard or sand bunker when those trees are in the way and the wind is so strong?  It might be better to go around the long way.    What do code golfers have that's analagous to athletic golfers' equipment, techniques and strategies? Sample answer to get this started: use the right club!  Choose GolfScript instead of C#. ",
        "Best answer": "I'd say that thorough knowledge of the syntactical oddities of your language help. Here is one I found in Ruby when doing a bit of code golf: Instead of require \"sequel\" require \"nokogiri\" require \"chronic\"  You can do something like this:  body_of_your_program if %w{sequel nokogiri chronic}.each{|i| require i}  With this kind of thing, you too can write incredibly elaborate Ruby one-liners! In Ruby and Perl, you also get the magic variables like \"$_\" which can be used to do all sorts of magic with strings and regexes. Is your data not strings? Well, you might want to turn it into strings. Obviously, in C, the preprocessor is your friend. "
    },
    {
        "ID": "568",
        "Question": "Managed OSes like Microsoft Singularity and JNode are quite an interesting concept. Essentially, the OS is bootstrapped with code written in a low-level language (C/C++/Assembly), which essentially implements a virtual machine. The rest of the OS (and all userland apps) run on the virtual machine. There are some great things about this. For example, you suddenly make arbitrary pointers obsolete. And if well written, you get rid of a ton of legacy crud that most modern OSes currently have.  However, as a disadvantage, you're that much farther away from the hardware, and as a developer, you lose the ability to drop down to a lower level of abstraction and get your hands dirty.  What are your opinions on this? ",
        "Best answer": "I think that this is another case where \"it depends\". If you're writing applications such as web browsers, word processors etc. where lightning fast performance is not necessarily an issue then this approach has it's merits. By using this approach you can offer your customers a safer, more controlled experience. Not only are you limiting the damage that can be done by malware, but you are also running in a more consistent environment. It's like the difference between console games and PC games. The former know exactly what hardware they need to work with so can make use of that knowledge whereas the latter have to be able to cope with a wider variety of graphics cards, sound cards, hard disk speeds etc. However, there will be applications (such as games!) that require the low level access and will still need to be run \"natively\". Like managed languages you will have to use the appropriate tool for the job. "
    },
    {
        "ID": "648",
        "Question": "We, as programmers, are constantly being asked 'How long will it take'? And you know, the situation is almost always like this:  The requirements are unclear. Nobody has done an in depth analysis of all the implications. The new feature will probably break some assumptions you made in your code and you start thinking immediately of all the things you might have to refactor.  You have other things to do from past assignments and you will have to come up with an estimate that takes that other work into account. The 'done' definition is probably unclear: When will it be done? 'Done' as in just finished coding it, or 'done' as in \"the users are using it\"? No matter how conscious you are of all these things, sometimes your \"programmer's pride\" makes you give/accept shorter times than you originally suppose it might take. Specially when you feel the pressure of deadlines and management expectations.  Many of these are organizational or cultural issues that are not simple and easy to solve, but in the end the reality is that you are being asked for an estimate and they expect you to give a reasonable answer. It's part of your job. You cannot simply say: I don't know.  As a result, I always end up giving estimates that I later realize I cannot fulfill. It has happened countless of times, and I always promise it won't happen again. But it does. What is your personal process for deciding and delivering an estimate? What techniques have you found useful? ",
        "Best answer": "From The Pragmatic Programmer: From Journeyman to Master:  What to Say When Asked for an Estimate You say \"I'll get back to you.\" You almost always get better results if you slow the process down and spend some time going through the steps we describe in this section. Estimates given at the coffee machine will (like the coffee) come back to haunt you.  In the section, the authors recommend the following process:  Determine the accuracy that you need. Based on the duration, you can quote the estimate in different precision. Saying \"5 to 6 months\" is different than saying \"150 days\". If you slip a little into the 7th month, you're still pretty accurate. But if you slip into the 180th or 210th day, not so much. Make sure you understand what is being asked. Determine the scope of the problem. Model the system. A model might be a mental model, diagrams, or existing data records. Decompose this model and build estimates from the components. Assign values and error ranges (+/-) to each value. Calculate the estimate based on your model. Track your estimates. Record information about the problem you are estimating, your estimate, and the actual values. Other things to include in your estimate are developing and documenting requirements or changes to requirements specifications, creating or updating design documents and specifications, testing (unit, integration, and acceptance), creating or updating user's manuals or READMEs with the changes. If 2 or more people working together, there's overhead of communication (phone calls, emails, meetings) and merging source code. If it's a long task, account for things like other work, time off (holidays, vacation, sick time), meetings, and other overhead tasks when picking a delivery date.  "
    },
    {
        "ID": "724",
        "Question": "When learning a new programming language you sometimes come across a language feature which  makes you wish you had it in your other programming languages that you know. What are some language feature which were at the time of learning very new to you and that you wish your other programming languages had.   An example of this is generators in Python or C#. Other examples may include list comprehensions in Python, template in C++ or LINQ in .NET or lazy evaluation in Haskell. What other semi-unique language features have you come across which were completely new and enlightening to you?  Are there other features of older programming languages which were unique and have fallen out of fashion? ",
        "Best answer": "Practically anything in Haskell  Monads. Yes - the big scary word that makes increadibly easy parsers, IO, operations on Lists and other things so easy (once you notice common pattern) Arrows. The same for advanced users ;) Standard stuff like lambdas etc.  Currying functions Algebraic data types Pattern matching  And many more. PS. Yes. I am Haskell fanboy if anyone asked. "
    },
    {
        "ID": "775",
        "Question": "On any team, you are going to have the need for more grizzled and grey developers and some young pups. Some reasons include:  Money. There are often tasks that don't require the same level of experience to deliver, so it makes sense not to pay top dollar to have those tasks fulfilled. Energy. There's an energy and enthusiasm that new people can bring to a team that stops it from getting too stale and set in its ways. There's also calmness and wisdom that the more senior people can bring. Knowledge transfer and career growth. Both in terms of the project and skills, it's useful and often fun to teach people and to learn new stuff. It's satisfying to help \"bring on\" new team members.  I realise there are some cutting edge projects where it may be important for there to be more senior people than junior, but in general, is there an ideal mix of experiences on a team, or is it totally dependent on the project? ",
        "Best answer": "I really like what Eric Brechner has to say on this subject  Think of your team as a river instead of a lake. A lake stagnates. There’s no energy or impetus to change. The same is true of groups that stagnate. They cultivate mediocrity and complacency; they abhor risk. A river is always running and changing with lots of great energy. You want a river. A river depends on the flow of water, and your team depends on the flow of people and information. You can think of the people divided into three groups: new blood, new leaders, and elders ready for a new challenge. Here’s how those groups should balance and flow:  The largest group should be the new blood. Not all of them will become technical or organizational leaders.  Sometimes you’ll have more new leaders than elders, sometimes the reverse, but ideally you should maintain a balance.  For flow, you want a steady stream of new blood becoming your new leaders, and new leaders becoming elders.  The key to flow is new blood coming in and elders moving out. For this to work, you WANT your elders to transfer before they clog the stream and disrupt the flow of opportunitiesfor others.   Not all technologies flow at the same rate. Central engines, like the Windows kernel, flow slowly, while web-based services, like MSN Search, flow quickly. You need to adjust for your situation, but even the most conservative technologies do change and flow. How do you successfully encourage and maintain a healthy flow?  Keep a constant supply of new people.  Instill information sharing as a way of life.  Shape the organization and roles to create growth opportunities.  Find new challenges for your elders.    "
    },
    {
        "ID": "812",
        "Question": "I originally tried asking this on StackOverflow, but it was too subjective :-(. I am interested in methods of defining the power of programming languages. Turing completeness is one, but it is almost universally satisfied. What would be nice is to define a measure of power that discriminates among programming languages that are actually in used. For example, can anyone propose a non-subjective method that would discriminate between assembly and Java? Turing completeness means that a language is maximally powerful in what it can output (which pretty much means it can do anything non-time based in the real world). So if we want to define a stronger measure of power, we need to take another approach. Shortness was suggested in the original question, but this is not easy to define at all. Does anyone have any other suggestions? ",
        "Best answer": "The notion you are looking for is called expressiveness and Matthias Felleisen has a mathematically rigorous definition:  \"On the Expressive Power of Programming Languages\" www.ccs.neu.edu/scheme/pubs/scp91-felleisen.ps.gz (Postscript version)  The intuition behind the idea is that if you have two equivalent programs in two different languages-- say, program A in language X and program B in language Y-- and if you make a local change to A that requires a global change to B, then X is more expressive than Y. One example Felleisen provides is assignment: In the Scheme programming languages you can remove the assignment operator and still have a Turing complete language. However, in such a restricted language, adding in a feature that would be localized if assignment was allowed would require a global change to the program without assignment. My discussion has simplified some details, and you should read the paper itself for the full account. To answer your other question: You can say that Java is more expressive than assembly because you can add a new class to your Java program, and then gain the benefits of polymorphism by having other parts of your program call its methods without global modification. Exception handling is another example where Java is more expressive than assembly: You simply need to write a single throw statement to transfer control up the stack. On a more elementary level, you can also add a new case statement near the beginning of a switch and you won't have to worry about recalculating any jump offsets by hand. "
    },
    {
        "ID": "827",
        "Question": "For decades, the accepted degree to get to become a software developer was \"Compter Science.\" We've had a few questions already about whether that degree really prepares students to develop software. Some schools have, in the last 8 years or so, started offering multiple distinct majors in programming.  Using the curriculum from my school:  Computer Science, which starts out with some intro programming courses in the first year, and then focuses on theoretical computer science, algorithms, and a bit of OS stuff.  Most classes involve several smaller projects and homeworks, done solo or in pairs. Software Engineering, which starts out with the same intro programming courses, does a few classes of theory, and then goes into software development practices (testing, process methodologies, sofware metrics, requirements gathering) and software design (distributed system design, info system design, real-time/embedded design, subsystem design, etc)  Different schools do it differently, so the above is just a real-world example I'm familiar with.  What I ask is: Is there a need for distinct majors in programming? ",
        "Best answer": "Yes, they should be. The relationship between computer science and software engineering is the same as the relationship between physics and mechanical engineering. One provides the theoretical background while the other takes those theories, along with good engineering principles, and applies them to the design and implementation of software. You need both in order to produce new and better software. A good computer science education trains people to produce new and better algorithms, data structures, programming languages and paradigms, compilers, and a number of other things that can be used to enhance software systems. A good software engineering education, on the other hand, trains you to take these tools and knowledge obtained through a scientific study of computation, along with a knowledge of the software development lifecycle and process models to actually build the system that a customer wants and needs. "
    },
    {
        "ID": "874",
        "Question": "In my current job it feels like we have a lot requirement changes. We are an \"Agile\" shop, so I get that we are supposed to adjust and what not, but sometime the change is large and nothing trivial.  My question is, how do you effectively communicate the cost of the change? Because of being agile, if a change is big enough something will get dropped from the current sprint, but it usually just get added next time around. Since our model is SaaS, the end customer is effectively the business itself, and they know they will get the cut feature n weeks later. I guess what I am trying to get at is the removal of a feature really isn't anything to use for communication as it was only delayed by n weeks. What other ways do you have to get the business to understand what a change costs? ",
        "Best answer": "@Joe \"We are an \"Agile\" shop, so I get that we are supposed to adjust and what not, but sometime the change is large and nothing trivial. \" If your process doesn't allow you to control the rate of change in requirements, your process is not agile, but haphazard. Agile does not mean \"taking anything that comes my way.\" To control requirement change/creep you can adopt - in your process - the notion that a requirement does not change (a notion that it's at the heart of Scrum.) Treat a requirement change as replacing an old requirement with a new one. You have to have a backlog of requirements, and you have to have the user choose which ones he/she wants to have implemented.  You wanted X and Y in two weeks, but all of the sudden you want Z. Well, then I can deliver you all three in 4 weeks. Or I can give a pair (X and Z) or (X and Y) or (Y and Z) in two weeks and deliver the remaining one later. Choose. This is how you negotiate with customers. This is how you communicate the cost of requirement change. If your group does not have that power, you are not in an agile shop, and there is nothing that you can do about it. It sucks, but it's true. In case where you can negotiate, you have to track (with precision) the time it takes to implement requirements and requirement changes. That is, you have to collect this data from past and present projects.  You collect the original time estimate and the actual completion time (in addition to resources like developer count) per request (or module affected by N requests). Better yet, estimate the size of the request/request change (in terms of lines of code or function points in past projects and requests.) Say you  have a metric that you can talk to the user with. You know that a new request will take, say, 1K lines of code, or 10 web pages with an average of 5 input fields each (50 function points).  Then by looking at historical data specific to your past projects (some by lines of codes, some by web pages, some by actual function points), and you can estimate how each of these cost in terms of absolute completion time. For those with sufficient data, you can also identify those requirements that track an actual developer head count.  Then you use that and you tell your customer that based on historical data; you argue that project failures tend to follow a exponential distribution follow; and then you are armed with the following argument for your customer:  Based on data from our past and present projects and available   resources, the requirement you are asking will take   X amount of time to complete with a 25% probability of failure (or   75% of success) 1.5 * X amount of time to complete with a 5% of failure (or 95% of success) 0.5 * X amount of time to complete with a 95% of failure (or 5% of success)   The probability of failure as a function of amount of time resources typically go 95%, 25% and 5% (resembling an exponential distro.) You convey the message that a certain baseline amount gives a somewhat decent chance of success (but with real risks). 1.5 of that might give almost a certain chance of success with minimal risk, but than much less than that (0.5 of the original guarantees almost certain failure.) You let them digest on that. If they still go for the risky proposition (done yesterday!) at least you have in writing that you told them so. If there is hope for your group of not just being agile but engineering-like, then, the customer might put serious consideration into your numbers and schedule this and future requests accordingly. It is your job as an engineer to explain in engineer, verifiable and clear terms that request changes are not a free meal. "
    },
    {
        "ID": "1070",
        "Question": "Dcpromo.exe is famous among MCSEs for being they only way to create a Windows Domain Controller (in other words an Active Directory Domain) ... which in turn is often used by the ASP.NET Membership system. I'm trying to determine if I should put content on \"www.dcpromo.com\" geared for developers or more towards sysadmins. To me, a logical extension of this \"domain controller on the internet\" is to use WIF for the ASP.NET Membership system.  We'd then have a website that will serve the developer's interests in the SSO, SAML, user managment and identity areas we all struggle with. So my question is:  Q:  Do programmers see a connection between the utility dcpromo.exe and ASP.NET Membership?  If so does it make sense to have a purpose of http://www.dcpromo.com  help developers in the areas of membership and cloud computing?   ",
        "Best answer": "I've been a Microsoft developer for 11 years, mostly as a web developer.  Been coding since 1.1 and I've used .NET membership in many of my applications, and I've never heard of dcpromo. "
    },
    {
        "ID": "1106",
        "Question": "This goes back to a conversation I've had with my girlfriend. I tried to tell her that I simply don't feel adequate enough in my programming language (C++) to call myself good. She then asked me, \"Well, when do you consider yourself good enough?\" That's an interesting question. I didn't know what to tell her. So I'm asking you. For any programming language, framework or the like, when do you reach a point were you sit back, look at what you've done and say, \"Hey, I'm actually pretty good at this.\"? How do you define \"good\" so that you can tell others, honestly, \"Yeah, I'm good at X\". Additionally, do you reach these conclusions by comparing what others can do? Additional Info I have read the canonical paper on how it takes ten-thousand hours before you are an expert on the field. (Props to anybody that knows what this paper is called again) I have also read various articles from Coding Horror about interviewing people. Some people, it was said, \"Cannot function outside of a framework.\" So they may be \"good\" for that framework, but not otherwise in the language. Is this true?  ",
        "Best answer": "You can't call yourself good at X.  Only other people can. "
    },
    {
        "ID": "1135",
        "Question": "Planning Poker Summary, in case you don't want to read the wiki article:  Get a list of tasks you want to do for the upcoming iteration For each task: 2.1 Discuss with the group what it entails 2.2 Everyone writes down / selects an estimation of how much effort is required for the task 2.3 Everyone reveals their estimation 2.4 The highest and lowest outliers explain their reasoning 2.5 Repeat until a consensus is reached  Usually something similar to numbers from the Fibonacci sequence like 0, ½, 1, 2, 3, 5, 8, 13, 20, 40, 100 are the allowed values, so you don't get long arguments over close values like 23 vs 27. Further, the numbers represent a unit-less value of effort, whose value is determined by a baseline task that everyone agrees equals about a 1, and all else is relative to that. Ultimately, the goal is to get a good feel for a given team's \"velocity\", which is the number of these points that can be completed in a given iteration.  With that, it's possible to make reasonably accurate estimates of how long any given feature will take.  We did this at iteration planning meetings at one company I worked at, and I thought it was one of the few good things about that particular company.  So, what I'm wondering is, has anyone used this?  Do you think it's a useful tool for estimation?  Does it work in all situations, or does it lend itself to certain teams, projects, etc? ",
        "Best answer": "We use it in our company for the project I'm involved in.  Some notes about planning poker are expressed in my recent blog post, and here's a bigger list of why it's cool:  It makes everyone agree.  People are not forced to accept any result; instead they're forced to make their own estimate!  The time to defend their own estimates is also allocated, if it's necessary. It keeps everyone busy.  You can't slack during the meeting, while trying to show that you're so involved. Also, necessity of moving your hands constitutes a good physical exercise to keep you off of sleeping. However, a downside of this is that sometimes you do need to do something else (for example, take some notes and write down the details of the agreement you've just reached).   It keeps meetings faster.  There's no need for a constant involvement of a meeting leader to keep everything on pace.  The game with clear rules is way better for that.  Yes, you need to make some extra moves to put cards on, reveal them, et cetera, but these pay their way. A lot of people just like to play cards, especially poker :-)  This increases motivation.  A company that sells decks of such cards accompanied their site with an article about Planning Poker, which is also worth reading. "
    },
    {
        "ID": "1371",
        "Question": "Having worked on a failed project is one of the few things that most programmers have in common, regardless of language used, industry or experience. These projects can be great learning experiences, soul-crushing disasters (or both!), and can occur for a multitude of reasons:  upper management change of heart under-skilled / under-resourced team emergence of superior competitor during dev cycle over/under management  Once you've worked on a couple of such projects, is it possible to recognise at an early stage exactly when a project is doomed to fail?  For me, a big sign is having a hard & fast external deadline combined with feature creep. I've seen projects which were well planned out and proceeding right on schedule go horribly off the rails once the late feature requests started to roll in and get added to the final \"deliverable\". The proposers of these requests earned the nickname of Columbo, due to rarely leaving the room without asking for \"just one more thing\". What are the warning signs you look out for that set off the alarm bells of impending doom in your head? ",
        "Best answer": "Heroic Coding Coding late into the night, working long hours, and clocking lots of overtime are a sure sign that something went wrong.  Further, my experience is that if you see someone working late at any point in the project, it only ever gets worse.  He might be doing it just to get his one feature back on schedule, and he might succeed; however, cowboy coding like that is almost always the result of a planning failure that will inevitably cause more of it soon.  So, the earlier in the project you see it, the worse it will eventually become. "
    },
    {
        "ID": "1386",
        "Question": "Sometimes I feel like a musician who can't play live shows. Programming is a pretty cool skill, and a very broad world, but a lot of it happens \"off camera\"- in your head, in your office, away from spectators.  You can of course talk about programming with other programmers, and there is peer programming, and you do get to create something that you can show to people, but when it comes to explaining to non programmers what is it that you do, or how was your day at work, it's sort of tricky.  How do you get the non programmers in your life to understand what is it that you do?  NOTE: this is not a repeat of Getting non-programmers to understand the development process, because that question was about managing client expectations.  ",
        "Best answer": "I don't even try. If they aren't tech oriented enough to have at least a basic understanding of programming, I am only going to bore them with the details. Usually I just go with something very high level like \"I create web sites\" or \"I write computer programs to do X\" "
    },
    {
        "ID": "1516",
        "Question": "A few years ago I considered myself somewhat of a web developer, knowing the basic 3 languages (HTML, CSS, JS) and lots of PHP. Moving on from simple text to actual websites was a pain because of the so called \"standards\" out there, which at the time were ridiculously complicated for me. It pretty much boiled down to this (minus the IE related stuff):  Standards are there to replace old ways of doing things in a simpler way. However when trying to actually implement some of the stuff (Entirely CSS based layout for example), it took me 10x longer to actually do it then if I did the simpler and still working solution. If it rendered the same, then why should I use the more complicated example that takes 10x longer and breaks once you change browsers? This sparked many long religious debates in ##php, ##css, and ##js in Freenode IRC and actually got me banned from ##css because I messed with their little world over there. My question: Should I follow every single standard and coding conventions even if they take me 10x longer but get me the same result as the simple one?  For the poll tag, those of you who have websites of any size (huge or small), do you follow all the standards?  ",
        "Best answer": "The standards writers have thought of things that haven't occurred to you, such as accessibility concerns. Standards exist for a reason. And, with HTML5, standards are fairly easy to follow. There may, occasionally, be reason to not follow the standard, but following it should be your default behaviour. "
    },
    {
        "ID": "1674",
        "Question": "i wonder apart from those very simple hello world apps, what other programs should i try developing for learning. i am entering uni next year and wonder what kind of programs will be developed in that environment.  this is not really a language specific thing, but if you want to know what i use currently mainly  windows: C#/WPF - i studied java/vb etc in poly but i think C# is the way to go, maybe even F# web: PHP/MySQL, Zend Framework/Doctrine, + HTML/CSS/JS(jQuery) of course. looking to try google app engine with python too.   for now, i am thinking of   todo lists apps that integrate with api eg. twitter/tumblr which i use text editor - i am currently trying to develop a text editor that uses markdown and generates html files for viewing (link to blog entry). not very pratical after i started developing it. cos when editing theres no formatting and in HTML format, i cannot edit directly  blog software (for web)  ",
        "Best answer": "Have a look at Project Euler.  There's nothing else like it for sharpening your foundational programming skills. "
    },
    {
        "ID": "1719",
        "Question": "In chapter one of \"The Pragmatic Programmer\" the first learning goal is:  Learn at least one new language every year. Different languages solve the same problems in different ways. By learning several different approaches, you can help broaden your thinking and avoid getter struck in a rut. [...]  To achieve this over a career, the list of languages is likely to get quite long (particularly if you do not want to \"progress\" into management). Clearly the education of a programmer (or whatever form) is going to get you started with a core of commercially useful languages (the usual list from job posting: C, C++, Ruby, Python, JavaScript, C#, Java, VB, ...). Additionally a formal or informal learning programme is likely to have covered functional approaches (via something like Haskell, LISP or an ML derived language) But once a reasonable subset of that list is learned- what's next, and why? ",
        "Best answer": "Make it interesting and spend each year writing an interpreter or compiler for your own programming language that fills up a niche you've never used a programming language for. Each year, write your next compiler/interpreter using the language you wrote the previous year. "
    },
    {
        "ID": "1750",
        "Question": "I work for a company that supports several languages: COBOL, VB6, C#, and Java. I use those languages for my primary work, but I often find myself coding some minor programs (e.g. scripts) in Python because I find it to be the best tool for that type of task. For example: an analyst gives me a complex CSV file to populate some DB tables, so I use Python to parse it and create a DB script. What's the problem? The main problem I see is that a few parts of these quick and dirty scripts are slowly gaining importance and:  My company does not support Python They're not version controlled (I back them up in another way) My coworkers do not know Python  The analysts have even started to reference them in emails (\"launch the script that exports...\"), so they are needed more often than I initially thought. I should add that these scripts are just utilities that are not part of the main project; they simply help to get trivial tasks done in less time. For my own small tasks they help a lot. In short, if I were a lottery winner to be in a accident, my coworkers would need to keep the project alive without those scripts; they would spend more time fixing CSV errors by hand, for example. Is this a common scenario? Am I doing something wrong? What should I do? ",
        "Best answer": "You need to get the situation formalised as it shouldn't really have got to this point. However, these things happen so you need to explain to your boss that you created these scripts for personal use, but they've \"escaped\" into wider circulation. Admit (if necessary) that you were at fault for not bringing this to his attention sooner. At the very least the scripts should be put under source control \"just in case\" - then at least if you aren't available (for what ever reason) your co-workers will have access to the scripts. Then you either need to convince your boss that Python is the way to go for these or accept that you are going to have to re-write them in a supported language. If the cost of documenting the scripts and educating your co-workers in Python is lower than that of the re-write you might even win the argument. "
    },
    {
        "ID": "1969",
        "Question": "I'm posting this here since programmers write viruses, and AV software.  They also have the best knowledge of heuristics and how AV systems work (cloaking etc). The EICAR test file was used to functionally test an antivirus system.  As it stands today almost every AV system will flag EICAR as being a \"test\" virus.  For more information on this historic test virus please click here. Currently the EICAR test file is only good for testing the presence of an AV solution, but it doesn't check for engine file or DAT file up-to-dateness. In other words, why do a functional test of a system that could have definition files that are more than 10 years old.  With the increase of zero day threats it doesn't make much sense to functionally test your system using EICAR. That being said, I think EICAR needs to be updated/modified to be effective test that works in conjunction with an AV management solution. This question is about real world testing, without using live viruses... which is the intent of the original EICAR. That being said I'm proposing a new EICAR file format with the appendage of an XML blob that will conditionally cause the Antivirus engine to respond. X5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-EXTENDED-ANTIVIRUS-TEST-FILE!$H+H* <?xml version=\"1.0\"?> <engine-valid-from>2010-1-1Z</engine-valid-from> <signature-valid-from>2010-1-1Z</signature-valid-from> <authkey>MyTestKeyHere</authkey>   In this sample, the antivirus engine would only alert on the EICAR file if both the signature  or engine file is equal to or newer than the valid-from date. Also there is a passcode that will protect the usage of EICAR to the system administrator. If you have a backgound in \"Test Driven Design\" TDD for software you may get that all I'm doing is applying the principals of TDD to my infrastructure.   Based on your experience and contacts how can I make this idea happen? ",
        "Best answer": "As you said in the question, it would have to work in conjunction with an AV solution.  In order for that to happen you would either need to write an AV engine, or become involved with an existing AV vendor. If such a thing did exist... Where does the benefit come in?  Just thinking devil's advocate here..  Couldn't the AV engine just report when it's database was updated? "
    },
    {
        "ID": "2042",
        "Question": "I've been working in the enterprise space for the past 4½ years and have noticed that generally speaking, enterprises are not conducive environments for the test-first style of development. Projects are usually fixed-cost, fixed-timeline and waterfall style. Any unit testing, if done at all, usually comes after development in the QA phase and done by another team. Prior to working for an enterprise, I consulted for many small to medium sized companies, and none of them were willing to pay for a test-first style of development project. They usually wanted development started immediately, or after a short design stint: i.e., something more akin to Agile, though some clients wanted everything mapped out similar to waterfall. With what types of shops, companies, and clients does test-driven development work best? What types of projects tend to be conducive to TDD? ",
        "Best answer": "Every line of code I write is using test driven development.  If management isn't on board with writing tests first then I don't tell management about it.  I feel that strongly that test driven development is a better process. "
    },
    {
        "ID": "2164",
        "Question": "So I'm sure everyone has run into this person at one point or another, someone catches wind of your project or idea and initially shows some interest. You get to talking about some of your methods and usually around this time they interject stating how you should use method X instead, or just use library Y. But not as a friendly suggestion, but bordering on a commandment. Often repeating the same advice over and over like a overzealous parrot. Personally, I like to reinvent the wheel when I'm learning, or even just for fun, even if it turns out worse than what's been done before. But this person apparently cannot fathom recreating ANY utility for such purposes, or possibly try something that doesn't strictly follow traditional OOP practices, and will settle for nothing except their sense of perfection, and thus naturally heave their criticism sludge down my ears full force. To top it off, they eventually start justifying their advice (retardation) by listing all the incredibly complex things they've coded single-handedly (usually along the lines of \"trust me, I've made/used program X for a long time, blah blah blah\"). Now, I'm far from being a programming master, I'm probably not even that good, and as such I value advice and critique, but I think advice/critique has a time and place. There is also a big difference between being helpful and being narcissistic. In the past I probably would have used a somewhat stronger George Carlin style dismissal, but I don't think burning bridges is the best approach anymore. Do you have any advice on how to deal with this kind of verbal flogging? ",
        "Best answer": "Don't just let them talk. Get them in front of a keyboard. The phrase \"ok, show me\" should do it. My experience is most blow hards aren't that great, and when they actually try to do what they say it doesn't work and things get real quiet. "
    },
    {
        "ID": "2252",
        "Question": "I've been asked this in a few interviews. And it always catches me off guard.My professional and academic background are already in the resumé, which the interviewer has obviously looked at. What more to tell him/her? Should I start with my hobbies? I like gardening, or looking at NSFW pictures on reddit in my free time? What and how do you answer to this specific question? Do you have a prepared answer for it? Am I wrong if I think this question is a bit silly? UPDATE There have been a lot of great answers to this question. I'm in pickle which to choose as the 'correct' answer, because most of them are very insightful. I found a great writing on this subject matter. It's a bit crazy for my taste, but it's interesting: How To Introduce Yourself… I Mean Practically ",
        "Best answer": "Don't assume the interviewer knows your resumé inside out. Often, they'll be interviewing several people for the position and may have just had a cursory glance over your resumé before starting the interview. With that in mind, and assuming this question comes early on in the interview, use this question as an opportunity to give a brief history of your career and why you are applying for the job, as well as what your stand-out skills or attributes are.  Your answer can effectively steer the course of the interview, giving the interviewer some \"jumping off\" points that could change what questions you get asked next. Focusing on your strengths with this answer means that it will be more natural to talk about what makes you great in answers to subsequent questions and not as something you have to try to shoehorn in to some other answer. "
    },
    {
        "ID": "2410",
        "Question": "I am referring to explaining to the non-programmer what programming is. I made sure to search for similar questions before creating this one, but the few ones I did find seemed to dodge the question, and I specifically would like to see some metaphors or analogies. I personally find it easier to explain something technical to someone through the use of metaphors or analogies. The reason I'm interested in this is because many people encounter the work of a programmer on a daily basis, but if you ask the average person what a programmer is or does, they don't really know. This leads to certain situations of misunderstanding (ex. \"[...] but I thought you were good with computers!\") I really would like to find the best one out there. I would like to be able to easily explain to someone what my career choice is about. Of course, at least the general idea. I personally don't have a solid one, but I have long thought about it and I have usually gravitated towards the 'language' metaphor, where we happen to know a language that computers understand, and therefore we are able to tell computers what to do, or \"teach\" them, to solve our problems. For example:  Imagine that in an alternate reality, humanoid robots with artificial intelligence exist, and some people are able to communicate with them through a common language, which is a variation of English. These people who can communicate with the robots are able to teach them how to solve certain problems or do certain tasks, like doing our chores. Well, although robots like that don't exist yet, programmers of our time are like those people, but instead of communicating with the robots, they communicate with computers. Programmers \"teach\" the computers how to perform certain tasks or solve certain problems by means of software which they create by using this \"common language\". Programmers and this \"common language\" are what give us things like email, websites, video games, word processors, smart phones (to put it simply), and many other things which we use on a daily basis.  I don't mean to put programming on the throne or anything, it's just the best metaphor I could come up with. I'm sure someone will find some issue with this one, it's probably a bit contrived, but then again that's why I'm asking this question. ",
        "Best answer": "It's like having to write detailed step by step directions for how to drive somewhere. But  you usually have to add contingency plans for things like 'what if there is a traffic jam', or 'what if a truck breaks down in the turn lane'.  And sometimes you have to dive even deeper and explain the rules of the road like which side to drive on or what to do at a red light.  And sometimes you even need to explain precisely how the steering wheel or the gas pedal works.   And usually, once you've got that all described in exacting detail, the customer says \"that's perfect, except it needs to work for someone driving a hovercraft\" "
    },
    {
        "ID": "2682",
        "Question": "We’re working on a .Net framework which ultimately amounts to a single DLL.  We intend to charge for commercial use of the framework, but make it free for open source/non-commercial use.  The rough plan at the moment is to administer this through some form of fairly simple licence which will be issued whether you’re using it for free or paying. We’re debating whether to make the source code available.  It’s our perception (and our own preference) that it’s far more appealing to use something where you have access to the source code. I’m interested in whether people think making the source code available will damage our ability to make money from the framework, or whether it will encourage more usage and enough “good” people will arrange to pay for the correct licence if using it commercially. My feeling is that, generally, commercial operations don’t mess about on the licencing front and so making the source code available will only encourage usage and therefore ultimately generate more revenue, but I’d be interested in others views/experience. ",
        "Best answer": "You should definitely make the source available.  Whether it's freely available or only available to those who buy a license is up to you, but I would never use a third-party library with no source.  Unlike Robert Harvey, I emphatically do not \"know that I will probably never need it.\"  Any library of non-trivial complexity is almost certain to have bugs in there somewhere, have missing or poorly-implemented features that could benefit from customization/extension, or most likely both.  (Yes, even yours.)  I've used a lot of different libraries, from different people and written in different languages, and I can't think of any that I've never needed the source from at one point or another. If you want to do it right, add a provision in the license like what the GPL and MPL have, that if they make changes to the code and end up publishing a product using it, they have to publish the changes they made. That way you get free bugfixes and (potential) features just by letting other people use your code. "
    },
    {
        "ID": "2699",
        "Question": "This is a \"Share the Knowledge\" question. I am interested in learning from your successes and/or failures. Information that might be helpful... Background:  Context: Language, Application, Environment, etc. How was the bug identified ? Who or what identified the bug ? How complex was reproducing the bug ?   The Hunting.  What was your plan  ? What difficulties did you encounter ? How was the offending code finally found ?  The Killing.  How complex was the fix ? How did you determine the scope of the fix ? How much code was involved in the fix ?  Postmortem.  What was the root cause technically ? buffer overrun, etc. What was the root cause from 30,000 ft ? How long did the process ultimately take ? Were there any features adversely effected by the fix ? What methods, tools, motivations did you find particularly helpful ? ...horribly useless ? If you could do it all again ?............  These examples are general, not applicable in every situation and possibly useless. Please season as needed. ",
        "Best answer": "It was actually in a 3rd party image viewer sub-component of our application.  We found that there were 2-3 of the users of our application would frequently have the image viewer component throw an exception and die horribly. However, we had dozens of other users who never saw the issue despite using the application for the same task for most of the work day. Also there was one user in particular who got it a lot more frequently than the rest of them. We tried the usual steps:  (1) Had them switch computers with another user who never had the problem to rule out the computer/configuration. - The problem followed them. (2) Had them log into the application and work as a user that never saw the problem. - The problem STILL followed them. (3) Had the user report which image they were viewing and set up a test harness to repeat viewing that image thousands of times in quick succession. The problem did not present itself in the harness.   (4) Had a developer sit with the users and watch them all day. They saw the errors, but didn't notice them doing anything out of the ordinary to cause them. We struggled with this for weeks trying to figure out what the \"Error Users\" had in common that the other users didn't. I have no idea how, but the developer in step (4) had a eureka moment on the drive in to work one day worthy of Encyclopedia Brown. He realized that all the \"Error Users\" were left handed, and confirmed this fact. Only left-handed users got the errors, never Righties. But how could being left handed cause a bug? We had him sit down and watch the left-handers again specifically paying attention to anything they might be doing differently, and that's how we found it. It turned out that the bug only happened if you moved the mouse to rightmost column of pixels in the image viewer while it was loading a new image (overflow error because the vendor had a 1-off calculation for mouseover event).  Apparently, while waiting for the next image to load, the users all naturally moved their hand (and thus the mouse)  towards the keyboard.  The one user who happened to get the error most frequently was one of those ADD types that compulsively moved her mouse around a lot impatiently while waiting for the next page to load, thus she was moving the mouse to the right much more quickly and hitting the timing just right so she did it when the load event happened. Until we got a fix from the vendor, we told her just to let go of the mouse after clicking (next document) and not touch it until it loaded.  It was henceforth known in legend on the dev team as \"The Left Handed Bug\" "
    },
    {
        "ID": "2806",
        "Question": "There's around a zillion \"PHP frameworks\". And most of them bill themselves as following the MVC pattern. While it's welcome to overcome osCommerce coding style (processing logic heavily intermixed with SQL and HTML), there are certainly simpler and easier to follow approaches to get a maintainable application design. The original MVC concept was targetted at GUI applications. And for Gtk/Python it seems feasible to follow it accordingly. But PHP web apps don't operate on live Views (GUI elements) and a persistent Controller runtime. It's quite certainly a misnomer if it just describes the used code + directory grouping or class naming. \"MVC\" seems to be used like a buzzword for PHP frameworks. And I've actually seen one or two mature PHP frameworks admit it, but redefining the phrase anyway to match interna. So is it generally snake oil? Why isn't better terminology used, and a more sensible concept for maintainable PHP propagated? Some elaborative reasoning Why I suspect that PHP implementations don't follow the real MVC pattern: Models: in theory, Models should be fat and contain business logic, and controllers should be thin handlers (input->output). In reality the PHP frameworks advocate shallow Models. CI and Symfony for example equate Model == ORM. Even HTTP input is handled by the controller, isn't treated as model. Views: workarounds with AJAX discounted, there can't be Views on web pages. PHP frameworks still pump out pages. The interface still effectively follows the ordinary HTTP model, there's no advantage over non-MVC applications. (And lastly, none of the widespread php frameworks can factually output to GUI Views instead of HTML. I've seen a PHP library that can operate Gtk/Console/Web, but the frameworks don't.) Controller: I'm unsure. Controllers probably don't need to be long-running and persistently active in the MVC model. In PHP framework context, they're however mostly request handlers. Not really something to get argumentative about, but it just feels slightly buzzwordish. Would there be better descriptors? I've seen acronyms like PMVC or HMVC thrown around. Though descriptions get more ambigous there, maybe these would describe the current web frameworks less hokey? ",
        "Best answer": "I think you are looking at this in completely the wrong way. A GUI app and a web page are worlds apart so the exact same definition of MVC will never work for both. MVC is more about the ideal: separating certain parts of the app like display and logic. In PHP (or the web in general), a View is the web page itself: the HTML output. It's not \"live\" as per your definition, but you simply click links to go back to the controller (i.e. another page request). The Controller and Model is where things do differ, like you explained. In PHP the model tends to be the data layer, interacting with the database and so on. But it is still modelling the situation, and the controller still controls the application flow, if only once per page load. So the name \"Model-View-Controller\" is perfectly logical, albeit a different implementation in GUI apps vs web apps. "
    },
    {
        "ID": "3033",
        "Question": "When writing software for yourself, your company or third parties, do you always consider certain principles, values, standards, or rules of behavior that guide the decisions, procedures and systems in a way that contributes to the welfare of its key stakeholders, and respects the rights of all constituents affected by its operations?  And can this code of conduct sometimes be overruled by business requirements, lack of technical skills or other friction during the development process?  Some random examples in order of severity. (yes that is controversial) :  Do you accept known bugs as a risk for the end-user? When writing applications, do you always give the end user the chance for a complete de-install? Do you always secure and encrypt private data delivered end-users in your web application? Do you alwask ask the end-user before submitting his entered data to the server? Did you ever wrote an application that sends unwanted e-mails? Did you ever work on harvesting or scraping projects only for the benefit of the business? Did you ever write software that is legal but moraly controversial, like  for weapons industry. Did you ever wrote software that ( can intentionally) or is be used for criminal activities  It would be nice if you can get a good case with explanation on your moral and ethical decisions.  note: Since ethics and law are quite local and cultural dependent, it would be interesting if you add the location of the \"crime scene\" with it. ",
        "Best answer": "Overall, I always keep the Software Engineering Code of Ethics in mind. However, to address some of your particular points:  Do you accept known bugs as a risk for the end-user?  It depends. If it's a mission critical system and the bug is a showstopper, that's unacceptable. However, if it's a minor flaw that has workarounds in a non-critical system, that's acceptable. I always consider the impact of the problem on the system and to the users (and people affected by) the system.  Do you always secure and encrypt private data delivered end-users in your web application?  If I was on a project where this applied, I would consult any applicable laws and guidelines and follow them. If there were no applicable guidelines, I would err on the side of caution and use some form of security. Of course, you have to weigh a number of factors, ranging from how the system is deployed (physical locations, connections between nodes) and performance of any algorithms or techniques used.  Did you ever write software that is legal but morally controversial, like for weapons industry.  All of my jobs (as you can see in my profile) have been in the defense industry (and I'm also planning on working in the defense or intelligence industries after graduation), including work on ISR systems and Command and Control systems. I don't understand anyone, especially really good software engineers with skills that these projects need, who says that they won't build such systems. The way I see it, by bringing the best software engineering practices to these systems, you are only making them safer and more reliable. And these systems that involve life and death need to be as safe and reliable as possible. "
    },
    {
        "ID": "3139",
        "Question": " Possible Duplicate: At which point do you “know” a technology enough to list it on a resume   I'm having trouble selecting exactly what to put in the computer skills section of my resume. I feel the need to list a lot of languages and the IDEs I work with, and perhaps mention that I use Mercurial too. But this seems, well, kinda fake; after all, where do I draw the line in the list of languages? Sure, I learned a little C in a class, I can conquer some simple printf and getchar projects, but I don't really think that counts as being able to list it on my resume. I seem to recall Joel or Jeff addressing this but I can't find it now. But I'm pretty sure they said something along the lines of don't put it on your resume if you don't want to be drilled on it. Well, I sure wouldn't want to be drilled on C... But is there no justification in my listing languages like C# that I don't work with daily but could pick back up after a short refresher? I mean, I wouldn't want to be drilled on the internals of .NET either, but I think I am justified in listing it in a list of languages I have used... How do you decide? What do you have in your 'Computer Skills' section of your resume? (and can you please find the Joel/Jeff posts I'm thinking of, if they exist?) ",
        "Best answer": "As little as possible, and only those relevant to the position I'm applying for. As someone who reads resumes on occasion, nothing is more annoying than going through a list of every single computer related piece of equipment, software, and skill the applicant has ever touched, read about, or has actual experience with. You applying for a job writing code?  Why the $*@( are you telling me you have experience with Outlook?  Seriously? Only include the skills relevant to the position you are applying for on your resume.   You are retooling your resume for each position you are applying for, aren't you? Aren't you? "
    },
    {
        "ID": "3233",
        "Question": "As an entrepreneur/programmer who makes a good living from writing and selling software, I'm dumbfounded as to why developers write applications and then put them up on the Internet for free.  You've found yourself in one of the most lucrative fields in the world.  A business with 99% profit margin, where you have no physical product but can name your price; a business where you can ship a buggy product and the customer will still buy it. Occasionally some of our software will get a free competitor, and I think, this guy is crazy.  He could be making a good living off of this but instead chose to make it free.    Do you not like giant piles of money? Are you not confident that people would pay for it?   Are you afraid of having to support it?  It's bad for the business of programming because now customers expect to be able to find a free solution to every problem. (I see tweets like \"is there any good FREE software for XYZ? or do I need to pay $20 for that\".) It's also bad for customers because the free solutions eventually break (because of a new OS or what have you) and since it's free, the developer has no reason to fix it. Customers end up with free but stale software that no longer works and never gets updated. Customer cries. Developer still working day job cries in their cubicle. What gives? PS: I'm not looking to start an open-source/software should be free kind of debate. I'm talking about when developers make a closed source application and make it free. ",
        "Best answer": "Because I don't want to feel obligated to provide technical support or offer refunds. "
    },
    {
        "ID": "3241",
        "Question": "This question is a little abstract but I'm hoping someone can point me in the right direction. My question is what amount of time can one expect to devote to a software project's bugs in relation to the original development time. I realize there are a huge number of determining factors that go into but I was hoping for a typical or average breakdown.  For example, if Project A takes 40 hours to complete and an additional 10 fixing bugs then this project would have a 4:1 ratio. If another Project (B) takes 10 hours to complete but another 8 on bugs then it would have a 5:4 ratio.  Is this a documented/researched concept? UPDATE Thanks for all the informative answers. I understand that it's impossible to put a standard to this kind of metric due to all the variables and environmental factors involved. Before I assign an answer I'd like to know if this metric has an agreed-upon name so I can do further research. I would like to get to a point where I can understand the measurements necessary to generate the metrics myself and eventually come up with a baseline standard for my project. ",
        "Best answer": "The equilibrium percentage of total capacity allocated to defect-fixing is equal to the defect injection rate. Many factors can affect this rate, among them, of course: what kind of product the team is developing, what technologies and technical practices they use, the team's skill level, the company culture, etc. Considering Team B, if they create on average 8 units of rework for every 10 units of work they complete, then working those 8 units will create new 6.4 units of rework.  We can estimate the total effort they will eventually have to expend as the sum of a geometric progression: 10 + 8 + 6.4 + 5.12 + ... The number of bugs will decrease exponentially with time, but Team B has such a coefficient in their exponent that it will go to zero very slowly.  Actually, the sum of the first three terms in the above series is only 24.4; of the first five, 33.6; of the first 10, 45; of the entire series, 50.  So, Team B summary: defect injection rate, 0.8; feature development, 10/50 = 20%; defect-fixing, 80%.  20/80 is their sustainable capacity allocation. By contrast, Team A is in much better shape.  Their progression looks like this: 40 + 10 + 2.5 + 0.625 + ... The sum of this series is 53 1/3, so Team A's feature development allocation is 40/(53 1/3) = 75% and defect-fixing allocation is 25%, which matches their defect injection rate of 10/40 = 0.25. Actually, all terms in Team A's series after the first three are negligibly small.  What this means in practical terms is that Team A can probably squash all their bugs with a couple of maintenance releases, the second release being pretty small in scope.  This also creates an illusion that any team can do that.  But not Team B. I thought about this equivalence while reading David Anderson's new book, \"Kanban\".  (The book is on a different subject, but addresses quality concerns, too.)  When discussing software quality, Anderson quotes this book, by Capers Jones, \"Software Assessments, Benchmarks, and Best Practices\": \"...in 2000... measured software quality for North American teams... ranged from 6 defects per function point down to less than 3 per 100 function points, a range of 200 to 1.  The midpoint is approximately 1 defect per 0.6 to 1.0 function points.  This implies that it is common for teams to spend more than 90 percent of their effort fixing defects.\"  He cites an example provided by one of his colleagues of a company that spends 90% of the time fixing their bugs. The fluency with which Anderson goes from the defect injection rate to the defext-fixing capacity allocation (failure demand is the term for it) suggests that the equivalence of the two things is well known to software quality researchers and has probably been known for some time. The key words in the line of reasoning that I'm trying to present here are \"equlibrium\" and \"sustainable\".  If we take away sustainability, then there's an obvious way to cheat these numbers: you do the initial coding, then move on to code somewhere else, and leave maintenance to others.  Or you run up the technical debt and unload it on a new owner. Obviously, no particular allocation will suit all teams.  If we decreed that 20% must be spent on bugs, then, if a team has an ultra-low defect injection rate, they will simply not have enough bugs to fill the time, and if a team had a very high rate, their bugs will continue to accumulate. The math I used here is way simplified.  I neglected things like transaction costs (planning and estimation meetings, post-mortems, etc.), which would affect the percentages somewhat.  I also omitted equations simulating sustaining one product and developing another one concurrently.  But the conclusion still stands.  Do what you can, in terms of technical practices, like unit-testing, continuous integration, code reviews, etc., to reduce your defect injection rate and, consequently, your failure demand.  If you can create only one bug for every 10 features, you will have a lot of free time to develop new features and satisfy your customers. "
    },
    {
        "ID": "3450",
        "Question": "Long winding if conditions should be avoided if at all possible, yet sometimes we all end up writing them. Even if it's a very simple condition, the involved statements are sometimes simply very wordy, so the whole condition ends up being very lengthy. What's the most readable way to format those? if (FoobarBaz::quxQuux(corge, grault) || !garply(waldo) || fred(plugh) !== xyzzy) {     thud(); }  or if (     FoobarBaz::quxQuux(corge, grault)  || !garply(waldo)  || fred(plugh) !== xyzzy ) {     thud(); }  or if (FoobarBaz::quxQuux(corge, grault)     || !garply(waldo)     || fred(plugh) !== xyzzy) {     thud(); }  or thudable = FoobarBaz::quxQuux(corge, grault); thudable ||= !garply(waldo); thudable ||= fred(plugh) !== xyzzy;  if (thudable) {     thud(); }  or any other preferences? ",
        "Best answer": "Often, a long if condition is the sign of code that needs refactoring, but sometimes you can't avoid it. In those cases, I prefer the first:  if (bar || baz || quux) { ... }  Because you're able to tell what's going on with one line. However, I'd much rather do something like this, when possible: function foo() {   return bar || baz || quux; }  if (foo()) { ... }  "
    },
    {
        "ID": "3482",
        "Question": "You've shipped, asserts are turned off, you receive a rare crash report indicating that a null pointer violation occurred in your code.  In a development environment, the problem would have been caught by an assert. All you have is a crash report, so reproducing the issue is near impossible.  Following the backtrace doesn't give any clues as to why the crash happened in the first place. Options: - Add pointer checking to prevent the crash.  This will prevent the crash, but you probably won't even find out why it happened in the first place. - let it fly, hope that it happens again with a repro scenario Let's say the application isn't intended for a guided missle or automatic braking system... Which would you choose? ",
        "Best answer": "I chose the second approach. There is no point in hiding the crash if the NULL pointer was unexpected at the point where crash has occured. This NULL pointer in most cases would just be one of the symptom of something else is wrong. If we hide it with a NULL pointer check it is almost certain that something else will break. I feel you have a better chance catching the scenario if you know the point where it crashes everytime instead at some random place. "
    },
    {
        "ID": "3622",
        "Question": "I often develop an application entirely myself. But did I really? I feel strange about that claim and never know when that is true. I mean I designed it, coded it, but I used XYZ plug-in. Can I still claim that I did it all myself even though I didn't create the plugin I used? Consider this conversation?  ME: I designed and developed this app entirely myself. Other: Cool, how did you program XYZ part? ME: I didn't program that part I used XYZ plugin. Other: So you didn't really program it ALL yourself than did you?  I mean if I must give them credit for the plug-in I used, then do I have to give the language authors credit for the language I used and the IDE authors credit as well? Where do I draw the line? This is just something that always crosses my mine as soon as I am about to take full credit for a project, and was wondering others opinions on the matter. ",
        "Best answer": "That depends on what \"XYZ part\" does.  Most code libraries are pretty modular things that take care of one simple task.  Even if someone else designed that, you're the one who integrated it into the overall vision of what your program is supposed to do. Unless the third-party code takes care of a major part of the application logic itself--for example, building a video game on top of an existing engine--it's just a component.  Saying that you didn't really build the program yourself because you used third-party controls is like saying that you didn't really build the house yourself because you bought premade nails, boards, drywall, wiring, etc... "
    },
    {
        "ID": "3713",
        "Question": "With parallel algorithms knocking at the door, it might be a good time to think about error handling. So at first there were error codes. Those sucked. It was free to ignore them, so you could fail late and produce hard-to-debug code. Then came exceptions. Those were made impossible to ignore once they occur, and most people (except Joel) like them better. And now we got libraries that help parallel code. Problem is, you can't handle exceptions in parallel code as easily as you could with non-parallel code. If you asynchronously launch a task and it throws an exception, there's no stack trace past it to unwind; best you can do is capture it and register it on the task object, if there's such an object. However, it defeats the primary strength of exceptions: you have to check for them and you can ignore them without any additional effort, whereas in single-threaded code an exception will necessarily trigger the appropriate actions (even if it means terminating your program). How should language implementations or libraries support errors in parallel code? ",
        "Best answer": "I'm rather fond of callbacks for errors that can be handled. And they can be made to work just fine asynchronously... But for errors that can't be handled, truly exceptional errors, I'd rather see the relevant information saved and the program terminated. Since this is usually accomplished via some sort of global error-handler anyway, I see no need to twist exceptions into something that works for this - but better platform support for detecting critical errors and producing memory dumps, etc. would be nice. "
    },
    {
        "ID": "3824",
        "Question": "I've been involved with many projects in several companies because I've been a developer for a long time and I'm a contractor. I estimate that less than 20% of projects are methodically tested. With methodically tested I mean any testing beyond ad-hoc no plan testing. I also estimate that less than 10% of projects are thoroughly methodically tested where they have dedicated testers as part of the team, test plan document, where developers write automated tests and then they also track test coverage and measure results.  Two questions  What are your percentage estimates about this issue? What's your professional experience regarding software testing?  Additional note Since methodical testing question may get quite biased answers (people like to brag about being superior to others) I encourage other developers (those that're not exposed to methodical testing) to provide their answer as well, because otherwise it will look like testing is being done everywhere... except at your company. ",
        "Best answer": "The pattern I have seen with testing over my career shows a strong correspondence with the risk of failure in a project. Big projects are more likely to be tested than small ones, mission critical applications are more likely to be tested than one off marketing web sites, in house systems are less likely to be tested than public facing ones. That said there are still projects that have been excessively tested and those that have not been tested enough, but these are the minority. "
    },
    {
        "ID": "3853",
        "Question": "I'm doing my first big project and I don't have a lot of experience in a professional programming environment. While researching anything programming-related I often see references to Unit Testing, but I am still unclear as to how to set those up or even if it would be beneficial to me.  Can someone explain unit testing to me, and how to set it up in a Visual Studio 2010 solution that has multiple projects? Is it something that occurs within your project's solution, or is it a separate solution? And is it something you'd recommend for a small development team or is it just a waste of time to setup?   Right now I just run the entire program to test whatever I am currently working on, but occasionally I have run into problems that are not easy to debug and it would be useful to run subsets of the code elsewhere... Sometimes I do setup another project with some of the libraries referenced to test a small part of the program, but I feel more time is wasted setting that up then just running the entire program because of all the dependencies involved ",
        "Best answer": "Setting up unit testing in VS2010 is fairly easy.   Add a new project to your solution and select the Test Project template from the New Project dialog box.  Usually you create one test project for project you want to test in your solution (ie. BusinessLogic.proj would have a mate called BusinessLogicTest.proj, just as an example). Once you have the project added, the easiest way (not necessarily the best way to do this, especially if you're goal is to get to TDD) to get started is to right-click on a function that you want to test and select \"Create Unit Tests\".  Make sure the Output Project is the Test Project you just created and click OK.  VS creates a new Test Class for you with a Test Method stubbed out. That's the basics of getting setup.  There's a lot to learn to test effectively, but you're at least on the right path by wanting to unit test.  I'll see if I can find a demo or article to get you a bit further. EDIT Found one.  This article is by Deborah Kurata (an MS MVP) and she has both C# and VB code samples.  She is using VS2008, but the basics are the same for VS2010 "
    },
    {
        "ID": "4107",
        "Question": "I was browsing some old code that I wrote (first year in the university) and noticed that I used to write comment titles preceeding various parts of the code. Stuff like (this is from a Monopoly game):  /*Board initialization*/ ...code...  /*Player initialization*/ ...code...  /*Game logic starts here*/ /*Displaying current situation*/ ...code...  /*Executing move*/ ...code...  /*Handle special event*/ ...code...  /*Commit changes, switch to next player*/ ...code...  This might be redundant, and arguably unnecessary if the code is really super clear, but as I scanned through the file it surprised me how strongly I felt like I know what's going on even though I hardly looked at the actual code. I can definitely see this as being fitting in certain circumstances, so I wonder- do you do this? Do you think it's a good idea? Or is it too much?  ",
        "Best answer": "This is a code smell. This says what and not why. If this is necessary, split the code in small functions. "
    },
    {
        "ID": "4112",
        "Question": "If you were to run across code like this in your travels would the short names bother you? let setPropertyAssignment =      parser {         let! a = setIdentifierName         let! b = propertyName         let! c = openParenthesis          let! d = propertySetParameterList         let! e = closeParenthesis          let! f = openCurlyBracket          let! g = functionBody         let! h = closeCurlyBracket         return Node(NodeType.PropertyAssignment, [a;b;c;d;e;f;g;h])     }  Here is the largest parser in the project: let iterationStatement =      parser {         let! a = doKeyWord         let! b = statement         let! c = whileKeyWord         let! d = openParenthesis         let! e = expression         let! f = closeParenthesis         let! g = statementTerminator         let symbols = [a; b; c; d; e; f; g]         return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = whileKeyWord         let! b = openParenthesis         let! c = expression         let! d = closeParenthesis         let! e = statement         let symbols = [a; b; c; d; e]         return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! c = maybeOne expressionNoIn         let! d = semiColon         let! e = maybeOne expression         let! f = semiColon         let! g = maybeOne expression         let! h = closeParenthesis         let! i = statement         let symbols = [a;b] @ c @ [d] @ e @ [f] @ g @ [h;i]                       return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! c = varKeyWord         let! d = variableDeclarationListNoIn         let! e = semiColon         let! f = maybeOne expression         let! g = semiColon         let! h = maybeOne expression         let! i = closeParenthesis         let! j = statement         let symbols = [a;b;c;d;e] @ f @ [g] @ h @ [i;j]         return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! c = leftHandSideExpression         let! d = inKeyWord         let! e = expression         let! f = closeParenthesis         let! g = statement         let symbols = [a;b;c;d;e;f;g]                        return Node(NodeType.IterationStatement, symbols)     }  +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! a = varKeyWord         let! c = variableDeclarationNoIn         let! a = inKeyWord         let! c = expression         let! d = closeParenthesis         let! e = statement         let symbols = [a; b; c; d; e]         return Node(NodeType.IterationStatement, symbols)     }  ",
        "Best answer": "Depends on the complexity.  With something this size, it's pretty obvious what you're doing and why.  Make it much bigger--a page or so--and it'll get very confusing very quickly. "
    },
    {
        "ID": "4250",
        "Question": "I'm sure many of us have seen method names like this at one point or another:  UploadTheFileToTheServerPlease CreateATemporaryFile WriteTheRecordToTheDatabase ResetTheSystemClock  That is, method names that are also grammatically-correct English sentences, and include extra words purely to make them read like prose. Personally, I'm not a huge fan of such \"literal\" method names, and prefer to be succint, while still being as clear as possible. To me, words like \"a\", \"an\", and \"the\" just look plain awkward in method names, and it makes method names needlessly long without really adding anything useful. I would prefer the following method names for the previous examples:  UploadFileToServer CreateTemporaryFile WriteOutRecord ResetSystemClock  In my experience, this is far more common than the other approach of writing out the lengthier names, but I have seen both styles and was curious to see what other people's thoughts were on these two approaches. So, are you in the \"method names that read like prose\" camp or the \"method names that say what I mean but read out loud like a bad foreign-language-to-English translation\" camp? ",
        "Best answer": "I'll agree that prose methods suck with one exception: Unit Test Cases These are generally never called in your code and show up in test reports.  As such, it's handy to have readouts with a bit more prose:  AddingACustomerOrderFailWhenCustomersIdIsInvalid : Failed OutOfBoundsPriceReturnsAnError : Passed CanDeleteAnEventFromASeason : Passed  Even this should be done sparingly, but I can see it as at least one case where grammatical additions can make it a little easier to express what passed and what failed.  This is, of course, unless your language/framework provides a good mechanism for test-descriptions in the test readout other than method names, in which case ignore this one too. "
    },
    {
        "ID": "4267",
        "Question": " Basically I am looking for what is it that you goof up and you are out from the remaining process ? Are elimination rounds a fair way to judge a person ? Anyone can have a bad hour :-(  Should you code the best possible or should you get the Algorithm right ? I generally first code a workable solution and then work on it till a level I think it looks beautiful to me. Is this a wrong approach ?  Recently I had a telephonic interview in which I was asked to write a variation of Level Order traversal in 20 minutes. I could get the Algorithm and working code in 20 minutes but couldn't get the Exception handling and the coding convention right, after which I didn't hear back from them :-( All the other questions in the interview went very well this was the only thing which was 'not upto the mark'. Needless to say I like the company and will apply again but want to get it right the next time :-) Please share your experiences and valuable suggestions. ",
        "Best answer": "When doing technical interviews, I'm honestly looking for people to hit a home run. If the candidate doesn't look like they know their stuff, they're not going to be effective in their role (I'm talking senior developers positions here). Look at it this way: Would you rather have a tough interview where you don't get the job (because you're not a good fit), or an easy interview where you do get the job, but then get let go after 90 days because you're in over your head? I've seen far too many developers in the latter camp. If you didn't get the job because you flubbed the technical part, consider it a blessing in disguise. If you don't like rejection, bone up on your technical skills. "
    },
    {
        "ID": "4274",
        "Question": "I'm an ASP.Net/C# programmer using SQL Server as a back end. I am the Technical Director of the company, I'm extremely happy in everything I do and consider the languages and system we use to be perfect for what we do. In the back of my mind though I know that over time programmers can become stale. I remember as a wee youngster that all those \"old\" developers were past it and couldn't keep up with the youngsters. So considering I'm happy in everything I'm doing. What options are there for keeping up with everything and avoiding becoming stale. One particular idea that I use is to let all the new developers use and showcase the things that they think are cool. If anything catches my eye then absolutely it will be something we all use going forward. Thoughts? ",
        "Best answer": "Learning many languages gives you different insights - different tools - into problems. I think it's very important to learn several very different languages. Maybe a functional language (Scheme, Haskell), a object-oriented one (Smalltalk, Ruby), a concurrency-oriented one (Erlang), a logic programming one (Prolog) and and and. The important thing here is that the languages shouldn't be more of the same. If you know C#, learning Java isn't going to teach you that much. If you know C, learning Pascal isn't going to expand your intellectual horizons. "
    },
    {
        "ID": "4399",
        "Question": "How much should programmers help testers in designing tests?   I don't think they should help at all.  My worry is that if they help testers in designing tests for their own code, they'll 'infect' the testers with their own prejudices and blind spots about that code.   I feel that the requirements should be sufficient for giving the information needed for testers to create their tests.  If there is some part of the implementation that the programmers find worrisome, then I think it's their duty to implement unit tests to test that part or even run their own informal system tests to test that part. Not everyone I know agrees with this though (and I understand some of their points to a certain extent).  What do others think about this?  Is this discussed in the literature anywhere? ",
        "Best answer": "I agree. Programmers can help the testers to understand the functional specs, to find resources for research but should not pollute the testers' minds with their own ideas about how to approach testing. "
    },
    {
        "ID": "4418",
        "Question": "While hacking on some static analysis tool, I realized I could make the task at hand (irrelevant for this question) much easier by saving the source files on-the-fly, while they were being edited. This seemed weird at first, but being a lazy cat I actually did it, and it turns out I find this pretty useful :     no need to check if all files were saved before running a compiler/interpreter forces you to make granular commits much more regularly  My editor has been behaving this way for a couple of days, I never had such a clean commit history, and didn't get burned yet.  In the days of DVCS when by saving we actually mean commit, do you think that manually saving files is still a relevant practice ? ",
        "Best answer": "I often use the opportunity to keep editing the file while the build is in progress.  If they were auto-saved, the build would break while I'm editing (a broken code would be complied), and I'd just have to wait while the code is compiling.  This is unproductive and boring. "
    },
    {
        "ID": "4475",
        "Question": "I have several projects coming up soon for public release, both commercial and open source. The projects are all downloadable, not web apps. I've worked on them alone and would like to get feedback during a beta period, but I don't currently have a large audience (though the markets are large). What are the best ways to get participation in the betas? Are there any existing sites or communities that specialize in software testing that I can reach out to? At this point, I'm specifically looking for technical testers who aren't intimidated diving into the code and can help spot security bugs, logical errors, etc. Edit: I'm looking for websites or communities similar to Invite Share. Invite Share itself would be perfect, but there doesn't seem to be any public information about how to submit a beta. Bounty Explanation: While Joel's article on running a beta is helpful, I wonder if there isn't an existing community available for beta testing of any sort, technical or user. As a self-taught and sole developer, I don't have a lot of technical contacts that would be appropriate approaching for testing. I did propose a Beta Testing site in Area 51 a few months ago, but it seems as if it either got buried, there wasn't a whole lot of interest, or it's a poor fit for StackExchange. If you know of existing testing communities, sites like InviteShare, or other ways to get testers, please share. ",
        "Best answer": " What are the best ways to get participation in the betas?  Joel (on Software) has an excellent article on this: Top Twelve Tips for Running a Beta.   Are there any existing sites or communities that specialize in software testing that I can reach out to?  I don't think there will be much \"We Test Your Code\" sites so you will have to start a webpage yourself and advertise it to the right audience...  At this point, I'm specifically looking for technical testers who aren't intimidated diving into the code and can help spot security bugs, logical errors, etc.  This seems more like a job description that you need technical testers in your company and is less like beta testing, perhaps it can still be alpha testing... But those aren't always technical either. Test-Driven Development helps you prevent bugs and errors, thinking about the security can help too... "
    },
    {
        "ID": "4614",
        "Question": "You know who they are.  They are the rock stars of programming:   They code 10X faster. Their code just works. They not only know their primary language inside and out, but they also know how it works under the hood. They know the answer to most any question before you ask it. A few of them invented the programming principles we all use. And they tend to be  uncharacteristically humble, as well.  What is it about these folks?  Is there something about their thought process that is fundamentally different from the above-average programmer?  Or are they simply very talented people that work hard? To put it another way: How can I be like them?  I know what I think I need to learn to be that good, but it seems like it will take me the next ten years to learn it, and then my knowledge will be obsolete. ",
        "Best answer": " Humble: An exceptional programmer will never claim their code is the best, in fact they will always be looking for a better way (Every chance they get.). Patient: An exceptional programmer will have boundless patience (This does not mean they will waste days on a problem. See: Troubleshooter). Troubleshooter: An exceptional programmer will be able to solve a problem in minutes that may take days for your average programmer. Curious: An exceptional programmer will be unable to resist trying to figure out why something occurs. Engineer: An exceptional programmer will engineer systems rather than hobble together a mishmash of frameworks (This does not mean they won't use frameworks.).  "
    },
    {
        "ID": "4654",
        "Question": "I'm tired of how luggish my developments PC is. It's Core2 Duo, 2GB RAM, Seagate ST3500320AS HDD - not the top model, but quite a decent one. Typically I open several copies of Visual Studio 2008, lots of tabs in Firefox, Outlook, MSDN, plus the programs I debug are quite huge, plus whatever Windows thinks it can't live without so I end up with Task Manager showing something like 2,5 GB pagefile usage. All the software above becomes luggish to such extent that it's really annoying. Something like I click on a menubar in Visual Studio - and instead of just opening the menu it works the harddisk for say 10 seconds. I'd like to have some magic \"don't make me think\" solution - so that it is installed once and then the lugs disappear or at least decrease significantly. It should not be very expensive - something like the current price of a hybrid drive. Will a hybrid drive magically help overcome my problem once and for all? Do you have experience using hybrid drives for similar purposes? ",
        "Best answer": "It sounds more to me like getting more RAM in your machine would be the best thing you can do.  "
    },
    {
        "ID": "4714",
        "Question": "It seems to me that rapid-development web platforms are going to radically change the world of web applications. It has been five years since Rails 1.0 was released for Ruby, and since that time we have seen Grails for Groovy, Django for Python, and Roo for Java. But to my knowledge (which is probably limited, being a Java/Groovy progammer) there is no similar framework for C#. Does such a thing exist?  If not, why not? Edit:  It's quite possible I'm not using the right words when I say \"rapid-development,\" but I'm talking about frameworks that can conceivably allow you to build a working blog engine in 30 minutes.  You couldn't reasonably do this with, say, Java, Spring, and Hibernate, given the various configuration needed to allow your controllers to be found, and both configuration and code necessary for your entities to persist and be retrieved.   So I'm talking about frameworks that handle all of the CRUD with a convention-over-configuration mentality.  If someone has the right words for what I'm talking about, let me know. ",
        "Best answer": "It seems to me that there is no name yet for this kind of framework you are all talking about in this thread. I call them for the moment just RAILS-like Frameworks: Frameworks that increment productivity by orchestrating other existing frameworks with the purpose of solving the basic needs of most web applications, but at the same time hiding all the complexities from the developer. By basic needs I mean the implementation of a Persistence Provider, a Dependency Inyection Container, a Logging tool, an MVC platform, an HTML Template Engine, a Web Site Template Starter Kit with CSS presets, a Security Framework and some Javascript Library for AJAX features and other cool stuff. The RAILS-like Frameworks orchestrate all these frameworks and tools on the basis of the Domain model (the entities of your system with its attributes). Thanks to the Convention-over-Configuration principle, these frameworks avoid the need of defining lots of configuration files usually required by the frameworks they orchestrate (like Spring, Spring MVC, Hibernate, Log4J, etc.), assuming configurations by default based on naming, structure and metadata included in the same classes definitions. Thanks to the dynamic languages that these frameworks make use (like Ruby, Groovy, Python, Clojure, etc.), with the exception of SpringRoo that implements dynamic behaviour in Java by using AspectJ, the functionality that belongs to the frameworks underneath are extended and are made available to the developer in such a uniform and elegant manner that he/she is just aware of the underlying technologies. Finally thanks to the Scaffold technique, unit tests, integration tests, controllers and views are automatically generated for the main functions (CRUD) over each one of the domain objects defined by the developer. In the .NET world nothing has been developed yet, following all the previous definitions. But nothing prevents that from occurring soon. There are great frameworks, tools and libraries already available in the .NET world that can be orchestrated by a new RAILS-like framework made for the CLR. There are Unity, Spring.NET and Castle Windsor among others for the Dependency Inyection needs. Entity Framework 4, NHibernate and iBatis.NET are pretty good .NET Persistence Providers. ASP.NET MVC have strongly arrived with support for various Template Engines besides the traditional ASP.NET. Even if nobody achieves to use a DLR language to build this kind of framework, anyone with enough will can follow the SpringSource path and implement a RAILS-like framework with some static language like F#, C# or VB.NET, making use of an Aspect-Oriented Container (like AspectSharp or Gripper-LOOM.NET) to obtain dynamic behaviour. I would love to know about any group of people trying to develop such framework in .NET. "
    },
    {
        "ID": "5119",
        "Question": "I'm freelancing on a project where I'm the only programmer, and find myself at the end of a line of four middlemen, who stand between me and the actual customer, each passing my work as internal to their own company. Communication is terrible and the requirements, made by an advertising company, are flimsy. I've managed to communicate with people upper the ladder by keeping asking questions that made people face their ignorance, but they won't let me contact the end client since, from his end, it's pretty much a done deal.  The project will soon be over though, and I've decided it's the last time I'll be working under these conditions. The middlemen, are pretty much useless from the perspective of shipping a product, but still necessary to me since they are the ones bringing the contracts in. Hence I'm not thinking about crossing them altogether, which would probably end badly. Rather I'm looking for a way to make them understand I need to be part of the requirements and design process, meet the clients, and shouldn't have to go through a whole channel of clueless people each time I require some information. Sorry for the venting :) Any ideas ? ",
        "Best answer": "Sell the middlemen on giving some progress demos to the client then lead the client into some of the issues that you are facing during the demo. "
    },
    {
        "ID": "5120",
        "Question": "Let me explain this a little. In a previous job, I had a coworker that has a good reputation with the management. He always finished on time. And the bosses were happy with his progress so het got certain privileges. The problem was that the other programmers knew his secret. He has optimized the 80/20 rule, so he worked his 20 percent time to finish 80 percent of the code. The other (hard) 20% was left to the maintenance programmers. Who (not surprisingly) got penalized because of their lack of progress. But because this programmer had a good reputation with the management, it was almost imposible to shift the blame to him. (Fortunately he left the company). My question is, what to do as programming team if you have such a programmer within your team. Do you try to warn management with risk of ruining your own chances? Do you accept the fact? Or are there other options. ",
        "Best answer": "Try to implement a code review team. It sounds like this programmer was working solo on a project with no team interaction. I'd try to encourage a more team-based workflow so that he can't just stomp over everything and then leave it on your door. "
    },
    {
        "ID": "5225",
        "Question": "Suppose I develop a useful library and decide to publish it as open source. Some time later I have a business need to do something that wouldn't comply with the open source licence. Am I allowed to do that? How should I publish the software in a way that I keep ownership and don't block myself from using the library in the future in any way? Keep in mind that at least in theory, other developers may decide to contribute to my open-source project. Can I specify in a licence that I as the original developer get ownership of their contributions as well? Don't get me wrong here, I'm not trying to be evil and get ownership of other's work - I just want to keep ownership of mine, and if someone posts an important bugfix I could be rendered unable to use the original code unless I use his work as well. ",
        "Best answer": "You always keep ownership under open-source licenses.  The work you created is your property, and you can do whatever you want to with it, (within legal limits, of course,) including allowing other people to use it under the terms of an open-source license.  If you want to use it for a proprietary project, you're welcome to do so, unless you have completely turned over the rights to someone else by contract.  But this is not what open-source licenses do.  They're about sharing usefulness, not about giving up ownership. Things get a bit sticker once other people start contributing.  It's their work, then, not yours, and you need to get their permission.  One thing you can do is publish your library under a dual license.  That's what Sam Lantinga, the primary creator and maintainer of SDL, does.  Because Apple doesn't like dynamic link libraries for iOS, and complying with the LGPL in a statically linked app is more trouble than it's worth, he publishes SDL under both the LGPL and a commercial license for static iPhone apps.  When anyone submits a patch, he explicitly asks them for permission to deploy their patch in the library under both licenses, and if they don't like that, he doesn't add it to the codebase. EDIT: My example is no longer accurate.  A while back Sam changed the model (not sure why; maybe he just got tired of the administration hassles) and now licenses SDL under a highly permissive zlib-style license.  But he used to do it this way. "
    },
    {
        "ID": "5297",
        "Question": "I've only been a year in the industry and I've had some problems making estimates for specific tasks. Before you close this, yes, I've already read this: How to respond when you are asked for an estimate? and that's about the same problem I'm having. But I'm looking for a more specific gauge of experiences, something quantifiable or probably other programmer's average performances which I should aim for and base my estimates.  The answers range from weeks, and I was looking more for an answer on the level of a task assigned for a day or so. (Note that this doesn't include submitting for QA or documentations, just the actual development time from writing tests if I used TDD, to making the page, before having it submitted to testing) My current rate right now is as follows (on ASP.NET webforms):  Right now, I'm able to develop a simple data entry page with a grid listing (no complex logic, just Creating and Reading) on an already built architecture, given one full day's (8 hours) time.   Adding complex functionality, and Update and Delete pages add another full day to the task. If I have to start the page from scratch (no solution, no existing website) it takes me another full day. (Not always) but if I encounter something new or haven't done yet it takes me another full day.  Whenever I make an estimate that's longer than the expected I feel that others think that I'm lagging a lot behind everyone else. I'm just concerned as there have been expectations that when it's just one page it should take me no more than a full day. Yes, there definitely is more room for improvement.  There always is. I have a lot to learn. But I would like to know if my current rate is way too slow, just average, or average for someone no longer than a year in the industry.  ",
        "Best answer": "If you're programming for a job, and your superiors are happy with the rate you're turning stuff out at, then I'd say you're doing fine.  As you've lasted a year, they're clearly not outraged with your output.  Also, you've only been there a year, and assuming they've been managing people for more than a day, they know that there's a learning curve when you're still green. As for estimates... I've been in the industry for 5 years now (certainly not veteran territory, I know!), and my personal estimates still suck.  I overestimate almost as often as I underestimate, and I do both far more than I get it right.  Something will come up, somewhere, and bite you.  Sometimes you'll find a library that does everything you thought you had to do yourself, and a week's work disappears in half a day.  Other times a stupid bug will stretch a day's work out to 2, 3, 4... If you're repeating a lot of the same work over and over, and you feel like you've maxed out your throughput on it, maybe you should ask to be moved to another task. 'Cross-pollination' and other PHB-friendly terms are definitely of benefit to devs.  If you spend a month or more on something else, maybe you'll find something you're better suited to.  If not, or you're not able to stay away from webforms, the change won't do you any harm, and you might come back with a bit more knowledge and experience that will help you. "
    },
    {
        "ID": "5331",
        "Question": "So you take a contract where you have solid experience with 75% of the technology necessary.  How do you handle your time to learn the other 25%? Work it into the billing time?  Expose the 25% in the contract as 'research'?  Do the learning on my own time (not billed)?  Not take the contract (too large of an unknown for me and the customer)? On the extreme end of this, I keep hearing a story about Mark Cuban (Dallas billionaire who started broadcast.com and sold it to Yahoo!) when he was at Indiana University.  Someone asked him if he could build a business app for them and he immediately said \"Yes\"... he had no idea how.  So he bought a book, stayed up nights, studied and coded... He finished it (I'm sure it was ugly), it worked and he kept going. I'm not suggesting doing contracts this way (the stress!), but there's a middle ground.  What is it, and how would you (or would you?) bill for the unknown? ",
        "Best answer": "If I'm learning something that I'll take away with me (like say a mainstream new API, or a new feature of .NET or a language that's somewhat useful) then I don't bill, I consider that time time spent sharpening my saw, and it's not the client's fault I didn't know that stuff yet. Now, if it's something obscure, I bill for it at my normal rate.  Some examples:  APIs and protocols which are not mainstream (industry specific, small 3rd party or just niche products); internal tools, configuration formats and services inside the client organization; a non-standard database schema, database query language or security model; etc. I've never had any objections about the way I do this, and I'm very transparent about it in my proposals. "
    },
    {
        "ID": "5356",
        "Question": "When you are setting up your system landscape for large and/or multiple application deployments, do you consider mainframe?  If not, why not?  If so, what factors are you considering. If you take a real TCO look at large ERP and/or consolidated application landscapes, mainframe is actually quite cost-effective. My own consultations have included recommendations for scale-up/mainframe/mid-size systems on some specific needs.  Honestly, I've never had a customer take said recommendation, rather defaulting to countless scale-out VMs on Intel boxen (in non-trivial cost) and yet still to this day have system management and performance issues. Curious your take on this.  We need to remember that the virtual machines we manage (and apparently love in IT departments) today have been done for decades on mainframe.  Most mid-size and mainframe shops have small fractions of support persons managing larger and more complex applications. Your thoughts appreciated. ",
        "Best answer": "It seems to me that you're doing more to express your opinion (\"If you take a real TCO look at large ERP and/or consolidated application landscapes, mainframe is actually quite cost-effective.\") than really ask a question. On that basis, I'm tempted to vote to close, but won't. As for the question (to the extent there is one), I'm going to assume that by \"mainframe\", you mean something like an IBM z series machine, rather than (for example) one of the big Fujitsu SPARC boxes. I think for many people, it's hard to recommend mainframes for a couple of reasons.   Every vendor has TCO numbers to \"prove\" their product is the best. Why should somebody trust numbers from IBM more than from Microsoft, Oracle, etc? Even if a mainframe really would make sense, such a recommendation is unlikely to be taken seriously, and whoever made such a recommendation will often be treated as a pariah. Even if the TCO would theoretically work out better in some respects, introducing machines with which existing staff are thoroughly unfamiliar would often be a mistake anyway. Using a mainframe would often lose versatility in other ways. Just for example, an Intel box can easily run Windows Terminal Server to make Word and Excel available, which is a bit tougher to do with a mainframe.  "
    },
    {
        "ID": "5415",
        "Question": "Frequently, I have been finding myself overloaded with contracts.  Most of the time, I find myself juggling with at least 2 projects, in addition to the numerous websites I have to upkeep and perform maintenance on.  Unfortunately, many of my clients will expect updates constantly - are constantly adding more to the to-do list than any one programmer could keep up with, and freaking out because the deadline was already overdue when I started on a project.  I constantly run into the fact most clients do not really understand the amount of work that can be involved behind the scenes, especially if it is non-visually-impacting. Does anyone know of good ways to handle these situations I might be overlooking? ",
        "Best answer": "Charge more. Learn to say no. Get some help. They're freaking out because they don't know what they are doing and are trying to motivate you. Every feature 'must' be included. They're all #1 priority. And they were due yesterday. Basically their boss is on their case. Take control of the project and bring some sanity to the situation. Start with a small piece of the project. Make a plan. Set a time-frame for your development and for their part (reviews, testing, approval, etc.). When they want to change the plan, just ask them, \"What other part should I remove or how far should I backup the due date?\" Remind them that this is what you agreed on and you don't mind making changes, but something has to give. This should help create a history of what they should expect from you in the future. So far, you haven't mentioned that anyone is trying to dump you. You must be doing something right or you found clients no on else wants. Maybe you could dump the clients and stick it to your competition ;) Edit: The art of saying no is in your mind you're saying no, but don't actually use the word. Features, Time, and resources are a constant compromise. It is important to let the client know the problems and don't just assume they will expect the form to take longer to load when you add 50 more fields. "
    },
    {
        "ID": "5452",
        "Question": "I've enjoyed a number of (fiction/non-fiction books) about hacker culture and running a software business in the 80's, 90's. For some reason things seemed so much more exciting back then. Examples are:  Microserfs (Douglas Coupland) Accidental Empires (Robert X. Cringely Almost Pefect (W.E. Peterson, online!) Coders at Work (Peter Seibel)  Today I'm an entrepeneur and programmer. Back in the 80's a I was a young geek hacking DOS TSR's and coding GWBasic / QBasic. In the 90's I was a C.S. university student, experiencing the rise of the Internet world wide. When reading these books running a software business seemed so much more fun than it is nowadays. Things used to be so much simpler, opportunities seemed to be everywhere and the startups seemed to work with much more real problems (inventing spreadsheets, writing word processors in assembly on 6 different platforms) than all our current web 2.0 social networking toys. Does anyone share these feelings? Does anyone have any good (personal) stories from back then or know of other good books to read? ",
        "Best answer": " In no particular order:  The Fugitive Game: Online with Kevin Mitnick The Cuckoo's Egg: Tracking a Spy Through the Maze of Computer Espionage  Insanely Great: The Life and Times of Macintosh, the Computer That Changed Everything Where Wizards Stay Up Late: The Origins Of The Internet  CYBERPUNK: Outlaws and Hackers on the Computer Frontier The Watchman: The Twisted Life and Crimes of Serial Hacker Kevin Poulsen  Takedown: The Pursuit and Capture of Kevin Mitnick, America's Most Wanted Computer Outlaw-By the Man Who Did It  Geeks: How Two Lost Boys Rode the Internet Out of Idaho  Soul of a New Machine - about DEC and one of their products in development. The Hacker Crackdown: Law and Disorder on the Electronic Frontier by Bruce Sterling tells the story of the 'meeting' of law enforcement and the cracker/phreaker subculture of the 1990s. Also, it describes in detail the Secret Service raid on Steve Jackson Games. That little incident almost put SJG out of business, all for a role-playing supplement not, as the Secret Service described it, a \"hacker's manual\".  Turns out that the Secret Service were actually after copies of a leaked Bell South E911 document. Programmers at Work - This likely inspired the Founders at Work and \"Coders at Work\" books. Free as in Freedom: Richard Stallman's Crusade for Free Software by Sam Williams.  It is a free biography of Richard Stallman, and contains lots of stories of programming culture at MIT.  If you're interested in the FSF and how the whole free software movement started this is worth a read. Hackers: Heroes of the Computer Revolution by Steven Levy (Wikipedia page on Hackers)  \"describes the people, the machines, and the events that defined the Hacker Culture and the Hacker Ethic, from the early mainframe hackers at MIT, to the self-made hardware hackers and game hackers.\"  Show Stoppers  Startup Founders at Work - interviews with startup founders, starting from the early 80's. It's more about how the founders built up their companies, but it has interesting insights into the programming culture prevalent then as well. The case of IBM 386 PC - A detective story for techies.  "
    },
    {
        "ID": "5494",
        "Question": "We're developing a new project that is to be deployed on a large number of client sites. The project contains a web-based GUI as one of it's \"access points\". Speed of the web interface is a priority for this project, second only to security. In the past, we've always created \"web sites\" in Visual Studio, which, when published, results in one dll and one aspx file for each page within the system. However, I am aware that you can actually create a \"web application\" and have it compile everything down to a single dll. To me (based on no real data, just gut feeling), compiling the site as a single dll sounds like it would be better for both security and speed (if only marginally). What are the considerations we should look at, and are there any obvious pitfalls we should be aware of when choosing the method that's right for us? ",
        "Best answer": "If it is a large application, there should be natural areas of division in your business logic (software tiers, if you will) which can go in their own DLL's.   It would be nice if the core website functionality could go into a single DLL.  It eases deployment concerns, and is a natural unit anyway. One DLL per page seems excessively granular. "
    },
    {
        "ID": "5749",
        "Question": "I work as the back-end developer, front-end developer, systems admin, help desk and all-around 'guy who knows computers' at a small marketing company of about 15 people.  I was wondering if others could share their experiences flying solo at companies that aren't necessarily inclined toward the technology industry.  I originally took the job in order to transition from front-end developer/designer to full-time coder. It's been a good experience to a point. I definitely get to occupy the role of 'rock star' programmer - because frankly, no one really understands my job. Lately, it feels like a very solitary position. I rarely get to bounce ideas off of people, and everyone looks to me like I have magic powers that will make all the computers work and land us first on Google searches. I've also felt a strong disconnect versus between what we say we want (projects with large, months-long development schedules) versus what we actually do (copy-edit our sites over and over). So who else finds themselves being the 'tech guy' in a company that thinks technology is all a bit magical, and what is your take on your situation? ",
        "Best answer": "Take advantage of the situation you have - to a certain extent, I think you have a little bit of \"grassisgreeneritis\".  Sorry, I'm not trying to be funny.  What I am saying is every position at every company has short-comings.  Yours are starting to get to you more because they are very familiar.  But, at tech companies, schedules and time commitments become an issue.  At larger non-tech companies, overcoming political stupidity and procedure can be big issues. So take advantage of what you have now; learn what you can.  Once you believe you can't really learn more, it is probably time to move on.  There is no harm in that; it sounds like you are one of those people that have to grow to be happy with a job. Your current company should understand that when you reach that point and honestly, if they don't, leaving is definitely the right thing to do. Having said all that, there is more you can do in your current position.   If you are feeling solitary, make some changes to eliminate that feeling.    Use on-line communities to bounce ideas off of people (StackOverflow is great for this). Do some research with Google to find out what it would take to land your company first and then put a proposal together to get it to happen.   When going through projects, take the initiative and change how things happen.  Don't go for the impractical, long projects. Instead, propose month long incremental improvements.  Over a year, those add up and can really help you feel like you've accomplished something.   "
    },
    {
        "ID": "5757",
        "Question": "I have found that there are only 3 ways to unit test (mock/stub) dependencies that are static in C#.NET:  Moles TypeMock JustMock  Given that two of these are not free and one has not hit release 1.0, mocking static stuff is not too easy. Does that make static methods and such \"evil\" (in the unit testing sense)?  And if so, why does resharper want me to make anything that can be static, static? (Assuming resharper is not also \"evil\".) Clarification: I am talking about the scenario when you want to unit test a method and that method calls a static method in a different unit/class.  By most definitions of unit testing, if you just let the method under test call the static method in the other unit/class then you are not unit testing, you are integration testing.  (Useful, but not a unit test.) ",
        "Best answer": "Looking at the other answers here, I think there might be some confusion between static methods that hold static state or cause side-effects (which sounds to me like a really bad idea), and static methods that merely return a value.   Static methods which hold no state and cause no side effects should be easily unit testable.  In fact, I consider such methods a \"poor-man's\" form of functional programming; you hand the method an object or value, and it returns an object or value.  Nothing more.  I don't see how such methods would negatively affect unit testing at all. "
    },
    {
        "ID": "5951",
        "Question": "A well-tested codebase has a number of benefits, but testing certain aspects of the system results in a codebase that is resistant to some types of change. An example is testing for specific output--e.g., text or HTML. Tests are often (naively?) written to expect a particular block of text as output for some input parameters, or to search for specific sections in a block.  Changing the behavior of the code, to meet new requirements or because usability testing has resulted in change to the interface, requires changing the tests as well--perhaps even tests that are not specifically unit tests for the code being changed.  How do you manage the work of finding and rewriting these tests? What if you can't just \"run 'em all and let the framework sort them out\"? What other sorts of code-under-test result in habitually fragile tests?  ",
        "Best answer": "I know the TDD folks will hate this answer, but a large part of it for me is to choose carefully where to test something. If I go too crazy with unit tests in the lower tiers then no meaningful change can be made without altering the unit tests.  If the interface is never exposed and not intended to be reused outside the app then this is just needless overhead to what might have been a quick change otherwise. Conversely if what you are trying to change is exposed or re-used every one of those tests you are going to have to change is evidence of something you might be breaking elsewhere. In some projects this may amount to designing your tests from the acceptance tier down rather than from the unit tests up. and having fewer unit tests and more integration style tests. It does not mean that you cannot still identify a single feature and code until that feature meets its acceptance criteria.  It simply means that in some cases you do not end up measuring the acceptance criteria with unit tests. "
    },
    {
        "ID": "5972",
        "Question": "This has nothing to do with having a favourite editor or anything like that. I was just wondering, per language, what are the most popular Integrated Development Environments? Maybe a top 2-3 if there is some contention. (Perceived popularity is enough) Thus Far: C# - Visual Studio, SharpDevelop Java - Eclipse, NetBeans, IDEA Objective-C - Xcode Delphi - RAD Studio Object Pascal - Delphi, Lazarus C, C++ - Visual Studio, Vim PL/SQL - RapidSQL, Oracle SQLDeveloper PHP - Eclipse, NetBeans, Nusphere PHPed Actionscript (AS2, AS3) - FlashDevelop Flex - Flash Builder 4 Python - Eclipse, IDLE Perl - Padre Common Lisp - Lispworks, Emacs Ruby - TextMate Haskell - Vim Fortran - Vim Visual Basic - Visual Studio ",
        "Best answer": "All languages - VIM I don't like IDE's.  If I'm on OSX I'll use TextMate at time, but mostly I do everything (JavaScript, Java, Python, PHP) in VIM. I'm also quicker then several colleagues who use an IntelliJ. "
    },
    {
        "ID": "6005",
        "Question": "To the outside world, programmers, computer scientists, software engineers, and developers may all seem alike, but that's far from the case for the people who create software for a living.  Any single programmer's ability and knowledge can range very widely, as well as their tools (OS, language, and yes, preferred editor), and that diversity spawns many sub-cultures in software - like programmers who actively use Stack Overflow and this site, versus many more who don't. I'm curious to hear from others what software sub-cultures they've encountered, belonged to, admired, disliked, or even created.  For starters, I've encountered:  Microsoft-driven companies and developers: their entire stack is from Redmond, WA.  E-mail is Outlook is e-mail.  The web is IE and IIS.  They have large binders of their MS Developer Network subscription full of multiple versions of VB, .net, Visual Studio, etc.  Avoids working with a shell/command-line.  Don't see what the fuss with open-source and such is all about.  MS-centric companies tend to be 9-5 and quite corporate (driven by business managers, not software people). Nowadays (given the wide availability of non-MS tools), this is the antithesis of hacker culture. Old-school CS people: they often know Lisp and Unix extremely well; sometimes, they may have written a semi-popular Lisp themselves, or a system utility. Few, if any, \"software engineering\" things are new to them, nor are they impressed by such.  Know the references, history, and higher-level implications of programming languages like Lisp, C, Prolog, and Smalltalk.  Can be bitter about AI outcomes of the 80's and 90's. Tend to be Emacs users.  Can type out multi-line shell commands without blinking an eye.  Their advice can by cryptic, but contains gold once understood. New-school web developers: played with computers and video games growing up, but often only really started programming in the late '90s or early '00's.  Comfortable with 1 to 1.5 scripting/dynamic languages; think C and languages outside of Ruby/Perl/Python are unnecessary/magical.  May have considered HTML as programming initially.  Tend to get a Mac and be fanatical/irrational about it.  Use frameworks more than build them.  Often overly-enthusiastic about NoSQL and/or Ruby On Rails. New-school CS: lots of training in statistics, Bayesian models and inference; don't say \"AI,\" say \"machine learning.\" More Java than Lisp, but could also be expert Haskell programmers.  Seeing major real-world successes by experts in their field (Google, finance/quants) often makes them (over) confident.  But big data, and the distributed processing of such, really are changing the world.  The examples above are by no means complete, correct, orthogonal, or objective. :)  Just what I've seen personally, and provided to spark some discussion and outline of the broader question.  Feel free to disagree! ",
        "Best answer": "I'd consider myself part of the Real-Time Systems group.   There are some 'Old School' characteristics but with less focus on CS, more on hardware.   The archetype:  Has expert knowledge of 'C'  Has an original copy of K&R Writes in other languages as if they were just an alternate syntax for 'C'  Can predict the assembler output from their code. Can read a circuit diagram Doesn't know how to write code without doing 'premature optimization'. Is quite comfortable with the command line.  "
    },
    {
        "ID": "6014",
        "Question": "A lot of us started seeing this phenomenon with jQuery about a year ago when people started asking how to do absolutely insane things like retrieve the query string with jQuery.  The difference between the library (jQuery) and the language (JavaScript) is apparently lost on many programmers, and results in a lot of inappropriate, convoluted code being written where it is not necessary. Maybe it's just my imagination, but I swear I'm starting to see an uptick in the number of questions where people are asking to do similarly insane things with Linq, like find ranges in a sorted array.  I can't get over how thoroughly inappropriate the Linq extensions are for solving that problem, but more importantly the fact that the author just assumed that the ideal solution would involve Linq without actually thinking about it (as far as I can tell).  It seems that we are repeating history, breeding a new generation of .NET programmers who can't tell the difference between the language (C#/VB.NET) and the library (Linq). What is responsible for this phenomenon?  Is it just hype?  Magpie tendencies?  Has Linq picked up a reputation as a form of magic, where instead of actually writing code you just have to utter the right incantation?  I'm hardly satisfied with those explanations but I can't really think of anything else. More importantly, is it really a problem, and if so, what's the best way to help enlighten these people? ",
        "Best answer": "It's basically because programming is fundamentally difficult.  It requires a lot of logical, structured thought in a way that a lot of people just don't know how to do.  (Or simply can't do, depending on who you listen to.) Stuff like LINQ and jQuery makes certain common data-manipulation tasks a whole lot easier.  That's great for those of us who know what we're doing, but the unfortunate side effect is that it lowers the bar. It makes it easier for people who have no idea what they're doing to start writing code and make things work.  And then when they run into reality, and find something fundamentally difficult that their simple, high-abstraction-level techniques are not well suited to, they're lost, because they don't understand the platform that their library is built upon. Your question is sort of on the right track, but much like the perennial controversy about violent video games \"turning kids violent,\" it has the direction of the link backwards.  Easy programming techniques don't make programmers stupid; they just attract stupid people to programming.  And there's really not much you can do about it. "
    },
    {
        "ID": "6133",
        "Question": "There are some really common usability errors in everyday software we used; errors that result from the ways the particular programmer has learned without learning of all the other ways there are. For example, talking about Windows software in particular, the following common flaws come to mind:  Failure to support multiple screens. For example, windows centered in the virtual desktop (instead of a specific screen) and hence displayed spanning the monitor boundary in a dual monitor setup. Failure to support serious keyboard users. For example, utterly messed up tab order; duplicate or completely missing accelerator keys. Alt+Tab order mess-ups. For example, a window that doesn't go to the end of the tab order when minimized. Subtle breakage of common controls that were reimplemented for one reason or another. E.g. failure to implement Ctrl+Left/Right on a textbox; failure to add an Alt+Space window menu to a skinnable window, failure to make Ctrl+Insert copy to clipboard, etc, etc. This one is a huge category in its own right.  There are a gazillion of things like this. How can we ever make sure we don't break a large proportion of these? After all they aren't all written down anywhere... or are they? ",
        "Best answer": "I think one thing to keep in mind is to remember the source reason for most software usability problems — usability is a human issue, and as such, is difficult to define with a set of rules.  This is totally at odds with the rules-world that most programmers want to live in. Because of that I think you need to throw out the belief that a checklist of usability problems could ever be helpful.  Believing that is thinking like a programmer and will only result in more usability problems that you simply hadn't thought of (or that are the result of sticking to a usability \"rule\" that never really should have been a rule).  One of the biggest differences can be made by designing first (read Alan Cooper's The Inmates are Running the Asylum). Second, make sure your software goes through usability testing with real users. Third, don't think like a programmer. The possible idea in your question is a perfect example of why this is important to remember.  Even good ideas (avoiding non-standard controls) are not always going to hold true.  Those controls may be faulty themselves or may be used for something they shouldn't.  The perfect solution for your form or user input may not have been invented yet, or may simply be not widely used or implemented (the iPhone is a great case study for this).  As another illustration of the problem with \"usability checklists\", the list you presented may well be common to you, and I agree they are problems, but I hadn't really thought of most of them prior to reading your list.  On the other hand, I've experienced tons of annoyances with Windows since being given a PC for my day-job:  (Windows 7) Mousing over a task bar button and then over a Window thumbnail drops all other windows to just outlines.  I've done this by accident a number of times and had what I was reading simply disappear. The Alt+tab pane can now be moused over to temporarily bring the window to the front.  However, when you let go you think it's going to stay there, and it doesn't. (MS Outlook - I know, sort of a cheap shot) If I open a new email and then close it, even if I haven't added any text it asks me if I want to save the draft.  Then if I say no, it moves it to my deleted items folder.  As an unread email, leaving me with a big blue \"(1)\" until I go delete it or \"read\" it.  "
    },
    {
        "ID": "6146",
        "Question": "I've recently been frequented by erroneous error messages from mod_security. Its filter sets cover outdated PHP exploits, and I have to rewrite my stuff because Wordpress&Co had bugs years ago.   Does this happen to anyone else?  Apache mod_security blocks possibly   dangerous HTTP requests before they   reach applications (PHP specifically).   It uses various filter sets, mostly   regex based.  So I have a nice shared hosting provider, technically apt and stuff. But this bugged me: Just last week I had to change a parameter name &src= in one of my apps because mod_security blocks ANY requests with that. I didn't look up its details, but this filter rule was preventing the exploitability of another app which I don't use and probably never had heard about. Still I had to rewrite my code (renaming parameter often suffices to trick mod_security) which had nothing to do or in common with that! And today, a silly regex blocks form submissions, because I wanted to submit php sample code. Given, this is the simple stuff that mod_security is there to protect against. But I don't believe mod_security can detect seriously obfuscated code, and just goes off at obvious (and in this case totally trivial) php snippets. Basically I'm getting penalized by mod_security because other people released bug-prone apps. (Not saying my apps are ultra secure - I'm pretty security wary, but make no hyperbolic claims.) I've already asked my provider to disable it anyway, the benefits are too minuscle IMO and for my apps.  What do you think? Does mod_security make much sense outside of WP hosting? Or is it really just a bunch of blacklists of long passed security bugs? Which of its rules are actually helpful? Is there an application level equivalent? ",
        "Best answer": "I personally see mod_security as a patch. I use it on some of our servers where we can't control the code that's uploaded (shared hosting servers, for example), but it's never really felt like a good solution to me. Based on it's wide and very general blacklist approach, it's more of a patch to cover up security holes than a good security policy. It can also provide a false sense of security. mod_security can reveal some common attacks but can by no means prevent any attack. Again, it's a blacklist of common known attacks. If you simply install mod_security and think that you're magically secure, you're horribly mistaken. I have found a much better policy to me managed servers where my team reviews all code that is placed on them, combined with lots of logs, logfile analysis, reporting systems, and intrusion detection/intrusion prevention systems (IPS). Everytime third-party or open-source software is installed (I'm looking at you, WordPress!) we keep a log of where it was installed, and when new versions are released we update every copy that was installed. Again, you're more likely to find mod_security on a shared hosting server, as you're experiencing now. As you grow you can move to a VPS or clod based hosting provider where you get your own managed environment and can more tightly control the available software. "
    },
    {
        "ID": "6166",
        "Question": "To quote Arthur C. Clarke:  Any sufficiently advanced technology is indistinguishable from magic.  Used to be I looked on technology with wonder and amazement. I wanted to take it apart, understand how it worked, figure it all out. Technology was magical. I'm older, I know more and I spend my days creating stuff that, hopefully, fills other people with that kind of wonder. But lately I've found my own awe for technology has been seriously curtailed. More often I'm just annoyed that it isn't as elegant or seamless or as polished or perfectly delivered as it seemed to be in my youth. It all looks broken and awkward, or cobbled together and poorly tested. Has programming ruined your ability to enjoy technology? Have you stopped wondering in awe and just started saying, \"They could have done this better\" every time you pick up a bit of technology? ",
        "Best answer": "It has ruined my ability to enjoy technology in fiction. I can suspend my disbelief whilst the hero of the [book / film / drama] can withstand numerous karate kicks, fire an infinite number of bullets, leap across a 50ft gap between two buildings, fall from a great height onto a pile of conveniently stacked boxes etc. What makes me shout at the screen in disbelief is when the hero then steps up to a computer, and:   performs a search with some application that has more apparent power than google. hacks into a supposedly secure system with a few key presses and a wink. copies the entire hard disk to a memory stick in a matter of seconds with a convenient \"% complete\" window (which just happens to work with the operating system of the computer he's copying) does anything that involves zooming an image from a CCTV camera to get a high resolution print out of the suspects face.  AAAARHG!!!!  "
    },
    {
        "ID": "6246",
        "Question": "Most software developers want to keep application logic in the application layer, and it probably feels natural for us to keep it here. Database developers seem to want to put application logic in the database layer, as triggers and stored procedures. Personally I would prefer to keep as much as possible in the application layer to make it easier to debug and keep the responsibilities of the layers separate. What are your thoughts on this, and what should or should not be ok to implement in the database layer? Edit This question is also covered on dba.se, from the DBAs perspective. As programmers.se & dba.se have different audiences and biases, future readers might want to review both sets of answers before deciding what works best for them. ",
        "Best answer": "Off the top of my head, advantages of putting logic in the application layer.  Testability.  This should be a good enough reason on it's own actually. Better code structure.  It's very difficult to follow proper OO-architecture with SQL.  This usually also makes the code easier to maintain. Easier to code.  Due to all the different language features available in whatever language you're using it's usually easier to code in the application layer. Code re-use.  It's a whole lot easier to share code with libraries than sharing code in the database.  "
    },
    {
        "ID": "6268",
        "Question": "Just read the question about the Big Rewrites and I remembered a question that I've been wanting answered myself. I have a horrible project passed down to me, written in old Java, using Struts 1.0, tables with inconsistent relationships, or no relationships at all and even tables without primary keys or fields meant to be primary keys but aren't unique at all. Somehow most of the app \"just works\". Most of the pages are reused (copy pasted code) and hard-coded. Everyone who's ever worked on the project has cursed it in one form or the other. Now I had long considered to propose to upper management a total rewrite of this horrendous application.  I'm slowly attempting to on personal time but I really feel that this deserves some dedicated resources to make it happen. Having read the articles on big rewrites I'm having second thoughts. And that's not good when I want to convince my superiors to support my rewrite. (I work in a fairly small company so the proposal has the possibility of being approved) TL;DR When is a big rewrite the answer and what arguments can you use to support it? ",
        "Best answer": "Sorry, this is going to be long, but it's based on personal experience as both architect and developer on multiple rewrite projects. The following conditions should cause you to consider some sort of rewrite.  I'll talk about how to decide which one to do after that.  Developer ramp-up time is very high.  If it takes any longer than below (by experience level) to ramp up a new developer, then the system needs to be redesigned.  By ramp-up time, I mean the amount of time before the new developer is ready to do their first commit (on a small feature)   Fresh out of college - 1.5 months Still green, but have worked on other projects before - 1 month Mid level - 2 weeks Experienced - 1 week Senior level - 1 day  Deployment cannot be automated, because of the complexity of the existing architecture Even simple bug fixes take too long because of the complexity of existing code New features take too long, and cost too much because of the interdependence of the codebase (new features cannot be isolated, and therefore affect existing features) The formal testing cycle takes too long because of the interdependence of the existing codebase. Too many use cases are executed on too few screens. This causes training issues for the users and developers. The technology that the current system is in demands it   Quality developers with experience in the technology are too hard to find It is deprecated (It can't be upgraded to support newer platforms/features) There is simply a much more expressive higher-level technology available The cost of maintaining the infrastructure of the older technology is too high   These things are pretty self-evident. When to decide on a complete rewrite versus an incremental rebuild is more subjective, and therefore more politically charged. What I can say with conviction is that to categorically state that it is never a good idea is wrong. If a system can be incrementally redesigned, and you have the full support of project sponsorship for such a thing, then you should do it.  Here's the problem, though. Many systems cannot be incrementally redesigned. Here are some of the reasons I have encountered that prevent this (both technical and political).  Technical   The coupling of components is so high that changes to a single component cannot be isolated from other components.  A redesign of a single component results in a cascade of changes not only to adjacent components, but indirectly to all components. The technology stack is so complicated that future state design necessitates multiple infrastructure changes. This would be necessary in a complete rewrite as well, but if it's required in an incremental redesign, then you lose that advantage. Redesigning a component results in a complete rewrite of that component anyway, because the existing design is so fubar that there's nothing worth saving. Again, you lose the advantage if this is the case.  Political   The sponsors cannot be made to understand that an incremental redesign requires a long-term commitment to the project. Inevitably, most organizations lose the appetite for the continuing budget drain that an incremental redesign creates. This loss of appetite is inevitable for a rewrite as well, but the sponsors will be more inclined to continue, because they don't want to be split between a partially complete new system and a partially obsolete old system. The users of the system are too attached with their \"current screens.\" If this is the case, you won't have the license to improve a vital part of the system (the front-end). A redesign lets you circumvent this problem, since they're starting with something new. They'll still insist on getting \"the same screens,\" but you have a little more ammunition to push back.   Keep in mind that the total cost of redesiging incrementally is always higher than doing a complete rewrite, but the impact to the organization is usually smaller.  In my opinion, if you can justify a rewrite, and you have superstar developers, then do it. Only do it if you can be certain that there is the political will to see it through to completion.  This means both executive and end user buy-in.  Without it, you will fail. I'm assuming that this is why Joel says it's a bad idea.  Executive and end-user buy-in looks like a two-headed unicorn to many architects. You have to sell it aggressively, and campaign for its continuation continuously until it's complete.  That's difficult, and you're talking about staking your reputation on something that some will not want to see succeed. Some strategies for success:  If you do, however, do not try to convert existing code. Design the system from scratch. Otherwise you're wasting your time. I have never seen or heard of a \"conversion\" project that didn't end up miserably. Migrate users to the new system one team at a time. Identify the teams that have the MOST pain with the existing system, and migrate them first. Let them spread the good news by word of mouth. This way your new system will be sold from within. Design your framework as you need it. Don't start with some I-spent-6-months-building-this framework that has never seen real code. Keep your technology stack as small as possible. Don't over-design. You can add technologies as needed, but taking them out is difficult. Additionally, the more layers you have, the more work it is for developers to do things. Don't make it difficult from the get-go. Involve the users directly in the design process, but don't let them dictate how to do it. Earn their trust by showing them that you can give them what they want better if you follow good design principles.  "
    },
    {
        "ID": "6442",
        "Question": "Assuming the language provides it, what are some things to do or libraries to use that every programmer should know? My list would be  Regular Expressions Named Pipes standard IO (std in/out/error) Executing outside executables (like imagemagik and your own scripts) How to grab an HTTP page as a string (mostly for updates and grabbing configs from servers)  I have a similar questions about tools. Also I am looking for specific answers. I don't want answers that can be done in different ways (such as learn how to synchronize threads). I know how to do the above in 3+ languages. There always seems to be things programmers don't know how to do which can make their lives easier. For a long time I didn't know what regular expression was (although I did hear of it) and I was surprised how many people who tried to make a toolchain or complex app and did not know how to (or that you could) get the stdout from an exe they launched (they checked for errors in a very weird way or just didn't and hope it produce the expected results). What do you think is useful, not well known and should be something every senior (or starting out) programmer should know? ",
        "Best answer": "Handling date calculations, date formating and localization issues dealing with dates. "
    },
    {
        "ID": "6476",
        "Question": "Lately I've been experimenting with using a collaborative text editor (or plugin such as NppNetNote for Notepad++) for two or more partners to edit source code. I've been met with unexpected success, and the workflow differs from anything I've ever experienced. My findings:  It's fun to fight over code, but also very satisfying to collaborate on it in real time. Two people can work collaboratively or separately, and be constantly aware of each other's changes. The comments end up becoming a free-form IM conversation about the code around them. The medium is enormously less restrictive than trying to work back and forth over IM proper. It's like pair programming, but with no overhead to switch roles between driver and navigator.  Has anyone tried this before? What were some of the advantages and problems that you encountered? For those that haven't tried it, I strongly encourage you to find a friend (or two, or more!) and make an attempt. Edit: See Wikipedia for something like more explanation, though in the context of pair programming specifically. ",
        "Best answer": "I often use GNU screen to share terminals (and terminal based editors) for pair programming and remote collaboration. I think one of the most important things that can make this go smoothly is a voice connection (phone, VoIP, etc.) with the other collaborators. Without a voice connection, you incur a lot of overhead and clunkiness as you have to IM (preferably in a separate window) at the same time. Short sharing the same terminal, each sharing a separate terminal (her read/my write, her write, my read). This allows for simultaneous use but also prevents you from working on exactly the same file. I've also been using tmux recently (a program similar to GNU screen) and while I find it better in some aspects I find other aspects less desirable. "
    },
    {
        "ID": "6526",
        "Question": "Prevalence is a simple technique to provide ACID properties to an in-memory object model based on binary serialization and write-ahead logging.  It works like this:  Start with a snapshot.  Serialize the object model and write it to a file. Create a journal file.  For every call into the object model, serialize the call and its arguments. When the journal gets too big, you're shutting down, or it's otherwise convenient, perform a checkpoint: write a new snapshot and truncate the journal. To roll back or recover from a crash or power hit, load the last snapshot and re-execute all the calls recorded in the journal.  The precautions needed to make this work are:  Don't let mutable object references escape or enter the prevalence layer.  You need some sort of proxy or OID scheme, as if you were doing RPC.  (This is such a common newbie mistake it's been nicknamed the 'baptism problem'.) All the logic reachable from a call must be completely deterministic, and must not perform business-logic-meaningful I/O or OS calls.  Writing to a diagnostic log is probably OK, but getting the system time or launching an asynchronous delegate is generally not.  This is so that the journal replays identically even if it's restored on a different machine or at a different time.  (Most prevalence code provides an alternate time call to get the transaction timestamp.) Writer concurrency introduces ambiguity in journal interpretation, so it is prohibited.  Is it because ...  people developed a bad taste for them after trying to use one on a project that wasn't well suited* to it?   Klaus Wuestefeld's strident advocacy turned people off? people who like the imperative programming model dislike separating I/O from calculation, preferring instead to interleave computation with I/O and threading calls? prevalence layers are so conceptually simple and so intimately bound to the characteristics of the framework they inhabit that they're usually custom-rolled for the project, which makes them too alien/nonstandard/risky? it's just too hard to keep straight what you have to be careful not to do? newbies' heads just seem to explode when faced with something that isn't the same kind of two-tier database-driven app they learned to write in school? ;)   *The entire data set fits in RAM, you don't need writer concurrency, and you don't need to do ad-hoc queries, reporting, or export to a data warehouse.  With apologies to SQLite, prevalence is an improvement on save-files, not a replacement for Oracle. ",
        "Best answer": "I think you first need to demonstrate that so many developers absolutely hate them. I don't think that's the case. Consider that fowler, a while back, formalized a pattern of sorts for this here. "
    },
    {
        "ID": "6543",
        "Question": "I'm starting to use the ORM recommended by the framework I choose, and though I like the idea of the added layer of abstraction the ORM provides, I'm starting to realize what this really means. It means I'm no longer working with my database (mysql) and any mysql-specific features are gone, out the window, like they don't exist. The idea the ORM has is that it's trying to help me by making everything database agnostic. This sounds great, but often there is a reason why I choose a specific database system. But by going the database agnostic route, the ORM takes the lowest common denominator, which means I end up with the smallest set of features (the ones supported by all databases).  What if I know that for the long term I won't be switching the underlying database? Why not access the database-specific features, too? ",
        "Best answer": "I see it the same way.  Any abstraction that doesn't allow you to get underneath it when necessary is evil, because it leads to all sorts of ugly abstraction inversions in your code. At work we've got a homegrown ORM that works fairly well, but for the times when we need something that its features don't explicitly provide for, there's a method that takes a string and drops it directly into the query it's generating, allowing for the use of raw SQL when it's necessary. IMO any ORM that doesn't have this feature isn't worth the bits it's compiled from. "
    },
    {
        "ID": "6591",
        "Question": "I'm trying to understand how I should evaluate writing a book, article, or conference presentation. Writing a book is a lot of work. Same for writing an article in a magazine or presenting in a conference. They need time and you may even make a mistake here and there that back fires (bad reviews, people calling you an idiot..) Also you do it for free (at least for magazine articles and conference presentations. For books you get something like a $5K deposit and you rarely get any additional sales royalties after that). So how should I evaluate the benefits? I would appreciate answers that call out if you have done this before. I may not write a book because it's way beyond what I'd like to commit time-wise, but should I bother giving conference presentations or writing shorter articles in magazines? ",
        "Best answer": "It all depends: what are your goals? ** [Note: my background is as a programmer, but I've been making a living as a tech writer/speaker for the last 12 years. After 15 titles, dozens of magazine articles, and speaking internationally, I think I'm at least as qualified as anyone else here.] ** If your goal is to make money, don't bother. Really. I know a lot of people in this business, and very few make a decent hourly wage from writing. Of the ones who do make a living at it, all of them write for beginners (tip: there are always more beginners than  intermediate or advanced users). However… IF you're currently working as a consultant and if you want more consulting gigs with bigger companies at a higher price and if you've been offered a book contract and/or speaking gigs … then go for it. Don't think of it in terms of work with low compensation; instead, think of it as just part of the training and prep you already do in order to get those consulting jobs. Screw writing articles for magazines/sites that don't pay — or say you'll write for them, on the condition that they run your article without ads. If they're making money, you should be too. However, if the magazine helps you get those high-profile consulting gigs, see the advice in the previous paragraph. ** Speaking gigs, though, are almost always worth it. At a minimum, you'll meet other presenters, which is how I've met some truly amazing people. Networking opportunities abound. ** On the other hand… IF you have an amazing idea for a great book that no one else has written and if you can't rest until you see that book in print … then go for it. In this case, it's about love, not money. If you can handle a life where this book doesn't exist, then don't write it. ** But it's really all about where you want your career to go. If a book helps you get to that place, then see if works for you. "
    },
    {
        "ID": "6595",
        "Question": "As part of being a programmer, you often are asked to provide estimates/ make slideware / do technical demos for Sales teams to present to end-clients. Sometimes we go along for the 'technical' discussions or 'strategic capability planning' or some similar mumbo-jumbo. Sometimes, you kind of know which ones are totally going to fail and are not worth pursuing but the Sales guys present fake optimism and extract 'few more slides' out of you or the 'last conference call'. These don't lead to anywhere and are just a waste of time from other tasks for the week. My question is how do you get out of these situations without coming across as non-cooperative. Updated after Kate Gregory's answer: The problem is related to projects we know are doomed (from the technical feedback we've received)  But Sales ain't convinced since they've just had a call higher up the management chain - so it's definitely going ahead ! ",
        "Best answer": "One-by-one, you can't. You're at your desk, sales calls and says excitedly that they've set up another meeting with Mr Big, and are you free at 2? And can you make another demo by then? It's just never going to be an appropriate response for you to say \"dude, Mr Big is totally not buying this software, that meeting would waste my time, no, I'm not going.\" Whether Mr Big is buying or not is something the sales guy is supposed to know more about than you. (In fact, he might even be right.) So for that call, you have to suck it up and say sure. And do a good job on the slides! Longer term, a chat with your boss about this \"supporting the sales team\" role would probably be helpful. Is your issue the time spent? Or that you object to investing time in something that is doomed? In my experience, it is highly unusual for the number of technical people in the room to shrink as the sales process goes on - normally the tech people only show up once things are getting warm, and more of them come to each meeting until the client is landed. So you wanting to bail from the process before the sale is closed will appear strange to management and sales. Discussing this with management may help you to understand how your presence increases the chance of a sale. I think it's unlikely management will grant you a pass to bail on selected client pitches based on your opinion of whether they are going to buy or not, but you could talk about it. Finally, you clearly are expected to produce slides and demos as part of your job, so approach them with the same dignity as \"real code\". Reuse, have a repository, be modular, don't repeat yourself, figure out how to waste as little time as possible giving sales what they need. Remember, when Mr Big does write that cheque, he's funding your salary. You want him to buy. You want to help sales. You just want to do so as quickly and efficiently as possible. "
    },
    {
        "ID": "6723",
        "Question": "Here is a theoretical problem.  It could apply to many systems.  Let's assume RESTful Web services for example.  You want to find out something but as it happens you can't get it in a single query.  You can make multiple queries.  For example, you have a joint bank account (or a company bank account) and you want to find out how much money is in there.  Meanwhile someone else transfers money from checking to savings. Checking:  $5,000   Savings:  $3,000   Me:  Query Checking.    Response: $5,000     Joe:  Transfer $4,000 from Checking to Savings  Checking: $1,000   Savings:  $7,000  Me:  Query Savings.  Response:  $7,000 Total $12,000. How do you avoid anomalies like this?  The example shows two accounts at one bank, but it can also happen with two accounts at two different banks.  ",
        "Best answer": "You need some form of concurrency control to deal with problems like this. Some possible solutions in your example:  Ensure that the service can return both Savings and Checking figures in a single query, perhaps as a Total. Implement some kind of session so that the user requesting values can lock the data until she has finished reading all the values in which she is interested. This approach is often called \"pessimistic concurrency control\". Design the service so that you can pass your previous Checking figure back when you request a Savings figure. If the Checking figure no longer matches its previous value, the service should indicate an error rather than return a value for Savings. This approach is a variation of \"optomistic concurrency control\".  "
    },
    {
        "ID": "6832",
        "Question": "Both in Code Complete by Steve McConnell and The Pragmatic Programmer by Andrew Hunt & David Thomas, they allege that most programmers don't read programming-related books as a habit, or at least not often enough. Does this still hold? Are programmers who do read such books still rare? I must admit that only quite recently did I start reading (and now I can't stop; it's burning a whole in my wallet!). Before I was actually against it and argued that it's better to write programs than to waste time reading about it. Now I realize that the best outcome, for me at least, is to do both. EDIT: I actually meant any kind of books that on way or another was related to programming; be it either on a particular language, technology, pattern, paradigm - anything as long as it may benefit you as a programmer. I was just referring to Code Complete and The Pragmatic Programmer because that's where I read that programmers don't usually read such books. It would have looked stranged if the title just read \"Are programmers who read books still rare?\" ",
        "Best answer": "Everyone seems to be answering this question personally, as in \"I do read such-and-such.\"  However, as a person hanging out at this site, you are already a cut above your \"average\" programmer IMO.  So this skews the numbers dramatically.  To answer the question directly: yes, programmers that read books are a rare breed.  I've worked with dozens of programmers, and only know two or three that would pick up a book to solve a problem.  Most of them would search Google for a particular program, or just stare blankly at the screen hoping for divine inspiration. :-) And the \"theory\" books such as Pragmatic Programmer and Code Complete are even more rarely read by average programmers.  If a programmer is going to read a book, it is more likely to be a technical book on the specific language, framework, or technology they are working on.  The \"soft\" books are not even on most programmers radar. "
    },
    {
        "ID": "6929",
        "Question": "I've got an itch and it needs to be scratched. I've got nothing to program these days. Normally I'd just pick my flavour of the month technology and fiddle with it, but I want to make something and take a break from serious research/experimentation for a while and just write something for fun (ie. not necessarily purely educational/professional development). Problem is,   I can't think of software that I need that I don't already have. Sifting through the open-source projects on various sites to help out on is excruciating (5 hrs down the tubes so far - not worth the trouble)  I'm not looking for a specific suggestion for what to work on because that would be a terribly hard task. I don't think that here knows nearly enough about eachother to effectively give good suggestions. So I ask, is there any mental exercise that you use to decide what to make when you're not \"fiddlin'\"? ",
        "Best answer": "When I run out of applications or utilities I want to write, I usually go solve math problems on Project Euler (I also often use those problems when I'm learning a new language).  Solving math-based problems programatically may not be the most rounded use for programming skills, but I've had a lot of fun and learned some things as well (ymmv). "
    },
    {
        "ID": "7126",
        "Question": "Note: this question is an edited excerpt from a blog posting I wrote a few months ago.  After placing a link to the blog in a comment on Programmers.SE someone requested that I post a question here so that they could answer it.  This posting is my most popular, as people seem to type \"I don't get object-oriented programming\" into Google a lot.  Feel free to answer here, or in a comment at Wordpress.  What is object-oriented programming?    No one has given me a satisfactory   answer.  I feel like you will not get   a good definition from someone who   goes around saying “object” and   “object-oriented” with his nose in the   air. Nor will you get a good   definition from someone who has done   nothing but object-oriented   programming. No one who understands   both procedural and object-oriented   programming has ever given me a   consistent idea of what an   object-oriented program actually does.  Can someone please give me their ideas of the advantages of object-oriented programming? ",
        "Best answer": "From your blog, it seems that you're familiar with both imperative and functional programming, and that you're familiar with the basic concepts involved in object-oriented programming, but you've just never really had it \"click\" as to what makes it useful.  I'll try to explain in terms of that knowledge, and hope that it's helpful to you. At its core, OOP is a way to use the imperative paradigm to better manage high degrees of complexity by creating \"smart\" data structures that model the problem domain.  In a (standard procedural non-object-oriented) program, you've got two basic things: variables, and code that knows what to do with them.  The code takes input from the user and various other sources, stores it in variables, operates on it, and produces output data which goes to the user or various other locations. Object-oriented programming is a way to simplify your program by taking that basic pattern and repeating it on a smaller scale.  Just like a program is a large collection of data with code that knows what to do with it, each object is a small piece of data bound to code that knows what to do with it. By breaking down the problem domain into smaller pieces and making sure as much data as possible is bound directly to code that knows what to do with it, you make it a lot easier to reason about the process as a whole and also about the sub-issues that make up the process. By grouping data into object classes, you can centralize code related to that data, making relevant code easier both to find and to debug.  And by encapsulating the data behind access specifiers and only accessing it through methods, (or properties, if your language supports them,) you greatly reduce the potential for data corruption or the violation of invariants. And by using inheritance and polymorphism, you can reuse preexisting classes, customizing them to fit your specific needs, without having to either modify the originals or rewrite everything from the ground up.  (Which is a thing you should never do, if you can avoid it.)  Just be careful you understand your base object, or you could end up with killer kangaroos. To me, these are the fundamental principles of object-oriented programming: complexity management, code centralization and improved problem-domain modeling through the creation of object classes, inheritance and polymorphism, and increased safety without sacrificing power or control through the use of encapsulation and properties.  I hope this helps you understand why so many programmers find it useful. EDIT: In response to Joel's question in the comments,  Can you explain what an \"object-oriented program\" contains   (other than these fancy defintions you've outlined) that is fundamentally   different from an imperative program? How do you \"get the ball rolling?\"  A little disclaimer here.  My model of \"an object-oriented program\" is basically the Delphi model, which is very similar to the C#/.NET model since they were created by former Delphi team members.  What I'm saying here may not apply, or not apply as much, in other OO languages. An object-oriented program is one in which all the logic is structured around objects.  Of course this has to be bootstrapped somewhere.  Your typical Delphi program contains initialization code that creates a singleton object called Application.  At the start of the program, it calls Application.Initialize, then a call to Application.CreateForm for every form you want to load into memory from the beginning, and then Application.Run, which displays the main form on screen and starts up the input/event loop that forms the core of any interactive computer programs. Application and your forms poll for incoming events from the OS and translate them into method calls on your object.  One thing that's very common is the use of event handlers, or \"delegates\" in .NET-speak.  An object has a method that says, \"do X and Y, but also check to see if this particular event handler is assigned, and call it if it is.\"  An event handler is a method pointer--a very simple closure that contains a reference to the method and a reference to the object instance--that's used to extend the behavior of objects.  For example, if I have a button object on my form, I customize its behavior by attaching an OnClick event handler, which causes some other object to execute a method when the button is clicked. So in an object-oriented program, most of the work gets done by defining objects with certain responsibilities and linking them together, either through method pointers or by one object directly calling a method defined in another object's public interface.  (And now we're back to encapsulation.)  This is an idea that I had no concept of back before I took OOP classes in college. "
    },
    {
        "ID": "7181",
        "Question": "I have been using mysql for the same amount of time I have been using PHP. However I feel about 10x more confident in PHP; or any other programming language. But I just can't seem to really really grasp SQL. I mean, I can select, update, insert, delete, join, etc. but when it comes to more complex stuff I am lost. How to write a more complicated select statement, how to use indexes properly, which engine to use, etc. For some reason I really find the mysql docs hard to follow, but the php ones easy to follow. I am starting to think that maybe my brain is just wired in a way that makes programming make since but sql confusing. I don't know. Most people think Regex is so confusing but to me it makes more scientific sense than how to construct an sql statement. Please don't get me wrong, I am not hating on SQL, I really like it, I just want to understand it better. Whats the best/fastest way to really master MySQL?  When I say \"master\" I don't mean the guru among gurus, I mean smart enough to figure out most of the questions I ask on SO myself without needing help.  ",
        "Best answer": "Fastest way I found to \"master\" anything was to challenge myself with it. Take a set of data and write down eight to ten different things you would like to query for. A group of students you might want to find the average age of everyone grouped by the grade point average rounded to the nearest whole number. Then write the queries for them. Start with one table, then add more. Practice data joins and built in functions.  "
    },
    {
        "ID": "7364",
        "Question": "This question is something that's been bugging me for the past 3 months since I switched from being a freelancer to working at a Web Design firm. Our sales people often ask us something similar to the following series of questions:  How much does it cost to program a widget How many hours will it take to convert this website to this software. (Without knowing what the website currently runs) etc    How can we give a quote without any information? (No, I can't ask for more info!)  I have another question if a project goes over budget it's bad. Recently, I missed an entire menu when calculating the cost of transferring a website over to a new platform so the project went over budget. My boss was not happy at all, and it's my opinion that some things like this can't be avoided.    2.  What is the general practice for dealing with going over budget            and do projects like web development often go over budget? If you work at a web development/design/similar company:    3.  How does your billable hour system work? For me, we have a time tracking application that we record how many hours we spend on which project and if they are billable or internal (AKA non-billable). If don't meet xx billable hours a week we can get in trouble/fired eventually. Work you do for the company or for clients that isn't billable isn't part of this system, and we often have to do internal work, so I'm wondering if any alternative systems exist. EDIT: Ok I am a developer at this firm not a designer :) Second, I am paid salary, but here is how management looks at it. You have 35 hours a week that you must work. You could be doing work that they bill to clients in that 35 hours and you should. If they figure out a project will take 50 hours and I take 55 hours, that 5 hours could have been spent on another project that wasn't over budget so we just \"lost\" money. Another example is that if I only have 1 project, that is due in two weeks and I spend a day  doing internal work, some how we lost money because I wasn't working. If I worked that day, I would finish a day early and still have no work. Either way, the work is contract so we will get paid the same amount regardless of which days I work! ",
        "Best answer": " Our sales people often ask us   something similar to the following   series of questions: How much does it cost to program a   widget How many hours will it take to   convert this website to this software.  Why are your sales people asking the designers? Sales should have a prestocked list of quotes and system for estimation that has little, if any, correlation to your actual costs. I'm assuming you are salaried.   How can we give a quote without any information? (No, I can't ask for more info!)  Short answer? You can't, don't try. The long answer is still short. If I call you up and say I have a website were people can login, post messages to other users, upload pictures, and make friends, what would it cost to build, what would you say? I could have described the worst social network imaginable, or Facebook. You don't have enough information so you can't give an accurate assessment.   I have another question if a project   goes over budget it's bad. Recently, I   missed an entire menu when calculating   the cost of transferring a website   over to a new platform so the project   went over budget. My boss was not   happy at all, and it's my opinion that   some things like this can't be   avoided.  Define \"over budget.\" Again, I'm assuming salary not hourly. If you went over your time budget, pull some long nights and don't make the same mistake (of missing something) again.  For me, we have a time tracking   application that we record how many   hours we spend on which project and if   they are billable or internal (AKA   non-billable). If don't meet xx   billable hours a week we can get in   trouble/fired eventually. Work you do   for the company or for clients that   isn't billable isn't part of this   system, and we often have to do   internal work, so I'm wondering if any   alternative systems exist.  I'm not sure how I would set that up if I had to create a record of \"billable\" hours. You would probably wind up with a hundred hours +/- a few ever week. I don't stop thinking about code, should that count?  "
    },
    {
        "ID": "7439",
        "Question": "If you were hiring programmers, and could choose between one of (say) the top 100 coders on topcoder.com, or one of the top 100 on stackoverflow.com, which would you choose? At least to me, it would appear that topcoder.com gives a more objective evaluation of pure ability to solve problems and write code. At the same time, despite obvious technical capabilities, this person may lack any hint of social skills -- he may be purely a \"lone coder\", with little or no ability to help/work with others, may lack mentoring ability to help transfer his technical skills to others, etc. On the other hand, stackoverflow.com would at least appear to give a much better indication of peers' opinion of the coder in question, and the degree to which his presence and useful and helpful to others on the \"team\". At the same time, the scoring system is such that somebody who just throws up a lot of mediocre (or even poor answers) will almost inevitably accumulate a positive total of \"reputation\" points -- a single up-vote (perhaps just out of courtesy) will counteract the effects of no fewer than 5 down-votes, and others are discouraged (to some degree) from down-voting because they have to sacrifice their own reputation points to do so. At the same time, somebody who makes little or no technical contribution seems unlikely to accumulate a reputation that lands them (even close to) the top of the heap, so to speak. So, which provides a more useful indication of the degree to which this particular coder is likely to be useful to your organization? If you could choose between them, which set of coders would you rather have working on your team? ",
        "Best answer": "Why choose? When you are hiring, you want to post your offer everywhere. Hiring based on reputation points or any other online results is a terrible idea and I'm pretty sure no one is doing it. Sure that having 30K on StackOverflow will helps getting the attention of the hiring guy, but you will get hired for many others facts.  The experience in your domain or industry of the candidate His physical location related to your office The amount of $$$ he is asking for the job His personnal interests (yes it has an influence, at least on me) His recommandations What people said about him during references checks And more importantely, his seduction abilities! (during interview, you have to seduce within the first 5 minutes)  They are so many factors you can't summarize the hiring process to programmer's (supposed) capabilities. "
    },
    {
        "ID": "7456",
        "Question": "Linguistic relativity is the idea that language shapes the way we think.  My question is, how much, and to what extent, does this apply to programming?  Are some native, natural languages better-suited for thinking about programming than others?  For instance, can the following be stated more concisely in a non-English language?  Select a pivot.  Move all the items less than the pivot to one side of the list, and all the items greater than the pivot to the other side. Does a Chinese-speaking programmer view programming in a radically different lens than an English-speaking programmer, or do the differences fade away when both are immersed in the subject? Are some programming languages and domains easier to think about in one language or another.  For instance, is it any easier to grok Ruby if you are Japanese because the creator of Ruby is Japanese?  Note that this question is not focused on \"how do programming languages affect the way people think about programming\", but rather \"how do natural languages affect the way people think about programming\". To get it out of the way, one language that clearly has a pragmatic advantage is English.  I think the advantage has little to do with programming languages choosing English keywords like if, for, while, and do, just as musicians who don't speak Italian aren't tripped up by words like forte.  It has more to do with communication of ideas with other programmers, as English is the lingua franca these days, at least in the programming world.  For instance, to ask a question in StackOverflow, you really need to know English and know it pretty well if you want good answers.  Although this sounds like an imperialist attitude, it really is true in practice. That aside, how do the intrinsic properties of languages affect how programmers who speak them think about data structures, algorithms, etc.?  Are any languages particularly concise when it comes to talking about logic and programming, allowing native speakers of those languages to think faster? ",
        "Best answer": "I don't know that any particular natural language lends itself to better programming (except maybe Latin?). I do know that knowing more than one language is pretty powerful. Dijkstra said in one of his last interviews (as reprinted in CACM Vol. 53 No. 8, p. 44):  There is an enormous difference   between one who is monolingual and   someone who at least knows a second   language well, because it makes you   much more conscious about language   structure in general. You will   discover that certain constructions in   one language you just can't translate.  "
    },
    {
        "ID": "7472",
        "Question": "Let's say a large corporation is planning to replace it's existing version control system. Let's say it is only considering systems from major vendors that cost hundreds of thousands of dollars because they have \"support.\"  Does version control in an enterprisey environment have to be expensive?  Does your medium/large corporation use a FOSS VCS such as SVN/Git/Mercurial?  What has the experience been?   I have to think it doesn't need to be expensive since there are so many free options, and there are probably companies that provide paid support for FOSS VCS if that is the main concern.   I don't intend this question to compare VCS or decide which is best, rather just understand experiences with VCS in a corporate IT environment. ",
        "Best answer": "Yes.    In my (admittedly limited) experience, the non-FOSS solutions tend to be more \"enterprise-y\".  That is,  They integrate with everything under the sun. They have more built-in controls for complex business logic (permissions, access control, approval, etc). They come with support contracts and reasonably responsive tech support lines. They're well advertised to the non-technical people making VCS decisions at a high level in big companies.  These attributes make them attractive to large companies, especially to people who don't have to use them.  The FOSS alternatives, as counters to the above:  Have plenty of third-party tools to integrate them with everything under the sun (by virtue of being more popular than proprietary alternatives), and tend to be easier to develop third-party tools for, being OS. See previous- easier to to get external tools around a clean, simple, basic tool. By virtue of being more popular, they have a wider community-based support. They don't need said advertising.  Aside from that, my experience with common free VCS (mercurial/svn/etc) has them being faster, more reliable, and easier to use. "
    },
    {
        "ID": "7482",
        "Question": "As per this question: I decided to implement the BitTorrent spec to make my own client/tracker. Now, I was going through the spec, I was about 70% done implementing the BEncoding when I found a link to an implementation of BEncoding in C# written by someone else. Normally, if I were working on production code, I'd use it as a reference to check my own work against, and a baseline to write some tests to run my code against, but I found myself thinking \"I'm making this, it's a for-fun project with no deadlines; I should really implement it myself - I could learn a lot\" but some voice in my head was saying \"Why bother re-inventing the wheel? Take the code, work it so that it's you're style/naming convention and you're done.\" So I'm a bit conflicted. I ended up doing the latter, and some parts of it I found better than what I had written, but I almost feel like I 'cheated'. What's your take? Is it cheating myself? Perfectly normal? A missed opportunity to learn on my own? A good opportunity to have learned from someone else's example? ",
        "Best answer": " If I have seen further it is by standing on the shoulders of giants. Isaac Newton  It is not cheating if the code is open source and you've taken the time to understand it. Now obviously this isn't always possible due to time constraints but try and always have high level overview of the code you are using. Always remember that C was derived from B. "
    },
    {
        "ID": "7530",
        "Question": "There are many stories about intentionally bad code, not only on TheDailyWTF but also on SO. Typical cases include:  Having a useless time-wasting construct (e.g. an empty loop counting to some huge value) so programmers can easily \"speed up\" the application by removing it when they are tasked to. Providing intentionally misleading, wrong or no documentation to generate expensive support requests. Readily generating errors, or worse, generating even though everything worked fine, locking up the application so an expensive support call is required to unlock.  These points display a more or less malicious attitude (even though sometimes by accident), especially the first point occurs rather often. How should one deal with such constructs? Ignore the issue, or just remove the offending code? Notify their manager, or speak to the person who introduced the \"feature\"? ",
        "Best answer": "I've never (in 20-odd years) come across intentionally bad code, but the examples you cite seem (to me at least, but IANAL) to be attempts to defraud either an employer or a customer, so you probably have a legal obligation to point it out to your manager. "
    },
    {
        "ID": "7536",
        "Question": " Possible Duplicate: int* i; or int *i; or int * i;   Thinking about where we place our asterisks; how do those that prefer to keep the \"pointerness\" away from the type and with the identifier (int *i) write code when the identifier is missing? void f(int*); // 1 void f(int *); // 2  The former seems much more common, no matter what your preference when with the identifier.  Is this a special case?  What makes it an exception? However, the first still isn't universal, because I have seen the latter style.  Besides consistency along the lines of \"there's always a space with the identifier, so we have one without\", are there any other reasons to prefer it? What about casts or array and function types?  How would you re-write these: (void*)var /*or*/ (void *)var  int[3] /*or*/ int [3] // more relevant in C++ than C: Example<int[3]>  void(int) /*or*/ void (int) // more relevant in C++ than C: std::function<void(int)>  The latter two would rarely, if ever, be used in C, but are seen with C++ templates. ",
        "Best answer": "I always separate the star from the type, so I use: void f(int *);  void f(int *x) {     char *y = (char *) x;     char x[4]; }  Obviously, I don't separate the brackets from the identifier. That's how I learned to do it from K&R, and who am I to argue? (Also, it seems to be the most readable way.) "
    },
    {
        "ID": "7559",
        "Question": "This happened at least once to me. I'm working on some part of the code base and find a small bug in a different part, and the bug stops me from completing what I'm currently trying to do. Fixing the bug could be as simple as changing a single statement. What do you do in that situation?  Fix the bug and commit it together with your current work Save your current work elsewhere, fix the bug in a separate commit, then continue your work [1] Continue what you're supposed to do, commit the code (even if it breaks the build fails some tests), then fix the bug (and the build make tests pass) in a separate commit  [1] In practice, this would mean: clone the original repository elsewhere, fix the bug, commit/push the changes, pull the commit to the repository you're working on, merge the changes, and continue your work. Edit: I changed number three to reflect what I really meant. ",
        "Best answer": "I have done 1 and 2 and in the end, I think I prefer #2. It allows for more visibility for the bug fix, which may be important for QA/release notes/other developers.  I've also come across a situation where what I thought was a bug actually wasn't (not saying that this is the case here), and \"fixing\" it in a separate commit allowed another dev to contact me and explain what was what instead of the \"fix\" just getting lost in my normal check-in. "
    },
    {
        "ID": "7566",
        "Question": "Developer interviews are different than most other places, because not only do you worry about the people you work with, benefits, job description, etc., you also have EXTRA to worry about after getting the job.  Writing two application with exact requirements can be vastly different if you're working in a loud vs quiet environment, using VS2003/.NET 2.0 vs VS210/.NET 4.0., using SVN vs VSS. Is it ok to give the potential employer the straight-up Joel Test?  I try to ask as many questions to get the type of environment I will be working in, which is extremely important from my perspective, but what's the best way to cut to the chase and just ask the tough questions (like they ask you during the same interview). NOTE: By the \"Joel Test\" I mean a specific list of things that are deal breakers that are important to you (not necessarily Joel), but you may not have time to get out using the traditional casual \"conversational\" way of asking them, so you decide to either email or schedule another meeting, or ask other people, etc. ",
        "Best answer": "A job interview goes both ways -- a company is interviewing you and you are interviewing the company. I wouldn't come out literally with a \"what's your Joel Test score?\", but I would ask the individual questions that were particular deal-breakers for me in a work environment. It doesn't need a huge build-up. A good time to ask these questions as at the technical part of the interview process, when they say \"do you have any questions for us?\". You can lead in with something along the lines of \"can you describe a typical day on the job here?\" and go from there. "
    },
    {
        "ID": "7588",
        "Question": "We currently use once cc.net server for our build process, which builds both .net (using msbuild & nant) and java (using maven and ant).    CC.net monitors source control, and triggers a remote build running on a separate server.  CC.net then collates the results. When we run the remote build, it typically:  runs nunit or junit or similar using mocked data optionally runs a DB script to create a fresh database instance or restore a database from a known position. runs selenium or similar to test UI runs emma or ncover for code coverage builds the system for various deployment environments (test, acceptance, production)  We may have several builds running at a time, some .net and some java (from different project teams). It's quite time consuming to get the remote builds working when we set up a new project and we feel that there must be something more suited to remote builds than cc.net.   Does anyone have any experience with remote builds with continuous integration systems? I don't really want feature lists of CI servers, I'd more appreciate hearing about how you've used them in a multi language, multi server environment. ",
        "Best answer": "Hudson (Update: in today's world, I would use Jenkins, a fork of Hudson.) I've used hudson in both Java and .NET enterprise environments for high visibility projects (you've probably been to some of the sites).  Hudson is solid from the start, but the best part is that there are lots of plugins to do just about anything you want.  Hudson is highly configurable, has a great community, and is really easy to set up in a cluster environment if you need multiple builds going at once.  It's my favorite CI server of all the ones I've used (CC.NET, Hudson, and TFS). Plus, you can use the ChuckNorris plugin to have him give you the thumbs up or down. "
    },
    {
        "ID": "7705",
        "Question": "When dealing with complicated algorithms in languages with support for nested functions (such as Python and D) I often write huge functions (because the algorithm is complicated) but mitigate this by using nested functions to structure the complicated code.  Are huge (100+ line) functions still considered evil even if they're well-structured internally via the use of nested functions? Edit:  For those of you not familiar with Python or D, nested functions in these languages also allow access to the outer function scope.  In D this access allows mutation of variables in the outer scope.  In Python it only allows reading.  In D you can explicitly disable access to the outer scope in a nested function by declaring it static. ",
        "Best answer": "Always remember the rule, a functions does one thing and does it well! If you can do so, avoid nested functions. It hinders readability and testing. "
    },
    {
        "ID": "7747",
        "Question": " It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration  -- Edsger W. Dijkstra I have deep respect to Dijkstra but I don't agree with everything he said/wrote. I disagree specially with this quote on linked paper wrote 35 years ago about the Dartmouth BASIC implementation. Many of my coworkers or friends programmers started with BASIC, questions below have answers that indicate many programmers had their first experience on programming at BASIC. AFAIK many good programmers started at BASIC programming. I'm not talking about Visual Basic or other \"modern\" dialects of BASIC running on machines full of resources. I'm talking about old times BASIC running on \"toy\" computer, that the programmer had to worry about saving small numbers that need not be calculated as a string to save a measly byte because the computer had only a few hundreds of them, or have to use computed goto for lack of a more powerful feature, and many other things which require the programmer to think much before doing something and forcing the programmer to be creative. If you had experience with old time BASIC on a machine with limited resources (have in mind that a simple micro-controller today has much more resources than a computer in 1975, do you think that BASIC help your mind to find better solutions, to think like an engineer or BASIC drag you to dark side of programming and mutilated you mentally? Is good to learn a programming language running on a computer full of resources where the novice programmer can do all wrong and the program runs without big problems? Or is it better to learn where the programmer can't go wrong? What can you say about the BASIC have helped you to be a better/worse programmer? Would you teach old BASIC running on a 2KB (virtual) machine to a coming programmer? Sure, only exposure to BASIC is bad. Maybe you share my opinion that modern BASIC doesn't help too much because modern BASIC, as long other programming languages, gives facilities which allow the programmer doesn't think deeper. Additional information: Why BASIC? ",
        "Best answer": "The Basics popular at the time of the quote were very different from what we had even 20 years ago.  (Are you counting those among your \"modern\" dialects? ;) Forget loops, subroutines, local variables, and everything that Structured Programming (of which Dijkstra and Knuth were big proponents) emphasized.  You had GOTO, and you liked it. In this context, programmers who only knew global variables, invented their own subroutines (using more global variables for parameters and return values!), and wrote spaghetti GOTOs really were mutilated. If you're 30-something or younger today and Basic was your first language, it wasn't the same language Dijkstra was talking about.  Even if you're older and your first Basic had some of these features, as Murph comments below, it still may not have been the same language Dijkstra was talking about.  You've updated the question with context I never knew before:  Dijkstra's quote is from 1975. It wasn't until version 6 that you got separately compilable procedures — which, I believe, shows the beginning of the shift in focus away from GOTO. \"In 1976, Steve Garland added structured programming features to create Dartmouth SBASIC, a precompiler which produced version 6 output...\" [Wikipedia] The context of the quote is clearly before what we now know as control structures, and many users of the time could've been more familiar with the second-to-latest version — i.e. two versions before Garland's, which is v5 and before separately compilable procedures. GOSUB/RETURN still only handle \"simple subroutines\". \"Variable names were limited to A to Z, A0 to A9, B0 to B9, ..., Z0 to Z9, giving a maximum of 286 possible distinct variables.\" ...and they're all global.  "
    },
    {
        "ID": "7823",
        "Question": "In the comment to this great post, Roy Osherove mentioned the OAPT project that is designed to run each assert in a single test. The following is written on the project's home page:  Proper unit tests should fail for exactly one reason, that’s why you should be using one assert per unit test.  And, also, Roy wrote in comments:  My guideline is usually that you test one logical CONCEPT per test. you can have multiple asserts on the same object. they will usually be the same concept being tested.  I think that, there are some cases where multiple assertions are needed (e.g. Guard Assertion), but in general I try to avoid this. What is your opinion? Please provide a real world example where multiple asserts are really needed. ",
        "Best answer": "I don't think it's necessarily a bad thing, but I do think we should strive towards only having single asserts in our tests.  This means you write a lot more tests and our tests would end up testing only one thing at a time. Having said that, I would say maybe half of my tests actually only have one assert.  I think it only becomes a code (test?) smell when you have about five or more asserts in your test. How do you solve multiple asserts? "
    },
    {
        "ID": "7826",
        "Question": "I like the GPL license but the project I'm working on is a more general purpose one that will be used inside other programs. It doesn't run independently like a CMS or application would. So I'm looking around at other projects to see what they've done.  JQuery has an interesting MIT/GPL license Zend framework has New BSD license Symfony framework has MIT license   As developers, is there an established preference that we expect from libraries. I rarely cared because I never build applications that were sold or distributed, which eliminated the possibility that I would be in conflict with the license of a library I'm using, but I want to know what others are thinking. Will you avoid using a library or a framework if it's GPL?  I'm a bit torn. I'd like to use GPL and as the only license (unlike JQuery for example) but I also worry that this will scare developers away who want to use the library to build distributable code. The other thing is I'm seeing that many libraries are frameworks are released as MIT, but I find the MIT license, well, a bit too \"loose\" for my taste. ",
        "Best answer": " Will you avoid using a library or a framework if it's GPL?  Yes. Using a GPL'd library would essentially require me to publish the source of my software that uses the library, or even GPL it (altough this is somewhat unclear - but better not take legal risks). Publishing sources (let alone GPL'ing a software product) is typically impossible with commercial software (depending on corporate policies). And even if I could publish the source, I dislike the idea that some 3rd party library's license defines how I should license my software. Consider using LGPL, or even better, another licence widely used in libraries the Apache License. Regardless of the details - all GPL licenses are quite incomprehensible and subject to continuous re-interpretation by lawyers and by the FSF - it's clear that the spirit of GPL is to make all software free via viral licenses. In practice they're better to avoid, unless, of course, you agree with their goals and understand them. "
    },
    {
        "ID": "7915",
        "Question": "I have been offered an interesting job, but there's a big caveat for me: they use pair programming. I hate the idea of pair programming, and I'm probably not suited for it: I like to do frequent pauses, I hate to see someone programming (I would constantly poke the pair away to code myself), I have to be in full control of the machine I'm working on, I like to work listening music, and basically I don't like to being tied to someone else. I'm not even a social person. I have however never actually worked with true pair programming (besides few times for a short time to help someone else or to solve a complex task together)... so it is pair programming really that bad? And given my attitude, should I refuse the job or should I leave my current one and give a try?  For people that asked about it: I'm looking for a job where formal design and development are used, as I hate my current job where we are \"coding in the wild\". The company is very interested in my technical profile so they insisted even when I specified I never worked with pair programming and that I probably wouldn't like it (besides being an unsociable loner programmer, I don't like and believe the very concept of pair programming). ",
        "Best answer": "Guess what - nobody likes pair programming at first. When I first tried pair programming I was against the idea and I had tons of objections similar to yours.  I don't like to watch someone else coding, I like to listen to music, I feel I'll be quicker working on my own, I don't want to stop and explain myself constantly, etc etc etc. Then I tried it.  And guess what?  It still sucked.  We were having all kinds of problems - I wanted to work different hours to the other guy, we had different keyboard settings and shortcuts, our desks didn't work very well for pair programming etc etc. This went on for about a week.  During that week we were rewriting the entire legacy login system for a distributed application.  We had to learn how some seriously difficult threading issues work, figure out how remoting sinks worked, change tons and tons of legacy code to work with our new login module and pretty much do some of the most hectic coding I've had to do in my career.  After a week of this we deployed it.. and everything just worked.  Not a single bug.  Not one. That's when I figured there might be something to this pair programming nonsense.  We started to do more pair programming.  We even started to force everyone to work in pairs.  It rocked.  I probably learnt more in a month of doing that than I did in the previous 2 years.  The other issues didn't go away.  Sometimes you'll be stuck with a pair that you dislike.  It's gonna happen.  Sometimes you'll struggle to find overlapping working hours.  Sometimes you'll want to just work on your own.  But the advantages of pair programming are just incredible. Since then I've always tried to do as much pair programming as possible.  It's simply the best way of learning.  This company forces everyone to do pair programming? Where do I sign up?  It seems to me they are really enlightened and you will be in a really intense working environment.  Awesome. Having said that, make sure that the pairs are rotated often.  You don't want to be stuck developing with one other developer for months on end - you'll go out of your mind.  We're all human, after all.  Also, check that this has been an ongoing practice for a while.  If it's not, it's not the end of the world, but if it's an accepted practice it's a massive green light. Trust me, you want to work for a company like that. "
    },
    {
        "ID": "7951",
        "Question": "I'm a student of computer science but I am only taking entry level web development classes. I'm looking for some out-of-class reading, tutorials, and other ways of learning but I'm not sure what I should start in.  What is a good set of programming languages (different paradigms?), frameworks, suggested projects, and maybe even some open-source communities that I should start to look at and start learning? Also, maybe even some books or blogs on development processes in the professional world. I'm looking to start getting in to professional development around the end of college. I understand practicing it will be the best way to learn anything but if I don't know what I should practice, I'm lost at that. :) ",
        "Best answer": "These would be my baseline recommendations for topics to cover, not necessarily in-depth, but at least a general understanding: (in no particular order)  A compiled language - C#, Java, C, or if you're brave, C++. Understand about source code gets compiled into something else and then run by the runtime. A scripted language - JavaScript, Python. Know the differences to (1) and the strengths in terms of dynamic typing and rapid development. HTML + CSS. Whether for documentation or test harnesses, you'll use it somewhere. SQL. Data lives in databases. They all have their own flavours, but a basic understanding of SQL helps a lot. Version Control. Pick any system - Subversion, Git, Mercurial, CVS - it doesn't matter which, just understand about the check out, modify, build, merge, review, build, commit workflow. Testing - whether unit testing, automated or manual.  Security. Software systems get attacked - even the un-sexy ones - and users' information is becoming worth more than their bank details. Algorithms - understand Big O notation and that choice of good algorithm matters much more than micro-optimisation. Design Patterns - no point in re-inventing the wheel. The Software Development Lifecycle. Doesn't matter which methodology you prefer, but go find out what they are.  and when you've got the first job: 11.. How your employer measures success. All of the above are moot if your employer has their own unique systems which you have to use. Find out how to be successful in your employers' eyes first and then introduce the items you've learned along the way. "
    },
    {
        "ID": "7966",
        "Question": "I have been listening to Scott Hanselman and Rob Conery's podcast, This Developer's Life. In the latest episode, they discuss personality traits:  1.0.4 - Being Mean. What makes people mean in our   industry? What about aggressive?   Confident? What's the difference?   Would you rather have a drill sergeant   for a boss, or a zen master? We talk   to Cyra Richardson and Giles Bowkett.  It got me thinking, what traits did the best managers you've worked for have in common? EDIT: Just to clarify, as there have been a few close votes, I'm interested in whether there are traits common to managers of developers that are not necessarily those traits that a manager of some other profession requires.  As for whether this is programming related or not, well I don't want to ask this question on a site that isn't about programming because, frankly, I'm not as interested in what people who make cans of soup for a living want from their managers as I am interested in what developers want from their managers. ",
        "Best answer": "Joel Spolsky calls it the \"Abstraction Layer\". Do what it takes to keep me programming. Let me know what's going on in the company, but keep me out of the politics. Eventhough I still have to do it, at least acknowledge that the request is bull sh!t.  "
    },
    {
        "ID": "8090",
        "Question": "Question first: What are some feasible alternatives to time tracking for employees in a web/software development company, and why are they better options Explanation: I work at a company where we work like this. Everybody is paid salary. We have 3 types of work, Contract, Adhoc and Internal (Non billable). Adhoc is just small changes that take a few hours and we just bill the client at the end of the month. Contracts are signed and we have this big long process, the usual. We figure out how much to charge by getting an estimation of the time involved (From the design and the developers), multiplying it by our hourly rate and that's it. So say we estimate 50 hours for a website. We have time tracking software and have to record the time in 15 we spend on it (7:00 to 7:15 for example), the project name, and give it some comments. Now if we go over the 50 hours, we are both losing money and are inefficient.  Now that I've explained how the system works, my question is how else can it be done if a better method exists (Which I'm sure one must). Nobody here likes the current system, we just can't find an alternative. I'd be more than willing to work after hours longer hours on a project to get it done in time, but I'm much inclined to do so with the current system. I'd love to be able to sum up (Or link) to this post for my manager to show them why we should use abc system instead of this system. ",
        "Best answer": "Take a look at evidence based scheduling.  Can really help you see how accurate your estimates are likely to be.  Over the last year or so at Fog Creek we’ve been developing a system that’s so easy even our grouchiest developers are willing to go along with it. And as far as we can tell, it produces extremely reliable schedules. It’s called Evidence-Based Scheduling, or EBS. You gather evidence, mostly from historical timesheet data, that you feed back into your schedules. What you get is not just one ship date: you get a confidence distribution curve, showing the probability that you will ship on any given date. It looks like this:  The steeper the curve, the more confident you are that the ship date is real. Here’s how you do it...  "
    },
    {
        "ID": "8111",
        "Question": "I'm a self-taught programmer and have just started a computer science degree to supplement my knowledge and fill in the gaps. However, I'm already debating the direction of my education. I want a 4 year bachelor's degree for sure, but after that, I'm not sure: Is it worth it to get an M.Sc? What about a Ph.D.? What opportunities do these degrees open up? Currently my goal is to be a software developer, but there are a number of fascinating fields in the software industry and I'm certainly interested in investigating many of them. It is in this respect that I think a higher degree may be worth it -- even if it wouldn't necessarily help a career of software development. So will it be worth it? Will grad school open doors? ",
        "Best answer": "Getting a PhD does two things to you and it uses up 4 or more years. You will need to decide whether those two things are worth the time. First, it gives you some initials after your name. For the rest of time, people who see those initials will think \"wow, you must be really smart!\" (and often, they will say it out loud.) On a resume it will generally help you, though in some circumstances it might hurt you, with people thinking you're overqualified or an egghead. Second, and more importantly in my opinion, is the changes in your brain and your attitude that happen over the course of the degree. You will end up knowing more about some small part of the world than any other person. You will stand in front of 3 or 4 people who are experts, sometimes world-renowned experts, and teach them about your work. When you walk in, those experts will be supervisor, examiner, \"bosses\" of a sort and when you walk out they will be your peers. You  will learn a lot about one corner of computer science and a lot more about yourself and your capabilities. You will be confident being \"the expert\" when required. And that changes everything. Unless you know now you want to be a prof, or to join a particular research lab, it seems unusual to me that you could decide about the PhD before doing the undergrad work. Go do the undergrad, and make friends with some grad students. They will tell you all you want to know about the life of a grad student, the life of a baby prof, the job prospects when you're out, and more. I don't regret the time I spent on my PhD. It has opened many doors for me. It has made me more memorable (\"right, you're the woman with the PhD, aren't you?\") and been an asset to some of my teams independent of my abilities or knowledge (\"we have a PhD on our team!\"). But the industry I joined then is not the industry you're joining now. Talk to people who are in the thick of it now and then make a decision as you finish the undergrad work. "
    },
    {
        "ID": "8145",
        "Question": "Looking at common Agile practices it seems to me that they (intentionally or unintentionally?)  force developers to spend more time actually working as opposed to reading blogs/articles, chatting, coffee breaks and just plain procrastinating. In particular: 1) Pair programming - the biggest work-forcer, just because it is inconvenient to do all that procrastination when there are two of you sitting together. 2) Short stories - when you have a HUGE chunk of work that must be done in e.g. a month, it is pretty common to slack off in the first three weeks and switch to OMG DEADLINE mode for the last one.  And with the little chunks (that must be done in a day or less) it is exact opposite - you feel that time is tight, there is no space for maneuvering, and you will be held accountable for the task pretty soon, so you start working immediately. 3) Team communication and cohesion - when you underperform in a slow, distanced and silent environment it may feel ok, but when at the end of the day at Scrum meeting everyone boasts what they have accomplished and you have nothing to say you may actually feel ashamed. 4) Testing and feedback - again, it prevents you from keeping tasks \"99% ready\" (when it's actually around 20%) until the deadline suddenly happens. Do you feel that under Agile you work more than under \"conventional\" methodologies? Is this pressure compensated by the more comfortable environment and by the feeling of actually getting right things done quickly? ",
        "Best answer": "The main idea behind the agile methods is to help you be productive - in a positive sense. No one cares if you spend an hour surfing every day if you meet the deadline. Everyone gets mad if you surf half an hour every day but miss your deadline. The solution: Make it easier for you to meet the deadline. As you noticed, pair programming makes sure you stay focused (among all the other advantages like improving skill/knowledge spreading, better code, less bugs, uniform design, etc.). I found that discipline is always a struggle for me. If I pair with someone, chances are that one of us wants some work done today and pulls the other along. So the \"work for a month\" often becomes turns into \"work together for one week\", being surprised how that huge amount of work resolved in the end, spend a day or so recovering (refactoring, fixing TODOs in the code, adding a couple of tests, surfing with a clear conscience) and then grabbing the next month of work. Net result: I'm much more relaxed (more because than despite the constant supervision), team cohesion is much better, work gets done more quickly, people don't hang around some minor issue for hours or even days (because someone else can spot the problem much faster). When you say \"you may actually feel ashamed\", isn't that a good thing? It means you feel that you did wrong and you should. You're not getting paid to get nothing done. Not getting anything done makes you feel helpless, unhappy, unworthy, miserable. Instead of feeling ashamed, stand back and think \"Why didn't I accomplish anything today?\" Do you need help? Is there something you don't understand? Is the current task too hard? You don't like it? Maybe you can switch the task with someone else. Maybe someone else can help you get through. Agile means: Assume responsibility instead of being micro-managed like a puppet on strings. You need a tool? Go to your boss and ask for it. Learn to argue. Learn to stand up and shout when you have to. As for tests, there is a sweet spot when your code suddenly collapses from \"nice\" to \"perfect\". That's the moment when you notice that you need to implement feature X and you thought that will be a nightmare and suddenly realize that the code is almost there. Just a small refactoring here and there. A new class and done. Four weeks of work suddenly became a day. Victory! Triumph! "
    },
    {
        "ID": "8157",
        "Question": "I work at a company where we have a lot of different skillsets in the development team.  We do all of the following (generally geared towards web):  .NET (MVC, Umbraco, ASP.NET, Surface) Java (Spring, Hibernate, Android)  PHP (Zend, Code igniter)  Actionscript 3 AIR Objective-C Html/Javascript (obviously)  We're trying to streamline our development process. We currently have a TeamCity server that builds and deploys .NET projects with msbuild/msdeploy/nant. What I want is something like maven that will give us a standard project template structure that works for most projects to allow people from different teams to move between projects easily. Currently this works on one platform because we tend to do things in a standard way for that platform (as long as certain people have been involved) however I want to use something like maven to standardise how a project is laid out and built. Has anyone tried anything like this before? Experiences? Books? ",
        "Best answer": "As for .NET, there are three projects to port Maven. See this answer on stackoverflow.com. Also this wiki article might be helpful. As for the other languages, I suggest to apply the same structure that Maven supports (all sources below src/language/main, etc) and then either write Maven plugins to build them or at least write generic \"Makefile\" templates which support this structure out of the box. "
    },
    {
        "ID": "8228",
        "Question": "I am rereading Refactoring by Martin Fowler. In Chapter 4, Building Tests, I came across the following passage. In fact, one of the most usefull times to write tests is before you start programming. When you need to add a feature, begin by writing the test. This isn't as backward as it sounds. By writing the test you are asking yourself what needs to be done to add the function. Writing the test also concentrates on the interface rather than the implementation (always a good thing). It also means you have a clear point at which you are done coding--when the test works. While I am an advocate of test-driven development now, I did not remember having been introduced to the concept when I originally read this book nearly 5 years ago. According to Amazon.com, this book was originally published on July 8, 1999. Is this the first published reference to test-first programming or is there something even earlier? ",
        "Best answer": "Jon Bently in Programming Pearls (originaly published 1986) does not specifically mention Test-First programming.   But in the chapter \"Writing Correct Programs\", he describes writing an algorithm by first defining the preconditions, invariants and postconditions, and in the next chapter describes an automated testing framework. It's not quite test-first, but he was definitely laying some of the groundwork. Also, CIO Magazine, March 1993, Bug Busters, by Lucie Juneau,  pg 84:  Test cases ... can be developed even   before any code has been written.    Ideally these cases are based on an   application's requirements ... If developers are given requirements-based tests before they begin to write code, they will design a product that can pass those tests ... \"  "
    },
    {
        "ID": "8236",
        "Question": "I just sent out emails to five local web design companies to my area asking to take drawings to HTML/CSS/jQuery. None of the ones who accepted the deal seem suitable to myself. Others rejected the offer because they wanted to 'provide an end-to-end solution' or are 'booked till June'. The local companies did not seem suitable to myself because my review process is this: goto their website, do a view-source. I'll see really weird things (contact us forms that go nowhere), really old things (mm_menu.js), and portfolios that are non-existent (aren't on the site, don't link anywhere, or otherwise). The company would like to hire as locally as they can rather than out-source to another country. Answers I'm looking for  Processes you use when searching for someone How you qualify their aptitude for the project Anything that you think I'm doing wrong, or should be doing also.  Answers I'm not looking for:  \"Hello sir please contact me we do everything for 10 dolla.\" My bud's great at this stuff, call him. example.com is the best for this.  ",
        "Best answer": "I might start out by searching for people on google, because if their own site isn't findable, I won't trust that mine would be.  Then I would want to see some portfolio sites, ideally ones where they use the technology I'm wanting to use. If they don't have any, they don't get to do the job unless they can show they clearly understand what I'm talking about and how to implement it.  Be ready to widen the net- there are a lot of decent designers and developers around so no point settling for someone weak just because they are very local rather than somewhat local. If I can't find anyone convincing that way, I might look the other way- find some sites that do something like what I am looking for and then find out who developed them.  "
    },
    {
        "ID": "8254",
        "Question": "I'm developing a statically- and strongly-typed, compiled language, and I'm revisiting the idea of whether to include function overloading as a language feature. I realized that I'm a little bit biased, coming mainly from a C[++|#] background. What are the most convincing arguments for and against including function overloading in a language?  EDIT: Is there nobody who has an opposing opinion? Bertrand Meyer (creator of Eiffel back in 1985/1986) calls method overloading this: (source)  a vanity mechanism that brings nothing to the semantic power of an O-O language, but hampers readability and complicates everyone's task  Now those are some sweeping generalizations, but he's a smart guy, so I think it's safe to say he could back them up if he needed to. In fact, he almost had Brad Abrams (one of the CLSv1 developers) convinced that .NET shouldn't support method overloading. (source) That's some powerful stuff. Can anyone shed some light on his thoughts, and whether his viewpoint is still justified 25 years later? ",
        "Best answer": "Function overloading is absolutely critical for C++-style template code.  If I have to use different function names for different types, I can't write generic code.  That would eliminate a large and heavily used part of the C++ library, and much of C++'s functionality. It's usually present in member function names.  A.foo() can call an entirely different function from B.foo(), but both functions are named foo.  It's present in operators, as + does different things when applied to integers and floating-point numbers, and it's often used as a string concatenation operator.  It seems odd not to allow it in regular functions as well. It enables the use of Common Lisp-style \"multimethods\", in which the exact function called depends on two data types.  If you haven't programmed in the Common Lisp Object System, try it before you call this useless.  It's vital for C++ streams. I/O without function overloading (or variadic functions, which are worse) would require a number of different functions, either to print values of different types or to convert values of different types to a common type (like String). Without function overloading, if I change the type of some variable or value I need to change every function that uses it.  It makes it much harder to refactor code. It makes it easier to use APIs when the user doesn't have to remember which type naming convention is in use, and the user can just remember standard function names. Without operator overloading, we'd have to label each function with the types it uses, if that base operation can be used on more than one type.  This is essentially Hungarian notation, the bad way of doing it. Overall, it makes a language far more usable. "
    },
    {
        "ID": "8257",
        "Question": "I'm currently writing a load test for a cache which should test how a cache will react to persistent requests. A colleague and I had differing opinions on how this load testing should be performed. I believe that a load test should be as random as possible. It should model real-world load as much as possible, and the way towards that is randomality. So I have created this random test as follows:  Test data is held in spreadsheets and is loaded into TestRunner objects at startup  The test data is not random  The load test will run 10 random TestRunners in individual Threads at the same time The object returned by the cache will be tested to make sure it is sensible, it is not thoroughly tested  Any tests that fail will be output at the end and each test has a unique ID to easily find failed tests  At random intervals, the cache will be cleared to model the real-world requirement of the cache being cleared at any time. The load test will run for a configurable amount of time  My colleague's idea of what a load test should do is:  Test data is held in spreadsheets and is loaded into TestRunner objects at startup All TestRunner objects are run in a sequential manner Each time the load test is run, it will run the TestRunner objects in the same order  Which methodology do you feel would produce the most reliable load test?  I personally think the random test will produce a more reliable load test as it will model real-world usage. It is not known what order requests will come in when this is released to production, so it should be tested with that unknown element taken into account. However, running all tests in the same sequence each time will make any failures reproducable, which is important during testing. ",
        "Best answer": "Do you have a way to reset the data once the test is run (if this is even necessary)? If so, what about running the non-random test first - to look for failures when run that way (and possible anomalies caused by the data itself)?  Then, after resetting (if necessary), run the random tests to reflect the real world load. "
    },
    {
        "ID": "8297",
        "Question": "My management just asked an unprecedented question in my (admittedly brief) history with the organization: \"What can we do to help you?\" Simultaneously, we're working several big projects for a fairly new client whose ability to push requirements around mid-project is legend. Developing for these guys is like tap dancing on quicksand. Seems like a prime opportunity to propose a shift to a more agile approach. The thing I know I'm going to get asked, and that I don't have any idea about, is how to quote/bid/bill for that sort of project. Do you go hourly? Do you bid a range of prices? Do you charge by the sprint? More generally, the aspect of the Agile Manifesto that reads \"We value customer collaboration over contract negotiation\" is GOING to scare my management. How do you value that in the real world of customers who want a lot for a little? ",
        "Best answer": "We have the same problem in my company.  There's a history of fixed-price, fixed-timeline projects, and our clients aren't generally very progressive. Regarding development with no up-front commitments, I've heard so many fundamentalist agilists say, \"I know it's hard, but you just need to push the benefits\", or, \"They might be skeptical but they'll see how well it went and come back to you next time\".  In some industries, maybe. In ours, that's a load of crap. I can't see any of our customers agreeing to just let us do our thing with no commitment on scope or price. What we've found is that it's not always necessary to change the way you quote/bid/bill customers for an agile project.  You can keep the agile process while sticking to your quote if you manage it properly. Quote the way you normally would (with padding), and set some boundaries around the scope of the project.  From that point on, follow your agile methodology:  Prioritise the work with the customer - develop the important stuff first Develop in small iterations, showing your progress Collaborate with the customer to make sure you're actually developing what they want Grow the spec as you write the software  But more importantly:  If a function turns out to be more complicated than what was originally requested, tell the customer immediately and make sure they're aware it will affect the timeline and/or price. Treat major (or even minor) changes as chargeable change requests.  You're still using Agile internally and getting the benefits, but the customer is seeing a more familiar fixed-price, fixed-timeline, fixed-scope project. Any changes cost money and blow out the time. The hardest part about this is setting the boundaries up front.  It's definitely not something that can be done by just your sales guy, BA, or project manager. You need an experienced developer in those meetings.  You need to nail down the areas that could cause problems and decide on expectations. "
    },
    {
        "ID": "8311",
        "Question": "This might be slightly off topic, but I'll risk it, as the site is about Programmers ! Programmers are good at constantly learning new programming languages, but how good are they at learning a new spoken language ? Have you taken up a foreign language (French/Spanish/etc) as an adult and mastered it? Was it easy? I ask because I have been trying to learn French for quite some time now, and I'm still at the annoying \"Je parle un peu de Française\" stage. I've attended two French courses, one where the majority of the class were programmers, and one where they weren't and the difference in ability was quite apparent. Does a mathematical / logical inclination hinder learning a spoken language where grammar is not in ones and zeros? Or am I just transferring blame instead of simply accepting that I am not good with languages. [It is important that you have not been taught the language in school, as early exposure really gives you the upper hand. I've picked up and got quite good at languages I've been exposed to under the age of 10.] ",
        "Best answer": "I find it easy and fun to learn new languages! The only reason I'm any good at programming is that I've got a strong inclination toward language. All human languages are fundamentally the same, and not even vast differences in phonology, grammar, and vocabulary can get in the way of the fact that all people work in basically the same way. I find it immensely rewarding to draw parallels between etymologies, to discover the underlying nature of what makes a language tick, and to learn how native speakers understand their own language. Not to mention that studying a wide variety of orthographies has given me great clerical accuracy, which is a big help in programming. However, your mileage may vary—I'm a programmer because I'm a linguist, not the other way round, and you can become proficient at programming in many different ways. Edit: Here are a few tips that I think can help programmers with language learning: Natural languages are not programming languages. Natural languages do not have rules, but they do have patterns. If you notice a pattern, don't claim it's a rule and then complain about all of the exceptions. Linguistics is not a black-and-white field. I've noticed that people of a technical mindset get caught up in whether they're \"correct\" and lose sight of the fact that it's more important to be understood. Natural speech has inherent meaning that transcends literalism. Learning a language is not about rote memorisation. No native speaker of Spanish says to himself \"voy, vas, va, vamos, vais, van\" to remember how to conjugate \"to go\". He just does it in running speech because he has developed a sense over time of what sounds right. Do not take a \"phrasebook\" approach to language learning: you will find yourself lost for the appropriate phrase because you won't be able to produce your own. Learning vocabulary is not the same as learning an API. Natural languages are redundant and compressible, and you can use this to your advantage as a student. If you pronounce or spell something wrong, chances are you will still be understood. Look up the etymologies of words to get a sense of their deeper meaning. Having a sense of the language as it was is just as important as knowing the language as it is. It's okay to make some mistakes. Step outside your comfort zone and experiment. Try to talk the way native speakers do. If you notice that you pronounce or articulate something differently, try to discern exactly how. If you don't understand everything someone says, it's okay to ask them to repeat themselves or explain. If you make a mistake, the worst that can happen is a misunderstanding, and if you're confident and outgoing then it turns into a funny situation rather than an awkward, embarrassing one. Have fun. "
    },
    {
        "ID": "8355",
        "Question": "The article \"Promiscuous Pairing and Beginner’s Mind\" (PDF) suggests that you put someone in the pair that knows the least about the particular area of the code base. It also suggests that you swap out the senior member of the pair every 90 minutes or so.  Not only will the newbies learn about that area of the code but they will also think differently compared to someone who already knows the area. Does anybody have experience with this strategy? Does it have any connection with reality? I found other questions about when to use pair programming and whether to accept a job where pair programming is required, but I didn't find any that are specifically about promiscuous pairing and this \"beginner's mind\" strategy. If you are unfamiliar with pair programming, there are interesting articles on Wikipedia and c2.com. ",
        "Best answer": "I think your question understates (perhaps, confuses) Pair Programming and Promiscuous Pairing. When you do pair programming and one of the programmers knows way more about the task at hand, the other programmer learns very quickly (the languages, the tools, the design or requirements of the product they're working on.  I do have experience with that and highly recommend it for bringing your co-workers or yourself up to speed. The idea of Promiscuous Pairing is when you have N programmers on the team and make all possible pairs out of them and rotate those pairs frequently, then such knowledge spreads throughout the team very quickly. "
    },
    {
        "ID": "8402",
        "Question": "I've had an argument recently with a fellow programmer. He was interviewing for a new position and was asked this question:  Give a sequence of numbers starting at   X and ending in Y but with one element   missing so N is Y-X-1, find the   missing element in O(N) or better.  Now, the answer is irrelevant here (but interesting). This started a discussion on whether this was even a good question to ask during an interview. One side: Algorithms are an inherit part of programming and a candidates ability to answer this question supports that this candidate will be a good programmer and be able to solve larger problems and can handle most programming tasks which are ultimately easy to understand and answer. Other side: Writing algorithms from scratch is rarely used in modern programming and therefore is irrelevant in the bigger question of whether the person will be a good programmer. A person could successfully answer this question yet still not be able to do more common programmings tasks. Your thoughts? Good interview question or not? ",
        "Best answer": "I agree with asking a algorithm question, but I disagree with insisting on a specific big-O quality level. Asking this sort of question is interesting to see how the person approaches the problem and what pitfalls they consider in their attempt, but unless they are writing something insanely incorrect or inefficient the actual detail of what they write is not as telling as the fact that they get through the problem solving / design steps in a coherent manner. I ask a similar question, but the people that I have had the best luck with after hire are the folks that gave flawed answers but had the correct idea in their approach. "
    },
    {
        "ID": "8415",
        "Question": "I'm looking for something that allows me to work out an algorithm on a computer similar to how I would do it on a whiteboard. At work, I have a huge whiteboard that I can use for this, but at home, I don't (and can't). This makes it difficult to design algorithms when I'm working on hobby projects. I think better when I use drawings as opposed to text, so pseudocode is no good. I guess what I'm looking for is something like a flowchart program, but that allows a less rigid approach to designing algorithms.  I'd like the program to be cross-platform (Mac, Linux), but I'll be happy even if it just runs on Macs. Free is preferred, but reasonably priced programs are good too. I would prefer programs that people have used and had experiences with, rather than whatever turned up on Google, as I know of some flowchart and mind-mapping software, but haven't really been satisfied with either. I realise that a computer isn't the best platform for doing this kind of stuff, but assume for a moment that it's the only medium I possess. ",
        "Best answer": "If I have to brainstorm some design stuff that takes more complexity than Notepad can cope with, I usually just use a pencil/pen and paper. "
    },
    {
        "ID": "8454",
        "Question": "I was having a chat with a coworker who is working on a polling app and framework. He was asking technical questions and I suggested he open source the application to get more quality opinions from developers who are interested in this problem and are willing to give it heavy though.  He has a different point of view which I think is still valid so I want to open this question for discussion here. He says he believes something like a polling framework should not be open sourced because it will reduce its security and validity as people reveal loopholes through which they can cheat. Can't say I completely disagree. I see a somewhat valid point there, but I always believed that solutions by a group of people are almost always better than a solution thought by a single person asking a small number of coworkers, no matter how smart that person is. Again I'm willing to accept that maybe some types of applications are different.  Does anyone have an argument in his favor? I'd really like to present your responses to him.  ",
        "Best answer": "In fact, being open source helps you to be more secure.  I personally believe that when a program began as closed source and is then first made open source, it often starts less secure for any users (through exposure of vulnerabilities), and over time (say a few years) it has the potential to be much more secure than a closed program. If the program began as open source software, the public scrutiny is more likely to improve its security before it's ready for use by significant numbers of users, but there are several caveats to this statement (it's not an ironclad rule). Just making a program open source doesn't suddenly make a program secure, and just because a program is open source does not guarantee security:  First, people have to actually review the code. This is one of the key points of debate - will people really review code in an open source project? All sorts of factors can reduce the amount of review: being a niche or rarely-used product (where there are few potential reviewers), having few developers, and use of a rarely-used computer language. Clearly, a program that has a single developer and no other contributors of any kind doesn't have this kind of review. On the other hand, a program that has a primary author and many other people who occasionally examine the code and contribute suggests that there are others reviewing the code (at least to create contributions). In general, if there are more reviewers, there's generally a higher likelihood that someone will identify a flaw - this is the basis of the \"many eyeballs\" theory. Note that, for example, the OpenBSD project continuously examines programs for security flaws, so the components in its innermost parts have certainly undergone a lengthy review. Since OSS/FS discussions are often held publicly, this level of review is something that potential users can judge for themselves.       One factor that can particularly reduce review likelihood is not actually being open source. Some vendors like to posture their \"disclosed source\" (also called \"source available\") programs as being open source, but since the program owner has extensive exclusive rights, others will have far less incentive to work \"for free\" for the owner on the code. Even open source licenses which have unusually asymmetric rights (such as the MPL) have this problem. After all, people are less likely to voluntarily participate if someone else will have rights to their results that they don't have (as Bruce Perens says, \"who wants to be someone else's unpaid employee?\"). In particular, since the reviewers with the most incentive tend to be people trying to modify the program, this disincentive to participate reduces the number of \"eyeballs\". Elias Levy made this mistake in his article about open source security; his examples of software that had been broken into (e.g., TIS's Gauntlet) were not, at the time, open source. Second, at least some of the people developing and reviewing the code must know how to write secure programs. Hopefully the existence of this book will help. Clearly, it doesn't matter if there are \"many eyeballs\" if none of the eyeballs know what to look for. Note that it's not necessary for everyone to know how to write secure programs, as long as those who do know how are examining the code changes. Third, once found, these problems need to be fixed quickly and their fixes distributed. Open source systems tend to fix the problems quickly, but the distribution is not always smooth. For example, the OpenBSD developers do an excellent job of reviewing code for security flaws - but they don't always report the identified problems back to the original developer. Thus, it's quite possible for there to be a fixed version in one system, but for the flaw to remain in another. I believe this problem is lessening over time, since no one \"downstream\" likes to repeatedly fix the same problem. Of course, ensuring that security patches are actually installed on end-user systems is a problem for both open source and closed source software.  Another advantage of open source is that, if you find a problem, you can fix it immediately. This really doesn't have any counterpart in closed source. In short, the effect on security of open source software is still a major debate in the security community, though a large number of prominent experts believe that it has great potential to be more secure.  Look at Linux... "
    },
    {
        "ID": "8560",
        "Question": "I'm currently working on throwing together some basic prototypes, partly to gather requirements and partly to design the final UI.  At the moment I'm trying building up the screen using Post-it notes, with yellow notes for information and pink for actions (buttons or menus). The idea being that you can easily move, remove and add information. But I'm sure there are more efficient methods out there.  What is the recommended way for developers to efficiently create non-interactive UI prototypes? And why?  I tried some pen, paper and Post-it note versions and they went down like a lead balloon (likely my drawing skills). In the end I used Balsamiq, which thus far is liked by most users and they get it is prototype. Alas, some people are still having problems with the idea that first one should get an idea of what the application should do via some lo-fi prototypes and really want to \"see something on the screen\" before committing to anything. ",
        "Best answer": "I prefer a whiteboard. It makes it easy to change as you make decisions without redrawing the whole thing.  It's easy to share with other (nearby) developers.  It's easy to annotate using sticky-notes or other colors. "
    },
    {
        "ID": "8565",
        "Question": "Loose coupling is, to some developers, the holy grail of well-engineered software.  It's certainly a good thing when it makes code more flexible in the face of changes that are likely to occur in the foreseeable future, or avoids code duplication.   On the other hand, efforts to loosely couple components increase the amount of indirection in a program, thus increasing its complexity, often making it more difficult to understand and often making it less efficient.   Do you consider a focus on loose coupling without any use cases for the loose coupling (such as avoiding code duplication or planning for changes that are likely to occur in the foreseeable future) to be an anti-pattern?  Can loose coupling fall under the umbrella of YAGNI? ",
        "Best answer": "Is programming practice X good or bad?  Clearly, the answer is always \"it depends.\" If you're looking at your code, wondering what \"patterns\" you can inject, then you're doing it wrong. If you are building your software so that unrelated objects don't fiddle around with each other, then you're doing it right. If you're \"engineering\" your solution so that it can be infinitely extended and changed, then you're actually making it more complicated. I think at the end of the day, you're left with the single truth: is it more or less complicated to have the objects decoupled?  If it is less complicated to couple them, then that is the correct solution.  If it is less complicated to decouple them, then that is the right solution. (I am presently working in a fairly small codebase that does a simple job in a very complicated way, and part of what makes it so complicated is the lack of understanding of the terms \"coupling\" and \"cohesion\" on the part of the original developers.) "
    },
    {
        "ID": "8660",
        "Question": "We're integrating Mercurial slowly in our office and doing web-development we started using named branches.   We haven't quite found a good convention as far as naming our branches though.   We tried:   FeatureName (Can see this causing problem down the line) DEVInitial_FeatureName (Could get confusing when developer come and go down the line) {uniqueID (int)}_Feature  So far the uniqueID_featureName is winning, we are thinking of maintaining it in a small DB just for reference. It would have: branchID(int), featureName(varchar), featureDescription(varchar), date, who etc... This would give us branches like: 1_NewWhizBangFeature, 2_NowWithMoreFoo, ... and we would have an easy reference as to what that branch does without having to check the log. Any better solution out there? ",
        "Best answer": "If you don't have an issue tracker, I recommend setting one up and then using {issue tracker name}_{ticket number}. When someone years from now files a bug and you don't know exactly how the feature was supposed to work, it'll be easy to annotate the file and get back to where the user may have requested that exact functionality. "
    },
    {
        "ID": "8721",
        "Question": "Here's a bit information about me, before starting with the question. I am a Computer Science Undergraduate, Java being my primary coding language. The basic problem in my University are the teaching standards. No one is concerned about teaching coding knowledge to students, rather than just theoretical knowledge. The effect being, most of my fellow college mates don't understand programming at all. Even I haven't been able to come out of the traditional programming environment, which limits my coding to an extent. What are the possible ways by which I can develop and expand my programming/coding skills. Also, can you suggest the sources for the same? Edited: Sources suggesting development of coding skills. ",
        "Best answer": "My favorite quote is from Confucius:  I hear, I know. I see, I remember. I   do, I understand.  All knowledge I got, was from applying one and single strategy:  Take the most challenging path, always.  You want to learn C#? Get a job as a C# developer. You want to learn Italian? Go there with a dictionnary english/italian, and talk Italian You want to learn coding ? Code! "
    },
    {
        "ID": "8748",
        "Question": "Quote from Wikipedia of the article \"High-level programming language\":  A high-level programming language is a programming language with strong abstraction from the details of the computer. In comparison to low-level programming languages, it may use natural language elements, be easier to use, or be more portable across platforms. Such languages hide the details of CPU operations such as memory access models and management of scope.  I.e., as the level of programming language increases, the further away the programmer goes from the hardware on which the program runs. Now I do not know the statistics of language usages across levels, but I would suspect that the higher level languages more and more are replacing the lower level languages. If so, can this lead to a deficit of programmers with computer architecture knowledge? Would this be a problem for the industry? ",
        "Best answer": "It can, but likely won't lead to a problem. It's just economics.  If the vast majority of people lose the ability to understand the underlying architecture, and there is still a huge NEED to understand the underlying architecture, then the ones who do will have jobs and get paid more, while those who don't will only have jobs where that is not needed (and may still get paid more...who knows?). Is it helpful to know? Absolutely. You'll likely be better.  Is it necessary in most cases?  No.  That's why abstraction is so great, we stand on the shoulders of giants without having to be giants ourselves (but there will always be giants around). "
    },
    {
        "ID": "8789",
        "Question": "I was browsing through this talk about \"Historically Bad Ideas\" over the history of Computer Science, and found an interesting presentation about the rise & fall of the Java Enterprise initiatives. Slide #16 grabbed my attention by suggesting that Container Managed Persistence is a \"naïve ORM solution\". Of course, I assume  the presenter made a live in-depth analysis of the problem, which is missing in the slide. So I was left intrigued by that statement. Is CMP Entity Beans just a heavy piece of naive engineering? Aside any bias from the author(s) of the presentation, what would constitute a more adequate ORM solution in the domain of languages like Java or C#? I'm not asking for specific tools or frameworks, but better approaches. ",
        "Best answer": "In general a good ORM solution should be easy to use and understand. It should promote the use of good design patterns (DAOs, DTOs, lazy loading, services, transaction boundaries, ease of configuration etc). It should be non-invasive - that is, it should not force you to extend special classes or implement special interfaces.   The EJB specifications fell over a lot in the early days leading to the mass migration away to the likes of Spring and Hibernate. EJB1 failed to adequately define CMP fields on the bean, EJB2 sort of implied that they should be abstract accessors rather than actual fields, which was just odd, and it wasn't until EJB3 that something close to what everyone actually wanted was created. By then it was too late, everyone believed EJB sucked and it took the JPA and JTA JCRs to put things right again. EJB1 and 2 generally forced the developer to put all their persistence configuration in a bunch of XML files well away from the actual code that was using it. This lead to lots of confusion and buggy code. Other ORM frameworks learned from this and decided to use annotations instead. Big win for them. EJB1 and 2 had very limited support for different kinds of relationships and how they could be implemented in the underlying relational database. All kinds of special interfaces had to be adhered to and the resulting code was hard to fathom. Anyway, all that is in the past and we can look forward to a bright future with the likes of Hibernate implementing JPA and JTA. All very harmonious. "
    },
    {
        "ID": "8890",
        "Question": "So the other day my boss (let's call him Colfax) asked me to work on a project, but that I should not discuss it with anyone (including the other programmers on my team) and that it would have to be done outside of normal work hours. In exchange, Colfax offered me \"off-the-book\" vacation days equal to the hours spent on the project. When I asked Colfax if his boss (let's call him Schuyler) knew what he was offering, Colfax said that Schuyler does not know and implied that he (Colfax) would get into trouble if Schuyler found out. My boss also said that if I were to go along with this, my efforts would be remembered for \"future consideration\".  The work is for our employer so everything there is on the up-and-up. However, I have an uneasy feeling about the whole thing. Part of me wants to work on the project -- as it's you know -- coding and coding something cool and fairly simple. On the other hand, the whole thing seems seedy and underhanded.  Would I be a \"bad employee\" for refusing extra work? Or am I morally justified to not do the work?  UPDATE I know it's been a while since I posted this question, but I thought the folks who participated in the discussion might be interested to know that Colfax quit a couple of months after this conversation. So, if I had followed along, it would have probably been for nothing. Regardless, thanks for the comments everyone. ",
        "Best answer": "If you have serious doubts about whether something is ethical, it's probably not.  That inner voice is there for a reason; listen to it. A real red flag should be the boss offering you vacation days \"off the book.\"  That could easily be interpreted as a confession of intent to commit accounting fraud, which is a pretty serious no-no.  Plus, if the boss is willing to hide things from his boss, how do you know he's not willing to hide things from you?  You could be getting set up for something here. Time to take this to someone with authority.  If what he's asking you to do is flat-out illegal, go to the police or the FBI.  Otherwise, go to Schuyler and explain what's going on.  You won't regret it. "
    },
    {
        "ID": "8917",
        "Question": "I've been using SVN for sometime and have been making an awkward, but soon to be rewarding transition over to git. Theres quite a few side / small projects that I'm working on which 90% will never see the light of day. As well - I also have my weekly school assignments / projects, and finally client projects that I have on the go. I've been rolling the idea or question of how or what the best way would be to back up my projects. The solutions I've sought out so far: github which offers an outstanding service - I'm ok with my work being open source, however It's the school work and client work I might not want the public to be open to. github, of course has a payment plan - but let's face it, im a poor colleage student doing what I can to at least put KD on the table! Theres the USB thumbstick that I can take around with me, work off of or back up, but that requires it to be around constently. Theres Dropbox, but thats a little overkill since it already is a form of a version control system, as well - I have quite a few things on the go - putting everything within dropbox would eat a lot of space Finally, theres a private server. I have a mac mini setup as a media server and I was thinking that I could backup to that. Anyways - what are your guys thoughts, how do you handle backing up projects, not only for version control but for the everyday backups. ",
        "Best answer": "If you have a private server with ssh access all you need to do is this: ssh my.server mkdir ~/repo.git && cd ~/repo.git git init --bare # bare repository that will serve as backup location exit # back on your box cd $my_local_git_repo git remote add --mirror backup ssh://my_server:repo.git git config remote.backup.fetch \":\" # don't replace our data when                                    # accidentally fetching from backup git push backup # copy everything new to backup repo  "
    },
    {
        "ID": "8986",
        "Question": "This isn't about the typical naming conventions, where to place your comments, should we use regions, etc.  This is about a coding standard that discusses specifics on how a programmer should design his applications. Stuff like: (on a .NET app and not in the exact wordings)  Make as few classes possible Classes should do as much as their real world counterparts can Modification is better than extension One DLL per category (One library for email functions, one library for File related functions)  Everyone's had their experience with bad code and possibly bad design, but what if you were placed in a team (leaving is not an option) where they had design standards that just don't feel right.  I'm unsure how to respond professionally when I feel like I'm dying inside whenever I'm asked to code this way. ",
        "Best answer": "I don't know who said this originally, but when faced with a situation that you don't like, there are only three options:  Change it. Accept it. Leave it.  Do you have the energy and influence within the company to make the mammoth effort to try the first option? Do you want to write better code by stealth and risk being fired? Can you accept the situation and write code to those \"standards?\" My advice, don't do this unless you really need the paycheck. It will slowly chip away at your morale. Which leaves option 3. Even if you need to accept the situation in the short term, you should start looking elsewhere for another job. "
    },
    {
        "ID": "9007",
        "Question": "I have been running my own home-based server for ages for my own personal code, and have tried one system or another (SVN, VSS, etc). But recently, I've enjoyed having some more precious (no source code) data off-site, without having to constantly worry about backups in case of hardware failure, using services like Dropbox, Skydrive, Google apps, etc. This has made me want to do the same with my source code for various projects, however, I have a few worries about them. Namely, how secure is my code from curious/prying eyes, and do I need to possibly have a fail-safe service, in case one goes belly up overnight, or simply has massive data issues? This wasn't actually a concern until I read someone's experiences with Mozy.com, which while not a code repository service, data storage is data storage. If you do use/recommend such a service, which would you recommend? I'm open to using any system, be it Mercurial, SVN, GIT, TFS, etc, so long as it has a Windows and a Mac client option (web-based would be a very nice \"bonus\"). And most importantly, why? ",
        "Best answer": "The advantage of a distributed system (Git, Mercurial, Bazaar) is that even if your remote host does disappear completely, you'll still have the full repository history locally, from which you can start again elsewhere.  With something like Subversion, if you lose the server you'll only have the working copy, which is better than nothing but means you've lost the project history. I use GitHub for open source projects and I'm very happy with it.  I don't use GitHub for commercial projects as their pricing gets expensive quickly for the way I use Git (lots of small repositories), so I host my own server using Gitosis. There are similar alternatives for Mercurial (BitBucket) and Bazaar (Launchpad). "
    },
    {
        "ID": "9013",
        "Question": "My team uses a Kanban system for tracking day-to-day progress and it's worked out really well for understanding progress on features (captured as user stories).  We have largely allowed our system design to emerge as we develop features which worked out well until recently.  In the last two week we've had several discussions about architectural trade-offs specifically related to performance and modifiability quality attributes. I think what's happening is as we implement features and design the system, we're implicitly making decisions about the architecture but not considering those decisions in terms of our known quality attribute requirements.  It would be really great if I could track/capture/visually depict how these important design decisions are being made so team members have a better chance to not create additional tension with in the system's architecture as it is being implemented.  And of course, to complicate things more, features on our board aren't exclusively functional and sometimes hide architectural complexity! How can I track progress on quality attributes (or other architecturally relevant decisions) visually on my team's kanban? ",
        "Best answer": " we're implicitly making decisions about the architecture but not considering those decisions in terms of our known quality attribute requirements.  I think you can visualize this by creating a \"architectural review\" step in your workflow.  The step would be represented by a Kanbad board column with its own WIP limit.  The WIP limit will depend on how many architects/designers you have to participate in these reviews. The development team is responsible for the horizontal, left-to-right flow of user stories.  The architect(s) will, in most cases, touch the stories only when they are in the architectural/technical review column.  The intersection of the column with a horizontal swimlane visualizes the meeting of developer(s) and architect(s). One of the teams where I work has a similar step where they do a database schema review with the chief data architect.  They don't use Kanban, but if they did, they would be very likely to have this column on their board. The known quality attributes could then be represented as:  the definition of done for the architectural review step. acceptance tests around the already-done user stories representing non-functional requirements.  Then the definition of done for a new user story would include not breaking those tests.  ADDED: The \"architectural review\" workflow step may be suspect to a problem called tragedy of the commons.  Here is a great blog post about how Kanban visualization can help deal with it and possible solutions. "
    },
    {
        "ID": "9099",
        "Question": "As a programmer, do you see any professional or other advantage in using your real name in online discourse, versus an invented handle? I've always gone by a single username and had my real name displayed whenever possible, for a few reasons:  My interests online are almost exclusively professional and aboveboard. It constructs a search-friendly public log of all of my work, everywhere. If someone wants to contact me, there are many ways to do it. My portfolio of work is all tied to me personally.  Possible cons to full disclosure include:  If you feel like becoming involved in something untoward, it could be harder. The psychopath who inherits your project can more easily find out where you live. You might be spammed by people who are not worth the precious time that could be better spent writing more of the brilliant software you're famous for. Your portfolio of work is all tied to you personally.  It seems, anyway, that a vast majority of StackOverflow users go by invented handles rather than real names. Notable exceptions include the best-known users, who are typically well established in the industry. But how could we ever become legendary rockstar programmers if we didn't get our names out there? Discuss. ",
        "Best answer": "The biggest thing I can think of is both an advantage and a disadvantage: everything you put online under your real name will follow you. This is good when you are posting good, constructive things. It's bad when you post a picture of you from that night or when you say something offensive or just plain stupid. I find that using my real name helps keep me in check -- I think more about what I say and how I say it. But it has on occasion been inconvenient when using my name invited personal attacks for various reasons. All in all, my approach is to use my real name when dealing with professional-ish stuff and to use a handle for personal interests and things I might not want to be as easily searchable. "
    },
    {
        "ID": "9180",
        "Question": " Possible Duplicate: I've graduated with a Computer Science degree but I don't feel like I'm even close to being an expert programmer   I recently graduated from university, and I have since then joined a development team where I am by far the least experienced developer with maybe with a couple work terms under my belt. Meanwhile, the rest of the team is rocking 5-10 years experience. I was a very good student and a pretty good programmer when it came to bottled assignments and tests. I have worked on some projects with success, but now I'm working with a much bigger code-base, and the learning curve is much higher. I was wondering how many other developers started out their careers in teams and left like they sucked. When does this change? How can I speed up the process? My seniors are helping me but I want to be great and show my value now. ",
        "Best answer": "The interesting thing about software development is that it doesn't matter how good you are -- there is always someone better or at least different enough to still teach you something. It's also not uncommon to look at code written a few months ago and think it sucks, regardless of your level of experience. For me, once I realized the gap between my skills and the skills of my coworkers, I started learning like I've never learned before -- reading other people's code, blog posts, books, paying attention to how my coworkers accomplished things, etc. University prepared me for computer science, but not really for software development. It's almost 4 years later, and I'm a much stronger software developer than I used to be. So, just hang in there and learn as much as you can from the people around you. It'll get better. "
    },
    {
        "ID": "9213",
        "Question": "There are times when I am working on programming project and I get the itch to change some stuff in my environment (OSX or Linux). Vim might not being doing exactly what I want, so instead of doing it the round about way I've been doing for a couple months (sometimes years) I go and figure out the right way. Or I may be doing something long handed in bash and I say to myself why don't I figure out a better way.   The thing is when I go off and do this hours can fly by. Sometimes I get stuck on trying to get what I want. I'll know I'm really close, so I don't give up. I usually always get it eventually, but its after hours of tinkering and googling. I hate the feeling of giving up and having to deal with something I know could work better.  When I'm done I have a warm feeling knowing that my environment is a little more smooth and personalized, but I wonder could my time be better spent. Where do I draw the line? It seems with the all the UNIX-style tools there is an endless amount to learn.  Always thought the sign of a superior programmer is someone who goes out of their way to make the computer bend to their will. Am I doing it right? I figure the bash shell, unix/linux, and vim will be around forever, so I see it as an investment.  But then again I just spend 3 hours trying to get some stupid thing the vimperator firefox plugin to work right.  So I wondering what this community think about this.   ",
        "Best answer": "To rephrase a time-honored axiom, if it wastes your time, don't do it.  Some developers are able to optimize their environment such that the amount of time invested up front is outweighed by the amount of time saved overall by having a highly-customized environment.  If you can't see the value in customizing your environment to suit your needs and find your time is better suited elsewhere, then by all means don't do it. The end goal is to do your job to the best of your ability, and what that takes is something only you are going to be able to judge. "
    },
    {
        "ID": "9268",
        "Question": "Requested re-post from StackOverflow: I'm working in a small development time with very limited time for development. We develop a tool that is important for the result of our work, but not used daily. I am the only person in the team that has a background as a programmer. My problem is that I have been pushing for code reviews before merging back to trunk for over a year. Everyone agreed on this, but still it's only my code that has been reviewed. Returning from a long vacation I come back to a trunk with code comments as \"this is an ugly solution - remove as soon as possible\" and \"quick fix\". What also is new is that a guy has been appointed the responsibility for the tool. (A role that first was offered to me but I turned down due to a non work related reason.) And he thinks that this is an ok way to work: As we have such limited time to develop, we should cut corners like that. My concern is that the other developers write ugly code: often breaking encapsulation, writing huge classes, adding internal classes at strange places, having few or no unit tests, and so on. It will eventually be impossible to develop the tool further. Should I insist that we perform code reviews before merging back to trunk or am I just a code quality bitch? ",
        "Best answer": "I think that code reviews and maintaining some coding guidelines is a good idea, but I think that doing it for each and every check in is a waste of time. It's a good idea when establishing a team and with young programmers, but experienced programmers can think for themselves and eventually you will have to trust them. That said - you can do the periodical code reviews to fresh thing up, but looking at each and every line of code that enters your VCS is really overdoing it. And a small comments regarding your colleague's fixes - some times making an ugly fix is the correct solution. It may be that this particular code is not important enough to invest lots of time in, it may be that the simple solution is good enough and it's better to invest time in other things. Making your code \"pretty\" is not your main goal as a programmer. Your main goal is to deliver and obsessing on every line of code simply won't get you there. What I'm trying to say is - you need to choose your battles. It's OK to lose the battle over some insignificant utility class to win the delivery war (or this REALLY IMPORTANT subsystem war, for that matter). "
    },
    {
        "ID": "9272",
        "Question": "Sometimes an algorithm can be written in two ways:  The short, fancy way; or The longer, easy-to-understand way.  For example, here is a longer, easier way of copying a string source to dest in C: *dest = *source; while (*source != '\\0') {     source++;     dest++;     *dest = *source; } (true);  And here is a short, fancy way. // Copy string source to dest while (*dest++ = *source++);  I've always heard and read that fancy code should be avoided, and I tend to agree. But what if we take comments into account? Assume that, as in the examples above, we have an uncommented, longer and supposedly easier-to-understand code, and a well-commented, short, fancy code? Is the non-fancy code still preferred? EDIT: Many have commented on variable names, so I've modified the example code as to not make that a factor when preferring on over the other. I tried to remove the double assignment in the first example, but that only made the code less readable. Perhaps this wasn't the best of examples because many find the 'fancy' code more readable and understandable than the longer code. The idea was to have one longer code which was much easier to understand than a very short but complicated code. EDIT2: Here's a new examle I got from SO: Commented fancy version: //direct formula for xoring all numbers from 1 to N int Sum = (N & (N % 2 ? 0 : ~0) | ( ((N & 2)>>1) ^ (N & 1) ) );  Non-commented long version: int Sum = 0; for (int i = 1; i < N; ++i) {    Sum ^= i; //or Sum = Sum ^ i; }  ",
        "Best answer": "I would generally prefer to extract the fancy code out into its own method.. Rather than comment the fancy code, its method name should be all it needs to make things clear. char *copy_string(char *s, const char *t) {         while (*s++ = *t++);      return s; }  "
    },
    {
        "ID": "9313",
        "Question": "Thought I post this to the best community of programmers I know.  David Johnston, the new Governor   General, has the digital world   confused.  Just what is the meaning of that 33   character-long string of ones and   zeros that is emblazoned across the   bottom of his fresh new Coat of Arms?  According to the GG's website, \"The   wavy band inscribed with zeros and   ones represents a flow of information,   digital communication and modern   media.\"  The binry is this: 110010111001001010100100111010011  It's not ASCII, is it just random?  Original article:   http://www.cbc.ca/politics/insidepolitics/2010/10/the-new-ggs-binary-banner-whats-it-mean.html  I'll accept the correct (if it can be solved) answer or failing that, the highest voted answer. ",
        "Best answer": "I'm pretty sure this doesn't mean anything and was only done for graphical effect. 33 characters in Binary doesn't leave much room for encoding data in the first place, and it's an odd length (not divisible by 4.) Add in the fact that it's palindromic, and the odds of there being something encoded in there falls to near zero. If you cut it into 3 even pieces, you can see the pattern emerge.: 11001011100 10010101001 00111010011  It really is just for looks. Edit:  The decimal conversion is also a prime number, so it's a prime number that has a palindromic binary representation.. pretty interesting without being an encryption puzzle. "
    },
    {
        "ID": "9432",
        "Question": "I have recently started work at a company using their proprietary CMS. Unfortunately, there is virtually no documentation for the application, and the source comments are also quite sparse.  Normally, given a particular task - I work down through the choices below, going to the next step if I can't find a solution 1: Consult the documentation (n/a) on how to do task X (i.e. create a page) 2: Read the source code - output variables - generally halt search after digging 4 or 5 steps into class/method chain 3: Ask the existing development team how they might achieve X in their application 4: Read the (uncommented) source again - go \"rabbit hunting\" - pursue each chain to completion 5: Make tea On the one hand, my employer is looking for technical capability. On the other, they are looking for speed of execution. They are averse to me asking the other developer for assistance (despite the lack of documentation), yet seem surprised when I've then had to spend an hour (or two) rabbit hunting through their application for the origin of variable or method X. Bear in mind that I've been employed as an intermediate developer.  So, I wanted to get an outside perspective, is my process of elimination unreasonable? I can't imagine anyone writing a Zend Framework app without the documentation (and sometimes they struggle despite that...) - so working on some proprietary app with none is proving somewhat tricky. ",
        "Best answer": "Explain to them that the lack of documentation is slowing you down and see if you can setup a block of time with the other developer for him/her to teach you the CMS system. If they really do not want you bothering the other developer, at least let them be aware that it is slowing you down because you are having to figure out an unknown system on your own (a nice time to bring up something like that is when they say \"hi how's it going\" - answer with some variation of \"slow because there is no documentation on how to use your CMS system and I'm having to figure it out myself\") Other then that, I've often found the best way to figure something out is to pick a task and get started on it. It will be slow at first, but as you work through it you often learn about the rest of the application. "
    },
    {
        "ID": "9584",
        "Question": "I realized I have to write down a convention specification about HTML, JavaScript and PHP coding for me and my team. In web development, just like in C++, I'm definitely a fan of indentation and comments. Nonetheless, often in my work I encounter HTML+JavaScript+PHP code which suddenly brings up the headache. I'm trying to make my code readable, but what seems to be better to me (to indent & comment) seems not to fascinate my teammates, so I was wondering if there is a best or at least shared good practice when writing \"hybrid\" documents just like today's web pages, which day by day become more and more complex.  I'm aware of the fact that probably it is in the nature of today's web pages' code to be a little bit intricated, but I wonder if a good convention concerning these aspects already exists. ",
        "Best answer": "Some general rules I follow: General  Indents are 4 spaces. Indent new levels Comments are < ~80 chars from the indent level. If I'm in two levels (8 spaces) that means the cursor stop will be around 88 characters.  Use multi-line comments. I prefer the look, however this is a subjective point. Comment now rather then later when you have no idea what's going on. Allman style braces. It's cleaner and is more readable. Subjective.  JavaScript  Use a library. jQuery in particular is very good. It eliminates all cross browser headaches. Understand that ID's are for particular elements, classes are for styles. ID's shouldn't be used more then once per page and they will have particular hooks attached to them. Use classes for things like navigation. Out source into methods. It's tempting to put all the code into the bind call, however putting it in it's own function will increase the flexibility of your code. Use functions instead of evals. That means setTimeout(function(){ /* Do something */ }, 1000); instead of setTimeout('doSomething', 1000); Use local variables with var.   HTML  Semantic markup. Use appropriate tags. Don't put <br />'s in there to add space, adjust margins and CSS rules. All tags are lowercase. All tags should end with a closing tag or be self closing. Make use of classes for layout that is similar. Have a couple of predefined classes like hide, clear, error, etc. Everything (scripts included) should go in <head>. Worry about optimizing (moving stuff around) when it presents a problem. External stylesheets and JavaScript source is a must unless it is page specific.  PHP  Frameworks are good, I recommend CodeIgniter. If you don't want to use a framework, try to use the latest version of PHP possible. (That means 5.3). Use includes to your advantage.  Clear injections or use prepared statements. Perform if checks on preconceived fail-secure values. $logged_in = false; if(check_user($user)) {      $logged_in = true;      $user = load_user($_SESSION); }  Know the difference between single and double quotes. Use single quotes when possible. Don't echo HTML.  "
    },
    {
        "ID": "9675",
        "Question": "I have been discussing a project with a client and they want to use a single language to deploy to as many platforms as possible.  (I am not sure that it's such a good idea, because each platform offers a different user experience and UI metaphor. In some cases, screen sizes vary as well. Also, iPhone is out of the running no matter what language we use. But, I digress...) The project is going to display an arbitrary block of text each day and will track if the user has \"read\" it. Some other features on the table are \"nags\" and the ability to go to another days' text.    I know Flash from the CS3 days, and I think that translates into AIR, but I dont know if AIR can do what we want.  On the other hand, I have (a) little experience with Java or Python, but those seem like good contenders also.  Which language should I use here? Why? Edit: This is going to run on desktop platforms too.  ",
        "Best answer": "These are languages that have a very good deployment factors: Javascript. Why? It runs on browser, and most platforms you'd care about have a browser. Though, some lower end mobile phones may have severely stripped browser (or Opera Mini, which supports Javascript only on the proxy-side, and very limited client-side Javascript). JVM Java Virtual Machine is ubiquitous in nearly every mobile phone, desktop, or server computer. All other things being equal, perhaps it's the best platform if you want easy portability. Nearly every mobile phone, old and new, supports JVM, the only major exception is iPhone. Python Python is very ubiquitous in Linux and Unix world; and the language has a very consistent behavior across different platforms, you almost need no additional work to write a cross-platform program in python. However, most Windows doesn't ship with Python by default, and most mobile phone doesn't support Python. OTOH Jython, compiles Python code to JVM; this combines the consistency and ease of use of Python with the ubiquity of JVM platform. If you don't need to care about iPhone, then JVM development is the way to go.  If you hate Java's language syntax, then Jython is a nicer alternative. If you need the most portability though, Javascript is probably the way to go; however writing anything in Javascript is painful. "
    },
    {
        "ID": "9741",
        "Question": "Background: I have a project where I need to provide the user a download package with some sensitive data in it.  The data needs to be encrypted.  After they download it, they need to be able to view it (no editing required).   For this question, let's approximate the data as a series of static html files.  Because the data is sensitive, it needs to be encrypted any time it is on disk. We are thinking of providing the user with a download option that would give them a zip file containing two files:  A data file (we'd probably use an encrypted zip file behind the scenes) with the data they asked for An application to view the data that would appropriately prompt for a passphrase and handle decrypting the data and displaying it via an embedded web browser.  Additional details:  Users are not under our control.  They are consumers. We are not worried about cross platform in this question.  This is just about Windows.  We will have a separate download for Mac users.  Get to the question already: For that application we need to create, we're internally debating if it is reasonable for that app to be a .NET winforms application.  We want a single .exe, and we want the download to be reasonably small (e.g. 100k).    Dare we use the .NET framework (we don't need to use a particularly recent version of .NET--2.0 would be fine)?   Is it reasonable to assume that most consumers have .NET on their machines now due to Windows Update?   Is it reasonable to ask the ones that don't have it to install it?  We know that not 100% of users will have .NET installed.  The real question is if it is reasonable to ask them to have it in this day and age. P.S.  Does anyone know of any reliable statistics of what percentage of people actually do have .NET installed already? ",
        "Best answer": "By default, Windows XP doesn't include any version of the .NET framework.  Vista, and Server 2008 include .NET Framework 3.0, Windows 7 and Server 2008R2 include .NET 3.5, and Windows 2000 can't support anything above .NET 2.0. With that in mind, it comes down to the OS spread in your target audience. I would list the requirement prominently on the download page and provide a link to Microsoft Update/Downloads section for the framework download, in addition to your application download. "
    },
    {
        "ID": "9788",
        "Question": "As x goes to 0, the limits of x^0 and 0^x go to 1 and 0, respectively. Some say that the best value for 0^0 depends on context, others say that the value of 0^0 should be 1. I'm interested in knowing what your language implementers say. There doesn't seem to be a consensus. For example, in ActiveState Perl 5.12.0: C:\\>perl -e \"print 0**0\" 1 While in Mathematica 6.0: In[1]:= 0^0         During evaluation of In[1]:= Power::indet:         Indeterminate expression 0^0 encountered. >> Out[1]= Indeterminate I'm also interested in knowing your opinion on the matter: What should 0^0 be? And also whether you have ever introduced a bug into a program you were writing (or had a hard time debugging a program) because of the way your language handles indeterminate forms? ",
        "Best answer": "According to this Wikipedia article, \"Most programming language with a power function are implemented using the IEEE pow function and therefore evaluate 0^0 as 1. The later C and C++ standards describe this as the normative behavior. The Java standard mandates this behavior. The .NET Framework method System.Math.Pow also treats 0^0 as 1.\" "
    },
    {
        "ID": "9810",
        "Question": "Ever since the update, people keep calling and saying \"Ever since the update X, Y and Z are slow, bad and crashing\"  This has happened ever since the dawn of updates.   What do people expect? Gamma comes after beta, and gamma testing always turns our users into The Incredible Hulks... Perhaps you've never heard this from a client, perhaps you're in college or a FLOSS Dev who can spread the blame around more than 5 or 6 guys, perhaps you unit test your code, perhaps you're not in that interesting situation where customers actually call you requesting the exact time of day you'll be releasing today's patch (I'd love to do that to Microsoft) or perhaps you're a sorry son-of-a-biscuit like me who just shipped a new update and went home and is dreading going back to work tomorrow. Anyway, ya'll are smarter than me anyway. How do you field criticism framed in \"You must be a bad programmer because you're making your software worse\"? ",
        "Best answer": "If this happens to you every time you deploy, there could be a serious flaw in your development process. I would suspect a couple of things that are causing the problems.  Do you develop against a database that is the same size (roughly) as production? If not then you will always have these problems because queries that are fine with small data sets are often dogs with bigs ones. Do you load test in QA? What works fine with one user testing is very different from how things will respond with 1000 users trying to do things at the same time. Do you assume the user's perception is wrong and treat them like they are stupid for complaining? If so, then your attitude is causing more complaints not lessening them. Are you doing a good job of testing? Do you regression test features not changed to see if they are affected by the change? Do you even care how long things take until they hit prod? Do you pay attention to when is a     good time for the users when you     deploy or do you merrily deploy a     change to the accounting system the     day payroll is run and wonder why     users are angry at the slow down? Do you have environmental differences between dev and prod. Sometimes those pesky differences in operating systems or database versions will cause issues like this as well. It is often a good idea to have a staging enviromnet that is exactly like prod, some equipment same operating system, same database with dat that is as close to prod data as possible. This is used to test your deployment. Run it first on this server and tehn have some users or testers go to it and run through some tests.  How good is your deployment process, do you often miss steps?  Can it be run by someone other than the developer because it is clear what code goes in the branch you are deploying? We got a lot better at deploying code when we got a configuration management team and nobody had the rights to sit with prod and babysit it making \"oopsie\" changes. Automating your build can help tremendously. No one should be guessing what needs to go to prod as it should have gone to QA and staging first and any deplyment issues worked out. Scripting database changes is key too. They shoud be in scripts and in source control, so the build process can pick them up without someone having to remember, oh yeah, we need to change the length on Column B to 241 from 50. I find that people often forget to treat database changes as code preferring to use the GUI which is virtually always a bad idea on prod.   "
    },
    {
        "ID": "9813",
        "Question": "I find myself lagging behind on new skills, techniques, language features, etc, and I am finding the time to do so is lacking. Between work, professional, personal and family obligations, I'm lucky to find a few stray hours here and there to focus on any new technologies or reading that I want or need to do. I do make it out to relevant local user groups, but even those are sometimes hard to get to? So I thought I would ask the community, where, or when, do you find the time to focus on honing your skills or learning new ones? Do you schedule time? Forgo some sleep during the week or weekend? Insomnia? Something else? EDIT I want to thank all of you who took the time to answer and offer up your advice. There were some things I knew or had thought about, and others I hadn't considered as an option until you shed new light on it. EDIT 2: In trying to find which answer among the many great ones to accept, I went with @Paddyslacker's since it is the one I feel is best suited for my current situation, although everyone had some very good nuggets of wisdom, such as @Elisha @Martin or @dash-tom-bang.   ",
        "Best answer": "Let me start by saying I know where you are coming from. I work for a small company with lots of stuff to do and I am a family man with two kids under the age of five. I have no intention of being an absent father or husband, or doing a poor job for my employers, so it is extremely difficult to find the time for new stuff. I think the trick is to make your goals extremely small and achievable and go from there. Earlier in the year I wanted to write some blog posts. Instead of setting a generic \"write some blog posts\" goal, I made a micro target of \"write for two hours a week; don't worry about whether it's good enough to post.\" I didn't worry about the larger goal, but instead made sure I achieved the micro goal that ultimately meant I achieved what I wanted. Right now, I'm interested in BDD frameworks, but rather than have a goal of learning them, my goal is simply to spend 30 minutes a day on them. Even if the 30 minutes is simply downloading and installing cucumber, I'm okay with that. Ultimately, I will achieve my goal. Someone much wiser than me once said that the only way to eat an elephant is one bite at a time; similarly the only way to hone your skills is in small chunks of time. Rather than stressing about some larger goal, if you focus on freeing up just two hours a week, over the next year that's 100 hours you've dedicated to something new. "
    },
    {
        "ID": "9814",
        "Question": "Here in my office, we made it a practice to come before 9 AM (in any condition as decided by management) and start with a Standup Meeting. Here we start with discussing with issues of previous day, any updates from manager (by leads) and the work plan for today. This meeting may sound good and process oriented , but some times if there is a show down then it ruins the morning mood or if there is a difference in opinion then again it is not a good thing to start with. So, HR and Management decides that initially we will discuss personal matters  for 10 to 15mins and then we will jump to technical and official discussion.For 15 minutes everyday, are you serious , I don't get a topic to discuss for 15mins every day with my lead who is much older than me and never discussed anything in personal till now and you want to start with it just like that!!! So it is back to square one. Every one (Amazingly in all the teams) is discussing again technical aspects only in the morning /stand up meeting. So here it comes, today I have a lunch with management and the topic for discussion is:    How to make Nice Day(Morning/Standup) meetings effective?  ",
        "Best answer": "In my experience, the best way to improve standups is to follow the rules exactly.  A lot of agile practices can be bent, but if you're having trouble, a good first step is to return to the prescribed way of doing it:  Stick to 15 minutes for the entire meeting.  People get bored easily, especially at mandatory, team-wide, daily meetings. Have each person say what they did yesterday, what they're doing today, and anything blocking them.  Nothing else.  This keeps it on topic and to the point. Have a team lead or senior developer (not management) lead the meeting, and be agressive about keeping things on topic as well as telling people to take discussions offline as they become too in-depth.  "
    },
    {
        "ID": "9843",
        "Question": "This is a question I asked a while back on SO, but it may get discussed better here... Where I work, we've gone back and forth on this subject a number of times and are looking for a sanity check. Here's the question: Should Business Objects be data containers (more like DTOs) or should they also contain logic that can perform some functionality on that object. Example - Take a customer object, it probably contains some common properties (Name, Id, etc), should that customer object also include functions (Save, Calc, etc.)? One line of reasoning says separate the object from the functionality (single responsibility principal) and put the functionality in a Business Logic layer or object. The other line of reasoning says, no, if I have a customer object I just want to call Customer.Save and be done with it. Why do I need to know about another class to save a customer if I'm consuming the object? Our last two projects have had the objects separated from the functionality, but the debate has been raised again on a new project.  Which makes more sense and why?? ",
        "Best answer": "If you consider that a Customer is a part of the domain model, then it makes sense (especially within the context of DDD but not limited to it) to have have both properties and operations for that object.  With that said, however, I think that the example you've used is a poor one, and is a cause of the argument. If you're talking about persistence, then Customers generally don't 'save' themselves; whatever you're using for persistence will. It makes sense that any kind of persistence should belong to the persistence layer/partition. This is generally the thinking behind the repository pattern.** A method like Customer.UpgradeWarranty() or Customer.PromoteToPreferred() makes the argument clearer. This also doesn't remove the possibility for having DTOs. Consider the situation where you're going to pass customer information to a remote service for instance. It may not make sense for a customer to create a DTO of itself for transport, that's an architectural concern, but a case could be made for it in the persistence or network layer/partition/code/what-have-you. In that case, such an objectcould have methods that look like this public static CustomerDTO GetDTO(Customer c) { /* ... */ }  public static Customer GetCustomer(CustomerDTO cdto) { /* ... */ }  So, in summary, it makes perfect sense to have operations on a domain object that are congruent with logical operations in the domain. Google for 'Persistence Ignorance' for a number of good discussions on the matter (this SO question, and its accepted answer is a good place to start). ** This gets a bit muddied with certain OR/M software where you are forced to inherit from a persistent base that has a 'Save' method. "
    },
    {
        "ID": "9881",
        "Question": "I'm looking for informative class documentation format for my Entity, Business Logic and Data Access classes.   I found following two formats from here  Format 1  ///----------------------------------------------------------------- ///   Namespace:      <Class Namespace> ///   Class:          <Class Name> ///   Description:    <Description> ///   Author:         <Author>                    Date: <DateTime> ///   Notes:          <Notes> ///   Revision History: ///   Name:           Date:        Description: ///-----------------------------------------------------------------  Format 2  // =============================== // AUTHOR     : // CREATE DATE     : // PURPOSE     : // SPECIAL NOTES: // =============================== // Change History: // //==================================  I feel following are the basic elements    Author   Created Date Description Revision History  as Namespace and Class name will be there in anyway. Please let me know your thoughts, which format is recommended and whether there is a standard way of writing revision history? ",
        "Best answer": "Most of the information you've suggested there would be found in the source repository. The only thing you really need is the purpose section, which says what the class is there for. Would it be tedious to look in repository every time you want to know the other information? I'd say no.  How often do you care who the original author was?  Or when the file was first created?  Plugins (such as Ankh SVN for Visual Studio) often allow you to right click within your current file and view the repoistory log for the file, so it's not that much of a hassle to actually see this information. Additionally, if you store the version history in a comment, this comment needs to be maintained. So over time there's a chance it could be lying to you.  The source code repository automatically keeps this historical data, so doesn't need that maintenance, and will be accurate. "
    },
    {
        "ID": "9991",
        "Question": "I once saw a program (can't remember which) where it talked about people \"experiencing flow\" when they are doing something they are passionate about. When \"in flow\", they tend to lose track of time and surrounding, concentrating only on their activity at hand. This happens a lot for me when I program; most particularly when I face a problem. I refuse to give up until it's solved. This usually leads to hours just rushing by and I forget to eat lunch, dinner gets pushed into far into the evening, and when I finally look at the clock, it's way into the wee-hours of the night and I will only get a few hours of sleep before having to rise early in the morning. (This is not to say that I'm in flow only when facing a problem - but I find it particularly hard to stop programming and step back when there's something I can't solve immediately.) I love programming, but I hate it when it disrupts my normal routines (most importantly eating and sleeping patterns). And sitting still for so many hours, staring a screen, is not healthy. Please, any ideas on how I can get my rampant programming activity under control? ",
        "Best answer": "Get married A partner yelling \"Dinner is ready.\" or \"Get to bed now, or you'll be grumpy in the morning\" will kick your right out of that zone. Seriously. The only reason I get to bed on time is because my wife hates me in the morning after late nights of programming. There's other benefits too. "
    },
    {
        "ID": "10094",
        "Question": "I'm interested in stories where office bureaucracy has had direct effect on the final code quality result.   For example, a friend just told me that at his previous work place the version control system was so bulky that programmers were not allowed to create new \"modules\" (root directories in the source tree) without asking permissions from the VCS gods. The result was that programmers were unwilling to go through the extra bureaucratic step and instead of properly componentizing their services they ended up piling unrelated functionality on top of existing modules even when functionality was just remotely related to the current definition of the module or the module's name was way in the past. (not to mention renaming a module...)   I'm interested in similar stories of office, operational or any other bureaucracy that eventually, perhaps unintentionally affected software quality  ",
        "Best answer": " I'm interested in stories where office   bureaucracy has had direct effect on   the final code quality result.  I don't think bureaucracy has so much effect on code quality as personal dynamics and office politics do. Bureaucracy has to do with process. When an existing process is done improperly (or exploited negatively... see further below), it has the potential to negatively affect the ability to delivery or react to sudden changes. A lack of process, however, will have a certain and significant impact on code quality. Or to be more precise, a process that does not govern code quality (also interpreted as a lack of code quality process) affect code quality. That is, it is not bureaucracy itself but specific, QA-related holes in bureaucracy that affect code quality when exploited (either accidentally or nefariously.) Personal dynamics and office politics, however, are much more of a culprit in bad code, however. Personal dynamics involves lack of professional ethics first and foremost. I don't really buy the argument that people write bad code because they don't know better or have not been properly trained. I've seen people w/o CS-related degrees writing decent code. It is a state of mind and a personal matter of being organized and meticulous. Office politics play an even more terrible role. Bosses that push the don't think, just code mantra (though there are times when we must just code and ship and clean the bodies later); developers who insist on delivering what they think is the perfect code even though getting something out of the door now is of the essence; code reviewers that are a**holes; cubicle wars and such. These things exacerbate problematic personal dynamics. The combination of both seep through any crack in the process (the bureaucracy) or lack thereof, causing a breakdown in code quality assurance. Hole in bureaucracy could be solved if there is a culture of post-morten reviews and continuous improvement. However, negative personal dynamics and destructive office politics prevent such corrections on the process to occur, thus perpetuating existing problems (including those related to code quality.) Bureaucracy by itself is rarely ever the culprit in bad code quality. I would actually say that code quality and bureaucracy are both negatively affected by negative personal dynamics and office politics. "
    },
    {
        "ID": "10166",
        "Question": "20% time it's the culture of an employer allowing it's employees to spend 20% of their time working on projects that they find interesting - it may be inventing a new app, or improving an existing process, etc. Some people may know this as skunk work, however that term may not mean anything (or something entirely different) to you. There are many documented cases of great products being born out of 20%/skunk work at a company. It seems like a win/win situation; the company potentially gain a great new product or application, and the developer has the opportunity to flex his/her creative muscles and innovate. I tried on numerous occasions to introduce some form of 20%/Skunk working at my previous employer with no success. How can I better justify it to management? What's the \"right\" way to approach this kind of work arrangement? ",
        "Best answer": "Fourteen months after I wrote this answer, I came up with a much better one. I haven't worked in a place where such works were officially recognized.  But from my conversations and attempts to learn about this practice, I found this - well, mostly what the \"20% time\" is not:  it's indeed a culture and not a policy it can't be decreed by the senior management it can't be instituted by a committee of developers it is not 32 hours on this and 8 hours on that  "
    },
    {
        "ID": "10172",
        "Question": "We have a small team in our department and typically employ 1-3 students/interns.  The problem we have is retention.  We like to hire a student that will stick around as part of the team for more than just a summer quarter.  The problem is though the last 4 students we hired, up front we discussed this with them and they all ended up leaving earlier than we would have liked.   The relationships all ended in a good fashion, the students either obtained teaching assistant or research positions within the university, or went on to bigger and better things.  This is all understandable and we do not hold it against them for doing what is best for them.  But my perspective is different as I worked within our department for 2 years during undergraduate at which point I was brought on full time upon graduation.   How can we retain students?  We give them hardware they need/want, we provide them with problems to solve in anyway they see fit (within reason).  All in all to me this type of job for a student programmer is so flexible and awesome I cannot see how anyone would want to leave. Thoughts ?  ",
        "Best answer": "I worked as a student programmer as an intern at two different jobs while in school. I am now graduated and work with another company full time. (a third). The main reason I didn't stick with those companies while as a student was because I didn't see an opportunity to move forward/up in the company. Also, they use one technology and didn't show any signs of expanding out. As a student about to graduate you feel that you shouldn't make a commitment that easily because you don't want thousands of dollars and 4 years of school to go to one job where you will sit and get stale (even though that's the case a lot of times.) Security isn't in the minds of those kids yet, but it's not their fault. Show them the company can grow, and they can grow with it in their careers, and show them you guys are open to new technologies and learning. "
    },
    {
        "ID": "10206",
        "Question": "I came across this list of management behaviors (http://suven.posterous.com/dos-and-donts-leading-software-development-te).   I think it has some gems, but I'm not 100% on some of them. I've marked those ones with italics and my name. Do you, as a software developer, think that these are appealing? Which three would be your TOP \"gotta have 'em\" items from your management? Don't  Don't scale teams vertically by adding more people Don't create a team with more that 10 people Don't call people resources, its not cool and is really offensive Don't assume that people in teams are interchangeable Don't compare teams to each other when highlighting weaknesses Don't pitch teams against each other Don't create fake deadlines Don't force standardisation of tools and processes across teams (I think this one can be argued for some situations - Todd) Don't hire product managers who don't have a clue about software development Don't exclusively use KPI's to drive your teams (Not only is it ineffective, but developers will find ways to drive the KPI metrics - \"You want lines of code? I've got your lines of code!\" - Todd) Don't force your teams to work overtime, even asking is bound to create tension Don't assume that double the people equals half the time  Do  Do scale horizontally by creating more teams of about 5-8 people Do have a vision for the product and team Do appreciate that every team is different, so allocate projects appropriately Do motivate your teams (Wow - that is one slippery, hard-to-define one. I agree with the sentiment, but it's like saying \"Be effective\" with no guidelines. -Todd) Do allow people to move between teams Do have sessions to discuss the product vision, strategy, technology and process Do involve the team when determining the team/product name Do allow your teams to make their own decisions especially if they are the ones with the expertise Do involve your team on any decision that impacts how or what they work on Do encourage a development methodology that matches the team and the project Do pay attention to every individual's personal development plan  ",
        "Best answer": "My guess is this list actually appeals to software developers because it validates their self-image as pampered creative divas rather than hard-as-nails problem-solvers (a là Winston Wolf) and expect to be treated professionally as a result. I also suspect if we improved software development techniques to the point where our trade was certifiable like that of architects, lawyers, medical professionals and the like, we would be better able to direct how software developers are managed. "
    },
    {
        "ID": "10208",
        "Question": "As someone who is now finding himself on the other end of the interview table, I'm wondering how useful these questions are from an employer's perspective. Some of my coworkers think they're good because you can see \"how they respond,\" but I'm not convinced it tells you anything useful, for several reasons:  It's not a very comfortable question and can lead people to twist their answers, even if not on purpose People may not fully know their greatest strengths or weaknesses (i.e. judge them by their peers) Explaining what a strength is isn't as good as showing it I still don't know any more about the candidate afterwards  The rationale of my coworkers is that it can help weed out people that give ridiculous responses, like one guy that said his greatest strength was \"his intellect\" or people that try to turn the weakness question into a strength like \"I work too hard.\" But I think there's more effective ways to determine such things. If you want to see if someone's smart, ask them technical questions. If you want to see if someone is productive, look at their work history. If you want to see how someone reacts under stress or change, ask them how they've dealt with it and ask for concrete examples. What are people's thoughts on these questions, from the perspective of an interviewer? What do they really tell you about a candidate, and what are better alternatives? How do I convince my colleagues of this? ",
        "Best answer": "Not very.  Any question for which a good percentage of candidates will have a canned response is of limited value, since you're often not getting the real them.  Everyone and their cousin has heard of \"what is your greatest weakness.\" The answer encourages lying: honest people will describe a fault and end up looking bad, while less honest people will spin a strength as a fault and look good. The question is so dated that, in my opinion, it reflects poorly on your company to ask it.  "
    },
    {
        "ID": "10230",
        "Question": "I recently started work at new office that uses a proprietary program written by a solo developer. He's occasionally around as part-time tech support, but the company has signed off on this software and he's no longer being paid to develop it. As a user of his software there are many issues that leap out to me as a source of concern:  very simple to directly view the DB as a low-privilege user passwords stored as plaintext dictionary admin passwords app uses the DB root account DB doesn't meet 1NF (eg. appointment1, appointment2, etc.)  I feel like these are serious issues and that if I were him I'd want them pointed out to me, but I'm not a qualified programmer (I'm a social worker) and I don't know if it would be rude of me to just buttonhole him and start blabbering about salted hashes and normal forms, especially when this is no longer a paid task of his. Is it out of line? If not, how would you broach it? Edit - more info prompted by comments:  The application holds sensitive data. It's for internal use, but it's running on lots of machines in several cities and many visitors come through our offices. My concerns are for the developer's future projects as well as this specific one.  ",
        "Best answer": "If you are not in a position to change the code yourself then you should decide which issue is the most important one and approach him about it. Rather than couching it as \"criticism\" do it under the guise of \"feedback\".  When I say feedback I mean a VERY distinct process of stating the behavior and the effect of it, along the lines of \"When you X, here's what happens...\" and speak STRICTLY of things are directly observable. Do not attack him, his professionalism, etc. Stick to the facts.  For a great overview on providing peer feedback listen to this postcast: http://www.manager-tools.com/2006/10/the-peer-feedback-model If you have success with the first item, move onto the next most important one. Good luck! "
    },
    {
        "ID": "10233",
        "Question": "I've been working here for about 3 months now, and this company has been around since 2003. I've the only full time developer, and I don't know what happened to the part time guy who was doing work before I got hired, I haven't seen him since a month after I started, aside from a couple emails. Essentially, that makes me the only developer, trying to balance adding new features to our main product (subscription website service), developing an Android-based version of the product, handling client issues, working on migrating clients from old version to new version (and the headaches that will result from this), fixing issues with a signup service for out ISP side of things, and helping with tech support for the ISP side. Not to mention the massive overhaul I'm fairly certain the database needs, that will mean at least a few months' work. My bosses hired an intern to help me with the Android version, but the one boss randomly went to England until December, taking the intern with him. Any tips I can use on convincing management to hire somebody else? Even if just a second full time support guy to help with the phone calls? ",
        "Best answer": "You need to build a case for it. Start by estimating the effort for every task on your plate.  In hours, days, whatever.  Make sure management sees this- mention this list to them the next time they ask you to do something.  Be sure this is written down. For each new bit of work, estimate its relative effort and importance, and be sure management knows where it's going in your todo list. When they start getting agitated that things aren't getting done as quickly as they'd like, point out the need for another developer.  1 person can get 1 hour of work done an hour.  2 people can get 2 hours of work done an hour (more or less).  If they want prompt support and improvements, they need to hire someone to help. "
    },
    {
        "ID": "10277",
        "Question": "In my experience as a freelance developper, I've had some difficulties with clients, whom insist on getting an estimate over flimsy requirements. They then expect me to deal with requirements changes, sometimes late in the project, and stick to that estimate. Since I've never come up myself with some T.O.S. tailored for this situation, I ended up bound by a contract of a more general nature, which inevitably became unsasatisfactory... Legal lingo not being my forte, I've been lookin for some publicly released T.O.S. to try to remedy this situation. Ideally these would be from an agile shop, under a licensing scheme allowing derivatives. This would help me lay out the contractual groundwork for a sane customer relationship, especially when dealing with changes. ",
        "Best answer": "Clients wanting you to be agile sounds more like clients that just want to change their mind (which is fine).   It seems like instead of:  Get requirements Sign the contract with Julien to deliver X for $Y. Julien delivers X Julien gets paid Y  They want:  Get very, very vague requirements Sign the contract with julien to deliver whatever they want (X? Y? X+Y? Z??), which is undetermined, and pay Julien $Y. Julien works toward ever-changing requirements. Requirements are never met. Julien never gets paid because they are never satisfied.  In this case I would simply charge per-hour. Don't worry about fixed price or an \"agile\" contract.  If you try to re-contract for every change, it will eventually become more pain than it is worth.  Your best bet here would be to have a per-hour charge, and have them sign off on timesheets you provide on a per-week or something similar basis.  This way both you and they can be as agile as needed while you have a better guarantee of getting paid.  Just do great work and any good client will see your value.  If not, you're better off just passing on the particular client if you have the opportunity. For a more Agile approach.. You could, essentially, have a shorter estimate contract per-iteration.  Each 2-week (or whatever time) iteration that you plan, have a shorter contract/amendment that outlines the requirements and estimates just for that iteration.  Let the client change requirements, but only give them that luxury at the end of each iteration when you do the planning for the next iteration. "
    },
    {
        "ID": "10358",
        "Question": "If you have been trained in the use of formal methods (FM) for programming:  How useful have you found it?   What did your FM training involve (e.g. a course, a book)?   What FM tools do you use?   What advantages in speed/quality has it given you compared to not using FM?   What kind of software do you create with FM?   And if you don't directly use FM now, was it at least worth learning??  I'm curious to hear as many experiences/opinions on FM as can be found in this community; I'm starting to read up on it and want to know more.  Background Programming and software development/engineering are some of the newest human skills/professions on Earth, so not surprisingly, the field is immature — that shows in our field's main output, as code that is typically late and error-prone.  Industry immaturity is also shown by the wide margin (at least 10:1) in productivity between average and top coders.  Such dismal facts are well covered in the literature, and introduced by books like Steve McConnell's Code Complete. The use of formal methods (FM) has been proposed by major figures in software/CS (e.g. the late E. Dijkstra) to address (one of) the root causes of errors: the lack of mathematical rigour in programming.  Dijkstra, for instance, advocated for students developing a program and its proof together.   FM seems to be much more prevalent in CS curricula in Europe compared to the US.  But in the past few years, new \"lightweight\" FM approaches and tools like Alloy have attracted some attention.  Still, FM is far from common usage in industry, and I'm hoping for some feedback here on why. Update As of now (10/14/2010), of the 6 answers below, no one has clearly argued for the usage of FM in \"real world\" work.  I'm really curious if someone can and will; or perhaps FM really does illustrate the divide between academia (FM is the future!) and industry (FM is mostly useless). ",
        "Best answer": "Absolutely useless for anything nontrivial. I had a course called, aptly, \"Formal Methods\" that focused on Alloy- I can't possibly see using that anywhere.  Had another class that focused on concurrency modeling with LTSA- equally useless. The problem is that most bugs and problems in software (at least in my experience) arise from complexity that occurs below the level of abstraction of those tools. "
    },
    {
        "ID": "10387",
        "Question": "My current project is a cloud deployment platform similar to GAE or Heroku. I'm trying to figure out what pricing method works best for us and the users. Assuming that I cannot currently provide a free plan, here are the options: 1) Have fully unlimited scaling (GAE-like), and charge for usage ($5 per project + x cents per CPU hour the project uses on our system, also charge a certain amount for bandwidth in/out, database storage + I/O, r/w data store, memcached, etc.)  This ends up being cheaper (around $20 for a moderately loaded website) and more \"fair\" for the user (pay only for what you use), but much more complicated with the many different factors that affect final price. This type of architecture is more resilient to what you throw at it, so if you get a traffic spike, it will work perfectly fine, you'll just get charged more. 2) Pay for \"application readiness\"/specific scalability (have a specific amount of processes per user available to accept requests, maybe $20 a month (3 cents per hour) per process), and for a specific tier of a database. No bandwidth/IO charges (within fair use, of course). Add-ons like email and memcached are extra. (Heroku-like) This option is a lot more simple, and much easier to project costs on. However, the pricing curve is rather steep. You do end up likely paying more (probably $40-$60 a month for a moderately-loaded website), but all of those miscellaneous costs are completely abstracted. Which would you more likely pay for? Also, is there another option I may be overlooking? ",
        "Best answer": "I much prefer the first model. So many sites want to extract $x/month and some of these sites make it very difficult to unsubscribe from. Even if the first model was more expensive, I'd prefer it as I would only be paying if I had a project, in which case, justifying the cost is very easy. "
    },
    {
        "ID": "10567",
        "Question": "Ok looking for some really subjective answers.  My company has traditionally been a Linux shop, we manufacture and sell purpose built boxes for video security.  We recently decided to build an Embedded Windows Standard 7 box because it shortened our development and time to market, and because of all the tools are available off the shelf for windows.   What I am looking for are some solid answers as it relates to the security of WES7 vs. Linux?  How stable is WES7, how prone is it to hack attempts, virus attacks etc. From a marketing point of view what could or should one say are the benefits of using WES7 over Linux?  Are there any?     What comparisons are there between the security of desktop Windows 7 and Embedded Windows Standard 7?   Any responses would be greatly appreciated.  Thanks  ",
        "Best answer": "You will need to do a few things, here are some off the top of my head (having been around this a few years ago):  You need lots of memory to run Windows Embedded 7. Check the data - you might be surprised at how much RAM you need. Make sure you understand how the write filter works, if you are using CF or similar as your \"hard drive\". If you have a real spinning HD then its not such a big deal. Do a course on using the tools. ESSENTIAL. I spent a couple of months on the previous generation tools and it was a pretty terrible time learning them. A 1 day course teaches what you will take 1-2 weeks to learn yourself. MS even offer some of these courses for free. Figure out how you want to lock down the platform. To do this you need to do things like disable web browsers, javascript, turn off file sharing and workstation services (about 70% of all the windows services CAN be disabled and probably should be - this will depend on your application though) Some aspects can only be locked down using manual steps on a master \"golden\" platform where you build the image, as manual steps hand entered, after the image build. WRITE DOWN A PROCEDURE to allow this to be replicated. Do everything (all your target building) in a VM, and check the VM into source control after. It will be between 8 and 10 GB - make sure that your VM splits the virtual disk into 2 GB chunks to make your source control systems life easier. This sounds over the top but it will save your life 2 years later. Check out and understand how the patch / update system works. We had to write our own, the standard one was not good enough. Things have moved on since but you still must understand this. Line up a consultant who has done this MANY times before, and get them in for a day or a week if you need to. Make them document everything they do, and WHY. You may need help, and a few days of an experienced consultant will save you a month. A classic turning of $ into time. MAKE sure they are experienced. There are lot of BS people selling Windows Embedded out there - doing the easy 90% is (surprise) easy. The other 10% is damn hard. Understand the requirements for branding - removing the MS logos and names and putting your own in. Its relatively easy but you must do it. You may need a graphic designer to make splash / start screens. Go through the license agreement 10 times. It is a LOT of fine print. Your legal department will need to look at it. You MUST understand it and the implications it imposes on your AND YOUR SUPPLY CHAIN. The agreement IS onerous. It is probably not a good idea to have windows updates enabled. You don't want your product doing various automatic unknown updates. (which leads to:) Make sure you know how to use and setup the windows firewall so that all ports are blocked except those used only by your application. This reduces the hack-attack surface.  If you get through all this, by all means use it. It is a very powerful platform. "
    },
    {
        "ID": "10655",
        "Question": "In my current developement, I have decided to keep the command prompt open so I can see debug messages. For example, if something wasn't happening when it should, I would put a cout statement at every function to determine where the broken link is. Sometimes, I might get hundreds of the same message (like WM_MOUSEMOVE) and I need to determine whether or not the messages are still coming, so I would using a static variable, and increment it, so I might have: ... 3> WM_MOUSEMOVE processed. 4> WM_MOUSEMOVE processed. 54> WM_CLICK processed. 5> WM_MOUSEMOVE processed. ...  in my cmd. (Don't ask me why there would be 54 WM_CLICK and 4 WM_MOUSEMOVE) What I would like to know is your opinion. Is this a good approach to debugging? What kind of syntax would you use for consistency? Does a syntax even matter? And would you make any suggestions to how this method could be improved. ",
        "Best answer": "There are a few ways. What you are using is a rudimentary form of logging. It's probably the most time-efficient way of debugging most errors. It's worth your time to write a solid logging module that can be redirected to screen or file, buffered/unbuffered, etc. It should have a timestamp and a message, if nothing else. There is also throwing exceptions - this can be more useful with languages that perform a stack trace using symbols, e.g., Java or Python.  Perl can croak()  or die() with similarly useful information. One other method is having a large array of unit tests which can pick up errors in a refined fashion. That of course implies well-written tests. The above are essentially non-interactive methods. There is also Ye Old Interactive Debugger, which functions in various levels of usefulness depending on development environment.  Lisp probably has the best debugging facilities available; C# probably has the most developed UI. Interactive debuggers can be very useful when your code \"logically\" should work, and you really need to do some serious variable-ogling and probe around. It's probably the most inefficient and hard to replicate of all the methods, but can be really useful. The interactive debugger can be extended to have various conditions and scripts run on variables hitting values, but I'm not sure if any current environments support it - the last one I heard of was developed in '93 for... Prolog? Can't recall off the top of my head. One method that was researched in the early 80s and is having a minor comeback today is replayable/reverse debugging. I am working on doing that in my thesis for a highly exotic embedded system. "
    },
    {
        "ID": "10656",
        "Question": "I have always been fascinated with the newest and best technologies available.  I graduate from college this year, and over the course of the past few years, I have spent a lot of time learning new programming languages, web frameworks, Linux distributions, IDEs, etc., in an effort to find the best of each. I have installed and played around with Ubuntu, Gentoo, Debian, Arch Linux, SUSE, VectorLinux, Puppy Linux, Slackware, and Fedora, I have spend a good amount of time in Vim and Emacs, and have played around with Visual Studio, Eclipse, NetBeans, gedit, and several more obscure ones. I have played with all sorts of languages - I started with the common ones like C, Java, Visual Basic, but always heard that they were \"bad\" (for relative definitions of bad). I then discovered the scripting languages and have quite a bit of experience in PHP, Perl, Python, and Ruby. Then I heard that functional languages are where it's at, so I played around with Scheme, Lisp, Haskell, Erlang, and OCaml. I've played around with obscure languages like Forth and J. When I do web development, I go back and forth between frameworks and languages. Should I use plain PHP, Ruby on Rails, Django, CakePHP, CodeIgniter, Yii, Kohana, or make my own? I have a very broad and shallow knowledge of computer science. As soon as I have learned a useful amount of one technology, I see something else shiny and go after it. My progression often goes like this: \"Perl is better than PHP, but wait, Python is better than Perl. Oh, but now I see that Ruby has the power of Perl and it is cooler than Python. Well, now that I have seen a little of Ruby, it is too complicated. Python is cleaner. Oh, but it is too hard to deploy Python, so I should use PHP if I want to do real web development.\" And so on and so forth. What  technology should I use for projects? Should I just pick one language/framework/IDE and sort of forget about the other things that are available for a while? I don't have all that much in the way of completed projects, because I never stay with something long enough to finish it. ",
        "Best answer": "Stop bikeshedding and start programming. This might sound cliche, but it's true: good tools don't make bad programmers good, and bad tools don't make good programmers bad (despite what Dijkstra says). You can spend 90% of your time bikeshedding over technology X vs. technology Y and get nowhere. Or you can choose X (or Y, it doesn't matter) and use it to do real work and get useful experience.  Just pick a set of tools and master them, even if they have some drawbacks. Once you really master your tools (that is, do more than half a dozen projects in them), you can play with others to try them out. In fact, it's recommended that you do so; you don't want to be a zealot who sees all other opinions as wrong. And sometimes, you might decide that another tool is indeed worth it over your current tool. If you can't decide what to choose on your own, a good way to end your inner debate is to let others choose for you. For example: Python, Django, jQuery, Vim and Debian. Now go and learn them. "
    },
    {
        "ID": "10731",
        "Question": "I just started doing PhD and we are supposed to do a project for a class, there are 14 people taking the class and we are supposed to develop a system all together. I was away from academia and working in the industry before, and I know it is very hard to manage even a couple of people towards the same goal. We are going to make the first meeting in a couple of weeks. First, I will suggest using a version control system like SVN. Second, I will try to take the lead for the architecture of the system, because I think I am more experienced. Since the class is about computer vision and I anticipate that most of the people's background is research related so there is a big chance that I am more experienced. I will gladly hand architecture to someone else if he/she is more experienced. What else should we do to progress without much hassle? PS. You can assume every one of us is going to work remotely, and meet once in a week at it's best (not everyone will attend though). And the project needs to be finished in 2 months. It does not need to be a perfect, complete product, we just need to make a prototype. PPS. The aspects of the group reminds me of open source project groups, maybe the answers will be helpful for those groups as well. ",
        "Best answer": " A distributed VCS is a much better choice than SVN. Mercurial should fit your needs nicely - it's similarly powerful to Git, but it is much easier to wrap your head around it and it has nice documentation. However, if it turns out that most people in the team (or even just you!) are much more familiar with SVN and are not in the mood to learn a DVCS, I wouldn't push for it. Get an issue tracker that is as light and easy to learn as possible. If the learning curve is too steep, people will not really use it, and you'll end up with no issue tracker at all. That's why I wouldn't advice JIRA or Bugzilla. Lighthouse has always worked great for me. Introduce some sort of lightweight agilish process. Having a slightly better structured environment may help the team deliver the prototype. 14 people looks like a rather large team. Maybe you could subdivide yourself into 3 smaller teams responsible for different parts of the prototype? However, try to enforce some sort of discipline when it comes to the non-essential (not directly related to research) parts of the project. After all, computer vision stuff is hard enough as it is - you don't want to fail because of a broken build system or something similar, right?  Last but not least: you shouldn't overdo the whole managing a project thing. After all, if I understand correctly, the goal of the project is to do science - not to have an industry level process. Try to keep all the tools as lightweight as possible - otherwise people will start working around them, and you'll end as if you were doing freestyle development (that's for example why I think Mercurial is a better choice than Git, even though I personally prefer the latter). Finally, remember that academia is more about Learning Cool Stuff on the Way than about Getting Things Done. Try to have fun! "
    },
    {
        "ID": "10736",
        "Question": "I am currently planning to develop a J2EE website and wish to bring in 1 developer and 1 web designer to assist me. The project is a financial app within a niche market.  I plan to keep the source closed. However, I fear that my would-be employees could easily copy the codebase and use it or sell it to a third party. The app development will take 4-6 months, perhaps more, and I may bring in additional employees after the app goes live.  But how do I keep the source to myself. Are there techniques companies use to guard their source? I foresee disabling USB drives and DVD writers on my development machines, but uploading data or attaching the code in email would still be possible. My question is incomplete. But programmers who have been in my situation, please advice. How should I go about this? Building a team, maintaining code-secrecy,etc. I am looking forward to sign a secrecy contract with the employees if needed too. (Please add relevant tags) Update Thank you for all the answers. I certainly won't be disabling all USB ports and DVD writers now. But I think I should be logging activity(How exactly should I do that?) I am wary of scalpers who would join and then run off with the existing code. I haven't met any, but I have been advised to be wary of them. I would include a secrecy clause, but given this is a startup with almost no funding and in a highly competitive business niche with bigger players in the field, I doubt I would be able to detect or pursue any scalpers. How do I hire people I trust, when I don't know them personally. Their resume will be helpful but otherwise trust will develop only with time. But finally even if they do run away with the code, it is service that matters after the sale is made. So I am not really worried for the long term. ",
        "Best answer": "You need to trust your developers. Virtually all professional developers won't steal your source. It's understood that if you work for somebody else, it's the employer that owns the code that you write. Developers might copy code for reference purposes, but it's highly unlikely they will offer it for sale to anyone else. If they did offer it for sale to a new employer then the likely outcome is them being shown the door and possibly even arrested (as Bob Murphy points out in his comment). Getting caught isn't worth the risk. More importantly, distrust breeds distrust. Disabling USB ports and DVD writers will engender a feeling of distrust which will, paradoxically, make it more likely that the developers will copy the code. By all means add a secrecy clause to your contract, but it's probably unnecessary to highlight it as the most important part of the contract.  "
    },
    {
        "ID": "10757",
        "Question": "The DRY Principle (Don't Repeat Yourself) states that \"every piece of knowledge must have a single, unambiguous, authoritative representation within a system.\"  Most of the time this refers to code, but it is often extended to documentation also. It is said that every software system has an architecture whether you chose it or not.  In other words, the software you build has a structure and that \"as built\" structure is the architecture of the software.  Since a built software system comes with an architecture, is creating an architecture description of that system a violation of the DRY Principle?  After all, if you need to know the architecture then you could always just look at the code... ",
        "Best answer": "Does duplicating your thoughts into code violate DRY principle? Geez, if you could just know the architecture by looking into code, there wouldn't be such things as \"architecture description documents\" in the first place.  It's not a repitition, it's another level of system description, which can't be trivially deduced from the code, and vice versa.  So it has its full right to exist even if you embrace DRY. "
    },
    {
        "ID": "10793",
        "Question": "I've heard in several places \"Don't make large commits\" but I've never actually understood whats a \"large\" commit. Is it large if you work on a bunch of files even if there related? How many parts of a project should you be working on at once? To me, I have trouble trying to make \"small commits\" since I forget or create something that creates something else that creates something else. You then end up with stuff like this:  Made custom outgoing queue  Bot -New field msgQueue which is nothing more than a SingleThreadExecutor -sendMsg blocks until message is sent, and adds wait between when messages get sent -adminExist calls updated (see controller) -Removed calles to sendMessage  Controller -New field msgWait denotes time to wait between messages -Starting of service plugins moved to reloadPlugins -adminExists moved from Server because of Global admins. Checks at the channel, server, and global level  Admin -New methods getServer and getChannel that get the appropiate object Admin belongs to  BotEvent -toString() also show's extra and extra1  Channel -channel field renamed to name -Fixed typo in channel(int)  Server -Moved adminExists to Controller  PluginExecutor -Minor testing added, will be removed later  JS Plugins -Updated to framework changes -Replaced InstanceTracker.getController() with Controller.instance -VLC talk now in own file  Various NB project updates and changes  ---  Affected files Modify  /trunk/Quackbot-Core/dist/Quackbot-Core.jar Modify  /trunk/Quackbot-Core/dist/README.TXT Modify  /trunk/Quackbot-Core/nbproject/private/private.properties Modify  /trunk/Quackbot-Core/nbproject/private/private.xml Modify  /trunk/Quackbot-Core/src/Quackbot/Bot.java Modify  /trunk/Quackbot-Core/src/Quackbot/Controller.java Modify  /trunk/Quackbot-Core/src/Quackbot/PluginExecutor.java Modify  /trunk/Quackbot-Core/src/Quackbot/info/Admin.java Modify  /trunk/Quackbot-Core/src/Quackbot/info/BotEvent.java Modify  /trunk/Quackbot-Core/src/Quackbot/info/Channel.java Modify  /trunk/Quackbot-Core/src/Quackbot/info/Server.java Modify  /trunk/Quackbot-GUI/dist/Quackbot-GUI.jar Modify  /trunk/Quackbot-GUI/dist/README.TXT Modify  /trunk/Quackbot-GUI/dist/lib/Quackbot-Core.jar Modify  /trunk/Quackbot-GUI/nbproject/private/private.properties Modify  /trunk/Quackbot-GUI/nbproject/private/private.xml Modify  /trunk/Quackbot-GUI/src/Quackbot/GUI.java Modify  /trunk/Quackbot-GUI/src/Quackbot/log/ControlAppender.java Delete  /trunk/Quackbot-GUI/src/Quackbot/log/WriteOutput.java Modify  /trunk/Quackbot-Impl/dist/Quackbot-Impl.jar Modify  /trunk/Quackbot-Impl/dist/README.TXT Modify  /trunk/Quackbot-Impl/dist/lib/Quackbot-Core.jar Modify  /trunk/Quackbot-Impl/dist/lib/Quackbot-GUI.jar Modify  /trunk/Quackbot-Impl/dist/lib/Quackbot-Plugins.jar Modify  /trunk/Quackbot-Impl/lib/javarebel.stats Add /trunk/Quackbot-Impl/lib/jrebel.info Modify  /trunk/Quackbot-Impl/nbproject/private/private.properties Modify  /trunk/Quackbot-Impl/nbproject/private/private.xml Modify  /trunk/Quackbot-Impl/nbproject/project.properties Modify  /trunk/Quackbot-Impl/plugins/CMDs/Admin/reload.js Add /trunk/Quackbot-Impl/plugins/CMDs/Operator/hostBan Modify  /trunk/Quackbot-Impl/plugins/CMDs/Operator/mute.js Modify  /trunk/Quackbot-Impl/plugins/CMDs/lyokofreak/curPlaying.js Modify  /trunk/Quackbot-Impl/plugins/CMDs/lyokofreak/lfautomode.js Modify  /trunk/Quackbot-Impl/plugins/listeners/onJoin.js Modify  /trunk/Quackbot-Impl/plugins/listeners/onQuit.js Modify  /trunk/Quackbot-Impl/plugins/testCase.js Add /trunk/Quackbot-Impl/plugins/utils/whatsPlaying.js Modify  /trunk/Quackbot-Impl/src/Quackbot/impl/SandBox.java Add /trunk/Quackbot-Impl/vlc_http Add /trunk/Quackbot-Impl/vlc_http/current.html Modify  /trunk/Quackbot-Plugins/dist/Quackbot-Plugins.jar Modify  /trunk/Quackbot-Plugins/dist/README.TXT Modify  /trunk/Quackbot-Plugins/dist/lib/Quackbot-Core.jar Modify  /trunk/Quackbot-Plugins/nbproject/private/private.properties Modify  /trunk/Quackbot-Plugins/nbproject/private/private.xml Modify  /trunk/Quackbot-Plugins/src/Quackbot/plugins/JSPlugin.java Add /trunk/Quackbot-Plugins/vlc_http Add /trunk/global-lib/jrebel.jar  Yea.... So for questions:  What are some factors for when a commit becomes too large (non-obvious stuff)?  How can you prevent such commits? Please give specifics What about when your in semi-early stages of development when things are moving quickly? Are huge commits still okay?  ",
        "Best answer": " To me, I have trouble trying to make \"small commits\" since I forget or create something that creates something else that creates something else.  That is a problem.  It sounds like you need to learn to break down your work into smaller, more manageable chunks. The problem with large commits are:  In a multi-person project, a greater chance that your commits will cause conflicts for other developers to resolve. It is harder to accurately describe what has been done in log messages. It is harder to track the order that changes were made, and hence to understand the cause of problems. It increases the probability of losing a lot of uncommitted work.  Sometimes large commits are unavoidable; e.g. if you have to change a major API.  But that's not normally the case.  And if you do find yourself in this situation, it is probably a good idea to create a branch and do your work in there ... with lots of small commits ... and reintegrate when you are finished. (Another case is when you do an initial import, but that's NOT problematical from the perspective of the issues listed above.) "
    },
    {
        "ID": "10804",
        "Question": "As a first-time part-time software developer at a small consulting company, I'm struggling to organise time to further my own software development knowledge - whether that's reading a book, keeping up with the popular questions on StackOverflow, researching a technology we're using in-depth, or following the front page of Hacker News. I can see results borne from my self-allocated study time, but listing and demonstrating the skills and knowledge gained through Professional Development is difficult. The company does not have any defined PD policy, and there's a lot of pressure to get something deliverable done now! when working for consultants. I've checked what my coworkers do, and they don't appear to allocate any time to self-improvement; they just work at the problems they're given, looking up specific MSDN references, code samples, and the like as they need them. I realise that PD policy is going to vary across companies of different size and culture, and a company like my own is probably a bit of an edge case. I'd love to hear views and experiences from more seasoned developers; especially those who have to make the PD policy choices in their team or company. I'd also like to learn about the more radical approaches to PD, even if they're completely out there; it's always interesting to see what other people are trying. Not quite a summary, but what I'm trying to ask:  Is it common or recommended for companies to allocate PD time? Whose responsibility is it to ensure a developer's knowledge and skills are up to date? Should a part-time work schedule inspire a lower ratio of PD time : work? How can a developer show non-developer coworkers that reading blogs and books is net productive?  Is reading blogs and books actually net productive? (references welcomed) Is writing blogs effective as a way of PD? (a recent theme on Hacker News)  This is sort of a broad question because I don't know exactly which questions I need to ask here, so any thoughts on relevant issues I haven't addressed are very welcome. ",
        "Best answer": "Company-backed PD time can encourage an atmosphere of learning and sharing.  I don't really like the style where companies schedule PD time for you (like one Friday afternoon a month), but I do like a company that makes allowances for it.  I worked at a company where we actually set PD goals, and there was a heavy emphasis on sharing what you learned.  Some people did presentations, but I personally like learning about new ideas and then implementing them.  Sometimes this is through projects in different languages, sometimes they are still related to the current project (and may even be incorporated later).  I believe it does pay off because it can lead to more skilled developers.  I think companies may see more benefit from PD when developers share their knowledge, even if most of it is individual.  At the very least, discussing what you have been working on is good. For example, I wrote a mock testing framework to see how it was done, and I also embedded an IronRuby shell that interacted with our C# application.  The mock framework helped me learn a lot more about reflection, expression trees, and testable code practices in general, and the IronRuby shell helped me learn about the DLR and brush up on my Ruby.  Experimentation is healthy here. However, it is very hard when the client is breathing down your neck, and I will admit that my previous company really slipped on PD towards the end because the project was so demanding.  Unfortunately, I find that companies that do not foster this kind of environment often attract a lot of people who aren't passionate about what they do, and they tend not to improve their skills over time.  That's a generalization, but it's my current experience. "
    },
    {
        "ID": "10807",
        "Question": "I'm not sure if it's just me or if this is common in our field.  The past year has been a bit intense for me. I've been learning a few different technologies to get some tasks done and sometimes I've had to completely focus on that one new technology I'm learning.  What I noticed though when I go back to using everything together in the full project is that I forgot how to do things that I already knew how to do before I started focusing on learning that other new technology. Sometimes I find that I forget even simple things like the syntax for selecting a div in jquery, that I have to go back to old files to get a quick peek.  Sometimes I could be coding something that I recognize I did before in another project, but can't quite remember which project it was for, so I have to go through several projects and look through the code to try and remember where I did that. What ways have you invented to prevent yourself from forgetting things you've done before, or to easily access code you've written in the past? ",
        "Best answer": "I keep a work log. It's just a plain text file. Every day I make notes on stuff I'm working on: what I did, what sort of problems I encountered, how I solved them, who I talked to, what we discussed, what decisions we made, relevant files/classes/webpages, etc. So any time I have a problem and I have a deja-vu feeling about it I just search in my work log and in most cases I can quickly find what I'm looking for. Work log also makes it easier to keep track of how you spend your time, write weekly/monthly reports, prepare for performance reviews. "
    },
    {
        "ID": "10816",
        "Question": "I have just completed a course on computability and logic which was an interesting course. The lecturer recommend a few books on his slides, which include \"Gödel, Escher, Bach\". I can see the book is quite famous, and looks very interesting. But I have a few questions to ask regarding its content.  Is the content still valid today? I guess most theoretical stuff doesn't change over night, but are there any major points which no longer hold today that I should be aware of? I assume we actually HAVE made some progress in the last 30 years or so. Can any of you recommend a book on the subject which includes this progress (logic, AI, computability)?  Another question: Do I have to know about Escher and Bach? ",
        "Best answer": " Is the content still valid today? I guess most theoretical stuff don't change over night, but is there some major points which does not hold today which I should be aware of?  The content is logic and math.  It doesn't change in any substantial way, not only over night.  It will be valid forever. "
    },
    {
        "ID": "10849",
        "Question": "I'm working with a new team that has historically not done ANY unit testing.  My goal is for the team to eventually employ TDD (Test Driven Development) as their natural process.  But since TDD is such a radical mind shift for a non-unit testing team I thought I would just start off with writing unit tests after coding. Has anyone been in a similar situation?  What's an effective way to get a team to be comfortable with TDD when they've not done any unit testing?  Does it make sense to do this in a couple of steps?  Or should we dive right in and face all the growing pains at once?? EDIT Just for clarification, there is no one on the team (other than myself) who has ANY unit testing exposure/experience.  And we are planning on using the unit testing functionality built into Visual Studio. ",
        "Best answer": "Practice on existing bugs/defects. This is a really tough situation.  I've never gone all the way to TDD from nothing before, but in my experience, getting a team to go from no unit tests to proactively writing them has been a very \"one step at a time\" approach. First, get them comfortable writing unit tests and knowing really what they are and their benefits.  For my teams, it's been best to write unit tests for existing bugs.  Current bugs in systems have two things that you need to teach people to write unit tests well:   an expected precondition and postcondition an outcome that currently is not what is expected and violates that precondition/postcondition  This gives members very concrete practice examples.  They can write a test before they fix the bug, so that it fails.  Then, they can fix the code so that it passes, and fixes the bug.  Once they're comfortable with this, then you can get them the rest of the way so that they can write unit tests with no code up-front and then write new code to get their tests to pass. I think the trick is to give them something to practice on where there are clear method pre/post-conditions.  If requirements for methods are fuzzy, it's hard for even experienced TDD people to know exactly where to start.  Take it a step at time and you'll get there.  Good luck! "
    },
    {
        "ID": "10857",
        "Question": "It seems a bit of a controversial subject to document everything, including the \"JavaBean\" syntax of getters and setters for fields: People say its needlessly long and repetitive breaking DRY (don't repeat yourself), that the naming convention should explain everything, and it clutters code/documentation. Sometimes those arguments work. But other times, you end up with this:  Above is common to open source projects that do boldly follow those principles. You're left with entirely useless documentation. That doesn't explain anything about what's going on underneath, the possible effects, or even what the expected value is (could it be null or never null? I don't know; the Javadoc doesn't tell me).  So when should I document? Do I document everything even if it occasionally clutters code? Or do I document nothing since in my eyes it's \"obvious\"? ",
        "Best answer": "Document everything that makes sense to document. In an ideal world, yes, you would document everything.   However, on Earth, we have deadlines, feature cuts, families and friends to visit, vacations to take, only 24 hours in a day and only 365 days in a year.  There's just not enough time to document everything.  So, optimally, document everything you can (you won't get done), but get the most bang for your buck by:  Make code readable and method signatures as obvious as possible so that documenting is less likely to be needed. Documenting the most obscure things first.  Help out others by documenting the crazy hacks you had to do to get things out the door. Document the why before the what - Don't comment what something does, like \"Iterate over the customer records where balance is less than zero and rating is less than one and add them to the exemptCustomers list\".  Document why you are adding them to the list in plain english (or your team's language), like \"Since these customers have a negative balance and low ratings, they are causing us to lose money, so exclude them from being able to check out.  "
    },
    {
        "ID": "10927",
        "Question": "I'm trying to think of the cleanest way to implement a couple of methods that open a file. Consider the following method signatures: public static DomainObject Load(Uri urlToFile) {     /* downloads file and calls Load(savedToFilename) */ }  public static DomainObject Load(string filename)  {      /* creates filestream and calls Load(stream) */ }  public static DomainObject Load(Stream stream)  {      /* does actual loading */  }  I'm trying to implement some concepts from Clean Code, specifically:  When constructors are overloaded, use static factory methods with names that describe the arguments. For example, Complex fulcrumPoint = Complex.FromRealNumber(23.0); is generally better than Complex fulcrumPoint = new Complex(23.0);  Now, I know I do not have overloaded constructors, per se (I refactored away from that), but I think the principle is the same. So that implies that my Load methods be refactored to something like: public static DomainObject FromURI(Uri urlToFile); public static DomainObject FromFile(string filename); public static DomainObject FromStream(Stream stream);  But, tbh, I think it's more intuitive to use what I've already got. From a consumer's perspective it feels like Open will take whatever source I happen to have whereas the other method requires me to think first about what my source is and then wonder if there is a specific method for that. So I ask, from your more experienced viewpoint, which is better and why? ",
        "Best answer": "I think \"Load\" is the way to go for a few reasons:  The parameter type is already in the parameter list- why specify it again in the method name? If you happen to have multiple methods that produce something a stream in a class, \"FromStream\" becomes a problem. In the age of intellisense, it's much more logical to type \"Load\" and get a list of possible parameter options, rather than look at three different methods (especially if more methods than just those in this set happen to start with \"From\"). Another decent rule of thumb is \"name methods after verbs when you can,\" which votes in favor of something akin to \"Load,\" or at least \"LoadFrom...\"  "
    },
    {
        "ID": "11121",
        "Question": "Often some element of a path is variable, but the path as a whole needs to be documented in some manner for later programmers. Especially when using *nix systems, almost any character is a valid symbol for the path. Given that, I would like to delimit the variable portions of my path to prevent misunderstanding, but in a way that also survives best across different display environments (especially the browser). Methods I have seen include (example path in users home directory):  /home/<username>/foo - needs special escape for web browser context /home/your_username/foo - unfortunately the variable element tends to be overlooked /home/{username}/foo /home/:username/foo  Which have you seen most often or had the most success with and why? If a double delimiter method (which seems to be the most common/successful), what lead your choice of delimiters? ",
        "Best answer": "I'd say #1 is the clearest, and whats probably used most often.  The < and > delimit it nicely, and clearly indicate something should be substituted there. "
    },
    {
        "ID": "11157",
        "Question": "It is easy for managers and customers to appreciate what they can see.  I have seen many GUI developers who are average programmers with minimal knowledge of design principles or other programming idioms. However, these shortcomings often go unnoticed, specially by management and customers, if the programmer can create an impressive looking user interface. So much so that many GUI developers I know spend hours beautifying the GUI at the expense of writing bad, unmaintainable code.  On the other hand, middle tier programmers who develop APIs or business functionality or database code (SQLs etc.) are at a disadvantage as there is nothing tangible to showcase. Perhaps a code reviewer or an architect may appreciate the elegance, good design, scalability etc. of code but it means nothing to the outside world. Your code may run for years without breaking, may be very easy to maintain and have good performance, yet it never elicits the 'wow' that a slick looking GUI does. In my opinion, a corollary to this is (and I am going to get heavily downvoted for this, I know) that there is less motivation for a GUI programmer to write good clean code. EDIT: I must explain here that by GUI programmer, I don't mean a full-fledged web/GUI designer but a front-end programmer e.g., a java-swing programmer. Does the rest of the community agree ?  ",
        "Best answer": "I think I see your point, but I suspect that there is also an opposite issue to consider. Essentially, I believe you are suggesting that, because the UI is the element of the application 'in the face' of the end users, the UI developers enjoy a higher visibility than the team members working in deeper layers of the app. Certainly I agree that there may be a higher visibility. For instance, developers working on the UI elements may get to interact with the end users more often (arguably, for good reasons, since they do focus on the Human/Computer Interaction aspect). Yet, I think that the higher visibility comes in play even in cases when there is a problem. For instance, end users are very likely to report issues as 'GUI Issues' even when they are not. It may all boil down to perception, and a mature organization should be able to recognize values, virtues and weaknesses of the various team members independently from which layer of the app they work on.  A mature organization may also have moved beyond distinctions like 'UI developer' and 'business layer developer', recognizing they are all team members anyway, with different expertise perhaps, but always trying to educate each other on those areas of expertise. "
    },
    {
        "ID": "11188",
        "Question": "I believe that an agile approach is best for projects where the requirements are fuzzy and a lot of interaction is required to help shape the end user's ideas. However... In my professional work, I keep ending up at companies where an \"agile\" approach is used as an excuse as to why no effort was put into an up front design; when the requirements are well understood. I can't help but thinking that if the agile approach wasn't around, I'd be sitting here with a nice high-level specification and not having to revisit the same screen and functionality every second day when something else crops up or so and so hadn't thought of that.   Are the benefits of agile methodologies really enough to outweigh the excuse for being lame it gives to cowboy technical leads?  Update: Ironically I'm now a certified Scrum Master. One of the papers presented on the Scrum course observed that the best development process was one where there was a single expert or guru making the design decisions, however that has obvious weaknesses. Scrum shifts the responsibility for producing quality software to the \"Team\" which means a sub-standard team can get away with churning out spaghetti which I guess is no different to other Agile and non-Agile development processes. ",
        "Best answer": " I believe if you're using Agile development as an excuse for cowboy-style programming, then you're not really following Agile development. Cowboys will always be cowboys, no matter what process you give them. "
    },
    {
        "ID": "11199",
        "Question": "At my company (3-yr-old web industry startup), we have frequent problems with the product team saying \"aaaah this is a crisis patch it now!\" (doesn't everybody?) This has an impact on the productivity (and morale) of engineering staff, self included.  Management has spent some time thinking about how to reduce the frequency of these same-day requests and has come up with the solution that we are going to have a release every week.  (Previously we'd been doing one every two weeks, which usually slipped by a couple of days or so.) There are 13 developers and 6 local / 9 offshore testers; the theory is that only 4 developers (and all testers) will work on even-numbered releases, unless a piece of work comes up that really requires some specific expertise from one of the other devs.  Each cycle will contain two days of dev work and two days of QA work (plus 1 day of scoping / triage / ...). My questions are: (a) Does anyone have experience with this length of release cycle? (b) Has anyone heard of this length of release cycle even being attempted? (c) If (a) or (b), how on Earth do you make it work?  (Any pitfalls to avoid, etc., are also appreciated.) (d) How can we minimize the damage if this effort fails? ",
        "Best answer": "You can certainly deliver every week - or even more frequently. At the moment we generally release every two weeks, but it isn't unusual to deploy functionality when something has arrived with no notice from one of our partners that would be irrelevant if we waited for the next cycle. At some point in the next few months I'd like us to move to continuous delivery (items are released as soon as is practical once they're 'done') as standard, but we're not quite confident enough yet to go that far. The critical thing is you need your website strongly covered by automated tests - both unit tests and end-to-end acceptance tests/executable specifications. By implication this also means that your build is fully automated. At the acceptance level we use Robot Framework which is excellent for quickly building up a maintainable test suite thanks to it's keyword approach. For look and feel our onsite tester makes some cursory checks, but we also have a couple of guys in India who do a more thorough check across different browsers (there are sites which help with this sort of thing by taking screenshots for you, e.g. BrowserLab). We don't fully automate the deployment (the very final step requires manual intervention, this is a concious decision for us) - but we do automate all the things like ensuring that the correct database connections are being used, etc, with short deployment cycles it'd be too easy to make a mistake with this sort of thing. There's a pretty good recentish book on continuous delivery that you might want to check out, I've skimmed it but not gone through it in detail yet. What I've read so far chimes well with our experiences though: Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation In summary you need a highly disciplined team, a high level of automation and - most important of all - an extremely high degree of trust in that automation. To me it seems that moving to weekly cycles in your case may be a mistake - crisis patches hint at other issues and you should work to eliminate those. Upping the tempo could potential make the situation worse... "
    },
    {
        "ID": "11257",
        "Question": "I'm interested in real experience and personal answers not just those standard worn out \"benefits of contributing to open source\" that we all memorized by heart by now.  I've met many people who contribute to open source project and almost never did this topic come up: why did they contribute to this open source project. So, if you've contributed to an open source project before in any way, can you please pause for a second and try to remember what really made you decide to contribute to that particular project.  Was it a random decision, was it because you were bored, was it because the company you worked for was already using it and you contributed as part of your job, was it because the project was too big you wanted to get contracts, or the project was too small you wanted to build it, or because your prof or co-worker asked you to help with his open source project, or.....  To substantiate your reasons, please mention the project name and rate your involvement (heavy, occasional, light, once).  ",
        "Best answer": "Because something was broken, and I needed to fix it for my own purposes anyway - why not share it with others in the same boat. "
    },
    {
        "ID": "11436",
        "Question": "I want to understand an existing project to improve myself more, but I am not sure if the project would be overwhelming for me or not. I am not sure if I am capable of understanding it. I even think that I will fail. When I look at projects at a glance, it makes me feel scared to see thousand line of codes.  I don't even know where to start reading code. Do I have to follow by debugging or is there something else? Can you please guide me in this situation? I am totally getting confused at the start. Edit : A few thing to specify about my question. I know C# language and I have learned some WEB technologies like ASP.NET, MVC 2, and Web Forms but I am sure there still will be many thing which I don't know. I label myself as beginner but it is just because of a lack of professional experience in this area. But my OOP understanding is good enough. I am wanting to improve myself on WEB, so I am looking for WEB projects like blog engines which are simpler than others for starting. ",
        "Best answer": "Best is to start off by fixing bugs. Start with small bugs. Fixing the first bug is a big milestone. Because when you do that, you already know how to edit version controlled code, build it, understand the desired functionality and test it. You also know the conventions used, libraries available, idiom/riffs used, debugging techniques, coding style/conventions of the project, overall code organization, the bug tracking system used etc. Do not be overwhelmed by the size of the project. Start small. Start with easy bugs. Ideally your mentor should assign something simple enough to get you familiar with your development environment. Then move on to small features, in modules that you are already familiar with. Then you can move on to more complex bugs or move to adjacent modules. In time you will be able to think in bigger chunks of the project and the size will suddenly seem smaller. "
    },
    {
        "ID": "11485",
        "Question": "We test our code to make it more correct (actually, less likely to be incorrect). However, the tests are also code -- they can also contain errors. And if your tests are buggy, they hardly make your code better. I can think of three possible types of errors in tests:  Logical errors, when the programmer misunderstood the task at hand, and the tests do what he thought they should do, which is wrong; Errors in the underlying testing framework (eg. a leaky mocking abstraction); Bugs in the tests: the test is doing slightly different than what the programmer thinks it is.  Type (1) errors seem to be impossible to prevent (unless the programmer just... gets smarter). However, (2) and (3) may be tractable. How do you deal with these types of errors? Do you have any special strategies to avoid them? For example, do you write some special \"empty\" tests, that only check the test author's presuppositions? Also, how do you approach debugging a broken test case? ",
        "Best answer": "Try making the individual tests as small (short) as possible. This should reduce the chances of creating a bug in the first place. Even if you manage to create one, it's easier to find. Unit tests are supposed to be small and specific, with low tolerance for failure and deviation. In the end, it's probably just a matter of experience. The more tests you write, the better you become at it, the less chance you have to make crappy tests. "
    },
    {
        "ID": "11523",
        "Question": "I often have to explain technical things and technical decisions to my extremely non technical manager and I'm pretty awful at it. What are good ways to essential dumb things down for the rest of the world who don't have a passion for programming? Example questions I've been asked:  Why are you using Django instead of Java (Didn't accept that it was cheaper either) Asking me to rephrase things in non technical words, my sentence was \"Certain HTML tags are not allowed\". How can I possibly dumb that down? Other stuff that makes perfect sense to me, but is just so basic I don't know how to explain it Why this, why that, why everything!  Also, how do I tell my manager to look the basic stuff up on Google, like \"What is Pylons?\" ",
        "Best answer": "I tend to use analogies. Take whatever the topic is, and think of something completely non-technical that they would understand, and explain it to them that way. Best example I can think of offhand is if I need to explain object orientation, I'll explain it using a deck of cards. Or, when I was trying to explain the idea of wireless internet to my great aunt (who's never used a computer), I used cordless phones to explain it. I've yet to come across any topic I can't dumb-down this way. Update I see this continues to get upvoted, so here's some of how I'd explain OOP with a deck of cards:  A card is essentially a copy of the same object, a piece of stiff paper.  Each card has a set of properties (value [A-K], suit, face up/down), which may or may not be unique. A card can be used in many different ways, without altering anything about the card (held in a hand, put in a deck, played on the field, etc.) If you want to get into interfaces: A card must conform to certain standards, such as size and shape. If you do something to one card, that doesn't affect any other card.   A deck is a \"container\" object, which holds <= 52 card instances.  The deck can have various operations done on it, such as shuffle, show the top card, draw 5, etc. The deck doesn't need to know or care about a card's value/suit, only that it is a card.   A hand is another object, with a certain number of cards, and its own set of operations (play, add, remove, sort)  "
    },
    {
        "ID": "11637",
        "Question": "I'm in the planning stages of developing a web application that I want to make as secure as possible. I'm pretty clear about how to do that from a technical point of view, but there is one massive potential security hole: I want the site to have users.  As anyone who has ever been a user knows, users forget usernames, forget passwords, sometimes forget that they even had an account with your site. Users respond to phishing emails and give away their credentials. If you do anything that they find too complicated they won't use your site, it has to be simple and all happen in as few clicks as possible, but we have to balance that out by making it as hard as possible for users to accidentally give away their credentials and to keep access to the service as secure as possible. What I'm particularly interested in are strategies that go beyond the standard username and password combination and ways of recovering or resetting passwords that make things easy for users but hard for anyone trying to steal their account. I know a lot of sites provide an extra user-set security question, but because I don't trust users not to create a stupid question like \"what is 1+1\" I don't see how this can guarantee security. Are there strategies that could be useable to the most clueless user but challenging to a determined and targetted attack aiming to break into their account? If not what are the closest things we could get? As a user what are the best/worst sites for this you have encountered and why? ",
        "Best answer": "The only way I know of to build a truly secure web site is to require a password and have a challenge/response device.  For example, TreasuryDirect.gov sends you a card with a grid of random characters. As part of the logon process, you are asked to provide the characters that are contained in specific cells of the grid. But this is expensive and time-consuming; it requires to get the cards, to track them and to send them out. Plus, if a person loses the card, they can't log in until you get them a new card. In the case of TreasuryDirect, where you could be managing millions of dollars, this hassle is worth it. Only you can determine if it will be worth it for your site. Another option is to add a validation picture, like a lot of banks do. I believe this option only helps thwart some phishing attacks so if you think phishing will be a problem, this might be an option. I believe the other alternatives, including what most banks use, seem to be secure but aren't really. Security questions, for example, I believe compromise sites more than they help. Others think this too. One other thing will be to encourage users to use a \"Passphrase\" instead of a password as Jeff Atwood recommends. Amazon has taken this approach as well with their newish PayPhrase. "
    },
    {
        "ID": "11721",
        "Question": "We've all heard it; whenever there's a discussion about CS grads having poor development skills someone eventually says,  Computer science isn't software engineering.  The problem that I see is that programming and software development are taught in CS courses. They're just commonly taught poorly. If it's being taught, then why not teach it right the first time? * So I would like to see what the opinions are on 2 questions:  Is the CS != SE argument a copout or excuse for not properly teaching programming skills. Regardless of your answer to question 1; if you were in the almighty position of making such a decision: would you force an emphasis on proper teaching of programming in CS courses?  *. I have a suspicion, based on anecdotal evidence I obtained throughout my education, that most academics in the field haven't had to write code to be maintained and haven't maintained code, and as a result don't have the knowledge/skills to teach it effectively. ",
        "Best answer": "I don't think it's a copout, but rather an assertion that computer science isn't software engineering, it isn't programming -- it's, well, the science of formalized computation. In essence, it's math (in fact, many CS programs started off as applied mathematics programs). I think the problem lies more with industry than academia. Industry sees a CS degree listed on a résumé and thinks, \"Great, this guy's good with computers,\" and hires him to do anything related to a computer: IT, programming, software development, whatever. Those things don't necessarily lie in the realm of expertise of a CS graduate. In turn, a lot of people who like building computers or playing videogames enter a CS program expecting to do that kind of stuff, and get a rude surprise; i.e., a lot of students going into CS don't really belong there, and would be better off in a more targeted degree program. Secondly, there's a very limited time to teach CS -- a very broad subject -- in a typical undergraduate curriculum. My undergrad curriculum had 8 CS courses (for a BA) or 12 CS courses (for a BS), plus all the required math courses. That's not a lot of time to teach CS and math and programming and software engineering, so at some point you have to decide what's important, and what a student can pick up on their own. That last point is crucial: I think a good student -- in any subject area -- will explore ideas on their own. I have a CS degree, but I think I came out of college as a decent developer, because I studied and wrote software on my own. College isn't all about classes; it's also about giving students time to develop their skills semi-autonomously, while still giving valuable guidance. I maintain that teaching theory -- CS, math, etc. -- is just as important, if not more important, than teaching specific job-related skills. If you teach a student the why behind methods and techniques, and not just the what, you'll end up with someone who is much more adaptable when applying his knowledge. For example, I went to a much-maligned \"Java school\" and thus was taught in Java, but I also had courses in programming language theory, which explained the why behind many languages; as a result, I've learned to write much better software than if I didn't have that background. Sure, I don't write software in Haskell in my day job, but knowing a lot of programming language theory has allowed me to gain insight that has been applicable to my job as a programmer. I also think industry is expecting too much from college students. Industry wants to cut its own costs, so instead of training new recruits for their jobs, they expect colleges to become trade/technical schools; in effect, they want colleges to do their training for them. College graduates can't be expected to know everything fresh out of college. Being a good developer is as much about experience as it is about knowledge (especially in a relatively young field like software development). "
    },
    {
        "ID": "11785",
        "Question": "I was talking with someone today and this discussion came up. Suppose you purchase some software and X user licenses for it. You then want to install another instance of the software, however the company tells you that you need to buy another X licenses to use the 2nd installation. Is it ethical to duplicate the licenses since the same X users are going to be using them? (the security on the program we were talking about is not very good) EDIT Licenses are per-concurrent-user. In this situation users would (generally) not be logged into both installations at once. Regardless, we would keep the number of users logged in between the two installations limited to the number of licenses we purchased. ",
        "Best answer": "Is the license per-developer or per-machine?  That's really the determining factor. i.e. The Resharper and Sublime Text licenses I bought are per-developer.  I am allowed to install it on multiple machines as long as I'm the only one using it.   If the license is a concurrent user license, you are legally bound to stay within that limit.  If you have a 5 user license and 4 people are using it, then one of those people could use a second install..  If the 5th user shows up and launches it, you would be in violation of your license. That said, can they detect it? can they do anything about it?.. Perhaps not, but you asked about ethics... it would be unethical to violate your license. "
    },
    {
        "ID": "11802",
        "Question": "I'm planning on using Vincent Driessen's branching model which I found great for my current development. I think Vincent approach is very general to any development project but I'm finding it difficult to handle the following scenario. I'm working on an application that will be branded to several customers, so each one of my bundles is a different branch that I rebase from develop branch each time a new build is going to be released. Each brand has its assets (images and text) and they do not conflict with changes in develop branch because in that branch I never touch the assets. My question is, now that I'm planning on using it, how can I handle different versions based on the same code base but with different assets? ",
        "Best answer": "I've skimmed over the article, so I can't be sure I got everything 100% correct, but here's my idea: can you simply edit your assets in the master branch? Or, if you're keeping your \"branded branches\", modify your assets only there. You can then simply merge the code from your develop, since you're not touching any assets there. To be honest, if assets are the only problem here, I'd create a config file (or a database table if you have one) for your application which would tell it which assets to load dynamically. This would eliminate any \"brand branches\" and make it easy for you to maintain your assets easily. "
    },
    {
        "ID": "11813",
        "Question": "I once interviewed at a consulting company where it came up in the conversation that they use open source products (which is great, I've used Hibernate, JBoss, etc. extensively.)  One thing that did surprise me is that when I asked if they used GPL licensed OSS when writing applications for clients, they said \"Sure, all the time!  As long as the client gets what they want and are happy.\"  Now, I'm no lawyer or big license buff, but I was under the impression that using GPL code (let's say some library that you include), then you are required to release the entire application under the same license.  When I pointed this out, I was given a quick response of, \"Well, we give the clients all the source code when we're done, so that's really not an issue.\" Not wanting to press the issue further (interviews aren't the place for arguments like that), I let it slide.  However, that still concerns me about that particular practice of the business.  What is the official word on GPL licensed code and how \"open\" does it need to be?  Do you have to publish it and say \"My company used this library so here is the site where you can download our shopping and order fulfillment system application that we spend millions of dollars to build.\"?  In this situation, is the company right for using GPL code without the client's knowledge?  Is it enough to just \"give them the source\"? ",
        "Best answer": "Standard disclaimers apply: I am not a lawyer and neither are you. GPL, at its core, is about protecting the rights of those using the program to obtain and use the source as well. It does not mandate that you publish the source of any program you write that uses GPL'd code, merely that if you publish such a program you must also provide the source. I can write any number of apps for my personal use, pull in GPL'd code, and never give the source of any of this to anyone. I can write such apps for internal use by my company, and need only provide the source to those within my company who use the program (practically-speaking, I would have to do this anyway if they had any good reason to request it). If I write such a program for some other entity, I merely have to give them the code (and make it clear that the program is GPL-licensed) - if they go on to distribute it, they also must then make the source available, but I'm out of the picture. So the only worry your consulting company might have is that they were failing to make their clients aware of the license the code they provided fell under. Actually, I lie - if they had negotiated a different license with their clients (client owns all rights to code...) then they could be in hot water over that as well... But this is true for any third-party code: unless it's public-domain, you must comply with the license, and must not re-license it unless that right has been granted you by the copyright holder. "
    },
    {
        "ID": "11846",
        "Question": "I’m a long time developer (I’m 49) but rather new to object oriented development. I’ve been reading about OO since Bertrand Meyer’s Eiffel, but have done really little OO programming. The point is every book on OO design starts with an example of a boat, car or whatever common object we use very often, and they start adding attributes and methods, and explaining how they model the state of the object and what can be done with it. So they usually go something like \"the better the model the better it represents the object in the application and the better it all comes out\". So far so good, but, on the other hand, I’ve found several authors that give recipes such as “a class should fit in just a single page” (I would add “on what monitor size?\" now that we try not to print code!). Take for example a PurchaseOrder class, that has a finite state machine controlling its behavior and a collection of PurchaseOrderItem, one of the arguments here at work is that we should use a PurchaseOrder simple class, with some methods (little more than a data class), and have a PurchaseOrderFSM “expert class” that handles the finite state machine for the PurchaseOrder. I would say that falls in the “Feature Envy” or “Inappropriate Intimacy” classification of Jeff Atwood's Code Smells post on Coding Horror. I’d just call it common sense. If I can issue, approve or cancel my real purchase order, then the PurchaseOrder class should have issuePO, approvePO and cancelPO methods. Doesn’t that goes with the “maximize cohesion” and “minimize coupling” age old principles that I understand as cornerstones of OO? Besides, doesn’t that helps toward the maintainability of the class? ",
        "Best answer": "A class should use the Single Responsibility Principle. Most very large classes I have seen do to many things which is why they are too large. Look at each method and code decide should it be in this class or separate, duplicate code is a hint. You might have an issuePO method but does it contain 10 lines of data access code for example? That code probably shouldn't be there. "
    },
    {
        "ID": "11855",
        "Question": "I feel the MVVM community has become overzealous like the OO programmers in the 90's - it is a misnomer MVVM is synonymous with no code. From my closed StackOverflow question: Many times I come across posts here about someone trying to do the equivalent in XAML instead of code behind. Their only reason being they want to keep their code behind 'clean'. Correct me if I am wrong, but is not the case that: XAML is compiled too - into BAML - then at runtime has to be parsed into code anyway. XAML can potentially have more runtime bugs as they will not be picked up by the compiler at compile time - from incorrect spellings - these bugs are also harder to debug. There already is code behind - like it or not InitializeComponent(); has to be run and the .g.i.cs file it is in contains a bunch of code though it may be hidden. Is it purely psychological? I suspect it is developers who come from a web background and like markup as opposed to code. EDIT: I don't propose code behind instead of XAML - use both - I prefer to do my binding in XAML too - I am just against making every effort to avoid writing code behind esp in a WPF app - it should be a fusion of both to get the most out of it. UPDATE: Its not even Microsoft's idea, every example on MSDN shows how you can do it in both. ",
        "Best answer": "I've never heard of anyone suggesting to put everything in XAML. In fact, that would be a terrible idea, IMO. The problem, historically, came from Winforms apps having logic in their code behind that didn't belong there, because it was logic. That's where the importance of VM comes from.  It allows you to isolate the parts that tie together the View to the Model. Therefore, the View is left to only handle what is does best, which is UI. Now, if you happen to have situations where your UI requires code, then by all means go for it and put it in your code behind.  However, I would say that for 95% of the scenarios out there, you will most likely be better off leaving the UI in the markup file, and if you need some code behind, it's probably some logic that you're trying to write out that is more appropriately left to the View Model.  Exceptions to this are things such as Behaviors, but they are separate animals entirely, and require a special place (i.e. not in your code behind). "
    },
    {
        "ID": "11856",
        "Question": "I was involved in a programming discussion today where I made some statements that basically assumed axiomatically that circular references (between modules, classes, whatever) are generally bad.  Once I got through with my pitch, my coworker asked, \"what's wrong with circular references?\" I've got strong feelings on this, but it's hard for me to verbalize concisely and concretely.  Any explanation that I may come up with tends to rely on other items that I too consider axioms (\"can't use in isolation, so can't test\", \"unknown/undefined behavior as state mutates in the participating objects\", etc.), but I'd love to hear a concise reason for why circular references are bad that don't take the kinds of leaps of faith that my own brain does, having spent many hours over the years untangling them to understand, fix, and extend various bits of code. Edit: I am not asking about homogenous circular references, like those in a doubly-linked list or pointer-to-parent.  This question is really asking about \"larger scope\" circular references, like libA calling libB which calls back to libA.  Substitute 'module' for 'lib' if you like.  Thanks for all of the answers so far! ",
        "Best answer": "There are a great many things wrong with circular references:  Circular class references create high coupling; both classes must be recompiled every time either of them is changed. Circular assembly references prevent static linking, because B depends on A but A cannot be assembled until B is complete. Circular object references can crash naïve recursive algorithms (such as serializers, visitors and pretty-printers) with stack overflows.  The more advanced algorithms will have cycle detection and will merely fail with a more descriptive exception/error message. Circular object references also make dependency injection impossible, significantly reducing the testability of your system. Objects with a very large number of circular references are often God Objects.  Even if they are not, they have a tendency to lead to Spaghetti Code. Circular entity references (especially in databases, but also in domain models) prevent the use of non-nullability constraints, which may eventually lead to data corruption or at least inconsistency. Circular references in general are simply confusing and drastically increase the cognitive load when attempting to understand how a program functions.  Please, think of the children; avoid circular references whenever you can. "
    },
    {
        "ID": "11874",
        "Question": "We find it relatively easy to hire developers to work on various projects. The problem arises when the project is finished but still needs to be supported.  We really battle to get people to join the support team. It's seen as dead-end, career-limiting, boring, second-class etc. Currently, we get round this by getting the project team to assign some of their team to the support team for a while. Part of the assignment is to do a \"brain dump\" of the project so that the support team understands it. This works as long as the assignment is only for a fixed period. Trying to hire people to work in support full-time is a problem. There are few applications and the calibre is not particularly high. (The financial reality though is that support can be very lucrative for a company and once you get a reputation, you get approached by other companies to do their support even though you weren't involved in the original development.) ",
        "Best answer": "Don't To me the best option here is not to separate the developers into support and non-support in the first place.  IMHO there are three main reasons:  people that write things that are hard to support do not learn until they have to support these things. people doing only support will usually take the path of least resistance in correcting an error even if it hampers future work. the theoretical time savings in staying on schedule in the new development by having the separate support developers is always more than consumed by having to provide instruction or repeat work.  Within the dev team you can have people that have maintenance tasks or take an approach of letting the maintenance tasks be the training grounds for the newer team members, but if you try to sell it as the long term goal of the position you will only attract people that will give you heartburn or people that will soon be on their way out. There always needs to be a clear path to getting out of a 100% support dev role, and/or a certain percentage of new development work to keep good people interested. You do not want to attract the sort of people that are happy in that role indefinitely and you are never going to convince otherwise good devs to take that role and keep it long term unless you are offering the kind of pay that would never have them considering a career move. "
    },
    {
        "ID": "12133",
        "Question": "As much as programmers hate to document their code/system and draw UML (especially, Sequencing, Activity and State machine diagrams) or other diagramming notation, would you agree to do it if it kept managers from requesting a \"minor change\" every couple of weeks? IOW, would you put together visual models to document the system if it helped you demonstrate to managers what the effect of changes are and why it takes so long to implement them? (Edited to help programmers understand what type of answer I'm looking for.) 2nd edit: Restating my question again, \"Would you be willing to use some diagramming notation, against your better nature as a programmer, if it helped you manage change requests?\" This question isn't asking if there might be something wrong with the process. It's a given that there's something wrong with the process. Would you be willing to do more work to improve it? ",
        "Best answer": "Your problem is a lack of communication between you and your managers/stakeholders. They don't understand the problems that making frequent changes can cause - even if you have an agile process in place. But equally the developers don't understand the need that the program is designed to solve or the business process as well as you should. Why do I say this? If the managers were familiar with the development process then they'd know when was the appropriate time to request changes - before the start of the next sprint, or when the design document for the next phase was being written etc. If the developers understood the problem better then the product would be closer to the needs of the stakeholders. You need to address the communication issue and put in place procedures to manage the change - because change will happen. Hiding behind UML diagrams (or specifications, or even story cards) isn't going to solve the problem. "
    },
    {
        "ID": "12165",
        "Question": "My group at work has been trying to come up with a good process that we can use with TFS. I'm just wondering if some of you guys had some successful strategies with using TFS across multiple sites using multiple branches.  One specific issue that we have is probably our release engine. On the simple side, a developer should be able to check in his/her changes into the DEV Branch, and then ok certain date (say a freeze date) the DEV Branch will be \"reverse integrated\" into the Main Trunk (QA) in which then the changes will be pushed to the Production branch. The issue arose when a user check into the DEV Branch, but he doesn't want those changes to be moved into QA (because maybe other portion of the code is not done yet) ... any thoughts on that? ",
        "Best answer": "I've been using it for about 4 years on a couple different teams. What I've found to work best for our group is managing branches by release/iteration. So all changes for the iteration are made on a development branch and when all of those are ready to go to QA then we merge. This does become a little bit of a problem if you have something on the branch that isn't really ready to merge but then you can choose not to merge those change sets. That gets messy if the code that you aren't including changes some key piece or something but that has been rare in my experience. What we have done is manage the iterations so that everything really is complete by the end. If there is a piece that is risky you can do a branch off your DEV branch just for that so that you can manage it seperately. I would keep to as few branches as possible so that you don't spend to much time merging. "
    },
    {
        "ID": "12170",
        "Question": "Good Day Everyone, I am realy out of option for interpreting the GPL for EXTJS for my work/idea/personal project. I can see that this is an active forum I realy hope, I have a closure on this. First let me explain my project, Iam creating this website which is like a webportal which is intented for End-User, now this webapps uses an EXTJS library which is GPL'ed as with my understanding on this GPL Any application uses GPL license libray should be released to as Open Source a a GPL compatible Linsence, this is when I or my application is a derivework or I have modified the Library and released it. or destribute it. But EXTJS has this dual license which is typically giving me the rights to do what I want. without giving out my code. If my application is intented for end user only, not a derive form of work, not a library, not a development tool, I will not distribute it because it is on the web. with this also the Libraries i will use will remain untouch as it is. and I will have list of library I use and thier respective license to credit them Given this can I not close-source my application and not violate the GPL? is it ok for me to use GPL library so long as the above is meet? The question in short is can i used GPL'ed Library, do not released my code as OPensource compatible License and still NOT violate the GPL'ed terms? Thanks in Advance Nick Ace ",
        "Best answer": "Based on the letter of the GPL, you may be right in interpreting that it's possible to use Ext without releasing any code, depending on the circumstances. However, there's also something as the intent of the people who make it. In your place, I would mail licensing@sencha.com and ask them this question. If you don't like their answer, use a different library. Dojo, YUI and a ton of other libraries are available under more permissive licenses, so you have a ton of options. If you think that Ext JS is a step up from those other libraries, and the guys at Sencha want you to pay for it, then the honest thing to do is to pay for it. Good products deserve getting paid for if their authors want that. "
    },
    {
        "ID": "12189",
        "Question": "I am looking into learning Python for web development. Assuming I already have some basic web development experience with Java (JSP/Servlets), I'm already familiar with web design (HTML, CSS, JS), basic programming concepts and that I am completely new to Python, how do I go about learning Python in a structured manner that will eventually lead me to web development with Python and Django? I'm not in a hurry to make web applications in Python so I really want to learn it thoroughly so as not to leave any gaps in my knowledge of the technologies involving web development in Python. Are there any books, resource or techniques to help me in my endeavor? In what order should I do/read them? UPDATE: When I say learning in a structured manner, I mean starting out from the basics then learning the advanced stuff without leaving some of the important details/features that Python has to offer. I want to know how to apply the things that I already know in programming to Python. ",
        "Best answer": "First learn Python well Here are some online resources for learning Python  The Python Tutorial Wiki-Book Byte of Python Building Skills in Python Version 2.5 Python Free Online Ebooks  Python Bibliotheca Think Python Data Structures and Algorithms in Python How to Think Like a Computer Scientist: Learning with Python Python for Fun Invent Your Own Computer Games With Python Learn Python The Hard Way Thinking in Python Snake Wrangling For Kids  For Django you can refer  The Django book  What I suggest is   The Python Tutorial Wiki-Book The Django Book  Also check out this video "
    },
    {
        "ID": "12318",
        "Question": "Questions revolving around ways of getting customers to embrace new web technology / browsers so one can deliver better web software to the end user. It's hard to manage expectations of customers. Expectations such as:  Websites should work on older browsers. (ambiguous). Websites should not require specific hardware. Giving system / computer specs for running a website is unacceptable.  Web development isn't as easy as many think. There's a lot that goes into creating a properly run web application (not just a website). Take a look at Google Docs or Microsoft Office online. These are more than just regular websites, and they force users to use newer browsers. MS Office Online will not work with IE6, and they are trying very hard to push people to use IE8 (soon IE9). Google pushes as well, same with many other strong web entities. You can do a lot on the internet, from playing games, watching movies, doing work, even coding and have the server you're connected to compile your code. With everything the web can do, I find it amazing that people still want to put unrealistic expectations on web applications just because it will require someone to use a browser that is only... 2-3 years old.  I understand people don't like change. And we all know that many corporations will provide days/weeks of training to help their employees understand new internet browsers. There are also cases where people are forced to use old browsers because the archaic system they use for internal work only runs on that browser (ActiveX+IE6). My Questions How can you tell your end users that they will need to upgrade their browser to use the latest version of your website without a huge outcry? Why does the expectation exist that it's ok for software to require people to upgrade Windows/Mac versions, but a website cannot require a new browser version? ",
        "Best answer": "First of all, I don't really think people have any greater motivation \"to upgrade Windows/Mac versions\": Here are the usage share of web client operating systems (August 2010): Windows XP (48.32%), Windows 7 (19.81%), Windows Vista (18.43%), Mac OS X (6.42%), iOS (iPhone) (1.40%), Linux (1.34%). So nearly 50% are using an outdated OS (XP). By contrast, here are the usage  share statistics for browsers: Overall- IE (31.1 %), FF (45.1%), Chrome (17.3%), Safari (3.7%), Opera (2.2%). Breaking down IE by version- IE9 (0.2%), IE8 (17.3%), IE 7 (8.0 %), IE6 (5.6%). And for Firefox- FF4 (0.8%), FF3.6 (35.3%), FF3.5 (5.6%), FF3.0 (2.9%). So over 50% use the latest stable (or beta) versions of these browsers. As for your other question--\"How can you tell your end users that they will need to upgrade their browser to use the latest version of your website without a huge outcry?\"--you must understand (i) the factors motivating people to upgrade, and (ii) the factors inhibiting people from upgrading; then you must use these factors to bolster your appeal to your end-users. Motivators What rewards do end-users get by upgrading? Skimming Microsoft's IE8 marketing materials, these are the motivators they stressed most:  Appeals to Efficiency/Laziness:  Faster surfing (i.e., you will gain more free-time if you switch) You can accomplish more work with fewer clicks, because of a more intuitive design. IE7 had put certain buttons in strange places, etc. (I.e., you will lose less effort [as measured in clicks] if you upgrade).  Appeals to Security/Fear:  SmartScreen protects you from malicious software (i.e., you will lose safety if you don't upgrade). Compatibility View allows you to view older pages correctly just as the website’s designers intended (i.e., you won't lose anything if you upgrade).   So motivators boil down to what the end-user will gain by upgrading (or lose if they don't upgrade). These things must be important to the end-user: Time, effort, financial security, compatibility, etc. Reinforcers Reinforcers aren't rewards, but they help increase the rate of adopting the desired behavior. Here's an example: Your web site can detect old versions of browsers, and direct users to download and install the latest versions by providing links and motivators. Inhibitors  Nuisance (cost in terms of time and effort) to upgrade Nuisance of learning something new New versions are inevitably buggy and suffer from incompatibilities that haven't yet been discovered  You must anticipate these arguments, and develop effective counter-arguments:  There is a risk/reward trade-off, and the rewards outweigh these risks. New versions of browsers are fully supported, and bugs will be worked out. By contrast, older versions aren't well supported; and the oldest version have lost support entirely.  "
    },
    {
        "ID": "12322",
        "Question": "TBH:  I've never used an observer pattern.  But it looks pretty useful for this situation: I'm developing a very simple strategy game.  It involves two combatants who simultaneously bludgeon each other over a short period of time, the winner being the combatant with the most health remaining in the end. Now, there's a lot of configurable strategies for the combatants, different pieces of equipment that can change how often they attack, etc.  I'm also planning on including some \"outside\" factors like weather that will affect the combatants. So....    I could create a Fight object that is the subject.  It keeps track of the time and notifies the observers every \"tick\" of the clock. The observers are two instances of a Combatant object and one Weather object. It seems like a good idea, but where are the pitfalls?  Actually, as I'm typing this I think one problem is that observers are probably ignorant of each others' parameters.  Or must they be? Has anyone used an effective design pattern to do something like this? ",
        "Best answer": "If the two Combatant instances and Weather object are notified as the result of the clock tick count being updated, and not by direct intervention by the Fight object, then it's the Observer pattern. If your Fight object triggers the notifications directly you can use the Command pattern to encapsulate the receivers or have the Fight object send messages directly to the Combatants and Weather objects individually. "
    },
    {
        "ID": "12369",
        "Question": "EDIT: As gavenkoa's answer points out, Oracle Database 12c (released a couple of years after this question was asked) has support for Identity Columns.  As far as I know, Oracle's RDBMS is one of the few (the only?) SQL database products that doesn't support identity/autonumeric columns. The alternative offered by Oracle is database sequences, a feature in many ways much more powerful than auto-numeric columns, but not equivalent. It is not that I don't like sequences. What I hate is having a different programming model for generating row identity values between Oracle and any other database. For example, I often try to setup HSQL or SQLite for java apps that will eventually run over an Oracle database when I'm not working specifically on the data layer (just as a stub or mocking database). I cannot do that easily because I need different set of SQL DDL scripts: one for Oracle, and one for everyone else; I also need two sets of Hibernate mapping files if I'm using Hibernate. What I find intriguing is that Oracle Database, being one of the most complete and robust enterprise software packages of the last decade hasn't put that seemingly basic feature in their product, but almost any other RDBMS, even the smaller ones, has it. Why?  Why doesn't oracle support a sequence-based identity column shortcut syntax that dumb and lazy people like me can use? The only reason I can think of is that Oracle does that on purpose as a vendor lock-in strategy so your code is harder to migrate to other RDBMS where your database sequences cannot be used. Or maybe I'm just wrong and confused? Please enlighten me. ",
        "Best answer": "I'm only guessing here but its probably for legacy reasons.  Sequences & identity columns have annoying properties like not respecting transactions. Sequences actually provide you with more flexibility than a plain identity column as it allows you, the developer, to decide how and when to apply the sequence.  Sequences also give you the ability to know your assigned sequence number before having to insert the record.  On a side note, if you plan in the future to support either replication or any form of disconnected'ness (eg mobile devices or offline connections to your database) i would suggest using GUIDs as your key. As this removes issues around sequence partitioning etc.  "
    },
    {
        "ID": "12394",
        "Question": "I've done some open source projects, and I plan to do more in the future. So far, I've released all my code under GPL, but I've read a few articles which claim GPL is too restrictive for any code to be used in corporate environment. This, supposedly, reduces contributions. Here's what I wanted to accomplish: For full applications:  no commercial use with the exception of selling support for the application (i.e. the app cannot be sold, but everything around it can)  For libraries (components, plugins, ...):  can be included into commercial projects without modifications any modification the the library/component must be open sourced (contributed back) - the rest of the project, commercial or not, is not affected  For applications, GPL still seems the logical choice. For libraries, my primitive understanding of licences makes me think that LGPL is a good match, but, I'm not sure. I've looked at MIT licence, and that seems too permissive. Most of the time, I want people to use my code anywhere they want to, as long as any improvements are contributed back. This brings me to my question(s): is LGPL a logical choice for open source libraries, components, plugins etc? Is there a better alternative? Is GPL a good choice for my applications or is there something better? Update: For those who are interested in my final decision, I've decided to release my libraries under multi-license scheme, MPL, LGPL and GPL. This enables virtually everyone to use my code with no obligations, unless they modify it under MPL, in which case it would have to be contributed back. This means the code can be used by both FSF and proprietary software, but \"bad\" commercial exploitation is prevented (or so I'd like to think). ",
        "Best answer": "  no commercial use with the exception of selling support for the application (i.e. the app cannot be sold, but everything around it can)   Beware that the GPL doesn’t forbid selling the application, and forbidding this would indeed render your license non-free, as observed by Huperniketes. The only thing the GPL ensures is that the company selling the software would also have to provide the code base for free. But they don’t have to provide the software package as-is for free. That’s quite a big difference, since the source code of a software is not a readily usable product. "
    },
    {
        "ID": "12401",
        "Question": "[Disclaimer: this question is subjective, but I would prefer getting answers backed by facts and/or reflexions] I think everyone knows about the Robustness Principle, usually summed up by Postel's Law:  Be conservative in what you send; be liberal in what you accept.  I would agree that for the design of a widespread communication protocol this may make sense (with the goal of allowing easy extension), however I have always thought that its application to HTML / CSS was a total failure, each browser implementing its own silent tweak detection / behavior, making it near impossible to obtain a consistent rendering across multiple browsers. I do notice though that there the RFC of the TCP protocol deems \"Silent Failure\" acceptable unless otherwise specified... which is an interesting behavior, to say the least. There are other examples of the application of this principle throughout the software trade that regularly pop up because they have bitten developpers, from the top off my head:  Javascript semi-colon insertion C (silent) builtin conversions (which would not be so bad if it did not truncated...)  and there are tools to help implement \"smart\" behavior:  name matching phonetic algorithms (Double Metaphone) string distances algorithms (Levenshtein distance)  However I find that this approach, while it may be helpful when dealing with non-technical users or to help users in the process of error recovery, has some drawbacks when applied to the design of library/classes interface:  it is somewhat subjective whether the algorithm guesses \"right\", and thus it may go against the Principle of Least Astonishment it makes the implementation more difficult, thus more chances to introduce bugs (violation of YAGNI ?) it makes the behavior more susceptible to change, as any modification of the \"guess\" routine may break old programs, nearly excluding refactoring possibilities... from the start!  And this is what led me to the following question: When designing an interface (library, class, message), do you lean toward the robustness principle or not ? I myself tend to be quite strict, using extensive input validation on my interfaces, and I was wondering if I was perhaps too strict. ",
        "Best answer": "I would say robustness when it doesn't introduce ambiguities. For example:  When parsing a comma separated list, whether or not there's a space before/after the comma doesn't change the semantic meaning. When parsing a string guid it should accept any number of the common formats (with or without dashes, with or without surrounding curly braces). Most programming languages are robust with white space usage.  Specifically everywhere that it doesn't affect the meaning of code.  Even in Python where whitespace is relevant, it's still flexible when you're inside of a list or dictionary declaration. I definitely agree that if something can be interpreted multiple ways or if it's not 100% clear what was meant then too much robustness can end up being a pain though, but there's much room for robustness without being ambiguous. "
    },
    {
        "ID": "12435",
        "Question": " NEW YORK - With a blast that made   skyscrapers tremble, an 83-year-old   steam pipe sent a powerful message   that the miles of tubes, wires and   iron beneath New York and other U.S.   cities are getting older and could   become dangerously unstable.  July 2007 Story About a Burst Steam Pipe in Manhattan  We've heard about software rot and technical debt. And we've heard from the likes of:  \"Uncle Bob\" Martin - Who warned us about \"the consequences of making a mess\". Michael C. Feathers - Who gave us guidance for 'Working Effectively With Legacy Code'.  So certainly the software engineering community is aware of these issues.  But I feel like our aggregate society does not appreciate how these issues can plague working systems and applications. As Steve McConnell notes:  ...Unlike financial debt, technical   debt is much less visible, and so   people have an easier time ignoring   it.  If this is true, and I believe that it is, then I fear that governments and businesses may defer regular maintenance and fortification against hackers until it is too late. [Much like NYC and the steam pipes.]  My Question:    Is there a way that we can avoid the software equivalent of NYC and the steam pipes?  ",
        "Best answer": "Having supported a variety of government and private industry applications I would say that most companies and at least the US Government is well aware of the dangers of letting code rot and not staying on top of the latest security trends. We regularly have to get our software certified for various susceptibilities and most government electronic systems, even old ones, get regular updates to keep them secure.  Of course there are exceptions, and hackers are always on the move, but on the whole I think people are pretty aware you can't just throw something out there and never touch it again. "
    },
    {
        "ID": "12466",
        "Question": "C is ubiquitous, and C++ nearly as much, but I'm curious which other languages follow a similar model of header files.  Specifically, in which other languages is it idiomatic to edit pairs or groups of \"header\" and \"implementation\" files?  (A group as in multiple headers with one corresponding implementation file or a single header with multiple implementation files, as also seen in C.)  For example, the pair \"blah.h\" and \"blah.c\". C's header files are tied into the C preprocessor, but that's not the detail I'm focusing on here.  For example, PHP has various include mechanisms, but you don't have a pair/group of \"blah-header.php\" plus \"blah-notheader.php\". Some languages generate a file from source in a one-to-one mapping, such as Java, but that's not what I'm talking about either.  I'm interested in cases where the programmer directly edits both/all files in the pair/group. Perhaps to put it another way: which languages have a declaration file (\"header\") and definition file (\"implementation\") such that a person would generally edit both files in tandem? ",
        "Best answer": "Ada  Any Ada package on the other hand consists of two parts, the specification (header) and body (code). The specification however is a completely stand alone entity which can be compiled on its own and so must include specifications from other packages to do so. An Ada package body at compile time must refer to its package specification to ensure legal declarations, but in many Ada environments it would look up a compiled version of the specification.    --file example.ads, the package specification. package example is : : end example;  --file example.adb, the package body. package body example is : : end example;   Source: http://www.adahome.com/ammo/cpp2ada.html#2 "
    },
    {
        "ID": "12528",
        "Question": "You probably know the list of open source licenses officially approved by the OSI. Most notably I guess would be the GPL, MIT, [insert your favorite license here]. I recently ran into a project which although was open source (the creator made all source code available), was not officially open source under one of those official licenses.   It released the source, but made no promise to release the source in the future.  It allowed modification suggestions, but made no promises to accept patches and disallowed external distribution of externally-patched versions.  It allowed the use of the software in commercial or paid projects, but disallowed the sale of the software itself.   I suppose it could be called \"available source\" not open source as we like to think of it. I can see why the management team of a company wouldn't want to do business with this software. They can't fork it, they can't sell it, they can't create their own version of the software and distribute it or sell it. But would it matter to you as part of a software engineering team who's just using this software? I can still get my work done with it, I can use it in a project for which I'm paid (but I can't sell the software itself, which I'm not in the business of doing anyway), and I can make changes to the code to make it behave differently for my needs (but I can't make those modifications public), and if I do want those modifications officially made available to others, the approval is up to the project itself and they choose whether to incorporate them in an official release or not. So we know that a company that wants to base its business on this \"available source\" software can't do that, but as someone from the software engineering team, would those differences matter to you or do they seem less relevant? Curious what others think of this. ",
        "Best answer": "For projects which would have had to develop from scratch the functionality provided by this software, it's a definite convenience not to do so. But whether a comparable open source package would be better depends on other factors:  will it be used to provide some service or bundle it as a part of another product? do they have the resources to enhance and maintain the product independently? is there a competitive advantage to use this software over the open source version (either in the code or the project management)?  Answering no to any of these factors indicates the OSS is a better choice. Most of the time, the code itself isn't the determining factor. One needs to examine the bigger picture. SIDEBAR OSS projects can't legally promise they'll keep future versions open, or that there will be future versions. That's one reason why having an open license is so advantageous. Also, OSS projects aren't required to accept patches from contributors (particularly without a transfer of ownership or rights). "
    },
    {
        "ID": "12556",
        "Question": "I'm a student at the University of South Florida who's frustrated with the educational environment in the Computer Science program. Right now, I'm taking \"Program Design.\" I should be learning how to organize my source code into functions and modules in order to make my programs readable and maintainable; instead, I'm learning about arrays and recursion in C. Next semester, I have to take \"Object-Oriented Design,\" which is taught through C++ (shudder.) Two years ago, I fell in love with programming, and I've been learning as much as I could since then. The prospect of taking another C++ class bores me almost to tears. For that reason, I thought I would start a programming club in order to meet similarly ambitious students, learn new languages, discuss software development topics, and work with other students developers.  However, I'm beginning to realize that there may not be any other students who share my software development experience. It's not because of a lack of motivation but a lack of opportunity: I know of only one other programming class (\"Programming Languages\") and no classes on real-world software development. Everybody else only has experience writing trivial scripts in C and C++. I've realized that if I want to work with other student software developers, I'm going to have to train them myself. Now, I'm planning to make the club a software development bootcamp, teaching members how to develop software with modern tools and languages. Obviously, starting an unofficial software development course is a monumental task with many possible approaches. My question to you, dear reader, is  What's my plan of attack? Should I  lecture the club myself, trying to balance club work with homework? ask the CS faculty to teach on topics within their expertise which may be less than relevant to members? try to find a sympathetic, experienced developer inside or outside the school who can share my workload? show video lectures (from MIT OpenCourseWare, Google Tech Talks, etc)? hold hands-on programming workshops? assign homework? do something else?  ",
        "Best answer": "Frankly your whole attitude concerns me. You haven't even gotten the group together and already you assume you will be only one who has any knowledge to impart and the one who should decide what the group will do.  The best bet is to get the intial group together and, as a group, brainstorm what they want to do. What you personally want to do is irrelevant in terms of what the group wants to do. Deciding what to do without the input of the other group members before the first meeting,  you will come across as an arrogant idiot that I wouldn't want to work with voluntarily. Thus you would kill the group before it got started. "
    },
    {
        "ID": "12572",
        "Question": "Not because I care how long it takes someone to read a book, but more because I'm interested in how people get the knowledge from a book (or I guess other sources) into their brains in the best, most efficient way. Personally I take a multi-pass approach (from my answer to the original question):  Skim through contents, dip in and read anything with an interesting looking heading and finally gawp at any nice diagrams and illustrations. I won't take much in at this stage, but it gives me a mental view of the book (an hour or so at most) First pass through the book, generally I'll read the opening chapters thoroughly for a book that is either very heavy going or introduces something completely new. For books that cover a subject I already know about I'll skim or skip bits that seem trivial. The remainder of the book I'll go through reasonably quickly but not so quickly that I'm just page flipping. (about a week) Not all books that I read make it this far, but if I find a book interesting or useful enough I'll then study it properly. I will go through the book at a slower pace and do some or all of the examples, try out code, etc. I will often skip entire chapters here unless the book is really good (1-3 weeks depending on the book). Finally when I've finished reading it and am reading other books I will often dip into it again and again to cross-reference, compare, look things up, browse, etc - so many of my favourite books don't just end up gathering dust on the bookshelf.  I rarely take notes when reading (although I may do some planning on paper if I'm working through something like a code sample). I've also considered starting to use a personal kanban for organising my progress, but have never quite got around to using that technique. Mindmaps are another thing I like the idea of but rarely do. What other methods to people have? How successful do you find them? Are there any commonly recommended techniques that you feel are a waste of time? ",
        "Best answer": "Do the exercises. If there aren't any, invent some. "
    },
    {
        "ID": "12589",
        "Question": "When choosing what we want to study, and do with our careers and lives, we all have some expectations of what it is going to be like. Now that I've been in the industry for almost a decade, I've been reflecting a bit on what I thought (back when I was studying Computer Science) programming working life was going to be like, and how it's actually turning out to be. My two biggest shocks (or should I say, broken expectations) by far are the sheer amount of maintenance work involved in software, and the overall lack of professionalism:  Maintenance: At uni, we were all told that the majority of software work is maintenance of existing systems. So I knew to expect this in the abstract. But I never imagined exactly how overwhelming this would turn out to be. Perhaps it's something I mentally glazed over, and hoped I'd be building cool new stuff from scratch a lot more. But it really is the case that most jobs are overwhelmingly maintenance, bug fixing, and support oriented. Lack of professionalism: At uni, I always had the impression that commercial software work is very process-oriented and stringently engineered. I had images of ISO processes, reams of technical documentation, every feature and bug being strictly documented, and a generally professional environment. It came as a huge shock to realise that most software companies operate no differently to a team of students working on a large semester-long project. And I've worked in both the small agile hack shop, and the medium sized corporate enterprise. While I wouldn't say that it's always been outright \"unprofessional\", it definitely feels like the software industry (on the whole) is far from the strong engineering discipline that I expected it to be.  Has anyone else had similar experiences to this? What are the ways in which your expectations of what our profession would be like were different to the reality? ",
        "Best answer": "I feel you man.  I just graduated little over a year ago in fact, jumped on the first job offer that came my way and got the biggest shock of my life. Things I didn't expect: School stress and Work stress aren't the same - The stress of working on a school project with friends, or working solo, even with that looming thesis deadline or special project defense does not compare to the stress of seemingly unreasonable work deadlines, communication problems, (a little of office politics) and crunch times. Lack of Best Practices - Same as your experience on professionalism. Before taking my first job and during my training period, I rushed off reviewing and reading about best practices in both programming and software engineering.  These aren't followed as well as they should for impractical and, to be fair, practical reasons. And sometimes, your knowledge counts very little against others who are merely afraid of the unknown and treat these practices with disdain. What they taught in school was just the tip of the iceberg - Thinking that what I learned self-studying and from classes was enough to get me through, I was shocked to say the least as I stared dumbfounded at the first piece of code I was supposed to maintain.  A lot of the skills I use now were learned on the job or during my job that I keep on wondering if I could've made it without a college degree at all. XD The Importance of Communication - Made me realize what all those English classes were for. Before the real world, I could not see the relevance of having three to four different English classes in college when it's been taught since we were in the first grade. You're no use in your job when you can talk to a computer but fail to talk to people. "
    },
    {
        "ID": "12626",
        "Question": "For most open source project, there is a well-founded project team and corporate sponsorship, and a lot of active contributors. The procedure for filing bug reports are clearly documented. However, there are also some open source project(s) that have been in existence for more than 10 years (maybe 15), and were included in all sorts of free and commercial products (OSes and linux distros, etc), and everyone just assumes it is correct, despite some parts of it in a state of despair and full of bugs. It appears to me that the real users (programmers in-the-know) simply choose to use the library in a certain way as not to trigger the bug. Few choose to speak up. There are also big-name companies that fix the bugs quietly (in their own products) without giving out any patches. And use that to their business advantage. There is no leading developer. There is no information as to who are the active developers, except that you can browse the mailing list and see who has recently submitted patches, and assume that they might know someone who is helpful. How should I handle a vulnerability case, without leaking information in a way that gives ammunition to the bad guys? This question is a spin-off from: https://softwareengineering.stackexchange.com/questions/5168/whats-the-biggest-software-security-vulnerabilty-youve-personally-discovered ",
        "Best answer": "Talk to Secunia (or any of the other bug databases), and let them handle it. They do this on a daily basis, and probably already have a procedure for if they can't identify an appropriate contributor for a project. (I would guess, if there's no contacts for the library itself, they'd contact major projects currently using the library, allowing any widespread software to fix/workaround any security issues, before releasing details to the public.) "
    },
    {
        "ID": "12645",
        "Question": "Some time ago I wrote a very small python script that periodically checked an xml feed for new entries, and alerted the user to new entries when present. I wrote this for myself, so it was essentially a console based program that anyone comfortable with a console interface could have used. After a while I decided it could be of more use to other people and began to tidy it up, sanitize inputs, remove bugs. It occurred to me that because I'd written the script I knew how to use it efficiently, accurately etc. Others might not, so I started adding a GUI. This started out as a simple menu, and then expanded to a more full GUI with both an interface and options menu. I then added stored user preferences and also storage for previously searched xml feeds to speed up repeat searches. I added logging to help debug the application in case things go wrong, brought the application up to the latest available stable python codebase for my chosen platform and improved dialog features. I've bugfixed and commented my code clearly, and yet I still have things I think can be done to improve the app before I make it available to alpha testers. It's a far far cry from my original 20-30 line script. What I anticipated would take me just an hour or two to go from proof of concept to an acceptable use program has taken 10-20 times that. (I'm still a noob, and stuff takes me a long time, but still....) How do you know when to stop adding/tweaking/fixing stuff and let your baby crawl out in the open? ",
        "Best answer": " When you hit the deadline.  If you have no deadline, this is your problem... Here is how I work:  I add new features/bugs in my product backlog. I prioritize the whole product backlog on business value and estimated (the last is optional in case of personnal project). I allocate work time to myself. The release date is the end of that time. I start with the very first in the list. I work on a feature a time. To be completed, a feature must be really complete including documentation (at the end of a feature, I can potentially ship the product). I take the next one until my allocated time is consumed. If the time is consumed when I'm building a feature, I discard it temporaly. When the allocated time is consumed, I take the latest build and make a release with it. I repeat the process from point 1.  "
    },
    {
        "ID": "12672",
        "Question": "Imagine the following scenario: You've detected that your (or someone else's) program has a bug - a function produces the wrong result when given a particular input. You examine the code and can't find anything wrong: it just seem to bog out when given this input. You can now do one of two things: you either examine the code further until you've found the actual cause; or you slap on a bandage by adding an if statement checking if the input is this particular input - if it is, return the expected value. To me, applying the bandage would be completely unacceptable. If the code is behaving unexpectingly on this input, what other input that you've missed will it react strangely to? It just doesn't seem like a fix at all - you're just shoveling the problem under the rug. As I wouldn't even consider doing this, I'm surprised at how often the professors and books keep reminding us about how applying \"bandage\" fixes is not a good idea. So this makes me wonder: just how common are these kinds of \"fixes\"? ",
        "Best answer": "Time/deadline pressures are one reason. If you are up against a tight deadline and you've got your boss breathing down your neck (possibly literally!) then doing this and thinking \"I'll come back and fix this later\" is very tempting and might be the only thing you can do. Of course the number of times you actually go back and fix it properly are very few and far between because you have a new problem that needs fixing yesterday. "
    },
    {
        "ID": "12777",
        "Question": "I've heard it said that the inclusion of null references in programming languages is the \"billion dollar mistake\".  But why?  Sure, they can cause NullReferenceExceptions, but so what?  Any element of the language can be a source of errors if used improperly. And what's the alternative?  I suppose instead of saying this: Customer c = Customer.GetByLastName(\"Goodman\"); // returns null if not found if (c != null) {     Console.WriteLine(c.FirstName + \" \" + c.LastName + \" is awesome!\"); } else { Console.WriteLine(\"There was no customer named Goodman.  How lame!\"); }  You could say this: if (Customer.ExistsWithLastName(\"Goodman\")) {     Customer c = Customer.GetByLastName(\"Goodman\") // throws error if not found     Console.WriteLine(c.FirstName + \" \" + c.LastName + \" is awesome!\");  } else { Console.WriteLine(\"There was no customer named Goodman.  How lame!\"); }  But how is that better?  Either way, if you forget to check that the customer exists, you get an exception. I suppose that a CustomerNotFoundException is a bit easier to debug than a NullReferenceException by virtue of being more descriptive.  Is that all there is to it? ",
        "Best answer": "The problem is that because in theory any object can be a null and toss an exception when you attempt to use it, your object-oriented code is basically a collection of unexploded bombs. You're right that graceful error handling can be functionally identical to null-checking if statements.  But what happens when something you convinced yourself couldn't possibly be a null is, in fact, a null?  Kerboom.  Whatever happens next, I'm willing to bet that 1) it won't be graceful and 2) you won't like it. And do not dismiss the value of \"easy to debug.\"  Mature production code is a mad, sprawling creature; anything that gives you more insight into what went wrong and where may save you hours of digging. "
    },
    {
        "ID": "12815",
        "Question": "I work for a mid sized Architecture and Engineering firm, our sub group focuses on developing tech solutions for engineers, mappers and technical managers. So we're heavy on desktop based apps for GIS and Civil/Env Engineering (some web). The company sells the services that our Engineers and mappers produce and our team develops tools that aids in them being more productive, efficient and help in adding value to their decisions and products, we DO NOT sell the technology. We are going through growing pains where initially we used to be extremely responsive and could rapidly prototype apps for engineers which immediately brought budgetary savings. That mindset has worked for us in the past. But this year we won a huge contract and our client base has basically quintupled (5 times?). What we are finding is that this rapid prototyping culture is hurting us, where project managers have started to expect short response times for tool development and robust production ready tools for all our engineers and GIS analysts. We've grown organically and now it seems that we are running into these issues were it appears we have to scale back our speed for more stability. Is this a legitimate tradeoff? Is there a win-win? How does one push back the engineer, project manager and analyst when they are our clients, they fund us and yet we need to be able to push back and tell them that if they want stability they have to be realistic about time frames?  This isn't Microsoft Word, these are specialized GIS software and Engineering models with a ton of interop components for other industry standard models, they aren't idiot proof tools, they need informed inputs and we can only test things so much. Has anyone dealt with similar growing pains? Recommendations/advice on a communication stance, books, blogs? ",
        "Best answer": "First of all I think the basic idea of quick development and delivery is fine and if you can keep it then great, do so (it is the gist of the Agile Movement). Question is why you have problems now?  Is the problem that you cannot deliver as fast because you have more clients to share your time?  Is the problem that the new employees cannot produce new code fast enough?   My personal guess is that you've found that \"talent doesn't scale\" and that you now have too few experienced programmers to do what you did before for more clients.  EDIT:  If so, you need to acknowledge this fact, as it is impossible to throw additional people at the problem to keep the scaling (Brooks's law).  Your experienced people will need to mentor new apprentices, and that will take some time. "
    },
    {
        "ID": "12861",
        "Question": "I know python is not suitable for things like microcontrolers, make drivers etc, but besides that, you can do everything using python, companys get stuck with speed optimizations for real hard time system but does forget other factors which one you can just upgrade your hardware for speed proposes in order to get your python program fit in it, if you think how much cust can the company have to maintain  a system written in C, the comparison is like that: for example: 10 programmers to mantain a system written in c and just one programmer to mantain a system written in python, with python you can buy some better hardware to fit your python program, I think that low level languages tend to get more cost, since programmers aren't so cheaply than a hardware upgrade, then, this is my point, why should a system be written in c instead of python? ",
        "Best answer": "The main reason is because the software was already written in C (or C++ or whatever) before Python became a legitimate choice. If it's a question of rewriting your million lines of C code into Python or continuing with the C code, then the choice is clear. If you spend 12 month rewriting the code in Python, then that's 12 months where you're not adding new features and you'll basically spend 12 months getting back to exactly where you were. Not to mention that fact that you'll probably have missed all those corner-case bugs that had been fixed over the years in the C version, but not really called out in the code so when your Python dev comes along to port it and says \"why does the code do this? That seems wierd... I'll just do it this other way\" he's actually forgetting about that customer from 6 years ago who lost 12 months worth of payroll because of this obscure bug in their RAID controller that was worked around with just that specific piece of code. It's basically the same answer people get when they ask \"Why doesn't Microsoft rewrite Windows or Office or (insert-product-here) in .NET? I thought .NET was the poster-child for all Microsoft development these days?\" Now, when it comes to developing new products, then Python (and all the other possible choices) become a more viable option. But  you've still to weigh the benefit of going with Python (or whatever) over C or C++, particularly when your new product may be interfacing with lots of existing code that you've already got written in C or C++. "
    },
    {
        "ID": "12864",
        "Question": "I am curious about the very early steps that a programmer's mind takes when it first encounters a problem that it is supposed to solve. How do you start reasoning about it ? What are the first questions you ask yourself ??.    Ok, let me make it more concrete for those of you who think the question is too vague or entirely situation dependent. Pick any of the following   requirements and think out loud about your very first hunches/impressions/tendencies about how to solve it. 1) build a simple word processor with about as much functionality as MS Word Pad  2) design a single person card game (e.g. Solitaire)  3) build a scientific calculator  4) Any  problem of your choosing with similar scale/complexity as the above three ",
        "Best answer": "1) Get it running.  Even if it does nothing but say hi. 2) Analyze.  Figure out what it needs, what would be nice, etc. 3) Have at it.  Add features one a time testing between each one. "
    },
    {
        "ID": "12915",
        "Question": "I am working as a PhD student developing scientific/engineering simulations and algorithms to be tested in these simulations. These days the first student started to work on my project (for his Bachelor thesis) and I am wondering: how should I organize the project now? I think I have some good C++ knowledge (although I still want to improve everyday!) and the code contains some design patterns, lots of templated classes etc. These techniques are new to the student and I wonder if it's a good idea to have him work directly in the trunk of the project. Do you have any experiences what happens if programming newbies and more experienced programmers are mixed? Does the code get messed up or do the newbies learn more by this? Is it wise to have a branch for the student to test his algorithms and maybe merge them into the trunk later? Should I first give him a book like The Pragmatic Programmer for reading (better suggestions?)? Thanks for every answer! PLEASE NOTE: I asked this question on Stackoverflow. The answer was about code reviews and I think this is a good way, but I was also advised that this site might be the more correct one, so I wanted to see if there are more things that might help or other opinions? ",
        "Best answer": "I'd suggest you create one or more branches for each student. Let them work with the code and mess it up, but don't merge it into the trunk until it is un-messed. If they can write more-or-less independent code, which is what your description sounds like, then to some extent you can just leave them to it and let them learn. If they need to modify core components then you'll need to be more hands-on with code reviews, testing and style requirements. Once you know how good a coder the student is then you can think about giving them commit rights to the trunk. It's difficult to apply industry principles (code review, style guidelines etc) to students because of the temporary nature of the work and the fact that research is the primary goal, not code writing. There is often some resistance to meeting industry standards because students often feel it is not worth their time writing unit tests and documentation: they've got a dissertation to write! That said, whatever tools you can set up to encourage best practice will help. I recommend Git for version control because it makes branching and merging very easy, and you can set up a continuous integration server like Hudson or Cruise Control to help encourage cleaner coding - the dashboards will show things like test coverage, style violations etc if set up properly. I'm not too familiar with static analysis tools for C++, but in Java you can use things like Checkstyle and Findbugs to encourage good style. There must be similar tools for C++. "
    },
    {
        "ID": "12929",
        "Question": "I have been going through Flask and it seems that now I have a decent understanding to go ahead and build a web application. However I want to know how would I approach the problem. For eg: I decide to build a blogging application. The first thing I do is write down all the things that come to my mind, from user registrations to posting data on the blog to publishing it on the site But after that I just get into coding. I know that is not the right approach. I have heard of UML diagrams, workflow diagrams and stuff. It would be great if someone could just outline the steps they would follow for building a large application and then I could go ahead and learn more about them.For eg,it could be something like the below:-  Collect requirements Draw UML diagrams Draw workflow diagrams Start coding  Please suggest an exhaustive list so that I can make my approach much more systematic. Thanks ",
        "Best answer": "Drop the diagrams for now, and only use them if/when you feel the need. Look into value-driven development as the overall approach and TDD for the specifics.  Begin by creating a bunch of stories. For instance, if creating a blog web site, the first step would probably be \"user should be able to write a simple blog entry\" followed by \"user should be able to display his blog entry\" etc.  Next, pick the most valuable story using this question as guideline: if you had to release today, what would be the single most valuable functionality? Then, break the selected story into small technical tasks such as \"Create a html form\", \"Write back-end for submits\" etc which can be completed in a few hours. Now:  Pick the next task Write a unit test for it Make the test pass Refactor code  Not satisfied? goto 2 Release it and/or goto 1  Make sure to use small incremental steps and keep your system working after each iteration so you don't get stuck in analysis paralysis. This is just scratching the surface of course, but it should get you going I think. "
    },
    {
        "ID": "12933",
        "Question": "I was wondering what is the best work rhythm for the job a programmer does? I am coding all day long and sometimes I get stuck in a problem and it keeps me occupied a few hours before I realize that maybe I need a break. Some say frequent and short brakes help you but sometimes when I am focused on a problem I feel like a break would not help, but rather loose my focus. So how often should a break be taken, and how long? The more basic question regarding this issue is comes from the fact that, you can get tons of \"good ideas\" ('promodo' for instance) on the net, that promise you will be more effective in whatever you do. Are these principles good or, this is something everybody should decide for himself? I wonder if any of them can accomplish what it promises! I mean what they promise is that (if the conditions are met) it works for everybody. Are there really such principles? And if there are, what are these and how can we find them? ",
        "Best answer": "The most important thing for good programming is good sleep. It does not matter what time you program at or for how long. Whether you drink caffeine loaded drink or munch on chocolate all the time. What matters is that you have a good long peaceful sleep every few days. As someone with a young child I can tell you for a fact that lack of deep rest saps your ability to be creative. There have been studies that have shown that artist that are the most creative require the most sleep and workers in non demanding repetitive jobs the least. "
    },
    {
        "ID": "12949",
        "Question": "If a piece of bespoke software was developed by a company and the Intellectual Property was retained by the company that wrote it, but now the client of the software company wants to get that source code (and its IP) how much should it cost them? How would you calculate a fair cost for the purchase of that source and IP? UPDATE: Just to add, the software in question is of no use to anyone else (for any legitimate purpose) as it ties in directly with the business processes of one company. It is not something that can be subsequently sub-licensed or installed outside the company in question. There are links of to third party services (but these were existing services that the bespoke software had to integrate with in the first place). ",
        "Best answer": "You saw there was value in retaining ownership in the IP and the source. So does your client. Are there royalties that would be eliminated by their purchase? Do they hope to deny competitors access to the IP? Are there other related markets that are served by the IP? The question is how do you value the market for the IP and source code? There are many sources on the web to help you determine their value for these properties. A paper by accounting and management consulting firm PWC outlines several possible methods:  Excess operating profits or premium profits method. Premium pricing method. Cost savings method. Royalty savings method. Market approach. Cost approach.  A guide by valuation specialists Valuation & Forensic Partners focuses on market, cost, and income valuation methods and warns,   Optimistic IP developers tend to   overestimate asset potential. So,   valuators generally view internal   projections skeptically, especially   when valuing unproven technology or   when management lacks industry   experience.  But another question to ask yourself is how well can you translate ownership of these properties into capital (in the form of money or some other asset) on your own (either marketing them yourself or licensing them) versus selling them outright? Edit: In response to your update, two questions that have direct bearing on how valuable the properties are:  Can you mine the source for generalized components that are useful elsewhere? Now that patents on business processes are deemed valid and have become a key factor in determining business valuation, is securing your IP crucial to patenting their business processes?  If none of these are valid, you might just have to settle for valuing your properties on a cost basis. "
    },
    {
        "ID": "13001",
        "Question": "Yesterday, a colleague asked me how to configure our VCS to timestamp the local copy of files pulled from the repository with the modification time in the repository.  I've always believed that they should be timestamped with the time they were pulled so that if you make a change, build, then back out by overwriting the changed file with the repository's copy, everything that depends on that file will build correctly. I thought about it overnight, but couldn't come up with a good reason to have timestamped with the repository modification time.  What would be the advantages of doing it that way? (Bonus question: repository modification time and local modification time are the only timestamping strategies that I'm aware of.  Are there others in use by VCS systems?) ",
        "Best answer": "See the tools in the contexts for which they were built.  Version control systems are primarily made to track content of source files.  They usually only track very little file metadata (executable bit, usually), because you don't need that for managing source code.  The timestamp of the file on disk needs to be changed whenever the file is actually changed on your local disk so that your local build tools behave correctly.  If you say you need to timestamp to be able to tell which file is which, well, the version control system is there to tell which file is which by means of the all the versions it controls. If what you actually need is  not a source code management tool but a deployment or backup tool, then you should use a tool made for that.  All the tools is that area, such as tar, rsync, rpm, dpkg, preserve file modification timestamps and other metadata, because that is necessary or desirable for their job. "
    },
    {
        "ID": "13006",
        "Question": "I've been interested in programming for a while now, and have been slowly, slowly, slowly working on things over the years. However, I don't feel like I've learned much. I've only really stuck with familiar languages (Java, C# and I've tried a little bit of PHP). The problem is that in those languages, I've only brushed upon the surface of them and have restricted myself to doing fairly simple things.  For example, as a Java project, I've downloaded the source code for some game and changed a few things to my liking, but nothing terribly difficult. In C#, I've made a few WinForm and console projects to do some repetitious or routine tasks for me, which is nice, but again nothing too complicated.  I've just barely begun to grasp the concept of classes, multithreading and some basic but essential tools that are required in modern object-oriented programming. (\"Why can't I just shove everything into one monster-sized class?\" \"Why can't I just run everything in one thread?\" are questions I used to wonder about) I've never made a large project by myself. Ultimately, my kind of \"dream\" hobby project is that I would like to create a game using DirectX technology. I say that because I've used XNA but it requires the user to install a framework to get it running, and I also feel that if I use the framework, that a lot of the back-end work is being obscured from me. (DirectX or OpenGL seem to be about as close to the machine as I can get without going overboard in complexity. DirectX is also installed on most [if not all] Windows machines that plan on running games.) But I haven't even begun to even think about starting that. The amount of things I need to know to undertake a task is absolutely overwhelming. I wouldn't know where to begin. Likewise, I also feel inadequate because I've restricted myself to [what I see as] easy languages. I feel like I should start learning some scripting language or learn this or learn that, which is quite daunting. -- tl;dr: I've been slowly teaching myself to program for a while now, but have been restricting my knowledge to limited languages and only scratching the surface of what I can do. I want to learn more and possibly start a decent-sized project (I can only make so many text-based RPGs and calculators), but the massive amount of knowledge I'll need is very intimidating. One of my concerns is that I know so little about programming languages in general. Is it better to learn several languages or to be able to delve deep into one single language? How can I motivate myself to learn more?  Am I thinking about this the wrong way? ",
        "Best answer": " One of my concerns is that I know so   little about programming languages in   general. Is it better to learn several   languages or to be able to delve deep   into one single language?  I don't know that I can expound on much of your question, but this particular line struck me as one that I have something to offer... While there's some merit in experimenting in a handful of different languages, what I've found (and it took me about 6 years to really see this) is that at its core, programming is programming, and languages are just syntax. The fundamentals are going to be the same across languages, and the longer you've programmed, the more you'll find that you can pick up a new language (at least to the point of basic proficiency) in almost no time. For that reason, I would say you'd be better served by diving deeper into one language and learning the craft than by trying to get a shallow understanding of several. As to how to motivate yourself to get deeper, your comment about \"another text-based RPG or calculator\" reminds me of this: the way I typically learn best is to have a project in mind. When I start out, I know that it's beyond my present capabilities, and I know that there are components in it that I have never done before and know that I don't even begin to know where to start on them. Because they are a defined part of the project, though, I have to do the research to learn. That way I'm not boxed into, \"Here are the tools I've already got. What new thing can I make with them?\", but rather, \"Here's what I want to make. How do I get there?\" "
    },
    {
        "ID": "13053",
        "Question": "Which was the first language with generic programming support, and what was the first major staticly typed language (widely used) with generics support.  Generics implement the concept of parameterized types to allow for multiple types. The term generic means \"pertaining to or appropriate to large groups of classes.\"  I have seen the following mentions of \"first\":   First-order parametric polymorphism is now a standard element of statically typed programming languages. Starting with System F [20,42] and functional programming lan- guages, the constructs have found their way into mainstream languages such as Java and C#. In these languages, first-order parametric polymorphism is usually called generics.  From \"Generics of a Higher Kind\", Adriaan Moors, Frank Piessens, and Martin Odersky  Generic programming is a style of computer programming in which algorithms are written in terms of to-be-specified-later types that are then instantiated when needed for specific types provided as parameters. This approach, pioneered by Ada in 1983  From Wikipedia Generic Programming ",
        "Best answer": "Before generics there were templates, and before that macro substitution. "
    },
    {
        "ID": "13061",
        "Question": "I was just thinking of something that would be really cool to have in my if-elif-else controls.  if condition:     stuff() elif condition:     otherstuff() then:     stuff_that_applies_to_both() else:     stuff_that_doesnt_aply_to_either()  So basically a then will be run when any of the conditions are run EXCEPT the else condition. Do you think this is useful? It's similar to the try-except-else of python. I think some of you are nitpicking a very preliminary implementation. The then block would be just like the else block in a try-except block in python. The real reason I suggest this is for situations like this.  m = {} if condition == '1':     m['condition'] = condition elif condition2 == '3':     m['condition2'] = condition2 elif condition3 == 'False':     m['condition3'] = True then:     run_test_that_relies_on_one_of_the_conditions_being_true()  return m  The then block is scoped to the first if just like the else is. So nesting works fine. And if you need to run a method before the if statements, that really has nothing to do with this use case. ",
        "Best answer": "I think it looks horrible. If you want code to run after a variety of conditions then either (a) recheck those conditions or (b) set a variable to indicated success status. "
    },
    {
        "ID": "13084",
        "Question": "Since a year I am a graduated Computer Engineer, and am now working in named field since 2 years. However, I'd like to get a Masters in a \"softer\" field and was thinking over the following choices:  MBA: Master of Business Administration, to bridge the business world with the world of computer science. Would give me an interesting job, travel potential, high salary etc. Techno-MBA: Sort of an MBA, but aimed at people who already have a degree in Computer Science. HCI: Human Computer Interaction degree, something I have been very interested in, but feel like it doesn't have the growth potential of MBA/Techno-MBA Marketing: My most recent brainchild is a degree in Marketing, is this at all a good idea given my background? Growth potential?  Main priority: Gaining a skill, and gaining a job allowing me to use my social skills. Which path has the best \"career potential\"? (Controversial I know) Any thoughts? Any feedback welcome. Edit: DOH. Realized that the first sentence makes no sense. Was running my own semi-successful business during last year of school. ",
        "Best answer": "Answer this question: \"What do I want to be when I grow up?\" If you can honestly answer that question, your choice will be made for you.  Any job is only as good as long as something about it makes you happy, and that's not always (and rarely primarily) money. "
    },
    {
        "ID": "13091",
        "Question": "Which of these is better for maintainability? if (byteArrayVariable != null)     if (byteArrayVariable .Length != 0)          //Do something with byteArrayVariable   OR if ((byteArrayVariable != null) && (byteArrayVariable.Length != 0))   //Do something with byteArrayVariable   I prefer reading and writing the second, but I recall reading in code complete that doing things like that is bad for maintainability.   This is because you are relying on the language to not evaluate the second part of the if if the first part is false and not all languages do that.  (The second part will throw an exception if evaluated with a null byteArrayVariable.) I don't know if that is really something to worry about or not, and I would like general feedback on the question. Thanks. ",
        "Best answer": "I think the second form is fine, and also more clearly represents what you're trying to do. You say that...  I recall reading in code complete that doing things like that is bad for maintainability. This is because you are relying on the language to not evaluate the second part of the if if the first part is false and not all languages do that.  It doesn't matter if all languages do that. You're writing in one particular language, so it only matters if that language does that. Otherwise, you're essentially saying that you shouldn't use the features of a particular language because other languages might not support those features, and that's just silly. "
    },
    {
        "ID": "13121",
        "Question": "At a meta level, I often find that when I grow a codebase organically, parts of the system that I eventually find need to know about each other (at least through some interface) have been mercilessly decoupled.  This often occurs in research prototype code, where one tends to frequently think of tweaks and small improvements that weren't planned for in the original codebase. Usually, some setting or piece of data needs to be plumbed through about 15 zillion layers of function calls in ways that I never anticipated when I designed the code.  This leads to a very ugly dilemma:  Use an ugly hack.  This can be yet another global variable, bolting the data onto a class that it clearly doesn't belong in, putting a bunch of print statements in the middle of code that previously had nothing to do with I/O logic, doing a seemingly straightforward task in a very roundabout way to make it fit in my original codebase, etc. Shotgun surgery refactoring.  This almost never gets done in practice because the code will likely have to be rewritten anyhow if it is to become production quality, was hard to get working the first time, is hard to test, or I just plain have higher priorities than writing clean, maintainable code when either I have a deadline or I know it's just a prototype for now.  Furthermore, the next tweak may break whatever nice abstractions I come up with.  Are there any good meta-tips or design principles that will help me avoid situations like this in the first place?  I am NOT looking for answers that simply recommend refactoring after the fact, as a nightmarish shotgun surgery refactoring session is exactly the kind of thing I'm trying to avoid. ",
        "Best answer": "Three things that I find useful, but some people dislike: Consider early prototype code - if you are doing something very different than what you have done in the past the first go of it is likely to teach you that you should have done it differently early on - work out your ideas then start fresh Minimize early decoupling - to me it is far easier to refactor something out of a coupled scenario once I have good reason than to juggle the bits across an arbitrary early decoupling border. Consider having fewer tiers or allowing functions to skip tiers.  In some projects the latter can approach a necessity at times. "
    },
    {
        "ID": "13142",
        "Question": "I'm the odd kind of developer that likes writing and explaining - emails, specs, you name it.  I enjoy helping people understand things deeply. I hate ping-pong communications where groups go through 30 or 40 emails, phone calls and meetings because 25% of the group misunderstood the previous dashed-off email in which the writer only half-explained their position, or was in a hurry and miswrote a critical word or left out a negative or two. This trait is great for documentation, but my emails are long, so people simply don't read them. A couple strategies I've used are formatting long emails as attached Word documents to encourage people to view them as documentation (works surprisingly well sometimes), or trying to use more bullet points instead of free paragraphs. These strategies increase readership, but the content is still long. Of course I want to yell and scream that complex technical topics require a lot of exposition and clarification, but that doesn't help anyone. Obviously there are tradeoffs that can be made, but I have a hard time justifying making statements that aren't clear or that will result in a million questions, misunderstandings, or added \"terms and conditions\" later on. How do you keep your communications short but complete? ",
        "Best answer": "Executive Summary: Put the important facts at the top then expand below.  Details: When I have a long technical email that I fear will lose half the audience, I make an effort to put a short, accurate \"executive summary\" at the top, and include the complete technical details below.   Selective use of bold or underlining also helps.   "
    },
    {
        "ID": "13143",
        "Question": "2 years ago I starded learning AS3 in order to get hired at a web design company. A few months later, I ditched it, because I discovered that I'm better at design; so now I'm a web designer. Recently, I decided that knowing JavaScript would be nice and helpful, so i started with Essential JavaScript Training from Lynda.com. Untill today, I was under the impression that my knowledge of programming was somewhat basic. What happened today...? I learned (through trial and error) that variables are only valid in the functions that they're defined in. This turned my world upside down. I was like: \"that makes sense now...\" thinkin of the all the headaches that the lack of this specific piece of knowledge gave me. How could I have missed such a fundamental thing while learning AS3 (and now JS)? I didn't. And I remember reading a at least three books on AS3, but none of them taught me about programming. So what I want to ask you, is if you could recommend me a good JavaScript book from which i can learn programming, too, besides syntax. Thanks! And sorry for my bad english :) ",
        "Best answer": "As the author of Lynda.com's JavaScript Essential Training, I feel like I kind of have to jump in here… IIRC, I covered variables and global versus local scope in Example 4 of Lesson 4 (\"Creating prompts for users\") when I first talked about var. If I wasn't clear enough, I apologize! If you're looking for a solid beginner-level JavaScript book, may I suggest JavaScript & Ajax for the Web: Visual QuickStart Guide, 7th edition? Yes, I'm the co-author (with Tom Negrino), but I've received a lot of compliments on it from people who had never previously written code—they said that other books just confused them, and ours was the first that didn't assume they were already programmers. Here's what it says on page 36:  Using var does two things:  It tells JavaScript to create a variable (that is, to set aside some space in memory for this new object). It defines the scope of the variable, that is, where JavaScript needs to know about this particular object (see the What Is Scope? sidebar). If a variable is created inside a function, other functions don’t have access to it, as it’s local to that function. If it’s created outside any function, it’s global, and everything has access to it. In this script, we’re creating the ans global variable.   You said your previous books were on AS, so hopefully JS&A:VQS wasn't one of the books you had trouble with… And if you have any further questions about particular bits of JavaScript code, StackOverflow is probably the place for them.  Edited to add… Just my 2¢: I suggest that you—for now—ignore the Douglas Crockford recommendations. Yes, he does know his stuff (although I disagree with some of what he says, and I suspect he disagrees with 90% of how/what I teach) -- but I cannot honestly recommend his work for someone who doesn't have solid programming experience. His stuff is for intermediate/advanced-level scripters, not for someone at your current stage. "
    },
    {
        "ID": "13190",
        "Question": "One of our junior developers has been assigned a new client (we don't have the client yet, we're still working with him to see if we can meet his needs) and the junior developer said the client will hire us if we can do the work on his project without getting access to his server. I've had a direct conversation with the client who turned out to have had his code stolen before by some offshore company that he outsourced. This made me more sympathetic but I still have mixed feelings about this.  On one hand I want to prove to the client that we're not all bad apples. Also if we do a good job with him, we get a loyal client who'll hire us for all his projects. I haven't heard of this happen before but I guess it happens more often than we'd all like to admit.  On the other hand I'm hesitant to accept working with him because deployment time is going to be a nightmare and no where in my career or education has anyone taught me how to work with clients like him. I (or the junior developer) would have to write a detailed description of exactly what to do with the source to deploy it and that is an annoying burden when I could deploy and test the whole thing in an hour myself.  As I said, I've never had to deal with this before (we're signing a non-disclosure but apparently so did the offshore company before us). We're not fully fully booked so it's not like I have an immediate replacement, but we're not begging for work either and I wonder if working under such restricted environment is worth the trouble.  Another side is that the experience itself could be rewarding for us, but is it experience worth having, as in what's even the likelihood of getting a similar client anytime soon. Are we even expected to comply with such clients? So since I don't have any first hand experience with this and it definitely wasn't covered in school, how would those with longer experience working with clients deal with a distrusting client like this? Would you even accept the job? ",
        "Best answer": "Work with the client, and add hours of extra (billable) time to your quotation for every task, to cover the hassles of deployment without server access. It's depressing to be limited like that due to (unfounded) trust issues, but really, it shouldn't be that burdensome. I've worked with a number of clients where we had to work this way, not due to them not trusting us, but simply because they were huge companies with blanket IT policies. It just means you need to be more disciplined about your deployments so you're not deploying, fixing a tiny bug and deploying again, realizing you forgot a file and deploying again, etc. etc. "
    },
    {
        "ID": "13207",
        "Question": "I'm wondering why we don't have some string classes that represent a string of Unicode grapheme clusters instead of code points or characters.  It seems to me that in most applications it would be easier for programmers to access components of a grapheme when necessary than to have to organize them from code points, which appears necessary even if only to avoid casually breaking a string in \"mid-grapheme\" (at least in theory).  Internally a string class might use a variable length encoding such as UTF-8, UTF-16, or in this context even UTF-32 is variable length; or implement subclasses for all of them (and optionally configure the choice at run-time so that different languages could use their optimal encodings).  But if programmers could \"see\" grapheme units when inspecting a string, wouldn't string handling code in general be closer to achieving correctness, and without much extra complexity? ",
        "Best answer": "Seems like the best way to get correctness is to keep programmers from doing \"string hacking\" ... it just isn't OK to write your own word wrap, hyphenation, word count, justification, cursor movement, etc. routines. All the modern UI frameworks will do this stuff for you these days. That is, the abstraction you'd usually work with is more of a \"paragraph display object,\" such as for GTK: http://library.gnome.org/devel/pango/stable/pango-Layout-Objects.html rather than a grapheme string, such as: http://library.gnome.org/devel/pango/stable/pango-Glyph-Storage.html To get to a string of glyphs you need info that's only available at the \"view\" level, so most uses of strings might not have this info. For example, you have to know the font, because fonts can have different ligatures. Aside from that kind of practical matter, glyphs probably aren't what you want. In many contexts, you want to use the proper Unicode attributes, shown in this API for example: http://library.gnome.org/devel/pango/stable/pango-Text-Processing.html#PangoLogAttr As you can see from that struct (which reflects the Unicode algorithms) doing various things at glyph boundaries is not any more correct than doing them at character boundaries. These two specs describe the algorithms to find different kinds of boundaries:  http://www.unicode.org/reports/tr14/ http://www.unicode.org/reports/tr29/  Doing text processing involves finding those boundaries with the algorithms and then working with the boundaries. If you start digging in on just how hard it is to handle all languages correctly, you'll very quickly realize you need a library that looks at whole paragraphs and handles them properly. Windows, Mac, Linux (Qt and GTK), and Java all come with facilities for this, plus there's http://site.icu-project.org/ for example. When writing web apps, unfortunately you pretty much have to let the browser (probably helped by the OS) do this stuff, as far as I know. All you can do in JavaScript or on the server side is mess it up. Maybe I'd sum up the answer as: most string manipulation on natural language text is broken, so not much point worrying about the string class, other than maybe to have one with no methods on it ;-) "
    },
    {
        "ID": "13265",
        "Question": "At my current workplace, we don't have any testers, the rationale for that from the management being: \"if we had testers, you wouldn't test your own code at all\". This kind of thinking seems to be detrimental to product quality, as while I do test my own code, there are a lot of things that I will miss just for the fact that I know the system inside out and don't know how to use it \"wrong\". Black box testing doesn't really work as I subconsciously avoid the pitfalls that a dedicated tester would fall into. A lot of my time goes into fixing bugs that have slid into production code and found by the end user. The system in question is large but is developed solely by me. This has also caused some managemental duties to fall on my lap, such as defining schedules and working on specifications. Should these kind of tasks be my responsibility? I see myself strictly as a programmer and nothing else. And if these are my responsibility, to what extent? When is a project so large that it requires testers? Should a programmer have to refine the specification, worry about management of the project or even provide customer support? Note Some might have got the impression that I am against widening my responsibilities – that is not the case, I'm eager to get a role that involves more management duties, but currently it is not in my job description. Until I'm officially employed as such or the additional duties start showing in my paycheck, I'm going to think of myself as 'just' a programmer. Unfortunately, as a junior developer, shifting to managerial duties is not going to happen very soon. Excellent answers so far, keep them coming if you have something to add or personal experiences to share! ",
        "Best answer": "You do have testers.  Only, you call them \"end users.\"  This is detrimental for all the reasons you describe; no matter how conscientious a coder you are, you're simply never going to be able to do a good enough job overcoming your own preconceptions about how the code is \"supposed\" to be used for you to find all the ways it can screw up. You need to re-open this issue with management.  By this point, it sounds like you have some hard data to back your case; the current hands-off approach to Quality Assurance both wastes your time and compromises your users' experience.  You need to be careful in how you present this so that it's clear this is a structural problem and not a case of \"You just suck at testing,\" but it sounds like a discussion that needs to happen. It sounds like you're coming to a crossroads with this employer.  If you're intent on remaining a programmer and nothing else, you may need to start pushing back and requesting that they start getting you the help you need to take some of the managerial tasks off your plate, either by bringing in somebody new or by expanding an existing co-worker's responsibilities.  (\"This isn't what you hired me for, and these tasks aren't going away.  Time I spend doing this stuff badly is time I'm not spending on what I'm good at.\")  But that may or may not be realistic.  Do you think you could handle moving into to a more managerial role if they gave you the resources and authority you'd need to get the job done right? As to how big does a project need to be before it needs testers, I'm not sure how to precisely define that line, but I definitely think you've crossed it.  You're spending more time than you'd like fixing bug reports coming in from actual users; to me that says it's time to spend more effort stopping the bugs from getting to the users in the first place. "
    },
    {
        "ID": "13341",
        "Question": "I have a problem: I suck at tracking time-on-task for specific feature/defects/etc while coding them.  I tend to jump between tasks a fair bit (partly due to the inherit juggling required by professional software development, partly due to my personal tendancy to focus on the code itself and not the business process around code). My personal preference is for a hard-copy system.  Even with gabillions of pixels of real-estate on-screen I find it terribly distracting to keep a tracking window convienient; either I forget about it or it gets in my ways. So, looking for suggestions on time-tracking.  My only requirement is a simple system to track start/stop times per task.  I've considered going as far as buying a time-clock and giving each ticket a dedicated time-card.  When I start working on it, punch-in; when done working, punch-out. ",
        "Best answer": "This may sound naive, overly simpleton even, but you can buy a chess clock and measure your spend time on two tasks with it. Know a few people who do that ... however, they are also chess players, so ... ;-) Looks nice on the table however (and most people will stop and ask why do you need two clocks and in what timezone is the second one).  "
    },
    {
        "ID": "13371",
        "Question": "We are currently developing a .net app in WPF, that will be a desktop app with a good bit of data-access to local (and some remote) servers. The business logic here is very complex, and currently it is built into the code along with the data access and whatnot (being WPF the UI is separated pretty nicely). Now, I have been tasked with reorganizing this system to make it more maintainable in the long run as certain logic changes. I know that workflows are supposed to separate the logic out pretty nicely and allow you to visualize it a bit. Is workflow a good choice for an app that does not need to be run over long periods? (since workflows can wait in idle for months) What has been your experience with the platform? Is it slow for dealing with a mid-sized company; around 150 simultaneous users? If you liked it, do you know of any good online tutorials (preferably not MSDN) that can give me a taste for the platform and do some testing before I commit to implementing it on a huge app? ",
        "Best answer": "I have a small amount of experience with WF - creating custom TFS workflows and activities. I found it generally painful and slow to work with - opening a workflow takes a long time (on a WEI 5.9 machine), it often hangs VS.NET, is missing some core features and is marginally buggy.  If you have anything more than a simple workflow you soon run out of screen real estate. I have a 24\" in portrait orientation (1200x1920) and often found myself wanting more vertical room. Overall I found it pretty frustrating and wouldn't recommend it. It didn't seem to allow me to do anything that I couldn't have done more easily in code. "
    },
    {
        "ID": "13391",
        "Question": "Say you have 5 customers, you develop 2 or 3 different projects for each. Each project has Xi tasks.  Each project takes from 2 to 10 man weeks. Given that there are few resources, it is desired to minimize the management overhead. Two questions in this scenario:  What tools would you use to prioritize the tasks and track their completion, while tending to minimize the overhead?  What criteria would you take into consideration to determine which task to assign to the next available resource given that the primary objective is to increase throughput (more projects finished per time unit, this objective conflicts with starting one project and finishing it and then moving on to the next)?  Ideas, management techniques, algorithms are welcome ",
        "Best answer": "Sounds like the company has taken on more work than it can handle and it's trying to dig itself out of a hole. Give your salespeople a vacation, pick the projects that have the higest profit ratios, finish them and postpone the rest until they can be worked on. Or get more people to work on the projects that you can't handle (it's not a mythical man-month scenario if there is no-one to work on them). "
    },
    {
        "ID": "13396",
        "Question": "I'm drowning in user emails and I'd like to implement a better way to manage all these requests I get and put them in a queue where those people on a team, as well as users, have access to them and can make common notes. I'm thinking about some sort of task management tool that would allow multiple tasks to be created under a project where emails, comments, ideas, etc. could be dropped/entered and easily accessible. I need something that all parties can be involved in - users, managers, team leaders, developers. I'm looking for a tool that can allow:  Users to just drag/drop an email to submit a request for maintenance or enhancement.  Developers to just see their queue and the weighted priority of each task/project.  A team of developers to see what everyone is working on in real-time.  Management to keep a a log of time spent on each task.  I I am starting to look in more of a Agile/Scrum direction for solving this problem. I found list of scrum agile sofware project management open source tools. Since I am limited on time, has anyone used these? Which one should I test to see if it will meet my needs? TeamPulse is a good direction, but think it is a little too bloated. I need something simple for all parties. ",
        "Best answer": "Redmine  ...project management web application. Written using the Ruby on Rails framework, it is cross-platform and cross-database. Redmine is open source and released under the terms of the GNU General Public License v2 (GPL)... Some of the main features of Redmine are:  Multiple projects support Flexible role based access control Flexible issue tracking system Gantt chart and calendar News, documents & files management Feeds & email notifications Per project wiki Per project forums Time tracking Custom fields for issues, time-entries, projects and users SCM integration (SVN, CVS, Git, Mercurial, Bazaar and Darcs) Issue creation via email Multiple LDAP authentication support User self-registration support Multilanguage support Multiple databases support...   "
    },
    {
        "ID": "13397",
        "Question": "Tell me if this sounds familiar:   something something something... as seen in figure 1-1 on the next page...  It's in practically every book I've ever read about programming. So when I was writing a small instructional booklet on how to use some in-house software, which had images of the screen that the user will be on at this certain step I wrote something like \"The screen that pops up looks like the one shown in figure 1-1 below.\" But I'm thinking: I'm used to that style of writing, but if my target audience is the 'average person' are they going to be confused? So, more generally, are there any common practices in technical books that should be avoided when writing documentation for average people? ",
        "Best answer": "If you want a better answer to your question and a real awakening as to how well you write instructional manuals, hand the lady at the desk your instructions and watch over her shoulder as she tries to follow. Don't interject; pretend your not there. Ask her to think outloud. Take lots of notes. Prepared to be shocked. Go back and rewrite the instructions with all the feedback you received.  At least with the romance novels, you know who's banging who. Our instructions are rarely that clear. "
    },
    {
        "ID": "13443",
        "Question": "A few weeks ago, my class was assigned to translate to Portuguese the book Real World Haskell. As I did the translation of the text and comments, I started to wonder if I should translate the code as well, as the instructor suggested. For example: data BookInfo = Book Int String [String]               deriving Show would become data InfoLivro = Livro Int String [String]                deriving Show Since I haven't read any software-related books in Portuguese, I don't know if that's a common practice, neither if it should be done this way. In the end, the code is a language mix (perhaps the example in Haskell is not a good one, since you can create synonyms quickly like type CadeiaDeCaracteres = String, but you get the point). So it doesn't really matter how hard you try, you'll have to rely on the reader previous experience with some sort of basic English words.  Knowing this, I really don't see the point in translating code, since we learn in the early days of our coding life it should be written in the universal language. Nevertheless, if the surrounding text (comments, for example, and text itself in a book) needs to be translated, what is possible and feasible in this matter? Can you provide me with some guidance of what to do? ",
        "Best answer": "In my opinion, no. Most programmers are forced whether they want it or not, to learn english, and nowadays it is the lingua franca of the software world.  In my experience, however, books written by authors in one of the stronger languages, have a tendency to do that - so far I've seen the practice in books in german, french and russian (well, russian is a little different of a case, since they use different alphabet as well). It is a rule of thumb that it depends on the country's dependency on english. The text is of course translated, the bigger comments also (one that describe the workings of a program in greater detail), then somewhat more rare, smaller comments and variable names. I've also seen the case where variable names were translated, but smaller comments were left in english. I believe the author however, in that particular case, was counting on his audience to have a working english knowledge. Sorry if this didn't solve your dilemma, but there still isn't a consensus on this one :-/ "
    },
    {
        "ID": "13470",
        "Question": "I'm thinking about leaving my current employer and wondering how one goes about that process in the programming world. We have a lot of projects on the go at the moment and I'm the only developer. We have 4-5 projects that are fairly big and need to be done in the next few months and even a few longer than that. I'm leaving because I'm the only employee and that's doing me no good. I'm young and want to learn, so a team would be nice. It's also too much work and the company is managed horribly.  I have no contract to worry about so I could theoretically quit and just not come back without notice. Just wondering how this is normally handled.   Should I write a resignation letter How much notice should I give Should I give a reason for leaving Should I go to my boss who is the main reason I'm leaving or go to his boss?  Overview of replies From the feedback here, it looks as though it's best to give 2-4 weeks of notice, and present a written resignation. Don't go into detail explaining why you're leaving in most cases. Don't burn bridges. Be professional. ",
        "Best answer": "Assuming that you've made your decision to leave, you should put it in writing. Whether this is an actual letter, an e-mail or a form you fill out will depend on the company and culture, but it should be written down and not a phone call, text message or even just face to face. If you do one of the latter things it's only polite to follow it up in writing. The amount of notice should be in your contract - assuming you have one. Even if you didn't sign the contract you should abide by its terms. By working and getting paid you and the company are working to that contract even if it's not \"official\". If nothing else you'll be seen to be doing \"the right thing\" and it will be harder for your employer to get you to work longer. If they want you to leave straight away you still should get paid as though you were working. You don't need to give any reasons for your decision. You should leave all files etc. you've worked on so that they are accessible to your manager, co-workers and anyone who follows you. A short document explaining what's what would be polite. Don't delete anything. The files/data aren't yours they are your employers. Once you've made your leaving official you should then talk to managers, co-workers etc. about how you can handle the hand over of information. "
    },
    {
        "ID": "13540",
        "Question": "There are many masters degree programs in computer science online offered by reputable organizations. However, I've found very few bachelors only degree programs offered by non for-profit institutions. I am looking for a degree in computer science and also any programs for entrepreneurship/management or algorithmic trading. I've found only 2 programs so far that fit: http://www.regis.edu/regis.asp?sctn=cpcis&p1=ap&p2=cs&p3=ol http://depaul.edu Requirements: 100% online undergraduate non-profit Options: Bachelor of Science Bachelor of Arts Bonuses: accelerated recognized ABET cost effective Update: I've answered my question with a list. If anyone knows an online program that combines skills in entrepreneurship (getting capital, starting a business) or algorithmic trading with a computer program that would be ideal.. maybe too much to ask though  ",
        "Best answer": "I ran across TUI International Online University which I thought looked promising. It's based in California and accredited. The University of California will test a fully-online program, but as the article says, \"it's unclear when students could enroll.\" Other possibly useful resources for you:  Online Degree Reviews Along with seeing what students think of their schools, you can also search for schools and programs   California Virtual Campus Catalog Lets you search for online programs by degree and subject.  "
    },
    {
        "ID": "13570",
        "Question": "I've inherited a project with a fairly large codebase, and the original developer rarely, if ever, replies to emails. There's a ton of different ways to do some things in it, and I don't know all of them. A lot of duplicated code along these paths (rather than functions included by, say, 5 pages that do relatively the same thing, it's code copied across 5 pages), and some subtle issues in the database (we've all heard of spaghetti code, but have you ever heard of a spaghetti database?) All of this I can deal with most of the time no problem. The issue is when a client finds a bug somewhere. They'll usually send a screenshot of the ending issue, and say, \"Could you take a look at this?\" while highlighting the specific thing on the page that's wrong, and sometimes what was expected. Very little more information is given, and trying to talk to them and get more (such as what they did to get the result) is like pulling teeth. Basically, it boils down to this:  Large and complex code base I'm not 100% familiar with Many many ways things can go wrong Very little information on how a bug came to be  Does anybody have any tips, tricks, suggestions, etc. on how to debug this sort of thing? ",
        "Best answer": "A good start might be this book.  I am using the definition below as it sounds like the developer isn't around to support it anymore.  Legacy code is source code that relates to a no-longer supported.  "
    },
    {
        "ID": "13623",
        "Question": "Suppose I give my developers a screaming fast machine.  WPF-based VS2010 loads very quickly.  The developer then creates a WPF or WPF/e application that runs fine on his box, but much slower in the real world. This question has two parts... 1) If I give a developer a slower machine, does that mean that the resulting code may be faster or more efficient? 2) What can I do to give my developers a fast IDE experience, while giving 'typical' runtime experiences? Update: For the record, I'm preparing my even-handed response to management. This isn't my idea, and you folks are helping me correct the misguided requests of my client.  Thanks for giving me more ammunition, and references to where and when to approach this.  I've +1'ed valid use cases such as: - specific server side programming optimizations - test labs - the possibly buying a better server instead of top of the line graphics cards ",
        "Best answer": "The answer is (I'll be bold and say) always  NO. Develop on the best you can get with your budget, and test on the min-max spec range of equipment you'll deploy to. There's emulators, virtual machines, actual machines with testers that can all test performance to see if it's a factor. "
    },
    {
        "ID": "13691",
        "Question": "I am very interested in Python for programming on the JVM, and I've worked in Java and Ruby (not JRuby) a lot, and to a certain extent in Groovy. What are the limits to the Java-Jython and Java-JRuby integration right now? What about Groovy: are there any things that a Java class can do that a Groovy class cannot? And what about IDE integration (mostly Eclipse, but IntelliJ and Netbeans are also interesting)? I'm not interested in whether Python, Ruby or Groovy is better/more-concise/whatever. I'm just interested in how well they are working in the JVM and tools (especially IDEs) right now. My main interest is not inside a web server, if that matters. Simple examples of getting a HelloWorld which extends a custom Java class and implements a custom Java interface would be very useful to me.  ",
        "Best answer": "I have no experience of JRuby nor Groovy. But Jython:  Excellent integration with NetBeans. NetBeans can run Python programs with Jython almost straight out of the box, just install the \"Jython Distribution\" plugin and you're done. Apparently Eclipse works as well. See chapter Using Jython in an IDE in the Jython Book. Java-Jython integration is excellent. Calling Java from Jython is super easy. Calling Jython from Java is not quite as straightforward (Java was not designed with Jython in mind), but still fairly easy. I've mostly had luck with creating a class in Jython (extending a Java class/interface), and then creating instances of that class in Java using an object factory. See Jython and Java Integration for how-to.  Extending a Java class in Jython works like this: from javax.swing import JFrame, JButton  class MyFrame(JFrame):      def __init__(self):         self.defaultCloseOperation = JFrame.EXIT_ON_CLOSE         self.size = (300, 300)         self.add(JButton('Click Me!', actionPerformed=self.print_something))         self.visible = True      def print_something(self, event):         print 'Clicked!'  Implementing interfaces works similarly, just import the interface, \"extend\" it using a class definition such as class MyClass(MyInterface) and implement what's needed.  My only criticism against Jython is that its raw performance is not very good (and that's mostly because it uses massive reflection to interact with Java). But then, raw performance is usually pretty irrelevant for a scripting language. "
    },
    {
        "ID": "13711",
        "Question": "We're dealing with an interesting problem on StackOverflow. We've got a whole bunch of little \"needs to be done soon-ish\" tasks.  An example is updating \"Related Questions\" lists.  What we've done in the past is to piggy-back those tasks onto some users' page loads. This was never ideal, but it wasn't really noticeable.  Now that SO has passed the 1,000,000 question mark, those unlucky users are starting to feel it. The natural solution is to actually push these tasks into the background.  There are two broad ways of doing this I'm considering. 1. In IIS as a custom Thread-Pool/Work-Queue Basically, we spin up a few (non-ThreadPool, so as to not interfere with IIS) threads and have them services some collections we're shoving Funcs into. The big pro here is simplicity.  We don't have to worry about marshaling anything, nor do we have to make sure some external service is up and responding. We also get access to all of our common code. The con is, well, that we shouldn't use background threads.  The objections I know of are all centered around starving IIS (if you use ThreadPool) and the threads dieing randomly (due to AppPool recycling). We've got existing infrastructure to make the random thread death a non-issue (its possible to detect a task has been abandoned, basically), and limiting the number of threads (and using non-ThreadPool threads) isn't difficult either.  Am I missing any other objections to in IIS process thread-pooling/work-queues?  Moved to StackOverflow, as it wasn't really addressed here. 2. As a Service Either some third-party solution, or a custom one. Basically, we'd marshal a task across the process boundary to some service and just forget about it.  Presumably we're linking some code in, or restricted to raw SQL + a connection string. The pro is that its the \"right way\" to do this. The cons are that we're either very restricted in what we can do, or we're going to have to work out some system for keeping this service in sync with our code base.  We'll also need to hook all of our monitoring and error logging up somehow, which we get for free with the \"In IIS\" option. Are there any other benefits or problems with the service approach? In a nutshell, are there unforseen and insurmountable problems that make approach #1 unworkable and if so are there any good third-party services we should look into for approach #2? ",
        "Best answer": "There is actually a third way in Windows to run background services, and it is very common in the UNIX world.  The third way is a CRON job that runs a piece of your infrastructure.  In Windows this is known as the task scheduler and is very common for running code on a scheduled basis.  To use this you would create a command-line app that is executed on a pre defined schedule.  The advantage of this is that you don't have to worry if the process stays up and running like a service, because if it fails for some reason, it will just start up next time. As for marshaling specific tasks, you really just need to store these tasks in a persistent binary storage.  Until the command line app picks them out of the storage and executes them.  I have done this in the past using the Cassandra database as a Session State Provider for stuffing background tasks for specific users in the Cassandra database, and then having the commandline pick them out and execute them for the user. This may not have been the typical marshaling solution, but it worked out very well for me and it turned out to be a very elegant solution, because the scheduled tasks survived shutdowns, network problems, and any machine could execute the task since it was centrally stored. Shameless promotion, but this is my project and the solution I just briefly detailed is why I created the project: http://github.com/managedfusion/fluentcassandra/ "
    },
    {
        "ID": "13779",
        "Question": "I'm not sure if there's an established protocol for this (even if it's not an official one), but thought those most experienced with open source might want to share with us.  I'm aware that random patches submitted to open source projects are never paid. They may be indirectly funded by a client but they're never paid for by the open source project itself.  But how about core developers? I heard for example that drupal has some 800 core developers behind it. Core developers means that they work on drupal core itself and together they push the main releases, so they're very important to the project. Of course drupal is just an example, but in general, is there any established protocol in the open source world that defines whether the company behind the project is expected to pay them and do these core developers expect such payment?  Any facts or first hand experiences? ",
        "Best answer": "As a particular example, a few of the core Squeak developers work for Teleplace. They hack on Squeak as part of their day job, so Teleplace gets the stuff they need, and then release the changes to the base Squeak image and virtual machine back to the larger Squeak community. There are quite a few other companies that pay their employees, in part at least, to hack on Squeak. "
    },
    {
        "ID": "13870",
        "Question": "A bit of background: I've got a small open source project that I've started on, a basic framework which provides an object oriented means of generating HTML code (since I don't really like HTML, and I do like PHP). It has some released source, and a few downloads, but primarily, the project is for me, with the Open Source portion just being a side benefit.  The original project which caused me to be able to develop on this project has mostly gone into hibernation for the time being, which means that all the development I get to sink into it at this point is personal time only. Unfortunately, I'm currently working toward my bachelor's degree, studying for certifications, and I have a three month old baby at home. In short, by the time I get around to \"me time\", I rarely feel like doing work, but rather usually feel like just chilling out. So, if there's anyone else out there who feels like they're in a similar position, what strategies have you used to keep yourself motivated toward working on the project? I would really like to at least be able to work on this until I have 100% spec coverage, but I haven't committed source in months. Anyone out there who can help? ",
        "Best answer": "If you are asking yourself how to find time working on your open source project, this may indicates that you are willing to work on too much things at the same time. You may not be able to physically handle everything and the consequence is that you will be bad in most things you will work on because of the lack of time and focus. Think about your baby! And your bachelor degree! It's already 2 FULL TIME projects believe me! Unless your open source project is more important than your baby or your bachelor degree, abandon your open source project for now. "
    },
    {
        "ID": "13940",
        "Question": "I've been working on a .NET personal project for a little while now and it's almost ready to go open source.  I've decided (arbitrarily) to host it on codeplex. Codeplex offers TFS or mercurial. I'm wondering which I should pick. Consider:  I've only ever used subversion. I'm using VS 2010 express as my IDE. Tools must be free (so the mercurial client if I go that route). From what I've been hearing, mercurial sounds interesting but I know very little about it so if there's a learning curve, then I don't want to add too many more learning objectives to the project. I don't expect any contributors.  So I guess the actual question is, is mercurial easy enough to use with codeplex and does it add anything that the TFS option doesn't? ",
        "Best answer": "Mercurial I like Mercurial provided you install TortoiseHG and VisualHG tools. The best feature is that you can create a \"branch\" to try out some funky new code by just copying your enlistment directory in Explorer.  This copy will sync back to Codeplex without changing any settings if all goes well.  If it goes poorly, delete the directory and 'all is forgiven'. "
    },
    {
        "ID": "13945",
        "Question": "Some time ago I decided to outsource web development projects (php coding & design). I've opened small office abroad with manager and couple of programmers and designer. I just started but already have a problem controlling their work. Some programmers are fast, others work much slower. I pay them fixed salary, so it matters for me if they do their job on time. When they work from office, I at least have a person responsible for the project to tell if they really worked. But sometimes they work from home and in that case I have no way to know if they really spent that time working on some issue. Some of my ideas were: controlling amount of code done with svn, strict deadlines, everyday reporting.. But nothing seems perfect to me. It creates even more work from my side. Can anyone suggest a way of fair judgement on how much really time was spent on actual work? How to stimulate people to work? Maybe create some bonus system if work is done fast? Any idea/experience on this topic would be highly appreciated. ",
        "Best answer": "Your only measure should be achievement. Is the work done? No? Get another developer. It is so easy I still don't understand why people try to control other people (impossible to do) while what you should do is stop working with them. Whether they are outsourced or offshore, the problem is the same. The only difference is the location and the fact that it's harder to try to control someone that is 4000km away.  "
    },
    {
        "ID": "13958",
        "Question": "I am a Software Engineer and over the past few years I have become the de-facto software project manager simply because there isn't one. So to keep our sanity in the R&D/Engineering department, customers have become accustomed to coming to me with their requests. I have no experience in this realm so it is my first time acting as a project manager for software projects. I have managed other things but not software. So, how do you manage software projects and mark priorities? Requests come in at infrequent intervals so we very well could be working on something for someone else and then another person comes in with a \"rush\" job that needs working on. Is it easier to just say First Come, First Serve or is it the person with the most money? ",
        "Best answer": "I've found that the more a customer complains about how urgent their request is, unless they are also a developer in their own right, it's usually a good sign that the request isn't urgent at all.  One of my professors in college always used to tell us not to let the urgent interrupt the important. I usually classify requests in this order (YMMV):  Issues related to a recent upgrade or migration (most important).  Security fixes.  Broken functionality of the existing system.  Broken functionality in RC and beta features.  Paid feature requests.  R&D feature requests from a large part of the user base.  R&D feature requests from only one or two users.  This last one actually takes a lot more time because they tend to be those \"urgent, I need it yesterday\" requests.  In reality, the user has rarely thought completely through what they actually need or how it will support their business model.  Most often, these urgent requests, once delivered, end up being used once or twice and forgotten about.  And once forgotten, they become an endless headache of security holes and unintended consequences. "
    },
    {
        "ID": "13961",
        "Question": "We are working on a large ongoing project that has continual feature changes. These features could take as long as 3 - 4 weeks to complete. This would of course be fine, except the client is always changing its priorities/focus based on pushes from THEIR clients who may want certain features implemented before others. So, we often have to shift gears in the middle of a major feature build and start a new feature that all of a sudden has a greater priority. We then of course have the time cost of merging everything together at some point, and also the cost of that lost momentum on the original feature. Our estimates are out the window at that point. Is there any good way to handle this? Some options I've thought of:  assume this is the way things will be, and apply a 'distracted client' factor of up to 100% on all estimates (this is dangerous in the case where we can actually complete a feature without interruption) educate the client on the costs of shifting gears, and perhaps apply a change premium to previous estimates if they want us to change to working on a different feature refuse to work on more than one feature/release at a time (aside from bug fixes). This is probably not realistic.  I'm looking forward to hearing about how others have tackled this. It can't be uncommon. ",
        "Best answer": "I think that your #2 is the way to go. Such problems can only be solved by talking to the customer. Tell them that changes are costly - both in time and money. Show them your estimations and indicate where their new changes interfere. "
    },
    {
        "ID": "14047",
        "Question": "I gave many interviews in the last few years and each time I found the interviewers are not satisfied with what I know. My first company only developed desktop Windows applications using .NET. They had nothing to do with features like: Remoting etc. We also had limited use of Generics, Reflection and Multi-threading.  When I appeared for the interviews, I was asked questions on above features even when I told them that I don't have real-life experience. Now the .NET interviews are even more complex. Seeing my experience, the interviewers target the latest framework. I have no real-life exposure to the new features and technologies like WPF, WCF etc. Please suggest me how to effectively prepare for the .NET interview. I have 3 years experience in .NET but I only developed Windows based applications. At present I work on .NET Framework 3.5. I never worked on ASP.NET, as in my present company I work on PHP for web-applications. ",
        "Best answer": "This is not meant to be a holistic answer, just to highlight a few points to get the ball rolling:  Know the core e.g. CLR, GAC, GC, OO etc Familiar with the language, syntax and features (C#, VB.NET minimal and their respective features e.g. C#3.0 vs C#2.0) Understand what your future company is doing. My company is still am doing v2.0 for some years to come, so 3.5/4.0 are nice to have but not mandatory. Some might be at the cutting edge, WCF/WF/Silverlight/Lamda etc Convince them that though you might not have experience, you can pick up new stuff quickly  "
    },
    {
        "ID": "14052",
        "Question": "Have you come across a super programmer? What identifies him or her as such, compared to \"normal\" experienced/great programmers? Also. how do you deal with a person in your team who believes he is a super programmer? Both in case he actually is or if he isn't? Edit: Interesting inputs all round, thanks. A few things can be gleaned: A few definitions emerged. Disregarding too localised definitions (that identified the authors or their acquaintance as super programmers), I liked a couple definitions:  Thorbjørn's definition: a person who does the equivalent of a good team consistently for a long time. Free Electron, linked from Henry's answer. A very productive person, of exceptional abilities. The explanation is a good read.  A Free Electron can do anything when it comes to code. They can write a complete application from scratch, learn a language in a weekend, and, most importantly, they can dive into a tremendous pile of spaghetti code, make sense of it, and actually getting it working. You can build an entire businesses around a Free Electron. They’re that good.  Contrasting with the last definition, is the point linked to by James about the myth of the genius programmer (video). The same idea is expressed as egoless programming in rwong's comment. They present opposite opinions as whether to optimise for such a unique programmer or for a team.  These definitions are definitely different, so I would appreciate it if you have an input as to which is better. Or add your own if you want of course, though it would help to say why it is different from those. ",
        "Best answer": "I would consider the term \"super programmer\" only for those who can do work that usually requires a team of good programmers, and do it consistently for a long time. This goes both for writing high quality code (documentation, tests etc) and solving very difficult problems requiring a lot of knowledge and talent to solve. But it requires a high performance on all accounts for a long time.  For those who write tons of code nobody can understand and maintain, the term is not applicable. How to deal with?  If you truly have such a person then do all you can to provide the scaffolding needed for the person to excel.  This means moving irrelevant stuff out of the way, and provide the resources the person needs.  Please note that I've found that very high performers tend to be humble. It is, unfortunately, much more likely that you have a person who thinks he is a super programmer and isn't.  The way to deal with those is in my experience to make their performance measurable.  \"FindBugs may not find any problems\", \"All code must have tests corresponding to the use cases\", \"Peer review\". If the code is truly hard to understand, consider weekly meetings where the not-so-super programmer explains any code the code he's written since the last meeting that anybody can request anonymously to have explained, and anybody can reject anonymously to have accepted in the code base for it to be unmaintainable.  Then at least you have shared the mindset and allowed for future maintainers to veto a piece of code.  This will also strongly indicate to the programmer which kind of code will trigger him having to do extra work. (EDIT:  The reason for the anonymous bit, is to avoid having the not-so-super programmer retaliate upon those who oppose him). "
    },
    {
        "ID": "14069",
        "Question": "I have graduated this year and got my first job involved with C programming, Linux administration and embedded systems development. I feel quite content with my job, but I'm afraid I won't become a successful programmer in this field. I'm a lone developer on my job now, with my teammates being hardware experts, there is no one to guide me or teach me in the ways of embedded programming, I have to study all on my own. So here are my questions. Is it possible to become a good embedded systems developer starting from an entry level position without any supervision by senior programmers? How could I become one (perhaps with the help of forums, IRC channels, good textbooks)? How long would it take? UPDATE: So far, I have received a handful of helpful answers, but I realized that I need some specific pointers on the subject. So, here are my particular questions:  What are some good textbooks one can use to learn embedded development? What specialized online communities can be helpful for an aspiring embedded developer (not counting general stuff like Stack Overflow, Reddit and so on)? What are the most interesting blogs dedicated to embedded development?  ",
        "Best answer": "You are right that being in your case will make things little bit harder. However there are many solutions to solve your issue.  First try to participate to communities like this one, but maybe more specialized in your field. By participating, you will increase your knowledge, and more importantly, meet other people. Try to participate to code camps, or any initiative where you meet other developers like you in real. It's easier to share knowledge when you have the person in front of you. Ask your boss to book you at least 10 days of training a year. No need to explain the advantages here, if he refuses, he is stupid. Try to go to 1 to 2 conference or trade show related to your business. Try to read a technical book every two months. If you can read more, don't hesitate. Get a mentor. It's not the easiest thing to achieve of course. Reserve some time in the week to experiment and do research & development, by trying new technologies of things you read in your books, community, trade shows, trainings, etc... Taking the time to practice what you have learn is VERY important. Today you are the only developer, but thanks to your hard work, the company you are in will grow, and will probably need to hire more people of your kind.   "
    },
    {
        "ID": "14092",
        "Question": "While looking for a job via agencies some time ago, I kept having questions from the recuitment agents or in the application forms like: How many years of experience do you have in:  Oracle ASP.NET J2EE etc etc etc....  At first I answered faithfully... 5yrs, 7yrs, 2 yrs, none, few months etc etc..  Then I thought; I can be doing something shallow for 7 years and not being competent at it simply because I am just doing a minor support for a legacy system running SQL2000 which requires 10 days of my time for the past 7 years. Eventualy I declined to answers such questions. I wonder why do they ask these questions anymore. Anyone who just graduated with a computer science can claim 3 to 4 years experience in anything they 'touched' in the cirriculum, which to me can be equivalent to zero or 10 years depending how you look at it. It might hold true decades ago where programmers and IT skills are of very different nature. I might be wrong but I really doubt 'time' or 'years' are a good gauge of competency or experience anymore. Any opinion/rebuttal are welcome! ",
        "Best answer": "I agree, this measure of competence is meaningless.  However, I am guessing that most recruiters do not know that.  They simply try to match your resume to the job description as best they can.  If the job ad says \"10 years of experience with Oracle\", then your resume may be rejected if it only lists 5 years, even if in those 5 years you have become an expert.   My advice is to answer these questions the way you did initially, to get past the initial resume filter.  At the same time, look carefully at the job description yourself, and try to gauge if it is really a good match for your skills.  "
    },
    {
        "ID": "14152",
        "Question": "I'm going to be managing projects for a very small (2-5) senior developer team.  They develop a subscription based web application.  It has been in use for many years so at this point the work involves:  Developing new features and updates to existing features Developing in-house tools and reports Fixing bugs Investigating customer service issues  Production releases are sometimes determined by customer obligations, sometimes by competitive advantage, but for the most part are not time-critical.  I'd like to have regular production releases to keep the product fresh -- once every 2 months or so. Since the team is so small, the developers themselves will have to be involved in designing, testing, deployment, and maintenance. Since we do not have a dedicated maintenance team I see the major challenge being development and planning disrupted by bug fixing and customer service issues.  They can distract us during new feature development and disrupt the release schedule. My initial thought is to do the following:  Hire a junior programmer to buffer the senior developers from maintenance tasks.  In between maintenance tasks this person can assist the senior devs. Adopt a scrum style approach where we have a series of sprints leading up to a production release.  This will allow us to keep the stakeholders in the loop and allow us to reflect and improve our process with each sprint. Shortly following the release, which is when most bugs surface, the team can focus on design for the next release with the expectation that each one will likely get pulled into bug fixing.  I'd love to hear your thoughts and lessons learned. ",
        "Best answer": "Scrum is the perfect choice here Forget the point 1. Let your cross functionnal team self manage itself. Do point 2, but implement everything in Scrum, not just what you think is useful. Also don't try alone, hire a certified Scrum Practionner or a Certified Scrum Coach. Forget the point 3. Hire a tester and put it in your Scrum team. Also using Scrum properly, that will help to reduce bugs in production. "
    },
    {
        "ID": "14162",
        "Question": "I'm trying to decide if I need to reassess my defect-tracking process for my home-grown projects.  For the last several years, I really just track defects using TODO tags in the code, and keeping track of them in a specific view (I use Eclipse, which has a decent tagging system). Unfortunately, I'm starting to wonder if this system is unsustainable.  The defects I find are typically associated with a snippet of code I'm working on; bugs which are not immediately understood tend to be forgotten, or ignored.  I wrote an application for my wife which has had a severe defect for almost 9 months, and I keep forgetting to fix it. What mechanism do you use to track defects in your personal projects?  Do you have a specific system, or a process for prioritizing and managing them? ",
        "Best answer": "Fogbugz (free individual license) if its a longish project or a simple to do list (using Google tasks) "
    },
    {
        "ID": "14176",
        "Question": "Does anyone know (or, I guess, I have gut feeling about) how the download size of a software product affects potential users, bandwidth not withstanding? For example: Does a bigger download make them    believe that is it more full-featured    than it is (like huge .NET Runtimes    if deployed with your package)? Reason is, I'm developing a VSTO add-in where there is my add-in (small MB), the VSTO run-time, .NET runtime, and Primary Interops. This could make it more than 70 MB in total size, just for an add-in that does a couple of things well. But that's all they are - just a couple of things. With VSTO deployments, I know I could deploy a smaller package size by assuming they've got the .NET runtime (and when they don't, initialize the download/install of that, but it seems like a clunky way to make just an ass out of me). What I really want is for people to just download the app (the trial version) to see if they like it. Is there some kind of magic happy threshold - like users of non-big name apps are more likely to download something that is under 20 MB than 50 MB? ",
        "Best answer": "Who are you targeting? Most people couldn't care less about download sizes. If you're targeting tech users, then I'd say most of them would prefer a smaller download, which signals that your software isn't bloated. But even then, it's really not a huge deal. I wouldn't include runtimes in a download if they are several times larger than the application/plugin itself, which sounds to be the case. I'd have the installer search for prerequisites, and if one isn't found, either (1) prompt the user to download and install (ask, and then launch a web browser), or (2) automatically download and install, after asking for permission to go online. "
    },
    {
        "ID": "14183",
        "Question": "I'm curious because I notice that the Ultimate subscription costs $12,000... for one year (after that, it's $3,800 to renew). Looking at the subscription chart, you do get quite a bit of software that is normally expensive by itself. Is the package meant for 1 person or is it meant for like a business-type bundle of some sort? If it's the latter, I can kind of understand that, but I can't fathom where anybody is going to have 12 grand to plop down on something that only lasts 12 months. Anyway, yeah, my question is, is this a good deal to anyone at all? Maybe it's just me not having any industry experience and this is actually a steal and millions of companies should be buying it right now, but I just can't personally picture this being worth the money.  ",
        "Best answer": "The most expensive MSDN option ($12,000 in the US) gives you Visual Studio Ultimate. There are several questions on Stack Overflow [eg this one] about whether VS Ultimate is worth it. (The answer is a rousing IT DEPENDS, of course.) Even if you only got the software for a year (and you don't, you have it forever, which in most cases means 3 years and then you'll go on to another version) it's $50/day. If it saves you an hour a day, you're ahead, right? If the price gives you pause, there are lots of ways to get it for less or for free. [This question on Programmers for example.] You could get a version less than Ultimate, all the way down to Express which is free. You could join BizSpark or WebSpark or DreamSpark, if you qualify. Your firm could join the Partner Program, either at the Registered level and buy Action Pack, or at the Certified level and get 5 MSDN licenses for $3500. You could become an MVP and be given a free copy (this is actually probably the most expensive way to do it.)  And yes, many companies buy some sort of MSDN (not always Ultimate) for each developer. And find it worthwhile. I could probably get by with Premium but I use something from Ultimate probably once a month. "
    },
    {
        "ID": "14254",
        "Question": "I have a blog that I use mostly to record solutions to problems I have had, that I had some trouble finding an answer to. Mostly problems where the online doc I googled provided too much info, and I found the answer to my question on the fifth page of my third google hit.  (Or if I asked the question here, I either didn't get an answer or I got slammed for asking a question that \"the answer could be easily googled.\")  I frequently look up stuff on this blog to remind myself of how I solved a problem, and it gets a decent amount of hits from others as well.  Anyway, I was wondering if mentioning this blog on my resume would help or hurt me in a job search? The topics are all over the map.  What I would hope it shows is that  I am a person who finds solutions to problems I have used many different technologies in my work I am not afraid to tackle a challenge   What I am concerned it shows is that  This person had trouble with something that simple? Why is this person bothering to blog this stuff?   ",
        "Best answer": "Any blog, if it covers technical topics rather than the adventures of your cat, can be useful in showing employers what you care about, what you know, and how you approach problems. Consider adjusting your entry style a little bit to eliminate the one negative, that it might show you once didn't know X, where X is some trivial thing that \"everyone\" should know. Next time you want to write \"I have been having such trouble for weeks now trying to figure out how to get the first 4 characters of a string with .NET, I was so confused but in the end I sorted it out, and along the way I learned a lot about the String functions ...\" and then writing a useful post about string functions, but possibly tainting yourself as someone who didn't know that basic thing, start it instead \"Some people are confused about all the different String functions. Here's a handy summary...\" and then carry on as you would have.  You're not lying, you're sharing the same information you would have otherwise shared, and you're helping others. Plus, it looks better when others investigate you. "
    },
    {
        "ID": "14271",
        "Question": "I guess you could call me a \"MicroISV\" because it's basically just me doing the work to create a product to be sold commercially. I'm sure there are many out there like me, so here's my question: Besides the programming/coding aspect to get your code out the door, do you DIY or vendor-out everything else? For example, web site creation, internet marketing, accounting/taxes, company minutes, customer support, setting up ecommerce/banking, server management, procurement, etc. All of those activities seem to be out of the realm of what I'm good at or have the patience/time for, but I don't know if the alternative is too untrustworthy or too expensive to make it worthwhile to try to farm out these tasks to someone else (like a local company, vendors on oDesk, etc.). I can certainly understand wanting to keep all these tasks \"in-house\" (i.e. me) to protect strategy, plans, code, private things like bank account numbers, but is it really that much of a risk to outsource these tasks? What is your experience? ",
        "Best answer": "We have a simple rule in our company, as we are a small software house.  Can we do it as well as a third party Will we save more money by doing the work internally or would it be more cost effective to work on another project Do we really have time for the additional work Is there any other elements that internal staff may not be aware of such as legislation etc.  The answers to simple questions like this should help you make an informed choice. I hope this helps a little. "
    },
    {
        "ID": "14295",
        "Question": "Although not a new idea there seems to have been a big increase in the interest in software craftsmanship over the last couple of years (notably the often recommended book Clean Code's full title is Clean Code: A Handbook of Agile Software Craftsmanship). Personally I see software craftsmanship as good software engineering with an added interest in ensuring that the end result is a joy to work with (both as an end user and as someone maintaining that software) - and also that its focus is more at the coding level of things than the higher level process things. To draw an analogy - there were lots of buildings constructed in the 50s and 60s in a very modern style which took very little account of the people who would be living in them or how those buildings would age over time. Many of those buildings rapidly developed into slums or have been demolished long before their expected lifespans. I'm sure most developers with a few years under their belts will have experienced similar codebases. What are the specific things that a software craftsman might do that a software engineer (possibly a bad one) might not? ",
        "Best answer": "I'd say the only difference between a professional and a craftsman is caring with a little bit of passion mixed in. There is no specific, observable practice that would classify one as a craftsman, but rather a collection of qualities:  A craftsman cares about the actual quality of his work, and not just the perceived quality. A craftsman has an interest in his craft that goes beyond getting the job done, and naturally gravitates towards his craft. A craftsman cares about his profession, aspiring to improve his skills and not only advance his career. A craftsman spends some amount of time outside of his paid working hours (even if it's a small amount of time) doing something with his craft, be it discussing, learning, or even thinking about it. A craftsman knows how little he actually knows, and is humbled by it. A craftsman is willing to teach those who are willing to learn, guide those who seek guidance, and seek those things himself when he needs them.  A little bit of passion covers all of these without breaking a sweat. "
    },
    {
        "ID": "14297",
        "Question": "I attended a software craftsmanship event a couple of weeks ago and one of the comments made was \"I'm sure we all recognize bad code when we see it\" and everyone nodded sagely without further discussion. This sort of thing always worries me as there's that truism that everyone thinks they're an above average driver. Although I think I can recognize bad code I'd love to learn more about what other people consider to be code smells as it's rarely discussed in detail on people's blogs and only in a handful of books. In particular I think it'd be interesting to hear about anything that's a code smell in one language but not another. I'll start off with an easy one:  Code in source control that has a high proportion of commented out   code - why is it there? was it meant   to be deleted? is it a half finished   piece of work? maybe it shouldn't have   been commented out and was only done   when someone was testing something   out? Personally I find this sort of   thing really annoying even if it's just the odd line here and there, but when you see large blocks interspersed with the rest of the code it's totally unacceptable. It's   also usually an indication that the rest of   the code is likely to be of dubious   quality as well.  ",
        "Best answer": " /* Fuck this error */  Typically found inside a nonsense try..catch block, it tends to grab my attention. Just about as well as /* Not sure what this does, but removing it breaks the build */. A couple more things:  Multiple nested complex if statements Try-catch blocks that are used to determine a logic flow on a regular basis Functions with generic names process, data, change, rework, modify Six or seven different bracing styles in 100 lines  One I just found: /* Stupid database */ $conn = null; while(!$conn) {     $conn = mysql_connect(\"localhost\", \"root\", \"[pass removed]\"); } /* Finally! */ echo(\"Connected successfully.\");  Right, because having to brute force your MySQL connections is the right way do things. Turns out the database was having issues with the number of connections so they kept timing out. Instead of debugging this, they simply attempted again and again until it worked. "
    },
    {
        "ID": "14335",
        "Question": "Similar to the question I read on Server Fault, what is the biggest mistake you've ever made in an IT related position. Some examples from friends:  I needed to do some work on a production site so I decided to copy over the live database to the beta site. Pretty standard, but when I went to the beta site it was still pulling out-of-date info. OOPS! I had copied the beta database over to the live site! Thank god for backups.  And for me, I created a form for an event that was to be held during a specific time range. Participants would fill out the form for a chance to win, and we would send the event organizers a CSV from the database. I went into the database, and found ONLY 1 ENTRY, MINE. Upon investigating, it appears as though I forgot an auto increment key, and because of the server setup there was no way to recover the lost data. I am aware this question is similar to ones on Stack Overflow but the ones I found seemed to receive generic answers instead of actual stories :) What is the biggest coding error/mistake ever… ",
        "Best answer": "Issuing a SQL UPDATE with a 'bad' WHERE Clause that matched everything. Lesson Learned: Always issue a SELECT first to see what will be changed. "
    },
    {
        "ID": "14401",
        "Question": "I work for a company that owns several closed source software. One of our software is used to service clients and generate income so it has commercial value to us (and it pays my salary).  I've talked about open sourcing the code, but the option of open sourcing under something like the MIT or GPL or any other open source license that allows competitors to install it and offer it to our clients for cheaper has met clear resistance. What about showing the code but not under an open source license?  The arguments I've been getting against this is that this move will confuse clients and developers and will get us more bad publicity than any good thing. The developer of PAINT.NET used to release his code and got a lot of flack when he started reminding competitors that the code is only for educational purposes and isn't really open source.  I've also been told that open sourcing for the sake of open sourcing doesn't make sense, bla bla bla, and that unless I have a plan for how that's going to bring more revenue, it's not for us. Well, honestly I don't have a plan, but I thought why not try. Of course I can't say if this will help us or hurt us. I don't want to take the responsibility for a move that might hurt a company that's doing well and get people laid off. As I said, an open source license does not look like an option, so it's between closed source or showing the code but still under a normal copyright license. But as I said there's resistance to showing the code because of the worry that competitors will use our code to create their own. Does anyone know of any facts that can help with this decision, whether in favor or against showing the code. ",
        "Best answer": "In a comment you mention when asked for a reason to open source you replied \"Obviously I'm hoping it will benefit the company somehow, but as I said, I don't know yet exactly how.\" That's not a good enough reason.  You absolutely must know how it benefits the company.  It's not a try it and see how it turns out kind of decision. Like @Vatine said there are many companies that will license the source in addition to the software, sometimes for an additional price.  It gives a little peace of mind in that if there's a critical bug affects their customer they can make a hot-fix while they wait for your company to fix it. The other big benefit of open-source is the community.  As @Konrad says, if the source is available but not open it greatly diminishes this effect. Also a community doesn't just establish itself, it needs promoting which costs time and/or money. Also think about what actually makes the money for your company.  Is it the software itself, or is it something external and the software is merely a facilitator?  Google makes most of their money through advertisement.  The fact they open source most of their software and is a huge proponent of open source isn't negatively impacted by having the source out there.  It's the user-base that is their biggest asset, not the software.  Open-sourcing helps build that user-base and build good-will. Does your company have a model that would benefit from a large non-paying user-base?  By non-paying I mean paying for the software.  If you're selling the software directly you can more or less forget it.  If you're providing support or consultation services then you can use open source to give you an edge over the competition.  Open sourcing then can help build brand recognition.  Notice I said \"can\" and not \"will\".  It still needs to be marketed and promoted properly otherwise it's useless.  It's all about building a community. In addition to the above, it's also a very personal decision for the company.  Is the personality of the company an open culture?  If it's not fully embraced by the company, especially at the head, it's not going to work.  Once it goes open source you can't just close it.  You can close future modifications, but what's out there will be out there, you can't take it back, so it's not a willy-nilly decision.  You absolutely must know how and why it will benefit your company specifically.   "
    },
    {
        "ID": "14435",
        "Question": "I have a subversion server setup that I need to look after several projects, grouped by language, then client (company) and then by project e.g. repos/  flex3      \\com1           \\project1           \\project2      \\com2           \\project1           \\project2           \\project3  flex4      \\com1          \\projectx      \\com2          \\projecty  java     \\projectz  repos is my repository root and then I have the 3 repositories inside (create with svnadmin create) flex3, flex4 and java, then flex3 should have two folders com1 and com2 for different clients, each will contain different \"project\" repos. I have a feeling I have done this wrong, should I simply create the top level structure as normal folders and then make the com1 and com2 etc into repos with \"svnadmin create\" so that different projects can be added directly below them. Is this correct? Thanks ",
        "Best answer": "I think you should ask yourself following questions before deciding repository layout for your company.   What data do you expect to live in your repository (or repositories), and   how will that data be organized? Where will your repository live, and how will it be accessed? What types of access control and repository event reporting do you   need? Which of the available types of data store do you want to use?   Here you will find full article and how to answer above queries... We have been using Visual SVN Server for quiet time to manage our project repositories.  Have a look at Visual SVN Server recommended repository layout Said that, we are not using Visual SVN recommended layout; because we have projects which may overlap technologies e.g. an ASP.NET project may have flash module or ASP admin (for some legacy applications). And there is access restriction each projects. This is what we are using, and it seems to be working fine so far. Some tasks are very easy, especially repo backup routines, access management and trunk, branch and tag structure. + Repositories (root)     + Project 1 (repository)     + .......     + Project 2 (repository)     + .......     + Project 3 (repository)     + .......  "
    },
    {
        "ID": "14492",
        "Question": " Possible Duplicate: How do you dive into large code bases?   I have worked as a developer developing C/C++ applications for mobile platforms (Windows Mobile and Symbian) for about six years. About a year ago, however, I changed job and currently work with large(!) enterprise systems with high security and availability requirements, all developed in Java. My problem is that I am having a hard time getting a grip on the architecture of the systems, even after a year working with them, and understanding systems other people have built has never been my strong side. The fact that I haven't worked with enterprise systems before doesn't exactly help. Does anyone have a good approach on how to learn and understand large systems? Are there any particular techniques and/or patterns I should read up on? ",
        "Best answer": "Personally I don't think anything beats just slogging through the code and learning it. There is not quick way to understand. When I go to a new job, I spend a week, doing almost nothing but understanding the design of the database (I'm a database specialist). If diagrams don't exist, I make them. I track things through from the top level of the queries all the way to the tables. I ask questions when something doesn't make sense. I look for the most critical of the queries to understand first (usually the search queries, they will give you a really good idea of the most important tables.). If possible get someone with some expertise to sit down with me for a good half a day and show me what he knows. Since you are way past your first week (when it is easier to get this time) this is harder. But start with one module you have worked on or are getting ready to work on and read the code, following the paths in the code until you have gone down to the lowest level. Make notes.  Make diagrams if you need to. Just work through one section at a time, but go through it thoroughly. Try to find someone who worked on the design to understand the design choices. Often they chose what was best at the time they did the design but something which five years later looks pretty awful to you. It helps to understand the constraints that caused the design to be the way it is.  Enterprise applications tend to be heavily database centric. Make sure to take the time to understand the database design as well as the structure of the application.  Are there requirements documents from the orginal design on out? Take the time to read some of them.  It is never easy to understand a large Enterprise system. There are alot of different little designs at different time periods involved in it so there is often very little consistency. It is likely that no one on the team understands the whole system completely. If you do have someone who is the acknowldeged expert in the system, listen to him or her very carefully, ask lots of questions, ask why they made the choices they did.  BUt do it respectfully, nothing will shut the expert onteh system up faster than asking questions in a way that makes them feel defensive. While you may or may not agree with all the choices they made (about a 100% chance you won't agree with all of them in my experience) at least understanding the whys helps you start to see the pattern of how this group of people approach design which will clue you in to how they did the next thing you look at as well.  "
    },
    {
        "ID": "14582",
        "Question": "Good afternoon I would like to know how do you guys organize your project folders? I had once a boss that suggest me to organize by Customers. Projects | |----Customer 1      |---- A Cool Solution 1            |---- source                  |---- version1.0                  |---- version1.1            |---- docs                  |---- analysis                  |---- meetings                  |---- manuals |----Customer 2 |----Customer 3  A friend of mine told me to organize tem by Technology Projects | |----.NET      |---- C#           |---- Customer 1                      |---- A Cool Solution 1                       |---- source                             |---- version1.0                             |---- version1.1                       |---- docs                             |---- analysis                             |---- meetings                             |---- manuals |----Ruby |----PHP  And you? Do you have a clever way to organize your project folders? ",
        "Best answer": "I'm pretty flat: /Projects Some varation getting there depending on box, but behind that there are just lots of individual folders for projects. Real deal lives in source control anyhow, so this is just the temporary local home.  "
    },
    {
        "ID": "14610",
        "Question": "Have you ever encountered a case of code duplication where, upon looking at the lines of code, you couldn't fit a thematic abstraction to it that faithfully describes its role in the logic? And what did you do to address it?  It is code duplication, so ideally we need to do some refractoring, like for example making it its own function. But since the code doesn't have a good abstraction to describe it the result would be a strange function that we can't even figure out a good name for, and whose role in the logic is not obvious just from looking at it. That, to me, hurts the clarity of the code. We can preserve clarity and leave it as it is but then we hurt maintainability.  What do you think is the best way to address something like this?  ",
        "Best answer": "Sometimes code duplication is the result of a \"pun\": Two things look the same, but aren't. It is possible that over-abstracting can break the true modularity of your system. Under the regime of modularity, you have to decide \"what is likely to change?\" and \"what is stable?\". Whatever is stable gets put in the interface, while whatever is unstable gets encapsulated in the module's implementation. Then, when things do change, the change you need to make is isolated to that module. Refactoring is necessary when what you thought was stable (e.g. this API call will always take two arguments) needs to change. So, for these two duplicated code fragments, I would ask: Does a change required to one necessarily mean the other must be changed as well? How you answer that question might give you better insight into what a good abstraction might be. Design patterns are also useful tools. Perhaps your duplicated code is doing a traversal of some form, and the iterator pattern should be applied. If your duplicated code has multiple return values (and that's why you can't do a simple extract method), then perhaps you should make a class that holds the values returned. The class could call an abstract method for each point that varies between the two code fragments. You would then make two concrete implementations of the class: one for each fragment. [This is effectively the Template Method design pattern, not to be confused with the concept of templates in C++. Alternatively, what you are looking at might be better solved with the Strategy pattern.] Another natural and useful way to think about it is with higher-order functions. For example, making lambdas or using anonymous inner classes for the code to pass to the abstraction. Generally, you can remove duplication, but unless there really is a relation between them [if one changes, so must the other] then you might be hurting modularity, not helping it. "
    },
    {
        "ID": "14728",
        "Question": "I have seen this happen multiple times: The candidate likes the company, succeeds at the interview. Then comes to work and at the end of the first day he is sure the codebase/project is not what he would like to spend time with. So he leaves quickly. I think that introducing candidates to the codebase at the interview could potentially solve this problem. Maybe even better: mixing this with interview questions like \"how would you improve this part of the code?\" This way, it would be obvious if the candidate is a \"good fit for the codebase\". Have you seen this approach applied anywhere? Would you show your own codebase to the candidates at the interview: if they asked/as part of the interview process? ",
        "Best answer": "We not only show the candidates our code, we get them to work on it. We do pair programming, so we do interviews by pairing the candidate with one of our programmers and working on a real problem (albeit a carefully chosen one - something that doesn't need masses of contextual knowledge). They get to see our code, and we get to see their coding, and we both get to see how they fit in with our culture. "
    },
    {
        "ID": "14744",
        "Question": "My first programming language was PHP (gasp).  After that I started working with JavaScript.  I've recently done work in C#. I've never once looked at low or mid level languages like C. The general consensus in the programming-community-at-large is that \"a programmer who hasn't learned something like C, frankly, just can't handle programming concepts like pointers, data types, passing values by reference, etc.\" I do not agree. I argue that:  Because high level languages are easily accessible, more \"non-programmers\" dive in and make a mess In order to really get anything done in a high level language, one needs to understand the same similar concepts that most proponents of \"learn-low-level-first\" evangelize about.  Some people need to know C; those people have jobs that require them to write low to mid-level code. I'm sure C is awesome, and I'm sure there are a few bad programmers who know C. Why the bias?  As a good, honest, hungry programmer, if I had to learn C (for some unforeseen reason), I would learn C.  Considering the multitude of languages out there, shouldn't good programmers focus on learning what advances us? Shouldn't we learn what interests us?  Should we not utilize our finite time moving forward?  Why do some programmers disagree with this? I believe that striving for excellence in what you do is the fundamental deterministic trait between good programmers and bad ones. Does anyone have any real world examples of how something written in a high level language—say Java, Pascal, PHP, or JavaScript—truly benefited from a prior knowledge of C? Examples would be most appreciated. ",
        "Best answer": "The advantage to knowing C is that you have a very good idea of how a computer works. Not just how your programming model works, but how memory's laid out, and suchlike. The only level below C is the assembly spoken by a particular CPU. (I'd add that knowing C also lets you appreciate how much less work you have to do in a higher level language. And hopefully an appreciation of the cost involved in working in that higher level language.) "
    },
    {
        "ID": "14781",
        "Question": "I just finished college a couple of months ago and I'm taking time out to improve my knowledge. I love programming but I feel like I don't know enough to be confident if I went for an interview. Last night I was browsing through questions and came across a question that asks interviewees how to work out a linked list. I learned these at college but if I was asked on the spot how to do it I wouldn't know. So that's another thing added to the list of what to learn.  This is when the anxiety hit me because I have so much to learn in so little time (at least it feels that way). Upon introspection, I think this anxiety is related to my perfectionism even though being perfect isn't rational e.g. Shakespeare and Einstein failed 80% of the time.  So the questions are, have you felt this anxiety of not knowing enough? If so, how did you deal with it? I suppose there's a point in time when you begin to feel comfortable in your abilities? ",
        "Best answer": "It's not about knowing everything.  It's about knowing where to find the information. I try to keep the core language I am using in memory.  The rest I am merely familiar with, so I use MSDN a lot to look up things.  Lately, I have been trying to get through the C# specification so that I can think more deeply about the language itself. But I learn best by doing.  Which means ultimately I am doomed to re-implementing Linq to learn lambda expressions, and such. As a programmer, I am capable of doing anything that the top developers can do.  It just takes me a little longer. :) "
    },
    {
        "ID": "14789",
        "Question": "In a now deleted question titled \"What naming guidelines do you follow?\", the author says:  Also I prefer to code using hungarian notation from Charles Simonyi.  I've run in to several programmers who still prefer to use Hungarian, mostly of the Petzold/Systems Hungarian flavor.  Think dwLength = strlen(lpszName). I've read Making Wrong Code Look Wrong, and I understand the rationale for Apps Hungarian, where domain-type information is included in the variable names.  But I don't understand the value in attatching the compiler type to the name. Why do programmers still persist on using this style of notation?  Is it just inertia?  Are there any benefits that outweigh the decreased readability?  Do people just learn to ignore the decorators when reading the code, and if so, how do they continue to add value? EDIT: A lot of answers are explaining the history, or why it is no longer relevant, both of which are covered in the article I cited. I'd really like to hear from anyone out there who still uses it.  Why do you use it?  Is it in your standard?  Would you use it if it wasn't required?  Would you use it on a new project?  What do you see as the advantages? ",
        "Best answer": "At the moment I still use Hungarian for exactly three reasons, judiciously avoiding it for everything else:  To be consistent with an existing code base when doing maintenance. For controls, eg. \"txtFirstName\". We often need to distinguish between (say) \"firstName\" the value and \"firstName\" the control. Hungarian provides a convenient way to do this. Of course, I could type \"firstNameTextBox\", but \"txtFirstName\" is just as easy to understand and is less characters. Moreover, using Hungarian means that controls of the same type are easy to find, and are often grouped by name in the IDE. When two variables hold the same value but differ by type. For example, \"strValue\" for the value actually typed by the user and \"intValue\" for the same value once it has been parsed as in integer.  I certainly wouldn't want to set up my ideas as best practice, but I follow these rules because experience tells me that it occasional use of Hungarian benefits code maintainability but costs little. That said, I constantly review my own practice, so may well do something different as my ideas develop.  Update: I've just read an insightful article (archive mirror) by Eric Lippert, explaining how Hungarian can help make wrong code look wrong. Well worth reading. "
    },
    {
        "ID": "14831",
        "Question": "How do you go about teaching Exception Handling to Programmers. All other things are taught easily - Data Structures, ASP.NET, WinForms, WPF, WCF - you name it, everything can be taught easily.  With Exception Handling, teaching them try-catch-finally is just the syntactic nature of Exception Handling.  What should be taught however is - What part of your code do you put in the try block? What do you do in the catch block? Let me illustrate it with an example.  You are working on a Windows Forms Project (a small utility) and you have designed it as below with 3 different projects.   UILayer BusinessLayer DataLayer  If an Exception (let us say of loading an XDocument throws an exception) is raised at DataLayer (the UILayer calls BusinessLayer which in turns calls the DataLayer), do you just do the following //In DataLayer try {     XDocument xd_XmlDocument = XDocument.Load(\"systems.xml\"); }  catch(Exception ex) {     throw ex; }  which gets thrown again in the BusinessLayer and which is caught in UILayer where I write it to the log file?  Is this how you go about Exception Handling? ",
        "Best answer": "To explain exception handling, explain the concept behind it: The code where an error occurs frequently does not know how to properly handle that error.  The code that knows how to handle it properly could be the function that called that one, or it could be further up the call stack. When you write a routine that calls a routine that might throw an exception, if you know how to handle that error correctly, put the call in a try block and put the error-handling code in the catch block.  If not, leave it alone and let something above you in the call stack handle the error. Saying \"catch ex, throw ex\" is not a good way to do exception handling, since it doesn't actually handle anything.  Plus, depending on how the exception model in your language works, that can actually be harmful if it clears stack trace information that you could have used to debug the issue.  Just let the exception propagate up the call stack until it hits a routine that knows how to handle it. "
    },
    {
        "ID": "14856",
        "Question": "\"Best practices\" are everywhere in our industry.  A Google search on \"coding best practices\" turns up nearly 1.5 million results.  The idea seems to bring comfort to many; just follow the instructions, and everything will turn out fine. When I read about a best practice - for example, I just read through several in Clean Code recently - I get nervous.  Does this mean that I should always use this practice?  Are there conditions attached?  Are there situations where it might not be a good practice?  How can I know for sure until I've learned more about the problem? Several of the practices mentioned in Clean Code did not sit right with me, but I'm honestly not sure if that's because they're potentially bad, or if that's just my personal bias talking.  I do know that many prominent people in the tech industry seem to think that there are no best practices, so at least my nagging doubts place me in good company. The number of best practices I've read about are simply too numerous to list here or ask individual questions about, so I would like to phrase this as a general question: Which coding practices that are popularly labeled as \"best practices\" can be sub-optimal or even harmful under certain circumstances?  What are those circumstances and why do they make the practice a poor one? I would prefer to hear about specific examples and experiences. ",
        "Best answer": "I think you've hit the nail on the head with this statement  I'd hate to take things at face value and not think about them critically  I ignore almost all Best Practices when it doesn't come with explanation on why it exists Raymond Chen puts it best in this article when he says  Good advice comes with a rationale so   you can tell when it becomes bad   advice. If you don't understanding why   something should be done, then you've   fallen into the trap of cargo cult   programming, and you'll keep doing it   even when it's no longer necessary or   even becomes deleterious.  "
    },
    {
        "ID": "14931",
        "Question": "Conundrum:  During the course of working on a new feature or fixing a defect, you find a legacy problem in code.  What should you do?  Fix it and risk altering the behavior of the code. It has either been working up until now by some fluke, or else the defect has not been detected or worth anyone's time to report.  Should you leave it alone and allow the problem to make the code harder to work with later?  Fixing the problem will only add to the time of your original task and force you to regression test. Few will appreciate the work. Fixing it, however, seems right somehow. Code with fewer problems is easier to refactor and build upon. I've been finding myself in this situation time and time again as we work to modernize a web application. I can't tell if I'm being obsessive or honorable when I go off-tangent working on these old bugs.  How do you handle these situations? Thanks, Corey ",
        "Best answer": "I work on a very small team, so it kind of depends on what the change is: If its a small, obvious bug fix, I definitely go for it.  I also throw in extra comments if I have to work through someone else's code and other little improvements that fall under the \"boyscout rule\" to me. If the code is so entwined that you have to ask \"Will changing this break something and require testing\" then no, you shouldn't change it.  Bring it up in your bug tracking system if it worries you.   This, incidentally, is why I try to code smaller methods with more obvious type-signatures as well.  If you know there aren't side-effects and can make the ins and outs match, you can fix, rearrange, or tweak any of the interior code without risk. But don't feel like lack of appreciation is a reason not to fix bugs you find or to improve the code base for any reason.  If nothing else, you're being kind to the future you who will assuredly be back in there to fix something else. EDIT: You also need to watch your time on the project.  Obviously under tight deadlines, you need to focus on getting the main work done, but if you're just under \"normal load\" then I think a little cleaning up here and there makes everyone happier in the long run. "
    },
    {
        "ID": "14942",
        "Question": "This question was prompted by a comment left on another question pointing to this article.  What Is Clean Code? I was thinking about how that sentiment can be applied to shared resources (servers, etc). What are some of the ways you've found that work to ensure that when others use a shared server they clean up after themselves?  Delete files / folders after one time use (after a fix or upgrade) Leave the desktop clean of crap Don't install software that they can use on their desktop  Don't create filesystem shares not needed for the solution  Yes, this applies mostly to a windows server environment, in a team of developers + support staff of about 25.  I know there are a few ways to tackle the problem, so what have you found that works? ",
        "Best answer": "The problem is very similar to \"dirty toilet\" problem. The classic approach is a shield \"please leave the cabin in a condition in which you would like to find it\" (forgive me my poor English). Combining this with a simple monitoring/mailing scripts should work fairly well. "
    },
    {
        "ID": "15050",
        "Question": "I have my own thoughts how \"ideal\" multilingual web application or web site should behave. Can you think of better solution? What are the pros and cons of them? What are cons of the solution I am presenting bellow? Any comments? My \"ideal\" solution:  application should read browser language (from Accept Language header) user should be able to override his/her default language in options (logged-on users will see the web site in this language no matter of current browser settings; useful when one is travelling for example) on top of this, when lang attribute is specified in URL (see example bellow), user will see the page in language specified by this attribute (both accept language and user settings would be overridden; this could be useful for book-marking, sharing, RSS feeds selection, web crawlers).  Example URL: http://www.example.com/index.html?lang=ex ",
        "Best answer": "One definite Pro from your approach is that it takes into account users traveling to international locations. I have recently moved to an Arabic speaking country and can't tell you how frustrated I get everyday with the numerous web applications that provide me with the Arabic version of the site, because they use my IP to find my location and present the site in that locations language. "
    },
    {
        "ID": "15094",
        "Question": "I've seen a lot of tests measuring execution speed of programming languages, but I've never found tests measuring development speed. I've heard development speed is increased when working with dynamic languages, but is there any proof of that ? I'm not trying to say dynamic languages aren't efficient, but I'd kind of like to see the results of a scientific study, if such a thing exists. Even better would be informations on maintenance facility. Are some languages easier to debug than others ? EDIT : I'm mostly interested in the differences between two languages that are close in syntax. For example, I guess programming in C will usually be slower than programming in Java; but is programming in Java faster than programming in C# ?  ",
        "Best answer": "Absolutely the programming language makes a difference. However, I think that difference is far more about how familiar the programmer is with each language than any particular aspect of the language, especially given your constraint of languages with similar syntax. "
    },
    {
        "ID": "15112",
        "Question": "I'm currently working on the following types of web sites and apps:  ecommerce like Amazon Reservation system (think of a hotel receptionist checking rooms availability) Invoice management like Freshbook  On Amazon, I didn't notice any breadcrumbs, just facets from the left panel.  However, newegg is using both breadcrumbs and facets. In a management system like hotel reservations or invoice management, you usually have unique reservation or customer number that you search through your system.  Each reservation can then expand to more sections, for instance: Reservations     > Reservation #123456         > Guests         > Room         > Airport pickup         > Payment  In each unique reservation page, I'm using breadcrumbs to show the location of the current page relative to the site.  Is that a good method to present that kind of information?  Should I use tabs or other techniques? ",
        "Best answer": "I find breadcrumbs a very useful feature.  I particularly like it on ecommerce sites where I might be in and out of a lot of different category products.  Its a wonderful tool that should be used more often and doesn't require a lot of screen real estate to implement. "
    },
    {
        "ID": "15159",
        "Question": "How many of you have ever used (or even know of)  the Visual Studio Text Templating Engine? I mean, T4 templates have been around for quite some years now. The where initially included in Visual Studio 2005 SDK, and they where moved to Visual Studio 2008 core just before releasing it. What I want to know is if this is really used out there. I mean, why doesn't the VS team invested in a descent minimal editor for the templates? It doesn't even have syntax couloring! I know there are a couple of good T4 editors out there, but as they don't come built in, I tend to think that this is an unknown feature for most developers. Have you ever used T4 templates in any of your projects? I'm really interested in answers that explain how they created their own T4 to accelerate, automate, or generate artifacts for your project. ",
        "Best answer": "We use T4 in our project for several things:  Auto-generating POCO classes in our DAL when our Entity Framework model is modified. Auto-generating SQL scripts for triggers when our Entity Framework model is modified.   So far they have been working great and have cut out a decent amount of dev time. "
    },
    {
        "ID": "15208",
        "Question": "I'm in the initial planning phase of a re-write project and I'm deciding between silverlight/silverlight oob/wpf. TL;DR at the end. It's a LOB app that handles leads/customers/appointment calendars. Not too complicated. I'm independently researching these options elsewhere, but I thought I'd ask around. Some rough initial requirements/foreseeable problems are: I have to be able to call an exe on the system with commandline args (sip phone).  Makes SL a problem  The userbase is distributed and I want to limit the traffic that goes over the wire as much as possible and avoid some nasty concurrency issues  I can see this being a problem using WPF  Software deployment/updating has to be dead simple. Some users are highly non-technical (see: 70 years old, on a computer for the first time)  This isn't a huge problem now with the ClickOnce app we are replacing, and I have control over the machines that it gets used on. However, it's simpler for users if they don't even have to click the clickonce \"Install\" button. I don't know how this is handled with Silverlight OOB.  The company is planning a hard expansion in 12 months so hardware deployment should be fast/easy. The idea is to get an internet connection at a new location, plug in some computers and be able to work without the need for dedicated IT people or server setup.   Makes SL appealing  Integration with other services (financial software, asterix server) isn't an immediate goal, but it is an eventual goal to be part of the system. This is made much simpler/more efficient if a single service is setup to integrate with those secondary services and doesn't have to transfer all of that data over the wire  Makes SL appealing  Making multiple 'versions' is out the window. I don't know what it's like maintaining a silverlight + silverlight oob version (if there's even any problems)  Might make WPF a better option.  TL;DR: From my vantage point, a silverlight app makes the best sense for 90% of the users - the other 10% can't use it because they need to run an exe. Silverlight OOB might be a happy middleground but I don't know at the moment what the execution model for it is like (is there still a concept of server-side code? If so, that would possibly be ideal) and I don't know how deployment/updating works for it. ",
        "Best answer": "Well - interesting problem. You forgot to mention that you can have a Silverlight full trust application as of SL4, so if you're thinking of WPF you might want to consider that instead. It would need installing (ClickOnce), but you seem to be moving away from that. I've not done anything with OOB yet, but I'm pretty sure that the same binary can be used both in broswer and out of browser as it's a project setting rather than a separate build target.  \"Enable running application out of the browser\"  on the Silverlight project's Silverlight tab. So there'd be no separate maintenance issue there. You can build a fair amount of code into the web application that hosts the Silverlight app and communicate using WCF RIA Services - again you'd have to go with .NET 4 and SL 4 to get version 1.0 of this. .NET 3.5 and SL 4 only supports the WCF RIA services beta. This would reduce a) the size of the download and b) the amount of code that would need to execute on the client but would increase the network traffic. On that score you can split your Silverlight code into several assemblies and using something like Prism set them to load on demand. This means that the user only downloads those parts of the application they are actually using. You can further reduce the amount downloaded by checking the  \"Reduce XAP size by using application library caching\"  option. If you have an \"extmap\" file to go with any external dlls (third party or .NET) then this means that they get bundled into a zip file and downloaded separately to be shared across all xap files in your project. This keeps the individual xap files to their minimum size and ensures that you only have one copy of these other dlls on the client machine. "
    },
    {
        "ID": "15209",
        "Question": " <body>     <!-- wrapper -->     <div id=\"wrapper\">     <!-- title -->    <div id=\"title\"><img src=\"title.png\" alt=\"\" /></div>     <!-- form wrapper -->    <div id=\"form_wrapper\">      <!-- form -->     <form action=\"thankyou.php\" method=\"POST\">      <!-- ... ... -->     </form>     <!-- /form -->    </div>    <!-- /form wrapper -->   </div>   <!-- /wrapper -->  </body>  I can almost understand the <!-- /wrapper --> at the end, because it's so far from the start of the tag/pair...  but seriously, what's the point of the opening comment lines?  I wouldn't ask the question if I didn't see this all the time.   I feel as if I'm missing something.  Maybe there's some unwritten best-practice involved, though I fail to comprehend what it could possibly be.  More than likely it's just obsessive-compulsive behavior. How do you guys normally comment your markup, if at all? ",
        "Best answer": "Some times CSS class names are generic and aren't specifically named. For example, what if you had <!--Code before-->    <!--Column one--> <div class=\"center-bold\">         <!--Tons of lines-->     </div> <!--End column-->  Wouldn't it be nice to know what your starting? I surely wouldn't want to scroll all the way to the end to find out that the whole div was a column.  "
    },
    {
        "ID": "15239",
        "Question": "Somewhat inspired by this question: For what common problems is functional programming not a good fit? - but nevertheless a question which I always wanted, but was too afraid to ask. I've been in ... well, let's call it engineering software development practically all my life, and in all that time, although OO had always been there (well, most of that time) I've never had the need to use \"its ways\", nor to learn that paradigm. We've always used rather simple program structures, routines/functions/modules and although it is opposite to today's best practices managing those programs (programs up to roughly 300k LOC, nothing too big) never proved to be difficult, let alone impossible.  So I wanted to ask you, what would be the sorta problems for which object oriented paradigm would not be a good choice? In comparison to procedural programming ? ",
        "Best answer": "Object oriented programming is procedural programming. The thing that makes OO object oriented is, as Robert Harvey mentions in a comment, that OO abstracts data in a particular way (to wit: bundling the functions that operate on a structure with that structure). William Cook explains the difference between objects and abstract data types nicely. So at the risk of sounding facile, I'd say that objects are not a good fit for when you need to easily extend the (number of) operations that perform on your data, and you don't need to have varying implementations of your data. Having said that, there are things you can do to bring the two closer together. "
    },
    {
        "ID": "15251",
        "Question": "I have been disassembling a large software project on my own, as a hobby. It is an educational exercise and I have learned a lot in the process. That said, I feel that my progress would be considerably quicker if I was to collaborate on the project with other like-minded individuals. Herein lies the problem -- maybe. I am reversing this software for personal interest and potentially (but unlikely; given the magnitude of the codebase in question) implementing an interoperable service to interact with this software, replacing the vendor's own service. My (very limited) understanding is that this particular use-case is protected under copyright law.  However, my wish is to collaborate with others freely on the Internet, in a similar system employed by open source projects: version control repositories of assembly files, wikis to coordinate and share knowledge, public mailing lists, et cetera. I have a feeling this may be iffy at best. Is what I want to do blatantly illegal, a gray area or even legally defensible? Would the situation be any different if the collaboration was in private rather than public? If it has any bearing on the answer, I am located in Australia and the software vendor in located in the USA. ",
        "Best answer": "Maybe read this http://lwn.net/Articles/134642/ I think reverse engineering is always a gray area legally. But that doesn't worry big corp, they will just use their lawyers to squash you legal or not if you are doing something they don't like. I think they key is to not make noise. "
    },
    {
        "ID": "15350",
        "Question": "Registering on an insurance company's website right now, and my password is 16 characters long, using a nice variety of letters, numbers, special characters, etc. However, here's their list of restrictions:  Note your password:  must be between 6 and 12 characters must not contain spaces, special/international characters must not contain your user name, first name or last name is case-sensitive should contain at least 1 number and 1 letter   I can understand minimum 6 characters, not allowing parts of your name, being case-sensitive, and needing at least 1 number and letter. The part I don't get is restricting your choice of characters you can use, and having an upper bound. Why do websites do this? The only thing I can think of it they don't know the basics of hashing a password, which would secure it better than anything, and get rid of any security concerns. If I choose to type DELETE FROM users WHERE 1=1 as my password, I should be allowed to. PHP's MD5 hash of it becomes fe5d54132b51b7d65ab89b739b600b4b which I don't think will harm anything. ",
        "Best answer": "It comes down to their programmers (or their management) being lazy and/or uneducated.  It doesn't take that much more work to make your system accept any characters, but it means you need to spend some time thinking about SQL injection attacks, cross site scripting, making sure that all parts of the system are able to deal with it, etc.  It can be cheaper and quicker just to forbid any characters that could be a problem. "
    },
    {
        "ID": "15391",
        "Question": "It is now more and more popular to have video tutorials of the software or technology instead of writing long articles. As for me, it makes perfect sense, especially for such areas like \"Getting Started\" or \"What's New\". What do you think is the most appropriate duration for such a videos? If you record the video tutorial yourself, would you just touch main points to keep it brief, or rather split it into a number of parts to keep the same level of details? And why? I know this question isn't implied to have one and only answer. I'd like to hear the reasoning behind various opinions. I have personally come across a video tutorial 45 minutes long today, and I got tired at around minute 10... So, my own impression is it's better to keep it from 5 to 10 minutes to gain visitor's attention fully. Thanks! ",
        "Best answer": "This is perhaps slightly off-topic, but hopefully still useful feedback for anyone considering doing video tutorials If the video is basically slides with speaking, please provide a non-video version with images and a transcript as well. I can read far faster than people can talk, and it can be incredibly frustrating having to sit through even a short video - especially so when I'm listening to music, and just want to know how to do whatever the tutorial is teaching. (This has the added benefit of allowing the content of the tutorial to be indexed, thus increasing exposure to the tutorial in general.) "
    },
    {
        "ID": "15405",
        "Question": "Do other people fix bugs when they see them, or do they wait until there's crashes/data loss/people die before fixing it? Example 1  Customer customer = null;  ...  customer.Save();  The code is clearly wrong, and there's no way around it - it's calling a method on a null reference. It happens to not crash because Save happens to not access any instance data; so it's just like calling a static function. But any small change anywhere can suddenly cause broken code that doesn't crash: to start crashing. But, it's also not inconceivable that correcting the code: Customer customer = null; ... customer = new Customer(); try    ...    customer.Save();    ... finally    customer.Free(); end;  might introduce a crash; one not discovered through unit tests with complete coverage, and manual user testing. Example 2 float speed = 0.5 * ((G * mass1 * mass2) / R) * Pow(time, 2);  People knowing physics will recognize that it's supposed to be R2 in the denominator. The code is wrong, it's absolutely wrong. And overestimating the speed will cause the retro-rockets to fire too soon, killing all the occupants of the spacecraft. But it's also possible perhaps having it over-estimate the speed is masking another issue: the air-bags can't deploy while the shuttle is moving too fast. If we suddenly fix the code: float speed = 0.5 * ((G * mass1 * mass2) / Pow(R, 2)) * Pow(time, 2);  Now the speed is accurate, and suddenly airbags are deploying when they shouldn't.  Example 3 Here's an example that i had recently, checking if a string contains invalid characters: if (StrPos(Address, \"PO BOX\") >= 0) {    //Do something }  What if it turns out there's a bug in the Do something branch? Fixing the obviously incorrect code: if (StrPos(\"PO BOX\", Address) >= 0) {    //Do something }  Fixes the code, but introduces a bug.  The way I see it there are two possibilities:  fix the code, and get blamed for breaking it wait for the code to crash, and get blamed for having a bug  What do you politically do?  Example 4 - Today's real world bug I am constructing an object, but calling the wrong constructor: Customer customer = new Customer();  Turns out that the \"parameterless\" constructor is actually an parameterized constructor from  further back in the inheritance chain: public Customer(SomeObjectThatNobodyShouldBeUsingDirectly thingy = null) public Customer(InjectedDependancy depends)  Calling it is a mistake, since it bypasses all the subsequent constructors.  I could change the object's lineage to not expose such a dangerous constructor, but now I have to change the code to: Customer customer = new Customer(depends);  But I can't guarantee that this change won't break anything. Like my Example 1 above, perhaps someone, somewhere, somehow, under some esoteric conditions, depends on the constructed Customer to be invalid and full of junk. Perhaps the Customer object, now that it is properly constructed will allow some code to run that previously never did, and now I can get a crash. I can't bet your wife's life on it. And I can test it from here to Tuesday, I can't swear on your daughter's life that I didn't introduce a regression. Do I:  Fix the code and get blamed for breaking it? or Leave the bug, and get blamed when the customer finds it?  ",
        "Best answer": "This depends wildly on the situation, the bug, the customer, and the company. There is always a trade-off to consider between correcting the implementation and potentially introducing new bugs. If I were to give a general guideline to determining what to do, I think it'd go something like this:  Log the defect in tracking system of choice. Discuss with management/coworkers if needed. If it's a defect with potentially dire consequences (e.g. your example #2), run, scream, jump up and down till someone with authority notices and determine an appropriate course of action that will mitigate the risks associated with the bug fix. This may push your release date back, save lives, wash your windows, etc. If it's a non-breaking defect, or a workaround exists, evaluate whether the risk of fixing it outweighs the benefit of the fix. In some situations it'll be better to wait for the customer to bring it up, since then you know you aren't spending time fixing/retesting things when it's not 100% required.    Mind you, this only applies when you're close to a release. If you're in full development mode, I'd just log the defect so it can be tracked, fix it, and call it done. If it's something that takes more than, say, half an hour to fix and verify, I'd go to the manager/team lead and see whether or not the defect should be fit into the current release cycle or scheduled for a later time. "
    },
    {
        "ID": "15449",
        "Question": "I'm a regular user here, but linking my screen name to my real identity is dirt simple, and the question I'm about to ask would lead to a very unpleasant conversation with my current employer were they to see it.  So, my apologies for this sock-puppet account; I promise Socky and I won't be voting on each other's stuff, and Socky won't be doing much of anything other than voting on answers to his own questions. I may have an opportunity to work with somebody I've worked with in the past.  Short, extremely understated summary:  It went well.  This time he's looking to bring me on as a partner to a fledgling web venture.  My job title isn't defined, but I'd basically be \"the guy\" in terms of the tech side of the company.  One of the first things I'd be doing is fleshing out the current and future requirements of the site and determining the best way to meet them -- overhaul the existing site?  Scrap the existing site entirely and replace it with something like Drupal?  I'd also be tasked with making sure that the site is not only a pleasant user experience, but that it looks and feels like a modern, professional website. One problem:  I'm not qualified. At least, not yet. I wasn't really qualified for the last time we worked together, but I taught myself what I needed to know and ... like I said.  It Went Well.  However, I was a lower-level code wrangler that time around, and a lot of key decisions had already been made.  I had more room for error and time to learn as I was going. This outing, not so much.  It's been a long damn time since I was completely up-to-date on web programming; my current gig has a healthy dash of web stuff, but the web interface isn't the main business and it's built on old technology.  (Technology it honestly doesn't use particularly well.)  I just don't spend a lot of time playing with new tech when I'm not at work; I generally spend that time on hobbies and passions I don't get paid to do. But I want this to happen.  And I want to do it right.  So my question is:  what resources would you recommend I use to both get myself up-to-date and stay up-to-date on professional-caliber web programming? I know I need to beef-up my jQuery-fu; I've been exposed to it a little, and holy crap does it make hard jobs easy.  I also know I need to acquaint myself with Drupal and other content management systems so that I can accurately gauge whether using one as the foundation for the site would be a good idea or a waste of time.  But I'm certain there are other technologies out there that would help me do that job that I don't yet know anything about.  What are some good resources for helping me figure out what I don't know? Websites, magazines, podcasts, whatever.  I need to figure out how to get back into the game properly. This is scary as hell, but it also feels like it could be a huge step forward in my career.  (Assuming it's not a step into a pool filled with laser sharks.)  My thanks in advance for any assistance anybody can offer in curing my ignorance. ",
        "Best answer": "There is so much going on in web technology nowadays you can not possibly be up to speed with everything. It is literally not possible for a single individual to know everything at the cutting edge of web development: HTML5, CSS3, the latest jQuery, pubsubhubbub, Node.js, Google APIs, Yahoo YUI stuff,whichever MVC framework you like, whichever javascript framework you like... are all in a constant state of rapid flux.  Repeat after me: You can not know it all. Now, breath a sigh of relief, and learn the stuff you need to for the new project. You have done it before and you will do it again.  The secret is not in the knowing but the learning. To do that, read the web, listen to relevant podcasts, and start building prototypes. There is no better way to learn than starting the doing. Aside: I personally don't generally bother with books. Web technology moves so fast now that by the time anything gets to print it will probably be, to some extent, out of date. "
    },
    {
        "ID": "15468",
        "Question": "Python seems all the rage these days, and not undeservingly - for it is truly a language with which one almost enjoys being given a new problem to solve. But, as a wise man once said (calling him a wise man only because I've no idea as to who actually said it; not sure whether he was that wise at all), to really know a language one does not only know its syntax, design, etc., advantages but also its drawbacks. No language is perfect, some are just better than others. So, what would be in your opinion, objective drawbacks of Python. Note: I'm not asking for a language comparison here (i.e. C# is better than Python because ... yadda yadda yadda) - more of an objective (to some level) opinion which language features are badly designed, whether, what are maybe some you're missing in it and so on. If must use another language as a comparison, but only to illustrate a point which would be hard to elaborate on otherwise (i.e. for ease of understanding) ",
        "Best answer": "I use Python somewhat regularly, and overall I consider it to be a very good language.  Nonetheless, no language is perfect.  Here are the drawbacks in order of importance to me personally:  It's slow.  I mean really, really slow.  A lot of times this doesn't matter, but it definitely means you'll need another language for those performance-critical bits. Nested functions kind of suck in that you can't modify variables in the outer scope.  Edit:  I still use Python 2 due to library support, and this design flaw irritates the heck out of me, but apparently it's fixed in Python 3 due to the nonlocal statement.  Can't wait for the libs I use to be ported so this flaw can be sent to the ash heap of history for good. It's missing a few features that can be useful to library/generic code and IMHO are simplicity taken to unhealthy extremes.  The most important ones I can think of are user-defined value types (I'm guessing these can be created with metaclass magic, but I've never tried), and ref function parameter. It's far from the metal.  Need to write threading primitives or kernel code or something?  Good luck. While I don't mind the lack of ability to catch semantic errors upfront as a tradeoff for the dynamism that Python offers, I wish there were a way to catch syntactic errors and silly things like mistyping variable names without having to actually run the code. The documentation isn't as good as languages like PHP and Java that have strong corporate backings.  "
    },
    {
        "ID": "15502",
        "Question": "When do you start writing your Exception Handling Code? When do you start writing Logging Statements.  For the purpose of elaborating this question, let us assume that we are on .NET platform with log4net logging but feel free to answer in a generic way.  Solution: A Windows Forms Project.  Projects: UI, BusinessRules, DataHandlers So, do you go about writing your DataHandlers which does your Data Manipulations such as Create, Read, Update, Delete first.  Then follow it up with your Business Rules And then your UI or any other permutation of the above. Test your Application for Functionality.  And then start writing your Exception Handling Code and finally your Logging code? When is a right time to start writing your Exception Handling code? PS: In the book Clean Code, they say Write your try-catch-finally block first. That, prompted me to ask this question. ",
        "Best answer": "You write your exception handling code when you're writing the thing that calls something that might cause exceptions. For instance, when you're writing something that sends data to a network, you should also be writing the code that handles connection timeouts. The exception's directly relevant to what you're doing, and the work is fresh in your mind. On the other hand, if you're writing a parser that raises exceptions when it encounters a malformed protocol data unit, then you can't write exception-handling code for the exceptions that parser produces. (But of course you're writing tests to show how and when and why the parser raises exceptions!) The reason behind the suggestion to write your try-catch-finally first is two-fold: finally is for cleaning up the resources the function creates/consumes, and writing the catch is a nice reminder that the functions you're calling may fail, and you need to handle (if necessary) those failures. I tend to add logging after the fact. I'm not sure if that's wise, but it's just what has worked for me. As I start running acceptance tests and the like, and start hitting issues, then I add logging so I can track errors. (And then I leave the logging in.) "
    },
    {
        "ID": "15567",
        "Question": "My company is getting PC refreshes with a choice of to have a desktop or laptop.  Each one seems to have its pros and cons. Would you want the flexibility and freedom of the laptop but take the risk of working extra hours at home? Or a desktop limited to one place, but not having to worry about taking work home and disrupting home time?   I'm not asking what would be better for coding, but more for convenience and productivity.  I currently have a desktop and work about 8 to 9 hours a day, with only coming in on the weekends for when we get behind on major deadlines. I love developing but I also have quite a busy life after work. I feel that having a work laptop will only haze the line of work and home. ",
        "Best answer": "Despite my habit of using my desktop as a leg rest, both at home and at work, I'd have to go with laptop. Get one that comes with a base station you can plug it into to connect 1+ monitors to it, and you've got a lot of screen space, possibly more than you could get with a desktop, since it has a monitor built-in already. You can also connect a keyboard & mouse to it, so it's indistinguishable from a desktop when being used. Even if you're not planning to take it home, just having the portability in case you need to bring it to a meeting, go on site and have it with you, or anything else, can be a blessing. "
    },
    {
        "ID": "15635",
        "Question": "So I've had a series of really bad experiences and am wondering what I'm doing wrong. I started out as a part time programmer at a major university.  I developed a problem with my wrist and asked (nicely) for some help with my ergonomic accomodations. My boss started screaming at me, then later claimed I screamed at her.  When questioned about it, she just started sobbing.  Luckily, there were others around who heard the screaming and knew I was telling the truth.  She was eventually let go. My contract went up and I switched jobs, this time to a startup.  Well, we went several months without my getting a development machine.  When the economy tanked, they started asking for insane amounts of overtime.  I complied because I was afraid of having to look for work.  The final straw was they contacted me right before my father's funeral to fix a problem. I was laid off, didn't receive my final check on time and the company screwed my COBRA up to the point where I couldn't get prescriptions for months.  They recently were fined by the State of X for what they did.  Now, even though I worked my heart out for them and they were willing before being fined, they are refusing to provide a reference. I took a new job quickly, mainly because I really needed my health insurance set up.  They wanted me to do several months of work in a couple of weeks.  They had no project plan.  Their customers were seriously not doing well, as whole sections of the application would stop working if someone ran a spell check.  Even though they had completely unreasaonble expectations about my work, they were happy to say it would take several months to repair the spell check bug.  There was virtually no training. They had me interviewing candidates and I selected one.  Apparently, that person was my replacement because I was let go after a few months. Recently, my former boss called my current employer and made up allegations that I am discouraging people from applying to open positions at his firm, which is insane because I didn't even know they were hiring. I feel like I have a T-shirt that says, \"If you're insane or would like someone to kick around and treat badly, hire me!\". Then again, when I look at some programmers' resumes, I do see horror stories, too. So are there things I can do differently to avoid these problems?  Is it part of our profession that people do not have standards?  Perhaps it is a personality issue, that resides purely with me? Has anyone else had a series of bad experiences and turned things around? ",
        "Best answer": "You sound like a reasonable guy.  I think you may have just had monumentally bad luck. There are good companies out there to work for; you just have to find them.  Start wearing suits to your interviews.  Be a professional.  Talk to people who have a job with a good work environment, so that you know what that looks like, and know how people at that kind of company behave.  Then, pick a job that has that environment.  Remember, at an interview, you are interviewing the company as well. Look for these things: The ideal job  Has enjoyable and meaningful work Pays fairly Has a pleasant, mentally positive, productive work environment Is challenging, but not excessively stressful Allows you to do your job well Has learning opportunities Has advancement opportunities Has sensible leadership Has minimal office politics Has a reasonable commute Provides the opportunity to work with motivated, like-minded peers who respect each other Has adequate work-life balance Provides an opportunity to work towards long-term goals such as retirement.  I should point out that my current job has most of these things, but it did, in fact, take most of my adult life to find it.  I've been in some pretty screwy work situations as well.   The thing that seemed to make a major difference for me: I went back to school and got my degrees.  I began to be treated differently after that, like I was somebody. "
    },
    {
        "ID": "15670",
        "Question": "Now that we know what is in store for c#5, there is apparently still an opening for us to influence the choice of the two new keywords for 'Asynchrony' that were announced by Anders Heijsberg yesterday at PDC10. async void ArchiveDocuments(List<Url> urls) {     Task archive = null;     for(int i = 0; i < urls.Count; ++i) {         var document = await FetchAsync(urls[i]);         if (archive != null)             await archive;         archive = ArchiveAsync(document);     } }  Eric Lippert has an explanation of the choice of the current two keywords, and the way in which they have been misunderstood in usability studies. The comments have several other propositions. Please - one suggestion per answer, duplicates will be nuked. ",
        "Best answer": "Given that I'm not clear about the meaning/necessity of async, I can't really argue with it, but my best suggestion for replacing await is: yield while (look! no new keywords) Note having thought about this a bit more, I wonder whether re-using while in this way is a good idea - the natural tendency would be to expect a boolean afterwards. (Thinks: finding good keywords is like finding good domain names :) "
    },
    {
        "ID": "15712",
        "Question": "For those of you who work in big-design-up-front/waterfall groups: how much critical thinking and design is skipped in your design phase and left to your implementation phase? How complete and detailed are your functional and technical specifications delivered at the end of the design phase? It seems to me that there's a lot of room for interpretation in the level of detail that needs to be provided at the end of the Design phase, and it irks me when the design phase is rushed and then managers get angry that the build phase doesn't progress like we're churning out widgets on an assembly line. On the other hand, a design phase that's complete enough to make the build phase operate like an assembly line practically includes the implementation phase with it - all that would be left in \"build\" is to type stuff into the editor. Of course, it also means your design phase is gigantic. I realize that this is a shortcoming of the waterfall model, but if there's anyone out there that can provide some constructive advice: Where does one draw the line? What should the expectations for the design and build phases be, and what kinds of design shortcomings or mistakes should/shouldn't be tolerated in the build phase? ",
        "Best answer": "i'm a huge fan of rapid prototyping, rapid incrementation, and rapid iteration. evolution rather than \"intelligent design\". you're writing software to solve problems for people and people tend to change their minds and needs over time, even short periods of time.  it's nice to get requirements, a birds-eye view, and \"sign off\" as squared-away as possible before coding, but there is little point being dogmatic, its all fluid. be prepared to edit, amend, and throw away code as you go. the funny thing is you never arrive at a completion point anyway...most code seems to change while people care about it, and then once no one cares, it just gets deleted. now to be certain this won't apply to must-work-the-first-time bits of software like aircraft control systems, nuclear reactor controls etc, but my guess is that isn't your case. "
    },
    {
        "ID": "15730",
        "Question": "The general rule is to keep check-ins small and check-in often. But sometimes the task requires large changes to the underlying framework. Then checking in before compeleting the task will break the project until you check in the finished work. So what strategies do people use to reduce the risk of losing work, or deciding something you are doing is the wrong approach, then changing your mind after removing the code and trying another approach? When I can, I'll check-in half done work commented out, or if it compiles and nothing is using new files I'll check them in. The larger the change the more likely I am to branch the project and then merge back when I have everything working again. Another option if the source control system allows is shelf sets, which are basically small branches. So when I finish for the day or come to a decision point, I'll shelve my changes, and then if something catastrophic happens, or I want to come back to that point, I can. ",
        "Best answer": "I use git, so my answer is \"branch\". Branch, and commit piece-meal as you complete the various bits. Push your commits upstream as you're happy, so your colleagues can review the changes without disrupting trunk. When everyone's happy with the code, merge and you're done! (What I tend to do for relatively long-running branches is periodically merge trunk (master, in git terminology) into my branch, so the two branches don't diverge too radically.) "
    },
    {
        "ID": "15804",
        "Question": "I wrote a .NET Windows application using framework 3.5. At that time, 3.5 was very new and I had knowledge of framework 2.0 only. Due to deadline, I didn't use any new features of version 3.5 and delivered the application. There was no time. It is now two years and the app. is running fine, but my clients have requested new features. There is a major scaling needed. The new code that I wrote so far uses many new features of .NET 3.5 and I am concentrating on reducing the new code size. The problem is that the new classes that I want to write are sometimes creating redundant methods for doing the same thing. I am not disturbing the old code. How to manage? If I replace old classes with new ones, it risks functionality and need thorough testing again. If I don't, the project size is increasing.  ",
        "Best answer": "We ship a number of windows applications. Some of the parts of each application use .NET2, some 3.5 and one application is still VB6. We have to make annual updates because the tax law changes every year (many of our users are accountants and actuaries). Refactoring code each year is a standard practice for us. When we added .NET 3.5, we were wanting WCF and Linq, and will be adding more stuff as time goes by (and we have time to learn what we want to add).   If I replace old classes with new ones, it risks functionality and need thorough testing again. If I don't, the project size is increasing.   I recommend adding unit testing to your solution. Many developers don't like to do testing, even automated testing. We have some who comment out tests that now fail due to their new code breaking some older edge case.  Three good books are: Brownfield Application Development in .Net  and Working Effectively with Legacy Code Growing Object-Oriented Software, Guided by Tests  If you cannot afford the versions of Visual Studio that include unit testing, then I recommend another books on using external tools for testing: Expert .NET Delivery Using NAnt and CruiseControl.NET While this book was written for .NET 1.1, it still has some useful information.   One practice I recommend strongly is \"continuous integration.\" The purpose of this is for some process that runs every time code is checked in to compile and test the code. While it is fiddly and annoying to set up in the beginning, it makes things very easy and very repeatable later on. If you are keeping score, continuous integration lets you answer \"yes\" to questions 2  & 3 on the Joel Test.  "
    },
    {
        "ID": "15813",
        "Question": "I have a difficult time trying to learn how to program from straight text-books. Video training seems to work well for me in my past experiences with PHP. I am trying my hardest to stay focussed and push through. Specifically I am looking to start indie game development. Over the last two weeks I have been trying to pick the \"right\" language and framework to develop with. I started going through Python, but I am not really enjoying the language so far. I am constantly looking through this website to compare this language to that, and keep getting distracted.  Aside from all of this, is it possible to become a programmer when you have trouble focussing? Has anyone been through this that can recommend some advice? ",
        "Best answer": "Commit to a language and framework. Once you have made that commitment, forsake all others.  Be faithful to that one language and framework, at least for awhile.  Then... Pick one thing to code, and work on that.  Focus on coding that only.  Get it done, quickly.  Then work on the next thing.  If you find yourself getting bogged down in a task, break it down into smaller pieces and work on each one individually. If you can control your focus, you will find that you have better productivity than your non-ADHD peers.  That is the great paradox of ADHD; once you are focused, you are hyper-focused. Do things quickly; stay in that zone.  But concentrate on one thing at a time.  That is the secret. "
    },
    {
        "ID": "15820",
        "Question": "Often when I hear about the switch statement, its put off as a way to replace long if...else chains. But it seems that when I use the switch statement I'm writing more code that I would be just writing if...else. You also have other issues like keeping all variables for all calls in the same scope. Here's some code that represents the flow I normally write (thanks to diam) String comment;   // The generated insult. int which = (int)(Math.random() * 3);  //  Result is 0, 1, or 2.  if (which == 0) {     comment = \"You look so much better than usual.\"; } else if (which == 1) {     comment = \"Your work is up to its usual standards.\"; } else if (which == 2) {     comment = \"You're quite competent for so little experience.\"; } else {     comment = \"Oops -- something is wrong with this code.\"; }  Then they want me to replace that with this: String comment;   // The generated insult. int which = (int)(Math.random() * 3);  //  Result is 0, 1, or 2.  switch (which) {     case 0:                comment = \"You look so much better than usual.\";     break;     case 1:                comment = \"Your work is up to its usual standards.\";     break;     case 2:                comment = \"You're quite competent for so little experience.\";     break;     default:               comment = \"Oops -- something is wrong with this code.\"; }  Seems like a lot more code in a much more awkward syntax. But is there really an advantage to using the switch statement? ",
        "Best answer": "For this particular situation, it seems to me that both if and case are poor choices. I'd use a simple array: String comments[] = {     \"You look so much better than usual.\",     \"Your work is up to its usual standards.\",     \"You're quite competent for so little experience.\" };  String comment = comments[(int)(Math.random() * 3)];  As a side note, you should generally compute the multiplier based on the size of the array rather than hard-coding the 3. As to when you would use a case/switch, the difference from a cascade of if statements (or at least one major difference) is that switch can semi-automatically optimize based on the number and density of values, whereas a cascade of if statements leaves the compiler with little choice but to generate code as you've written it, testing one value after another until it finds a match. With only three real cases, that's hardly a concern, but with a sufficient number it can/could be significant. "
    },
    {
        "ID": "15829",
        "Question": "For 4 years now, I've been the solo developer for a small company. We have a handful of well establish products in a niche industry. We will be hiring 1-2 developers soon, and that will likely change the way things operate around here. While I won't have a \"real\" title, I will be \"in charge\" of this team. What I want to do is establish a very organized and productive programming department for my company. I got this solo job right out of college, so while I've become proficient as a programmer in this industry, I lack a lot of team programming experience. I feel that starting out on the right foot will be key.  Right now it's just me, a few computers, and an SVN server. I'm looking for any general guidance on building a team from the ground up. ",
        "Best answer": "Get your house in order and ensure that things are setup for a collaborative effort.  Version Control - You mentioned that you already have an SVN server, which is great. Make sure that you have established the repo and organized the projects in a standardized way. Automated Builds Issue/Bug tracking software Unit/Integration tests Continuous Integration server  Having the following items setup and integrated into your development process makes it much easier to bring people on board, track what is being done, and keep things from getting \"messy\". "
    },
    {
        "ID": "15873",
        "Question": "Are there any systematic differences between software developers (sw engineers, architect, whatever job title) with an electronics or other engineering background, compared to those who entered the profession through computer science?    By electronics background, I mean an EE degree, or a self-taught electronics tinkerer, other types of engineers and experimental physicists.   I'm wondering if coming into the software-making professions from a strong knowledge of flip flops, tristate buffers, clock edge rise times and so forth, usually leads to a distinct approach to problems, mindsets, or superior skills at certain specialties and lack of skills at others, when compared to the computer science types who are full of concepts like abstract data types, object orientation, database normalization, who speak of \"closures\" in programming languages - things that make little sense to the soldering iron crowd until they learn enough programming.   The real world, I'm sure, offers a wild range of individual exceptions, but for the most part, can you say there are overall differences?   Would these have hiring implications e.g. (to make up something) \"never hire an electron wrangler to do database design\"?   Could knowing about any differences help job seekers find something appropriate more effectively?   Or provide enlightenment or some practical advice for those who find themselves misfits in a particular job role? (Btw, I've never taken any computer science classes; my impression of exactly what they cover is fuzzy.   I'm an electronics/physics/art type, myself.) ",
        "Best answer": "If I had to generalize, here's what my experience has been:  Engineers (or just EE's) tend to do better in the \"perfection of the small\".  Given a small programming task, they think very long and hard about all the edge cases, and are more likely to end up building a piece of software that's very robust.  It's usually driven from a top-down design-it-all-up-front approach, because that's what they're used to in hardware.  It usually involves the use of state machines, because they're used to designing them for hardware, and it fits with the \"big design\" approach.  On the flip side, they aren't thinking as much about scalability or maintainability. Your traditional developers are better at managing large complexity, mostly because the training pushes breaking down problems into smaller more manageable bits.  They're taught to avoid the big design, and just separate the concerns, write tests, and make the tests pass.  Typically there are lots of little missed edge cases, just due to complexity & time, but those eventually get covered off.  Developers tend to take advantage of the fact that it's just software and it should be (or is) easy to change.  When EE's work with hardware, they don't have this advantage, and I think it takes time to make the transition.  As I said, that's my generalized experience.  It's not true in every case. "
    },
    {
        "ID": "15897",
        "Question": "Hi We have a CMS application that lets people create websites under our domain. The system was built a few years ago and it used a method that transfers parameters such as website id, folder code and more using the url. this method created a giant url for every item in the website    For example: My domain is www.domain.com A users website on my domain is www.domain.com/user and every time that a user enters his website he gets a link like this www.domain.com/page.aspx?code=blablasdsdsdsdsds&folder=blablablablablabla and more. We are trying to reduce the string size in the url. What are our options? can we show the user one url like a virtual one and still work the same with the old url? We are trying to locate a solution that wont make us rewrite our entire application. the application is built in c# and the web server is iis 6. Thanks ",
        "Best answer": "You might want to consider URL Rewrite (or friendly URL) techniques. IIS6 could be a little tricky though. You can find lots of resource just by google-ing. http://www.google.com/search?q=url+rewrite+asp.net An example can be found in this SO post: https://stackoverflow.com/questions/2262/asp-net-url-rewriting "
    },
    {
        "ID": "15943",
        "Question": "Whenever I do a check-in, I always check in from the root of the project... i.e. check in all the files in my working copy, so after the check-in the source control repo contains exactly the same set of files that I just finished testing in my local copy.  I also make sure my source control is set to flag local files that are not under source control.  In general, there are none of these files... if there are, I either add them to source control or mark them as \"ignored\".  I also check in all my changes together in one check-in. A lot of colleagues check in much differently.  They carefully select each file to check in, as if they are a master jeweler selecting only the very best gemstones to set into the royal crown, and they check in each one as a separate check-in.  They rely only on their memory to figure out which files need to be checked in, or especially added to source control. The results are quite predictable... frequent broken builds because they forget to add their new files to source control or forget to check in a changed file (especially changed project files). I have mentioned this to them and they never seem to change.  When I mentioned it to the team lead he said, \"this is just a different way of working\".  To which I may respond: What if I want to drive my car with my eyes closed?  Is that just \"a different way of driving\"? Am I right in being bothered by this practice? ",
        "Best answer": "Don't be bothered by the practice of checking in individual files - if someone can do that and make it work, that's fine. Do be bothered by people checking in broken builds. The primary concern is the result. Addressing the root cause is absolutely a good idea, but the best first step is to find the appropriate motivation to stop checking in broken code. "
    },
    {
        "ID": "15956",
        "Question": "I found that this was the case with Eclipse.  When I first started learning Java a lot of people told me to start with NetBeans because it's easier, comes with a lot of predefined functionality and most of the configuration is done by default. Indeed it seemed like an easy IDE but I just hated the way it looked. The UI to me was horrible. Then I downloaded Eclipse. I was pretty impressed with it. Everything looked shiny, fonts looked awesome and the UI in general was amazing compared to NetBeans'. So I stayed with Eclipse.  Has something like this ever happen to you? Should the UI be of such a relevance to a programmer, is this bad ? The problem of the UI extends to everything for me, not only IDE's (e.g. even on linux I have to customize the WM the way I want before starting working with it).  ",
        "Best answer": "Yes I use VIM because it's beautiful.  Aesthetics mean a lot. If the UI is cluttered and ugly it will impact how you use the tool. NetBeans might do everything, but it looks awful and runs slow. I don't see many people using it. "
    },
    {
        "ID": "15984",
        "Question": "We are building a web service(SOAP, .Net) which would be talking to (mostly) native clients (windows, C++) and we are wondering what is the best way to communicate errors to the client (e.g. SomethingBadHappened like login service not available or something like user not found) and haven't been able to decide between throwing exception to the client or using some kind of error code model to do the above. What you would prefer on the handling on the client side: receiving a error code or handling a ServerFault exception which contains the reason for the error? 1) Why are we thinking exception: Because it would make server side code a lot more uniform 2) Why are we thinking error codes: Because we think it makes more sense from the client side perspective.   If 2) is really true we would probably want to go for error codes than exceptions? Is that the case here? Also, would the answer change if we were talking to managed clients instead of native clients? ",
        "Best answer": "SOAP has a concept of faults, you can convert an exception to a fault on the server side and on the client proxy the fault can again be converted back to an exception. This works remarkably well in WCF and Java metro stack, cannot comment on native C++ clients.   As regards to SOA best practice define one generic fault and few specific faults only if the client need to handle a certain type of error differently. Never send a exception stack trace to client in production deployment. This is because in theory the server trace has no meaning for the client and for security reasons as well. Log the full error and stacktrace on the server and send a unique reference to the log in the fault. In WCF I use the Microsoft Exception Handling block from Enterprise Library to generate a guid and also convert a exception to SOAP fault.   Check the guidance at Microsoft Patterns and Practices. "
    },
    {
        "ID": "16010",
        "Question": "I recently tried to implement a ranking algorithm, AllegSkill, to Python 3. Here's what the maths looks like:   No, really.  This is then what I wrote: t = (µw-µl)/c  # those are used in e = ε/c        # multiple places. σw_new = (σw**2 * (1 - (σw**2)/(c**2)*Wwin(t, e)) + γ**2)**.5  I actually thought it is unfortunate of Python 3 to not accept √ or ² as variable names. >>> √ = lambda x: x**.5   File \"<stdin>\", line 1     √ = lambda x: x**.5       ^ SyntaxError: invalid character in identifier  Am I out of my mind? Should I have resorted for a ASCII only version? Why? Wouldn't an ASCII only version of the above be harder to validate for equivalence with the formulas? Mind you, I understand some Unicode glyphs look very much like each other and some like     ▄ (or is that ▗▖ ) or ╦  just can't make any sense in written code. However, this is hardly the case for Maths or arrow glyphs.  Per request, the ASCII only version would be something along the lines of: winner_sigma_new = ( winner_sigma ** 2 *                     ( 1 -                      ( winner_sigma ** 2 -                        general_uncertainty ** 2                      ) * Wwin(t,e)                     ) + dynamics ** 2                    )**.5  ...per each step of the algorithm. ",
        "Best answer": "I feel that just replacing σ with s or sigma doesn’t make sense and is counter-productive. What’s the potential gain of such a replacement?  Does it improve readability? Nope, not in the slightest. If that were so, the original formula would have undoubtedly used Latin letters also.  Does it improve writability? At first glance, yes. But not really: because this formula is never going to change (well, “never”). There will normally be no need to change the code, nor to extend it using these variables. So writability is really not an issue.   But programming languages have one advantage over conventional mathematical notation: you can use meaningful, expressive identifiers. In mathematics, this isn’t normally the case, so we resort to one-letter variables, occasionally making them Greek. But Greek isn’t the problem. Non-descriptive, one-letter identifiers are. So either keep the original notation … after all, if the programming language does support Unicode in identifiers, there’s no technical barrier. Or use meaningful identifiers. Don’t just replace Greek glyphs with Latin glyphs. Or Arabic ones, or Hindi ones. "
    },
    {
        "ID": "16105",
        "Question": "I am a CS undergrad but I landed a programming job last year and I like it a lot. We are currently 3 programmer in the development section of the company and we have to work with pretty much anything were asked to do. We deal with many different languages and learn them as needed for some quick jobs etc etc.  We want to hire a 4th programmer and I'm asked to suggest some students in my class, a year younger, since I failed a class. I don't really know any of these guys except my teammates which I wouldn't suggest. We don't really want to interview them all so I thought we could make a little challenge to help us choose who to interview. We're in need of someone who understand the business even though they're new to it, and likes to learn new stuff and code. Any idea on a programming challenge or a kind of letter saying why we should take them? TL;DR: We need a new undergrad programmer, we want the best to come to us without interviewing them all. Any challenge or test you could suggest? ",
        "Best answer": "Run them through the Programmer Competency Matrix and see where they fall. Identify problem solvers. People who get 100% on assignments are great, but might not be the most out of the box thinkers. Look for people who ask questions and work around problems without following traditional routes.  We don't really want to interview them all so I thought we could make a little challenge to help us choose who to interview.  This line in particular worries me. You should sit down with every applicant for at least five minutes unless the interaction you have shows such a gross lack of knowledge it would be worthless. You might end up (as mentioned above) with people who are great at finishing specific tasks but lack an overall \"big picture\" view. "
    },
    {
        "ID": "16109",
        "Question": "Would a recruiter prefer a B.A in Comp Science from a more prestigious university or a B.S in Inf Tech from just an average university? Would it matter for someone already established in the field with plenty of experience? The only difference in the B.A and B.S is essentially the mathematics.  The B.S is geared for those going on to master's programs, although one isn't excluded from going on to a masters program with a BA, many do require more mathematics. Just for those curious, the information tech degree is typically geared more to real-world problems then the computer science and the quality of it will vary widely. Some inf tech programs are as in-depth as an ABET C.S degree, the information tech from Drexel looks extremely strong. The Management Information Systems is generally provided via the business school and is more of a traditional business degree. I'm not sure really how well versed the recruiters/g.p are in the actual differences in all of these programs. Some non ABET BS Comp Science may only require 1 or 2 Calculus versus the ABET certified programs. ",
        "Best answer": "Personally, I expect candidates to know C.S. fundamentals, but I don't pay much attention to their degree. I've hired lots of people with degrees in E.E. and other fields; I assess these skills myself. "
    },
    {
        "ID": "16117",
        "Question": "One thing working in Haskell and F# has taught me is that someone in a university smarter than me has probably already found an abstraction for what I'm doing.  Likewise in C# and object-oriented programming, there's probably a library for \"it\", whatever it is that I'm doing. There's such an emphasis on reusing abstractions in programming that I often feel a dilemma between: 1) just coding something short and dirty myself or 2) spending the same time to find someone else's more robust library/solution and just using that. Like recently one of the coders here wrote up a (de)serializer for CSV files, and I couldn't help but think that something like that is probably very easy to find online, if it doesn't already come with the .NET standard APIs.   I don't blame him though, several times working in .NET I've patched together a solution based on what I know, only to realize that there was some method call or object or something  ,often in the same library, that did what I wanted and I just didn't know about it. Is this just a sign of inexperience, or is there always an element of trade-off between writing new and reusing old?  What I hate the most is when I run across a solution that I already knew about and forgot.  I feel like one person just isn't capable of digesting the sheer quantities of code that comes prepackaged with most languages these days. ",
        "Best answer": "First, you need to learn to identify \"components\" that are generic/reusable enough that a library or 3rd party solution is likely to already exist. Once you do that, realize that, even if you are a good developer, the collective experience of countless developers spending countless hours on the same problem is likely to have produced a solution better than you'll ever be able to do. That doesn't mean you should never \"reinvent the wheel\", but if you choose to do so, you better have a DAMN good justification for doing so.  There's such an emphasis on reusing abstractions in programming that I often feel a dilemma between: 1) just coding something short and dirty myself or 2) spending the same time to find someone else's more robust library/solution and just using that.  It's worth mentioning that even if it takes you the same amount of time to find an existing library/solution as it does to write it yourself, remember that doing it yourself also means you'll have to maintain it forever. You're not just re-inventing the wheel, but also the entire pit crew to keep it running. Of course, some libraries are buggy or poorly maintained, but these are things you should keep in mind when picking a 3rd party solution. "
    },
    {
        "ID": "16141",
        "Question": "There seems to be a conflict over whether its better to use whitespace or tokens like brackets to indicate scope. I've seen many praise python's solution to the inconsistent indentation problem, but many disagree:  Any language that has whitespace as tokens needs to die.  posted later on the same answer:  I was sortof anti-whitespace-as-tokens, until I actually tried it. It probably helped that my personal white-space layout pretty much matches what everyone in python-land uses. Perhaps it's that I am a bit minimalist, but if you're going to indent anyways, why bother with the {}s?  I can see some clear arguments for each side: using whitespace:  helps reduce inconsistent indentation in code clears the screen by replace visible tokens with whitespace to serve the same purpose  using tokens:  much easier to cut and paste code to different levels (you don't have to fix the indentation) more consistent. Some text editors display whitespace differently. more popular currently.  Are there any points I missed? Which do you prefer? Any words of wisdom after having worked with one or the other for a long time?  PS. I hate it when languages don't use the same token for each control structure. VB is really annoying with its End If and End While statements, most other languages just use {}'s for everything. But maybe that's a topic for a different question... ",
        "Best answer": "I think a lot of us programmers (myself included) have a tendency to \"logicalize\" every decision.  That's fine, but not every question has a logical answer.  For instance, I doubt that chefs post questions on chefoverflow (if such a thing exists) asking for the pros and cons of apple pie vs cherry pie.  It's a question of which you like better. With that in mind, I think the simplest answer is to say \"Some people like braces, some people like whitespace\" and leave it at that. "
    },
    {
        "ID": "16159",
        "Question": "We're outsourcing some work to an external developer, so I'm busy writing up a contract about what constitutes a deliverable. So far I require that the code is shipped with automated tests.  But, what is a reasonable way to specify the detail of tests up-front in the contract in a measurable way? I'm loathe to say \"100% code coverage\" because it's been established pretty often that 100% is pretty meaningless, and the diminishing returns above about 70-80% would probably just be pushing up our costs unnecessarily, and possibly even pushing up the complexity of certain things that might otherwise be very simple.  Internally we pretty much leave it up to our developers to decide on the level of tests needed, based on their intuition and experience. With a contractor however there is a fixed price that has to be agreed to up front and we need some way to enforce a certain level of quality. Any suggestions or recommended reading matter would be appreciated! ",
        "Best answer": "When subcontracting out, it is up to you to ensure the code being written at least works the way you need it to.  For that reason, your team will need to write some automated acceptance tests.  Provide those tests to your subcontractor, so they can make sure their code works with it. Anytime you require percentage coverage in your unit tests, it is up to you to provide the tool which will be measuring the code coverage.  I don't know the environment you are running (.Net, Java, Ruby, etc.), but there are usually more than one tool available to measure coverage, and they are not all equal.  You also need to specify, or at least agree to the parameters used (i.e. coverage exclusions, type of coverage, etc.). It would be unfair and unproductive to require testing of:  Generated classes/methods (some ORM tools generate classes, .Net UI components generate classes and methods, etc.) System level exception catching code.  The code may be required by the language, and good practice, but if testing it requires hacking the platform itself, it's not worth the investment.  Don't require more of your subcontractors than you would of your own team.  If you are going to require a certain percentage of unit tests as an acceptance criteria, provide a range like 70-80%.  If they beat it, great.  I would consider 50% coverage an absolute minimum, with 70% a decent requirement.  Anything above 70% may cost more, but you'll have better piece of mind about it. Just a note about metrics like test coverage.  They are just numbers, and anyone can play with numbers.  I think your intent is a good one, but anyone who wants to game the system can.  The coverage number is a rough indication of the thoroughness of the testing, but not the quality of the testing.  In my experience, many programmers who are not used to writing unit tests tend to write integration tests, and merely run the application through the test framework without any assertions whatsoever.  Essentially they are just providing themselves a launching point to step through with a debugger.  It takes time and training to get unit tests that are useful. I would require an early initial delivery simply to evaluate the effectiveness of their unit testing, and to help fine tune both your expectations and theirs.  That will help both of you to get on the same page, and make future deliveries better. "
    },
    {
        "ID": "16165",
        "Question": "EDIT: This question at first seems to be bashing Java, and I guess at this point it is a bit. However, the bigger point I am trying to make is why any one single language is chosen as the one end all be all solution to all problems. Java happens to be the one that's used so that's the one I had to beat on here, but I'm not intentionality ripping Java a new one :) I don't like Java in most academic settings. I'm not saying the language itself is bad -- it has several extremely desirable aspects, most importantly the ability to run without recompilation on most any platform. Nothing wrong with using the language for Your Next App ^TM. (Not something I would personally do, but that's more because I have less experience with it, rather than it's design being poor) I think it is a waste that high level CS courses are taught using Java as a language. Too many of my co-students cannot program worth a damn, because they don't know how to work in a non-garbage-collected world. They don't fundamentally understand the machines they are programming for. When someone can work outside of a garbage collected world, they can work inside of one, but not vice versa. GC is a tool, not a crutch. But the way it is used to teach computer science students is a as a crutch. Computer science should not teach an entire suite of courses tailored to a single language. Students leave with the idea that all good design is idiomatic Java design, and that Object Oriented Design is the ONE TRUE WAY THAT IS THE ONLY WAY THINGS CAN BE DONE. Other languages, at least one of them not being a garbage collected language, should be used in teaching, in order to give the graduate a better understanding of the machines. It is an embarrassment that somebody with a PHD in CS from a respected institution cannot program their way out of a paper bag. What's worse, is that when I talk to those CS professors who actually do understand how things operate, they share feelings like this, that we're doing a disservice to our students by doing everything in Java. (Note that the above would be the same if I replaced it with any other language, generally using a single language is the problem, not Java itself) In total, I feel I can no longer respect any kind of degree at all -- when I can't see those around me able to program their way out of fizzbuzz problems. Why/how did it get to be this way? ",
        "Best answer": "This isn't a Java problem, it's a teaching problem. Not knowing how to program is not a languages fault, it's the students fault. Same goes for all your issues, GC, basic machine knowledge, how things work \"under the hood\" so to speak. Your major gripe about garbage collection throws me slightly. Unless you're doing C or C++ garbage collection is typically very good and not an issue. Would you rather they all learn assembly? Higher level languages that are strict are very useful for teaching. It gives you the flexibility of libraries, packages, and other niceties when you need it, without any of the confusing language \"sugar\" present in most other higher level languages (PHP, Ruby, Python, Perl). "
    },
    {
        "ID": "16243",
        "Question": "Anyone who has used R# or CodeRush knows how fast you can put together simple constructs (and refactor complex ones) with a simple keyboard shortcut.  However, do these productivity plugins cause a false evaluation of ability during interviews? Part of being a productive code writer (and making a good first impression in an interview) is writing good code - fast.   If I had two candidates:  Doesn't use plugins.  She thinks about the problem, sits down at a stock IDE at the interview PC that looks exactly like hers and types out the code in a minute or two, as usual.  Done.  Pass. Uses plugins.  He thinks about the problem, sits down at a stock IDE at the interview PC and realizes \"fe + tab\" no longer writes a foreach loop automatically, and all the shortcuts are gone.  He then bumbles around the keyboard hitting his normal hotkeys and popping up strange windows and getting flustered.  It takes him 3 minutes to write what normally would take 30 seconds.  Done.  Looked like they didn't know their way around the IDE at times.  Must be new to this IDE and thus not had much experience with it or maybe the language.  Pass, but a 'meh' mark beside their name.  In your experience, how do you handle plugins during interviews as the interviewer or interviewee?  What are the best practices to getting what the candidate really knows?  There can be candidates who don't understand code, and use R# as a crutch.  There can also be candidates who know the code in and out and use R# because it's just plain faster than the built in VS or Eclipse templates.  Is it best to just not use an IDE at all?  Let them bring their own PC?  Others? ",
        "Best answer": "I was a candidate 2 in an interview very recently. I was given a vanilla install of the IDE on a PC with a non-standard keyboard and unfamiliar testing framework, and I was asked to write a simple Fizz-Buzz app with unit tests. I fluffed it. I must have looked like a complete noob, stumbling around in the dark trying to hack out code. Needless to say, I wasn't offered the position. What I learned is that I rely very heavily on my plugins. They don't just get code typed faster - they actually shape the way I think about code and the way I go about coding. For example, I used to think very carefully about variable names because they could be a pain to change after the fact. Now, in contrast, I just make a half-baked guess about how I'll use the variable, hack out some code, let the variable tell me what it is for, and then hit Refactor->Rename to call it something more appropriate. Does this make me the less capable candidate? In some ways, I think it does. Someone who can write code in Notepad and have it compile and run correctly has certain advantages over someone like me who needs all the IDE goodness he can get. From that point of view, I perfectly understand why any company would choose not to hire a toolhead like me. On the other hand, I'm still a talented and capable Senior Developer. I've learned what works for me, and I practice the sort of laziness that makes me productive, given my own weaknesses and limitations. In short, I'm the kind of programmer that could really benefit a company like the one who turned me away. Interestly, I had another interview a couple of weeks ago. Following my previous experience I made a point of asking about additional tools or budget for buying them. Discovering that there were neither gave me one more reason for turning down to the (rather generous) offer that they made to me. So, to paraphrase Groucho, \"I would not join any company that would have someone like me for an employee.\" Not unless they let me use ReSharper, anyway. "
    },
    {
        "ID": "16284",
        "Question": "I'm learning about the open source web based project management tool Trac and I'm a bit confused about its role in the area of reporting defects vs bug tracking.  For those unfamiliar with Trac itself, it has a Ticket system which is the tool used to report issues, and you can choose between reporting a task, an enhancement, or a defect. It's that last option that I'm confused about.  I get the impression that Trac is not meant to be a bug tracking tool, but if that's the case then what is the defect field for? I looked at the way the Trac team uses their own system, and it does look like they post some things that appear to be bugs, but for the most part it's other kinds of issues, and that actually blurries the distinction for me even further.  Can someone explain to me how the \"defect\" option is to be used? Is defect==bug? Should tickets be opened for bugs?  ",
        "Best answer": "I use it to track features and tasks in addition of defects. I use the Defect option when the item I'm adding is a defect. (I'm using Task option when it's a task, and Enhancement when it's a feature) "
    },
    {
        "ID": "16296",
        "Question": "I have always wondered about this and perhaps experienced folks here can comment on this. I always read that big established software companies such as Google, Microsoft, etc are always wary of hiring software engineers/(or other related professionals) who don't exactly meet the job requirements, and would rather turn down a close \"Yes/Maybe\" decision.  There's also the general perception that start-ups are generally more willing to hire such \"less-than-perfect\" employees. But to me this sounds counter-intuitive: start-ups are generally much smaller and resource constrained so they would actually need a \"10/10\" hire while established companies with bigger teams and more resources would be willing to take a (small) risk on a hire. Anyone have any comments on this from first-hand experience? Thanks, ",
        "Best answer": "Having worked (and recruited for) a few start-ups I have to say my personal experience is that any start-up with their salt should be more fussy about programmers than a large corporate. Paul Graham backs me up nicely in his great How to Make Wealth Essay:  Steve Jobs once said that the success or failure of a startup depends on the first ten employees. I agree. If anything, it's more like the first five. Being small is not, in itself, what makes startups kick butt, but rather that small groups can be select. You don't want small in the sense of a village, but small in the sense of an all-star team.  It is naturally a lot harder for start-ups to find suitable candidates. Pay is lower, risk is higher. But there are a surprising number of people who are attracted to the lack of corporate BS and being able to make a visible difference (early stock options help too.) "
    },
    {
        "ID": "16354",
        "Question": "Planning is very difficult. We are not naturally good at estimating our own future, and many cognitive biases exacerbate the problem. Group planning is even harder. Incomplete information, inconsistent views of a situation, and communication problems compound the difficulty. Agile methods provide one framework for organizing group planning--making planning visible to everyone (user stories), breaking it into smaller chunks (sprints), and providing retrospective analysis so you get better at planning. But finding good tools to support these practices is proving tricky. What software tools do you use to achieve these goals? Why are you using that tool? What successes have you had with a particular tool? ",
        "Best answer": "We use Redmine -> http://www.redmine.org/ We log all our dev in there along with support calls so we can see how much time we have free to allocate to a sprint on our latest bit of development.  It is useful because it ties in nicely with our email system and our Version Control System (Git in our case, but it works with others). Easy to get going out of the box (written in Ruby, will run on most small servers) and with some fairly powerful addons which are easy to install and use. "
    },
    {
        "ID": "16463",
        "Question": "I generally use switch statements to simplify a block of multiple if statements - for example - returning a feedback string to a user based on a multiple choice input.  I also tend to construct classes so that there is one \"management\" method, to avoid sequential steps or chained method invocation within other methods of the class. I've found this helps to keep methods flexible and focussed - i.e. class MyClass{      // this method does nothing more than invoke the relevant method     // depending on the status following the previous. It's entire purpose     // is to control application flow     public function manageFlow($input){         $status = $this->stepOne($input);         if($status == false){             //exit routine         }          $status = $this->stepTwo($input);         if($status == false){             //exit routine        }     }      // this method has several sequential steps implemented direcly within it,      // for example to a user logging in     // it makes it impossible to re-use any of the intermediary steps     public function tangledFlow($input){          if($input == 'something){          //100 lines of code          }          //then handing on to the next bit          if($this->User->Authenticated){          //another 100 lines...          }     } }  Then it occurred to me that I could use a switch statement to control this kind of sequential execution - so my question is: has anyone used a switch statement for this kind of flow control? ",
        "Best answer": "What you're describing is a state machine, and yes, many people do use switch statements to implement those.  They're usually cleaner, but it can be more difficult to follow the logic flow. "
    },
    {
        "ID": "16544",
        "Question": "What build or deployment tools are available for a 1 person development effort, in the .NET space, that are capable of producing project outputs? I'm not looking necessarily looking for a CI server (though I can't think of anything else that does what I'm looking for) but I am looking for it to:  produce and publish documentation from xml comments produce and publish the project (web and/or clickonce app) handle basic versioning (automatic build number incrementing) work from a sln file be easy to setup (< 8-16 hrs for someone who knows little to nothing about the tool(s)) do this at the push of a button (after configuration obviously)  Things I don't need:  source control integration : I can point it to a sln if need be. Not a huge deal. unit testing : I run test suite before commits static analysis : again, I also run these before commits  I know that msbuild is capable of most or all of this, and I do have my msbuild book(s) with me, but I'm still very new to it and I don't have the time at the moment to learn it well enough to do what I want. ",
        "Best answer": "TeamCity is a CI server that does what you need (and a few other things too). Automatic assembly versioning requires some tricks (I can go into more details here if needed), but it will version build artifacts that it produces. Setting it up is very easy and there is a free version that supports up to 20 build configurations, which should be sufficient for a single developer. Edit: come to think of it, I'm not sure about setting up XML doc generation with TeamCity. But if it's something that can run as part of an MSBuild project, TeamCity will run it, I'm pretty sure. "
    },
    {
        "ID": "16638",
        "Question": "I am trying to learn more about the various ways that the problem of a Distributed Queue may be solved. So I would like know what products, services, implementations and research papers that are already out there. An implementation will face many challenges and will be forced to make tradeoffs:  Does it have strong or loose ordering? Does it have idempotent put? Can we have more queues than what can fit on a single machine? Can we have more data in a queue than what can fit on a single machine? How many machines can crash before we potentially lose data? Can it tolerate net-splits? Can it automatically reconcile data when a net-split is fixed? Can it guarantee delivery when clients can crash? Can it guarantee that the same message is not delivered more than once? Can a node crash at any given point, come back up, and not send out junk? Can you add nodes to, or remove nodes from, a running cluster without down time? Can you upgrade nodes in a running cluster without down time? Can it run without problems on heterogeneous servers? Can you “stick” queues to a group of servers? (example: “these queues are only allowed in the european datacenter”) Can it make sure to put data replicas in at least two datacenters, if so available?  I have no illusion that any implementation will be able to say “yes” to all of that. I am merely interested in hearing about the various implementations; how they work, what tradeoffs they have made and perhaps why they decided on their particular set of tradeoffs. Also if there are any challenges that I may have missed in the above list. ",
        "Best answer": "Writing a basic queuing system is fairly simple, but as you noted above with all of the challenges, doing it right is another matter.  I've used home grown systems for which I wrote the source code, 3rd party systems, and various JMS providers.  JMS (Java Messaging Service) by far is the most complete solution I've encountered thus far.  Much of what you ask is available in JMS.  My favorite JMS provider is ActiveMQ.  Free, performant, easy to install, and more importantly easy to embed in my app with Spring.  JMS providers don't provide everything you asked for out of the box, but they provide a set of tools to handle much of what you asked about should your application need it.  I haven't found lots of applications need everything you listed.  Ordering might not be important (it's best if it isn't), durable topics might not be important, guaranteed delivery, etc.  You just have to stick to the problem and use what it demands. http://activemq.apache.org/what-open-source-integration-solution-works-best-with-activemq-.html Does it have strong or lose ordering? Yes.  It has both depending on your programs needs.  Here are the details: http://activemq.apache.org/total-ordering.html. Does it have idempotent put? No, but this is trivial to implement in your application layer should you need that. Can we have more queues than what can fit on a single machine? Yes.  You can have clustered servers, and if you wanted to setup multiple machines with different queues you could, and pull from either. Can we have more data in a queue than what can fit on a single machine? Yes most JMS providers have to use some sort of DB/persistent storage to ensure messages aren't dropped or lost if the JMS provider goes down. How many machines can crash before we potentially lose data?  This is a little harder to answer because it's timing related.  However, you can crash a JMS provider and provided the disk isn't corrupt it will come back up and start where it received the last commit.  This means messages could be delivered twice, but if you code your app to handle this it's not a problem.  As long as you have at least one of each type (producers, consumers, or JMS servers) it will complete. You can also have load/balance/failover for redundancy should a disk go out on you. Can it tollerate net-splits? I think I understand what you mean by \"net-split\", but I'm not entirely sure.  I guess you mean if the JMS servers are clustered, and we loose connection with one of the servers will it jump to another server and pickup where it left off.  Yes, but again these types of situations can lead to duplicate messages depending on at what point the client lost connection. Can it automatically reconcile data when a net-split is fixed? If you are using transacted sessions it will only redeliver any message that has had a commit called on it to existing clients that are up. Can it guarantee delivery when clients can crash? Yes this is one of the main goals of JMS.  Guaranteed delivery means that if a message is queued it's guaranteed to be handled by a client. Can it guarantee that the same message is not delivered more than once? Yes if the transacted sessions are being used.  That means a client has accepted the message and called commit/rollback.  Once the commit is called it won't redeliver the message. Can a node crash at any given point, come back up, and not send out junk? In the case where you have durable clustered queues.  Yes it won't spew \"junk\" if the other node in the cluster has delivered the message.  It can still redeliver anything that hasn't been acknowledged. Can you add nodes to, or remove nodes from, a running cluster without down time?  Yes. Can you upgrade nodes in a running cluster without down time? This is a little trickier for me to answer, but I believe that yes you can do this. Can it run without problems on heterogeneous servers? What does this mean exactly?  I've found most JMS providers are very easy to run in environments using different hardware, OS, etc.  Although, if you mean performance, that's a whole another thing.  Any distributed processing system can be negatively impacted by a slow node.  I had 2 8 Core Intel servers running the queue and the consumers.  That's 16 cores together, and I got better performance from using only those two boxes, than when I added a single core machine as a consumer.  That single core machine was so much slower it slowed down the entire grid by a factor of 2x.  This had nothing to do with JMS per se. Can you “stick” queues to a group of servers? Short answer yes.  I can think of a way where you can run a cluster that's only in the european data center, and configure the queue there.  Then in your spring config setup your consumers to consume that queue as well as other queues on other clusters.  You might want to consult the docs: http://activemq.apache.org/clustering.html Can it make sure to put data replicas in at least two datacenters, if so available? Again I believe so, but it's best to consult the clustering docs. Again JMS has lots of options you can tweak as your need dictates.  Using transacted sessions and durable queues comes with a performance cost.  I've seen turning on all the bells and whistles impact performance as much as 10x.  When I used JBossMQ if we turned off some of these features we could get around 10,000 messages/s, but turning them on brought us down to 1000 messages/s.  Big drop. "
    },
    {
        "ID": "16646",
        "Question": "I've always been of the mindset that properties (ie, their set/get operations) should be fast/immediate and failure-free. You should never have to try/catch around getting or setting a property. But I'm looking at some ways to apply role-based security on the properties of some objects. For instance an Employee.Salary property. Some of the solutions I've run across that others have tried (one in particular is the AOP example here) involve throwing an exception if the accessor doesn't have the right permissions - but this goes against a personal rule that I've had for a long time now. So I ask: am I wrong? Have things changed? Has it been accepted that properties should be able to throw exceptions? ",
        "Best answer": "when you set the value of a property, throwing an exception on an invalid value is fine getting the value of a property should (almost) never throw an exception for role-based access, use different/dumber interfaces or facades; don't let people see things they can't have! "
    },
    {
        "ID": "16647",
        "Question": "A couple of friends at work an me are going to set up a little startup / create our own software, probably moonlighting at first, since we cant yet afford to quit our day jobs. Neither of us has have this experience, we've all worked for other companies before, where a set of guidelines are set, and I think this is the time to establish good practices to follow (like avoiding meeting-itis). For people that have gone this way, what piece(s) of advice would you give us? I'm looking more for the technical side of things, things such as:  Is it worth to have some kind of build server or is that going to far ahead?  Would you do extensive TDD or do you think it would be too much overhead for a small team that is not too experienced with it?  But wouldn't mind to listen to the management side of things.  The project is a web application done in ASP.NET MVC, I'm thinking of using Mercurial and BitBucket or Kiln + FogBugz or some other online project tracking tool, since we are going to be working remotely. ",
        "Best answer": " Release as fast as possible. Chances are 90% of the code you start with will not make it past the first 6 months. So there's no point in engineering it like crazy. Code as quickly as possible to get to market, then let your users decide how to develop it further. If TDD is how you code quickest, use TDD. Otherwise, just hack it. Early-adopter users are pretty forgiving of a few bugs when your product is in beta.  Don't waste your time being sys admins. You've got the right idea with hosted platforms for bug-tracking (e.g. FogBugz) and source control. Use an online document repository such as Google Docs. If you do store anything locally, use an online cloud backup service such as Carbonite. On your live environment, rent a fully managed hosting solution if you can afford it. Try to tend away from having to maintain your own servers. Concentrate on what makes you unique. If you find yourself writing code that seems like it must've been done before, use what's already there. Become experts at solving your business problem and don't get distracted by problems outside your domain.  "
    },
    {
        "ID": "16687",
        "Question": "Situation is I have a number of in-house clients who continue to have trouble grasping the concept of defining clear requirements for a project and committing to them. Unfortunately my job situation is that I can't decline such projects, and I am often forced into situations that I can tell are doomed to scope creep (or worse) from the very start. Telling them that detailed requirements are vital and why we need them (using examples of past troubled projects due to this) have not helped.  I am looking for techniques, tips, words, etc that you might use to motivate or illuminate clients in such situations to the necessity of requirements,setting milestones, etc. I am only a developer, and not a manager. ",
        "Best answer": "Draw the Time-Quality-Cost triangle on their whiteboard: .  Explain the trade-off: It's physically impossible to satisfy all 3 properties. They want good quality in a short time? It's going to cost them dearly. No time, no money? Then they're going to see it in the shoddy quality. High quality, low cost? That'll be ready whenever it's ready!!  This doesn't exactly address requirements and milestones, but it may plant some kind of seed that says, \"Hey, if I leave stuff too late I'm going to end up paying more or get worse quality.\" "
    },
    {
        "ID": "16689",
        "Question": "Today, my manager told me that I must work over time to make up for a lack of planning and project management. Instead of incentivizing this unpaid mandatory overtime, my manager has made it clear I have no choice in the matter. This isn't a single project. We currently have a dozen projects all on the go at one time, and I simply can't get them all done. Therefore, I must work overtime so we don't have to push deadlines.  Is this a sign of an ignorant or disrespectful manager, or simply an inexperienced one? I'm in this position because of a lack of planning and project management (I think). How might I avoid this in the future? I'm no Project Manager, it isn't my strength. What are good ways to get an employee to work overtime if you can't directly pay them? Good incentives, etc.  From what I hear, gaining your employee's respect is the single best way to get your employees to work over time, although you should never make a habit of it. ",
        "Best answer": " My manager told me that I must work over time to make up for a lack of planning and project management. Instead of incentivizing this unpaid mandatory overtime, my manager has made it clear I have no choice in the matter.   This is a clear sign of a death march. I strongly recommend the book Death March. It will give you ideas of how to deal and cope with death marches as well as helping you decide if and when it is time to quit. Sadly, death marches are the norm in software development, and not quite the flaming emergency they are made out to be.  An article written some years ago pointed out why other industries got rid of \"crunch mode\" (or \"death marches\") - they were the worst way to get work done.    As a side question, what are good ways to get an employee to work overtime if you can't directly pay them?   Again, I refer you to the book Death March. Some organizations (notably big-4/3/2/1 accounting and consulting firms) use a \"Marine Corps\" mentality: \"Sleep is for sissies! There will be time to sleep when we are dead!\" The movie 300 has some entertaining examples of this sort of mentality. There are other methods for motivating (or trying to) workers in death marches.  If this is a one-time screw up by your mismanager, then probably the only thing to do is suck it up and get to work. If this happens all the time, then it is his/her/its incompetence at work and things need to change. A useful quote to remember comes from the movie Goldfinger:    Once is happenstance. Twice is coincidence. The third time it's enemy action.   "
    },
    {
        "ID": "16732",
        "Question": "To what extent do you unit test internal/private components of a class/module/package/etc? Do you test them at all or do you just test the interface to the outside world? An example of these internal is private methods. As an example, imagine a recursive descent parser, which has several internal procedures (functions/methods) called from one central procedure. The only interface to the outside world is the central procedure, which takes a string and returns the parsed information. The other procedures parse different parts of the string, and they are called either from the central procedure or other procedures. Naturally, you should test the external interface by calling it with sample strings and comparing it with hand-parsed output. But what about the other procedures? Would you test them individually to check that they parse their substrings correctly? I can think of a few arguments: Pros:  More testing is always better, and this can help increase code coverage Some internal components might be hard to give specific inputs (edge cases for example) by giving input to the external interface Clearer testing. If an internal component has a (fixed) bug, a test case for that component makes it clear that the bug was in that specific component  Cons:  Refactoring becomes too painful and time-consuming. To change anything, you need to rewrite the unit tests, even if the users of the external interface are not affected Some languages and testing frameworks don't allow it  What are your opinions? ",
        "Best answer": "Case: a \"module\" (in a broad sence, i.e. something having a public interface and possibly also some private inner parts) has some complicated / involved logic inside it. Testing just the module interface will be sort of an integration testing with relation to the module's inner structure, and thus in case a error is found such testing will not localize the exact inner part / component that is responsible for the failure. Solution: turn the complicated inner parts into modules themselves, unit-test them (and repeat these steps for them if they are too complicated themselves) and import into your original module. Now you have just a set of modules simple enough to uniit-test (both check that behavior is correct and fix errors) easily, and that's all. Note:   there will be no need to change anything in tests of the module's (former) \"sub-modules\" when changing the module's contract, unless the \"sub-module\"'s no more offer services sufficient to fulfil the new/changed contract. nothing will be needlessly made public i.e. the module's contract will be kept and the encapsulation maintained.  [Update] To test some smart internal logic in cases when it is difficult to put object's inner parts (I mean members not the privately imported modules / packages) into appropriate state with just feeding it inputs via the object's public interface:  just have some testing code with friend (in C++ terms) or package (Java) access to the innards actually setting the state from inside and testing the behavior as you'd like.   this will not break the encapsulation again while providing easy direct access to the internals for testing purposes -- just run the tests as a \"black box\" and compile them out in release builds.   "
    },
    {
        "ID": "16755",
        "Question": "Our software dev team is currently using kanban for our development lifecycles, and, from the reasonably short experience of a few months, I think it's going quite well (certainly compared to a few months ago when we didn't really have a methodology). Our team, however, is directed to do work defined by project managers (not software project managers, just general business), and they're using the PMBOK methodology.  Question is, how does a traditional methodology like PMBOK, Prince2 etc fit with a lean software development methodology like kanban or scrum? Is it just wasting everyone's time as all the requirements are effectively drawn up to start with (although inevitably changed along the way)? ",
        "Best answer": "I'm not a massive expert in this area. But I think that you can try two things here: 1 Use a Kanban board to visualise the PMBOK/Prince2 process that's being used.  Simply show the Project Managers this 'visualisation aid', for me that's what Kanban is for - showing you where you have bottlenecks and blockers. Assuming you get the usual waterfall style problems, you'll be able to show this on your Kanban board as the workflow gets slowed or blocked at various points.  You can then use this as a starting point to get them to come around to agreeing to letting you run the project in a more agile manner. And/Or 2 Take the requirements and split it into a Kanban style project anyhow, but do some analysis/preparation up front on how you can report progress back to the Project Managers in a Prince2/PMBOK manner (that will be comfortable to them).  You might find that the PM's don't care how you run the project, as long as the reporting meets their needs. "
    },
    {
        "ID": "16779",
        "Question": "As a software developer, I've worked on projects ranging from tiny home-made apps to medium-size enterprise applications. In nearly every project I used a database or regretted choosing not to use it from the beginning. Now, I am wondering a few things about the databases and their usage in general applications:  Why Windows itself doesn't use any \"central\" SQL database? For example:  Errors Reporting data is stored in a bunch of files, Windows Update stores everything in flat files, Icons cache is stored in a very strange single file which doesn't seem to be accessed through SQL, etc.  Why so many large applications avoid using databases? For example, wouldn't Microsoft Outlook gain by using a real database instead of reinventing the wheel by having its own format for .pst files and storing some data in registry?  If database adds an additional layer of an overall complexity and a tiny performance loss, it is a price of a huge advantage of making the code simpler in most circumstances, especially when it comes to the storage of small organized chunks of data instead of large binary streams. So why so few products are actually using databases? Probably the only application I know which actually uses Sqlite database is Firefox, and maybe Microsoft Exchange (but the last one is not a desktop application)? Also, wouldn't a set of applications, like Microsoft Office or Microsoft Expression, benefit from having an unified SQL database, making it easier to deploy the applications, to update/upgrade the data, to share data between those applications, to make backups, etc.? ",
        "Best answer": "Many applications embed SQLite. Quoting from their website:   SQLite is a software library that implements a self-contained, serverless,  zero-configuration, transactional SQL database engine. SQLite is the most widely deployed SQL database engine in the world. The source code for SQLite is in the public domain.\".   You just use one of the available APIs, create a database, tables, etc and the SQL engine will store everything in a binary file that you can put anywhere within the user's file system. Best of all, it's FLOSS.  "
    },
    {
        "ID": "16836",
        "Question": "I think the world now programs in English-based programming languages not only because of historical/economic circumstances, but because the English morphology in particular has some properties that suit algorithmic thinking best. But anyway it would be interesting to hear your opinions on this, especially if you are multilingual yourself. I've seen some mentioning of German-based languages (see Plankalkul for example, in fact the first ever programming language we know very little about, thanks to WW2), also a Russian-based flavor of Algol which existed back in the 80's at least on paper, not sure if it ever existed in binary or not. Both looked a bit sluggish because there were more shortened words and weird abbreviations than full words like in the EN-based languages. So if you know of any other national language-based PL's, even completely archaic and irrelevant today, purely theoretical or whatever, would be interesting to take a look at them. And back to the main question: so what, if any, makes the Shakespeare's language so good for programming? (There is actually a list of Non-English-based programming languages on Wikipedia (of course, where else?), but it would be interesting to hear opinions of native speakers of those languages on how a given \"national\" programming languages really feels like.) ",
        "Best answer": "Disclaimer: My native language is German. I don't think there is any reason English as a language to take keywords from would be better than any other natural language. I do think it's the one all-important language in IT, but not because of linguistic properties, but because most tech people speak it to some degree, it's the native tounge of quite a few important people in the field, most tech-related terms are already English, etc. But since we talk about programming languages, not about documentation/API/names/etc, I have to object: Programming languages are not based on English - or on any other natural language, for that matter. Programming languages are formal languages. They do use, to varying degree, a handful of words from (usually) English. Some even try to mimic its grammar -- but utterly fail to read like English regardless. To add insult to injury, they only associate one single (in rare cases a handful of) meaning(s) with each word they borrow. Often, this meaning is very jargon-y, specialized, or based on a questionable analogy. Therefore, knowing the myriad natural-language meanings of a word borrowed by programming language doesn't really help understanding the programming concept behind the keyword. Examples off the top of my head: array, type, goto, class, void. (Fun fact that sprung to mind as I re-read the question: All of these, except goto, have German translations which are at most one character longer: Feld, Typ, Klasse, Leere. They all sound weird to me, but that's probably a matter of habit.) "
    },
    {
        "ID": "16867",
        "Question": "I'm in the throes of getting myself enrolled in school to get a CS degree. The school I am looking at actually offers both Java- and C++-based for the introductory software development courses (object-oriented programming, design patterns, that sort of thing). It is student's choice which track to follow, but there is only time to follow one. Knowing what you know now, if you had the choice, would you lay down your CS curriculum foundation in Java or C++? My current debate currently looks like this: A good friend (who has a PhD in AI) is touting Java as the better choice regardless of what I do, if only to open up more job opportunities later, though he might be biased since all of his work has been in Java (he loves it). I live in the Boston, MA, USA area and I see an equal amount of Java and C work.  On the flip side, although I haven't entirely yet settled on what I want to do with the degree when I'm done, my preference would be to develop for the Mac, which I am doing now albeit in a limited capacity. To that end, I'm getting some limited exposure to C++ already, but I've had none with Java, and looking at my projects at my day job I don't see a need to use it anytime soon, \"soon\" measured by at least two years. I probably should note that I'm an adult going back to school after 20 years (I currently have no degree of any kind) so I'm looking to maximize the opportunity and time spent as best I can. I'm kind of leaning towards C++ but I'm still ambivalent, and some outside, objective advice would help here. Or I could just be thinking too hard about it. UPDATE: It turns out the language selection wasn't so clear cut as I originally surmised. While a couple of core courses focused on Java, some of the other core courses work in primarily C and Java, but also a few others thrown in for good measure. In fact, my rest of my semester is going to be in Objective-C after spending time in Java and Javascript. Last semester was C, Javascript, and PHP, plus a few others thrown in as assignments required. Since things were pretty much split down the middle overall, and I am still getting answers to this, I am now trying to work my curriculum such that I meet all of the requirements for the degree but to absorb as many languages as I can reasonably handle. So far, my grades have not suffered trying to do this. ",
        "Best answer": "I'd personally go with C++ as it will give you insights into how parts of Java work under the hood (Pointers for example).  Moving to Java from C++ is fairly trivial, whereas moving the other way around is arguably more difficult. The truly difficult thing about the Java eco-system is it's vast number of frameworks, libraries etc - they're unlikely to cover all of that at University anyhow. At the end of the day it's not going to matter that much what language you choose, as long as you learn the principles. My JUG is going to kill me for endorsing C++ ;-) "
    },
    {
        "ID": "16908",
        "Question": "This is one of the things that I hate most when I see it in someone else's code. I know what it means and why some people do it this way (\"what if I accidentally put '=' instead?\"). For me it's very much like when a child goes down the stairs counting the steps out loud. Anyway, here are my arguments against it:  It disrupts the natural flow of reading the program code. We, humans, say \"if value is zero\" and not \"if zero is value\". Modern compilers warn you when you have an assignment in your condition, or actually if your condition consists of just that assignment, which, yes, looks suspicious anyway You shouldn't forget to put double '=' when you are comparing values if you are a programmer. You may as well forget to put \"!\" when testing non-equality.  ",
        "Best answer": "Ah, yes, \"Yoda conditionals\"  (\"If zero the value is, execute this code you must!\").  I always point anyone who claims they're \"better\" at tools like lint(1).  This particular problem has been solved since the late 70s.  Most modern languages won't even compile an expression like if(x = 10), as they refuse to coerce the result of the assignment to a boolean. As others have said, it certainly isn't a problem, but it does provoke a bit of cognitive dissonance. "
    },
    {
        "ID": "16913",
        "Question": "Before my current job, I was always involved in the technical aspects of a project like:  architecture design performance security etc  Now I'm team lead of a project that's a game on a web site (not mine) and somehow got involved in the business side of the project:  what users expect ads showing in which pages of the site mechanics of the game etc  But I quite don't agree with business people's (customer or product owner if you like) decisions of the directions of the site. Of course I raise my concerns, some of them are taken into account, most of them aren't. I continue my work as usual as I like working here but I feel like the product would be better than what it is now. I think that's because my goal is to make an interesting and challenging game, and theirs is to attract as many people and earn money as possible (it's a paid game). Have you guys ever happened to be in this kind of situation? What are your experiences? ",
        "Best answer": "In many projects getting involved in the business side of things is not only inevitable, but good.  If you understand the owner's reasons for wanting things you can better react to/suggest/repair/anticipate features you need to work on. Specifically to your final question: I have, and most of the time you loose more of these than you win until you start to get some positive history on being right with your suggestions. One thing that sometimes has helped me is to make a demo with things my way and show it to the users as an alternative or conversely make a demo of enough of their way to point out the problems. In most enterprise situations it is your job to bring the things up, but ultimately their decision if they think that their crazy is better. When they choose the crazy the best thing you can do is make certain you have a clear exit strategy back to sane worked out when they realize their error. If the crazy starts to get the better of you consider a different employer... "
    },
    {
        "ID": "16975",
        "Question": "I've worked in a number of small teams over the last 9 years. Each had the obvious good practices, such as short meetings, revision control, continuous integration software, issue tracking and so on. In these 9 years, I've never heard much about development methodologies; for example, there's never been a \"we're doing scrum\", or \"lets do agile\", or anything more than a passing reference. All of the teams seemed to function fine, without following much process, we were just freeflowing and just naturally worked well. Has anyone else progressed for long periods of time without encountering scrum/agile/etc? The only exposure I've had to these is through sites like this one. I read questions like Sprint Meetings - What to talk about ... and all the talk seems to describe almost robotic like people who follow a methodology finite state machine. Is it really (though exaggerated) like that? I wonder if the  people posting on the internet just loud supporters of \"best practice\", with similar textbook views, not really reflecting how people work... Or that I've encountered some teams making their processes up naturally. Furthermore (I am in the UK, which may be relevant)... I think if a methodology were introduced to any of the teams I'd work on, they'd just reject it as being silly and unnecessary... then carry on. I'd tend to agree, following processes seems a bit unnatural. Is this typical or common? ",
        "Best answer": "Over 20 years of development experience here, and I've never used a formal methodology.  Never needed them, and I dont plan on using one in the future.  Methodologies might be fine for some people, but they are no substitute for skilled programmers who write good, tested code. Personally, I think it would behoove a lot of people to care less about following the day's hottest new methodology, and focus more on code quality. "
    },
    {
        "ID": "17005",
        "Question": "Many times I have witnessed a sad tragedy.  Here's what happens:  A team design review for a new project. I see a simple design that has quite a few holes. I casually mention the holes and ways to avoid them. The warnings are ignored with comments like \"that 'never' happen in real life\" Eventually the things that \"will 'never' happen\" happen An emergency team design review for a broken project.  So what do I do?  Copping the \"I told you so\" attitude is not going to win friends and influence people.  Sometimes years go by and the comments from step 3 are forgotten anyway.  I definitely don't want to be the annoying pest reminding the world of the gotchas.  I often sit back and watch the Titanic sail off to Europe. It's frustrating to see bad designs move forward.  It's also frustrating that I can't seem to convince others of the pending peril of the current path.  I do worst on team meetings where everyone has different ways of understanding different terms.  Also, egos tend to win of reason and thought.  I'm looking for good tactics to convince groups people to use some new and complicated ideas. ",
        "Best answer": "Try to build a reputation as being the person who can identify what will work and not just what you think will have a problem in some rare use case. When you see these potential problems, just consider them a footnote that may need to be addressed later. People get crazy in congregations. Mention your concerns to a key person outside of the meeting. They will see this as less threatening and may take the time to hear your argument instead of thinking about their defending the design. They may also take more time to explain to you the circumstances why you may have a valid point, but it is not feasible to address in v1.0. Key! Go into the meeting with a complete understanding of what your direct supervisor's agenda is. Maybe they see this as a minor project and the last thing they need at the meeting is a naysayer taking time away from more important issues. Ask them to help you help them. "
    },
    {
        "ID": "17079",
        "Question": "Lately we had a project, in which client was busy touring. As usual scrum team was formed, management decided to appoint our analyst as Product owner since Client won’t be able to participate actively. Analyst was the one who worked closely with client for requirement analysis and specification drafting. Client doesn’t have the time to review first two releases. Everything went smoothly until, client saw third release; he wasn’t satisfied with some functionalities, and those was introduced by make shift Product Owner (our analyst).   We were told to wait till design team finished mock-up of all pages and client checked each one and approved to continue working. Scrum team is there, but no sprints – we finished work almost like classic waterfall method.   Is it a good idea to appoint scrum team member or master as product owner? Do we need to follow scrum in the absence of client/product owner participation? ",
        "Best answer": "It was only a few weeks ago that Mike Cohn wrote about combining scrum master and product owner roles on his blog.  I don't think I can put it any better than he did, but my short summary of his post is this:  it's a bad idea SM and PO perform very different kinds of tasks (\"star tasks\" and \"guardian tasks\" in Cohn's words) the person combining the two roles is unlikely to be a good fit for all tasks involved in both roles the team may be hurt by the combined SM/PO neglecting the tasks they are not the best at.  I think there is nothing wrong per se with taking any member of a scrum team and moving him/her to Product Owner.  But you have to realize that it's like a promotion or an internal transfer; it creates a hole in the team and the hole needs to be filled.  Maybe the team can \"self-reorganize\" to fill the hole; maybe it needs to hire a new employee to fill the vacant position. "
    },
    {
        "ID": "17111",
        "Question": "We are starting the process of internationalizing our software. To do this we will be writing a series of filters to convert all of our different resource files (resx, text, java resource, etc) into a common format. We will then be outputting xliff files for translation. After translation the process will run in reverse. We would like to keep the central store updated with any changes that developers make to the resource files ideally using ant during the build process, maintaining knowledge of which branch or version of the software is using which resources. We'd also like to keep track of which version of software the resources were taken from which are sent for translation, the dates when the files were sent and received back and also who translated them. The idea then being we apply the translations to the correct branch, we can take diffs of the central store in future to send partial resources for translations and we can report on quantity of translation at a point in time. We are considering storing all of this data within the svn repository as a tree of xliffs, perhaps branched in a mirror of the main code repository. Is this sufficient or would we be better off just using a database to maintain this information? ",
        "Best answer": "I've found SVN (or other good source control system with branching and diff tools) to be really good for this (oh and don't forget the power of patch files).  For example, we use it to compare and contrast Jboss Application server configuration files across the various versions they release as well as our modifications.  Using branches and diff we can pretty quickly figure out that X, Y and Z changed in this point release and we can patch on our regular A & B patches. But do take the time up front to design your branches and your workflow.  Test it out with small examples first, we had to go through a few iterations before we found a logical way of working. "
    },
    {
        "ID": "17121",
        "Question": "Two examples spring to mind:  One of the reasons that .Net programmers are encouraged to use .config files instead of the Windows Registry is that .config files are XML and therefore human-readable. Similarly, JSON is sometimes considered human-readable compared with a proprietary format.  Are human-readable formats actually readable by humans? In the example of configuration data:  The format doesn't change the underlying meaning of the information - in both cases, the data represents the same thing. Both registry and .config file are stored internally as a series 0s and 1s. To that extent, the underlying representaion is equally unreadable by humans. Both registry and .config file require a tool to read, format and display those 0s and 1s and convert them into a format that humans can read. In the case of configuration stored in the Windows Registry, this is a Registry Editor. In the case of XML it could be a text editor or XML reader. Either way, the tool makes the data readable, not the data format.  So, what is the difference between human-readable data formats and non-human-readable formats? ",
        "Best answer": "Human Readable means I can open the thing in Notepad if I want to, and change \"password=foo\" to \"password=bar\" if I so desire. I don't have to use a proprietary tool to look at or edit the content. Contrast to a PDF which you cannot edit with a simple text editor - you need a specific tool that knows the format. Or a binary .dat file that came with some application from 25 years ago that you can't read, edit, or understand. "
    },
    {
        "ID": "17151",
        "Question": "The best way to get data on computer language popularity that I know is the TIOBE index.  But everyone knows that TIOBE is hopelessly flawed.  (If someone provides a link to support this, I'll add it here.) So is there any data on programming language popularity that is generally considered meaningful?  The only other option I know is to look at the trends at indeed.com, which is inherently flawed, being based on job postings. It isn't like I would make a future language decision solely based on an index, but it might provide a useful balance to the skewed perspective one obtains by talking to ones friends and colleagues. To illustrate that bias, I'll point out that based on the experience of those I personally know, the only languages used professionally today (in order of popularity) are Java, C#, Groovy, JavaScript, Ruby, Objective C, and Perl.  (Though it is evident that C, C++ and PHP were used in the past.) So my question is, everyone bashes TIOBE, but is there anything else?  If so, can anyone explain how we know the alternative has better methodology?  Thanks. ",
        "Best answer": "One alternative would be to look at the number of questions for each programming language tag on StackOverflow. I set up a Quick Query to do that: Post Counts By Tag Unurprisingly, given the nature of that community,  C# has a considerable lead. Of course this also is biased against languages that because of ease of use might have fewer people asking questions. "
    },
    {
        "ID": "17173",
        "Question": "I have a common scenario where I have an object (a) that logically affects another object (b), like a makes a note on b, or a marks b as disabled (usually a is a user object of the system and b is some kind of business object like a customer or an appointment). In the past, the method was on the affected object like: customer.Disable(user); // user is marking the customer as disabled  ... but it doesn't read well (it appears backwards). The opposite: user.Disable(customer);  ... reads better, but there's something about it that I can't put my finger on that just doesn't seem right. The only other option is to have a static intermediary: ThirdPartyClass.DisableCustomer(customer, user);  ... but the domain objects end up becoming more like DTOs and I prefer the domain model approach because it makes more sense. So the question is: which makes the most sense? Is there an option I haven't considered? Who actually owns an operation like this? Edit With the second option, the actionable objects in the system (users usually) end up becoming huge because the users of the system are the ones that pretty much do and affect everything. ",
        "Best answer": "How about customer.SetDisabledBy(user);  or if using C# 4.0 or a different language with similar capabilities: customer.SetDisabled(by: user);  or if you're using C# 3.5 or newer, you can write the following: user.DisableCustomer(customer);  while having the DisableCustomer method be an extension method sitting in a class called CustomerActions which sits together with the Customer class. That way, when you're using the Customer namespace, User will have that extension method. When you're not using it, it's gone. User is no longer a god object but the intermediary class is nowhere to be found in the code. You might also have an OrderActions class which will provide user.CancelOrder(order), etc. Good question, made me think! "
    },
    {
        "ID": "17183",
        "Question": "It seems like there are a plethora of methods for dialing in and checking out what's going on on a client's PC.  But they seem to boil down to just 2. Dialing in and checking it out with VNC and Remote Desktop. VNC seems abysmal with more graphics laden OS's and it sometimes is too much to ask to get a access to a terminal server (especially when the client doesn't have one). What other methods are out there for connecting with clients to diagnose and fix bugs they they can reproduce, but may not necessarily be able to adequately describe over the phone.  Alternatives to VNC and Remote Desktop are appreciated, but really looking for a better way to do it than just to dial in and check it out - or the most effective way to use VNc and Remote Desktop. ",
        "Best answer": "We have had good success teaching end users to respond to unhandled exceptions (which are thankfully rare, but which plagued one installation because of networking issues) with these steps (Windows only)  press Shift+PrntScrn Open Word press Ctrl+V Return to the application and click the More Info button Click in all the words that don't make any sense, and press Ctrl+A Ctrl+C Return to Word Click below the picture and press Ctrl+V Save the file and email it to me.  This lets you see the exception, what they were doing in the background, what data they were looking at (employee id or store id or whatever) and other useful information. I have debugged a lot of things after the fact armed with these screenshots and the stack trace. The big difference between this and GoToMeeting, WebEx etc (which we use with these clients for demos and training) is that they don't need to repro the bug on demand. When they are working away and the bad thing happens, they follow these steps and we make it all better. (Sometimes by calling their IT rather than changing our code, but honestly the end users don't care.) The impact on them is minimal. If I needed to, I might also teach them Problem Steps Recorder on Windows 7 which is way cool if you've never used it. But the screenshot and stack track approach carries a LOT of information. "
    },
    {
        "ID": "17214",
        "Question": "There are lots of books about programming out there, and it seems Code Complete is pretty much at the top of most people's list of \"must-read programming books\", but what about The Art of Computer Programming by Donald Knuth?  I'm a busy person, between work and a young family I don't have a ton of free time, so I have to be picky about how I use it. I'm wondering - has anybody here read 'TAOCP'?  If so, is it worth making time to read or would some other book or more on-the-side programming like pet projects or contributing to open source be a better use of my time in terms of professional development? DISCLAIMER - For those of you who sport \"Knuth is my homeboy\" t-shirts, don't get me wrong - I want to read it, but I'm just wondering if it should be right at the top of my priority list or if something else should come first. ",
        "Best answer": "TAOCP is an utterly invaluable reference for understanding how the data structures and algorithms that we use every day work and why they work, but undertaking to read it cover-to-cover would be an extraordinary investment of your time. As one family man to another, spend the time with your kids. "
    },
    {
        "ID": "17226",
        "Question": "... and coded the functionality \"manually\"? As a rather metaphorical example, you'd hardly need a library for solving quadratic equations, although such libraries do exist and someone takes them seriously. As a more questionable case, depending on the circumstances I may ditch jQuery (for example when I don't need to support some stone age browsers): it does simplify some things but it adds another layer of complexity and unreliability to DOM. And overusing jQuery leads to absurd problems, such as one seen recently on SO: how do I assign an empty href to an a tag with jQuery? Turned out it was a HTML question, not even JavaScript. Another absurd case and yet non-obvious for many is using some templating engine/language built on top of another templating system: PHP. Third level of templating anyone? And another one: sometimes just spitting out XML with (figurally) printf is far easier than doing it with some monstrous XML engine. Any other cases from your experience? ",
        "Best answer": "Much of MS enterprise library and most 3rd party controls for .net have left me with this feeling after a bit of use. Your mileage may vary "
    },
    {
        "ID": "17305",
        "Question": "There has been a few remarks about white space already in discussion about curly braces placements. I myself tend to sprinkle my code with blank lines in an attempt to segregate things that go together in \"logical\" groups and hopefully make it easier for the next person to come by to read the code I just produced. In fact, I would say I structure my code like I write: I make paragraphs, no longer than a few lines (definitely shorter than 10), and try to make each paragraph self-contained. For example:  in a class, I will group methods that go together, while separating them by a blank line from the next group. if I need to write a comment I'll usually put a blank line before the comment in a method, I make one paragraph per step of the process  All in all, I rarely have more than 4/5 lines clustered together, meaning a very sparse code. I don't consider all this white space a waste because I actually use it to structure the code (as I use the indentation in fact), and therefore I feel it worth the screen estate it takes. For example: for (int i = 0; i < 10; ++i) {     if (i % 3 == 0) continue;      array[i] += 2; }  I consider than the two statements have clear distinct purposes and thus deserve to be separated to make it obvious. So, how do you actually use (or not) blank lines in code ? ",
        "Best answer": "Always Whitespace is crucial to clean readable code. A blank line (or two) help visually separate out logical blocks of code.  For example, from Steve McConnell's Code Complete, Second Edition chapter on Layout and Style:  Subjects scored 20 to 30 percent higher on a test of comprehension when programs had a two-to-four-spaces indentation scheme than they did when programs had no indentation at all. The same study found that it was important to neither under-emphasize nor over emphasize a program’s logical structure. The lowest comprehension scores were achieved on programs that were not indented at all. The second lowest were achieved on programs that used six-space indentation. The study concluded that two-to-four-space indentation was optimal. Interestingly, many subjects in the experiment felt that the six-space indentation was easier to use than the smaller indentations, even though their scores were lower. That’s probably because six space indentation looks pleasing. But regardless of how pretty it looks, six-space indentation turns out to be less readable. This is an example of a collision be tween aesthetic appeal and readability.  "
    },
    {
        "ID": "17428",
        "Question": "I'm trying to learn more about Erlang than the toy projects I've been playing with. To this end, I'm reading through Programming Erlang and some of the archives from Armstrong on Software.  I would also like to read over some example (preferably production) code that showcases the sorts of things you can build with Erlang that would be disproportionately difficult in other languages. Aside from (I assume) Yaws, are there any publicly available examples of beautiful Erlang code that I could read through to gain a better understanding of the language and/or see the idiomatic uses for various language constructs?  I'm specifically not looking for code that \"gets the job done\" but uses questionable practices, or examples along the lines of \"here's how you write factorial in Erlang\". In the same vein, can anyone recommend any good literature for learning this language (other than the mentioned \"Programming Erlang\")? For preference, something you yourself used to learn it, but if there's some community standard books for it, throw those in too. ",
        "Best answer": "Another good reference is Erlang and OTP in Action. About the code samples: it is kind of hard to find best practices for Erlang, but I would suggest you try these websites:  Trapexit Erlang Programming Language Erlang Factory Planet Erlang Erlang Solutions  Check the links from this article (open source software written in erlang) and you might find interesting code. "
    },
    {
        "ID": "17438",
        "Question": "I'm developing a website which includes a section for reviews. I want to include Google's microformat for review information so that details of the review show up in Google's snippet thingy, like this:  However, according this page, Google supports three different formats for the review data, \"microformat\", \"microdata\" and RDFa. They all seem to do exactly the same thing, though, and I have no idea which one to choose... RDFa looks nice - in that the data is fairly separate from the actual content (which makes my life a bit easier). Except it's based on XML namespaces, which isn't going to play well with HTML5 (the site isn't HTML5 yet, but I imagine it will be ported eventually).  The \"hReview microformat\" seems to use special CSS classes to denote the various sections of the review, which seems like a bad mix of concerns to me... So I'm kind of leaning towards the \"microdata\" format, which looks like it's closer to the HTML5 way of doing things (i.e. no special namespaces). An example of that would be (from the page I linked above): <div>   <div itemscope itemtype=\"http://data-vocabulary.org/Review\">     <span itemprop=\"itemreviewed\">L’Amourita Pizza</span>     Reviewed by <span itemprop=\"reviewer\">Ulysses Grant</span> on     <time itemprop=\"dtreviewed\" datetime=\"2009-01-06\">Jan 6</time>.     <span itemprop=\"summary\">Delicious, tasty pizza in Eastlake!</span>     <span itemprop=\"description\">L'Amourita serves up traditional       wood-fired Neapolitan-style pizza, brought to your table promptly       and without fuss. An ideal neighborhood pizza joint.</span>     Rating: <span itemprop=\"rating\">4.5</span>   </div> </div>  But as I said, all the formats are basically the same, just slightly different... which one should I choose? ",
        "Best answer": "microdata is part of the HTML5 spec.  That for me means a lot more future that the others.  I'd only consider RDFa if you already use some RDF infrastructure. In that case, RDFa is just another serialization of semantic data. "
    },
    {
        "ID": "17498",
        "Question": "When I worked at Sun, we used a DVC system called Forte SCCS/Teamware, which used the old SCCS file format, but was a true distributed source code revision control system. One nice feature is that it had strong GUI support:  You could bringover and putback changes by simply clicking and dragging. It would draw trees/graphs showing how workspaces relate to each other. You also could have a graph view to display a single file's complete history, which might have had several branches and merges. Allowing you to compare any two points. It also had a strong visual merge tool, to let you accept changes from one of two conflicting files.  Naturally, many of the current DVCSs have command line support for these operations, but I'm looking for GUI support in order to use this in a lower-level undergraduate course I'll be teaching. I'm not saying the Forte Teamware solution was perfect, but it did seem to be ahead of the curve. Unfortunately, it's not a viable option to use for my class. Question: What support do the current DVCSs have with regards to GUIs? Do any of them work on Windows, and not just Linux? Are they \"ready for prime-time\" or still works in progress? Are these standalone or built as plug-ins, e.g., for Eclipse? Note: To help keep this discussion focused I'm only interested in GUI tools. And not a meta-discussion if GUI tools should be used in teaching. ",
        "Best answer": "TortoiseHg for Mercurial and TortoiseGit for Git are quite ready for serious use, in my experience. Don't know about Eclipse, but NetBeans supports Mercurial straight out of the box, and NbGit plugin gives Git support. "
    },
    {
        "ID": "17519",
        "Question": "I considered posting on Stack Overflow, but the question strikes me as being far too subjective since I can't think of a reasonable technical explanation for Microsoft's choice in this matter. But this question has bugged me for so long and the issue keeps coming up in one of my projects, and I have never actually seen an attempt at explaining this: OpenGL uses a right-handed coordinate system, where the +Z part of the world coordinate system extends toward the viewer. DirectX uses a left-handed system where the +Z part of the world coordinate extends into the screen, away from the viewer. I never used the Glide API, so I don't know how it worked, but from what I can gather, it uses a left-handed system as well. Is there a technical reason for this? And if not, is there some conceptual advantage to a particular handedness of a coordinate system? Why would one choose one over the other? ",
        "Best answer": "They are both essentially equivalent, as one can be easily transformed into the other.  The only advantage I can find for the left-handed system is: as objects are farther away from the observer, in any direction (x, y, or z), the distance is a higher value. But I have no idea if this is why Microsoft chose one over the other. POV-Ray also uses a left-handed corridnate system. "
    },
    {
        "ID": "17525",
        "Question": "[This question was originally asked on Stack Overflow, but recommended to move the question here.] I can't find anything quite like the question I'm about to ask, so please forgive me if there's something just like it already, please feel free to point me in the right direction. It'll take a bit of background explaining too, please forgive me for that. [backstory] Basically, I graduated from University about 18 months ago with a degree in Business Information Systems and Japanese. The Japanese took up half of the degree so the BIS was only joint. I only learned PHP in terms of languages and basically no computing theory - everything was vocational (Networking, programming basics, CMS development, Office and VBA and then loads of Business theory courses). Since this, I decided to teach myself C# and ASP.Net and try to get a position as a programmer. I created an online shop style website and a small CRM application in Windows Forms to both teach myself and build a portfolio, and luckily I managed to snag a developer position. Bad side? I'm the only developer at my company. Now don't get me wrong, in the last year I've learned loads and loads, I did some devpt. before Uni so knew the basics anyway, but it was very much a \"learning from books\" job - every night. Now then... I am now at a point where I'm building software on a regular basis, making good judgements for time scales, and have even been told my code and methodology are good by other professionals that have been in the game longer than me, and they have offered me jobs. [/backstory] What this whole thing boils down to, is that I now want to study up on the topics I'll have missed by not doing CS. More importantly, could you recommend books / free online courses? I want to learn about Computer Science theory, not just better coding. Thank you! ",
        "Best answer": "Math. You need to be really good at math.  Not quite as much because you are likely to encounter the need to solve differential equations in order to write your code, but because you need the same skills it takes to solve math problems to be really good at software engineering.  I recommend taking a few upper lever math classes at your local community college.  Perhaps a numerical methods or discrete math class first. I took a class in software engineering for my degree in computer science a few years ago that the professor used some of the material from MIT's open course-ware.  He used course 6.170.  I recommend going through that class and a good course on object oriented programming and especially design patterns.  If you still have time and want to learn more, move on to artificial intelligence and theory of computation studies.  If I had to to pick a single course that was most helpful in my career as a developer, it would be the studies on design patterns. Because I have a low reputation, I can't post more links.  Do a Google search for \"gang of four design patterns\" for the book that is classically used to teach that subject. Good luck! "
    },
    {
        "ID": "17568",
        "Question": "Timesheets are something that I've never been fond of, but none the less something that is a requirement within my company. They don't bother me so much, but they seem to really grind some other people's gears. I suppose I have a few questions, and feedback would be great.  Are you required to do timesheets, assuming you aren't a contractor? (That is understandable to me). What is the granularity of timesheets that you would be comfortable with or that you use? (ex: all entries must be under two hours). Would timesheets ever factor into your reasons for not accepting a job or leaving a current one? How has management within your organization justified timesheets if you aren't billing to a client?  ",
        "Best answer": "As a manager yes I get the team to do timesheets.  Here's why and a few notes on how they're implemented to, hopefully, minimise disruption:  As a business much of our work is done on a time and materials basis.  Without timesheets that obviously doesn't work.  We have 10 clients and a range of different projects and products but we're not a big enough to devote people to clients or projects full time which means that we have to have some way of working out how long things took. Even if this weren't true to manage a team you still need to understand what takes time and how much. Think that old app the mailroom guys use is taking more time to support than it's worth? What about when someone asks how much work went into feature X on the new website which doubled sales? Or when your developers say you should recruit someone else and you get asked to breakdown what they do to help justify it? Categories exist for all reasonable \"non-work\" including mentoring, general technical discussions, support, meetings and so on.   Bug fixing - we record time against a whole project rather than bug by bug.  This tends to make things a lot easier - spend the day fixing bugs, 7.5 hours bug fixing goes against the project and you're done.  No need to try and work out how it was divided between the 13 bugs you fixed. When we implemented them I promised that no-one would be penalised / rewarded for what was on their timesheet so long as it was accurate.  So there is no input into reviews based on profitability or utilisation or anything else.  This means that there is no incentive to distort. By accurate I mean roughly.  People really shouldn't have to spend too much time worrying about what happens when they make a coffee or go to the toilet.  Basically if you make a note on a pad of each thing you worked on during the day, then at the end of the day roughly break it down across the hours you worked and that's it.  If shouldn't take more than 5 minutes max. If I don't like what I see - for instance someone has spent too long on task X - the investigation is into what we can do to make X faster, rather than anything to do with the timesheet. Knowing how long you spent doing something is a great way of improving estimates.  The anti-timesheet feeling among many programmers seems to come from two things - (1) badly implemented timesheets which take too long to complete, demand more information than is really needed and encourage lying and distortion so the information is worthless anyway, and (2) a feeling that every single thing that slightly inconveniences a developer should be done away with. The first one is fair but you should blame the implementation and the rules someone has attached, not the whole idea of timesheets which can be done in ways that don't have these issues.  The second one is just unrealistic - there are many parties involved in projects, both inside and outside the company, each of whom have many demands on them.  Yes we want to do everything we can to make programmers productive, but it has to be balanced with the needs of other parties. "
    },
    {
        "ID": "17590",
        "Question": "I have searched for many websites for a definition of \"task pool\", but most descriptions are very vague. Anywhere I can find a precise definition of it? More details.  Are dependencies allowed in task pool? Can a running task exchange data with other running tasks?   To phrase conversely, are tasks permitted to accept data only prior to start and permitted to publish results only after the task has finished?  What keeps track of the temporary resources (memory, CPU etc) that are used by tasks? How to define weights that will encourage efficient task assignment?  For example, if one task needs to pass a lot of data to the next task, it may make sense to run the next task immediately following the first task, and as close to the first thread / CPU / machine / cluster (locality) as possible  What about tasks that are not known (do not exist) in the beginning, but were \"created\" later?  ",
        "Best answer": "I'm pretty sure you won't find one authorative answer, because it's a term that could mean different things in different contexts. In terms of C# 4.0 and the Task Parallel Library, a task pool is a collection of pending work items that need to be run. To grossly simplify the situation(*), tasks are taken from the pool and run by various worker threads in parallel. (*) In the actual implementation, Tasks aren't taken from the pool one by one as that introduces too much overhead. Instead, they're taken in batches - and not necessarily in the order they were added to the pool. "
    },
    {
        "ID": "17645",
        "Question": "I was trying to parse a java properties file to avoid some Properties loading pitfalls (non ordering, loss of comments). (Property are of the form 'key = value' where the value can span on multiple lines using the \\ special char) I have tried to get the right regexp to parse the properties but after I gave up after fighting for more than one hour. (a version for groovy found here: ~ /^([^#=].*?)=(.+)$/) I manage to write the same thing using a single loop and very trivial code in 5 minutes... I'm an average programmer when dealing with regexp: I have to re-read the doc each time I want to use them and advanced feature are still obscure for me. Often when dealing with regexp I have the feeling that some very skilled developers can produce/decipher very elaborated regexp in seconds, but others have trouble to handle them. Question: should I take time to deeply learn regexp and produce cryptic and powerful parser, or should I keep using some ugly easy to debug/understand parsing algo ? ",
        "Best answer": "For this specific task (parsing java properties file), first look for an existing solution (sounds like someone would have solved this same problem already), and if you can't find one, it's not necessarily bad to create a non-regex parser to do it with - you use the tools you know. If you do go the regex route, do not be compelled to use a single regex, when two (or more) will often give a much simpler and faster result.  should I take time to deeply learn regexp  YES! There are a lot of times when knowing regular expressions can greatly speed up your programming - I probably use more regex in the tasks of creating & maintaining code than in actuall code itself. But perhaps the main reasons for learning regex deeply is so that you can quickly say \"this is a job for regex\" or \"regex is the wrong tool here\" - otherwise you can waste a lot of time trying to get regex to do a task it just isn't suited for. However, it's important to also state that Regex does not need to be crypic - you can write complex regex and format it nicely so that it is just as understandable as any other code. "
    },
    {
        "ID": "17650",
        "Question": "I was at work last week and one of our \"lead developers\" (who said he has programmed C++ before, but I don't believe him) was talking about C++ and how our team of developers who only know PHP would not have a hard time at all learning C++. My first reaction was pure shock and then I laughed at his comment. He gave me a bad look and i asked if he was serious. Me personally I program a lot of C and C++ for mostly school projects and coding competitions. I don't think it's at all like PHP. I asked him why they were the same and he could not give me an answer (he doesn't know C++ at all I was thinking). Then I said \"OK, yeah they both use curly braces and have conditional statements. But C++ you have to worry about memory management and all the easy to use built in functions in PHP, for example the array functions, are non-existent in C++. You actually have to know the algorithms behind them.\". There are some great C++ libraries (STL, Boost) that will make your life easier but C++ is like PHP, come on! If you already know one language it will make it a little easier to learn another because you will already be good at boolean login (if this then that) or looping but there is just so much more to C++ than PHP. Like you have to compile the language and actually have to define what type of variable you are using and returning from functions. What do you think? EDIT: Actually what he said was \"C++ is not much different than PHP\" ",
        "Best answer": "As someone who went from PHP to C++ (though only limited C++) when I started writing C++, I often thought \"Wow, this is a lot like PHP\". When you consider that PHP itself is written in C, and whenever the developers faced a design decision they couldn't resolve internally, the final answer almost always ended up being \"Do it like C++\" it's really not that absurd a statement. Your response about Memory Management and Array Sorting/whatever is a non-starter. That's not a PHP/C++ answer, that's a C++ with zero libraries versus just about every other popular language in existence answer. It feels to me like you're asking this question so that you can get people to agree with you -- that people who write C++ are so much better than people who write PHP. Personally, I think that's a foolish point of view which will lead to nothing but poor decisions on your part. "
    },
    {
        "ID": "17675",
        "Question": "I was asked today if I had experience with \"Service Oriented Architecture\" and although I think I do. The concept, to me, seems so muddled I don't know how you could honestly answer that question anymore.  I resorted to Googling the term in an effort to get a concise definition of the concept and how it differs from other architectures. After reading a number of articles on it, the only common thread I seem to be able to find is a system with multiple components that talk to each other over some kind of interface, with perhaps a slight preference for XML/SOAP. It seems like almost any application could be defined as SOA, especially a web application. Has this term fallen into the \"Web 2.0\" trap and become a term meaning whatever you want it to mean?  Am I way off base here? When you guys hear the term does it mean anything specific to you? If so I'd love a concise definition that clearly demonstrates what is and what specifically is NOT SOA. ",
        "Best answer": "The strict definitions of SOA are far past the cost/benefit line as to be theoretical in many cases. Unless your product is the services themselves you often need a different point of view. A USABLE definition of SOA means that your overall architecture is service friendly.  A system built completely from atomic services is usually not the right plan, and some services are going to organized functionally while others are going to be single responsibility.  I may have black boxes, I may have offline processes, but if there are a discoverable collection of services through which I can get a meaningful amount of work done that is my minimum definition. Aside from the debate on what it actually means, the concept (whatever it means) has suffered in many circles by being applied to places it simply does not fit. For example, If I am building something that is meant to be a black box process and scales through parallelism not segmentation and distribution I may have a service to expose/talk to the black box, but some people keep trying to put the services inside the box. As a strict technical definition it has always been undefined, but the idea is not without merit where it fits. "
    },
    {
        "ID": "17729",
        "Question": "I am programmer who just started working on a startup idea. At the moment I want to bring onboard at least one programmer. This programmer should be a ninja - a 10x engineer.  Since early days are probably the most risky for a startup, I want to make sure I approach this problem the best I can.  How do I find these people? and How do I convince them to come onboard? I would love to hear from people who started their own companies and what their thoughts are about hiring Update: I would like to get the ninja as a co-founder so besides being a ninja (ie. great programmer with computer science background) he/she has to have a healthy appetite for risk (for great programmers this is not a big deal because they can be hired anytime into mainstream jobs if the startup doesn't work) ",
        "Best answer": "Pay lots of money. If they can't do that they offer stock options and nice perks like free food, drink, nice working environment with latest equipment and good benefits. Basically you have to give them something worthwhile, no one is interested in making you rich for their toil. "
    },
    {
        "ID": "17898",
        "Question": "In your own studies (on your own, or for a class) did you have an \"ah ha\" moment when you finally, really understood pointers? Do you have an explanation you use for beginner programmers that seems particularly effective? For example, when beginners first encounter pointers in C, they might just add &s and *s until it compiles (as I myself once did). Maybe it was a picture, or a really well motivated example, that made pointers \"click\" for you or your student. What was it, and what did you try before that didn't seem to work? Were any topics prerequisites (e.g. structs, or arrays)? In other words, what was necessary to understand the meaning of &s and *, when you could use them with confidence? Learning the syntax and terminology or the use cases isn't enough, at some point the idea needs to be internalized.  Update: I really like the answers so far; please keep them coming. There are a lot of great perspectives here, but I think many are good explanations/slogans for ourselves after we've internalized the concept. I'm looking for the detailed contexts and circumstances when it dawned on you. For example:  I only somewhat understood pointers   syntactically in C. I heard two of my   friends explaining pointers to another   friend, who asked why a struct was   passed with a pointer. The first   friend talked about how it needed to   be referenced and modified, but it was   just a short comment from the other   friend where it hit me: \"It's also   more efficient.\" Passing 4 bytes   instead of 16 bytes was the final   conceptual shift I needed.  ",
        "Best answer": "Someone much wiser than I once said:  The nun Wu Jincang asked the Sixth   Patriach Huineng, \"I have studied the   Mahaparinirvana sutra for many years,   yet there are many areas i do not   quite understand. Please enlighten   me.\" The patriach responded, \"I am   illiterate. Please read out the   characters to me and perhaps I will be   able to explain the meaning.\" Said the nun, \"You cannot even   recognize the characters. How are you   able then to understand the meaning?\" \"Truth has nothing to do with words.   Truth can be likened to the bright   moon in the sky. Words, in this case,   can be likened to a finger. The finger   can point to the moon’s location.   However, the finger is not the moon.   To look at the moon, it is necessary   to gaze beyond the finger, right?\"  "
    },
    {
        "ID": "17971",
        "Question": "Personally, I break out in hives if I don't put ADO objects that implement IDisposable in using statements.  But at my current contract, I've found that their homegrown enterprise framework \"data access provider\" code does not 1) implement IDisposable and 2) call Dispose() on anything it uses, at any point, ever.  Users have been complaining a great deal about performance issues in the Winforms applications that heavily use this framework for data access, and though there are a LOT of other problems in the code that could be hitting performance, this one just screams at me and is more low-hanging-fruit than the others.   So, beyond saying something like \"Dispose is there for a reason, use it,\" what can I tell these people to convince them that this is really, really bad? ",
        "Best answer": "If you do not call the Dispose method on a SQL Connection, when you done using it, that connection does NOT get returned back to the connection pool. If you are having performance issues, my guess is that the maximum connections are opened on your database.  A DBA could easily confirm this. Microsoft's Best Practice calls for you to place your connection code inside of a Using statement, ensuring the connection is disposed of and that the connection is returned back to the pool. "
    },
    {
        "ID": "17979",
        "Question": "What I am looking for is the most optimal solution for storing content revisions in an MSSQL database. We have an application that allows users to make changes to small HTML content blocks similar to a wiki, but we need to have tight audit control over the changes. At any point in time, the manager might want to look at previously submitted contents or revert the entire HTML block back to the previous state. I had it setup in the past where we had a primary table that stored the HTML information (along with various other meta tags) and then an audit table that kept a copy of the entire data row everytime a change was made. What I am wondering is if this is the best way to go, or should I just keep 1 table with all the current records and the changes in it and just have a flag that lets me know which one is the current one? ",
        "Best answer": "I have had decent success with doing a bit of both. One table that has the current version of the record and one table with every version (and a timestamp and an edited by) in another table. This gives you the advantage of having a complete copy of each version without the overhead of having to sort out the current version from all the others at runtime.  it also allows you to set much stricter permissions on the audit table and optionally implement the audit copy with triggers and completely hide the audit information from the rest of the app if you like that sort of thing. "
    },
    {
        "ID": "17995",
        "Question": "The specific example I have in mind involves the currently-alpha game Minecraft, but this is a general question that I think warrants some conversation. Minecraft is written in Java, and stack traces usually look like this (real example): java.lang.NullPointerException     at d.a(SourceFile:247)     at gi.b(SourceFile:92)     at bd.a(SourceFile:33)     at bn.a(SourceFile:69)     at bn.e(SourceFile:115)     at bn.d(SourceFile:103)     at net.minecraft.client.Minecraft.i(SourceFile:1007)     at net.minecraft.client.Minecraft.run(SourceFile:596)     at java.lang.Thread.run(Unknown Source)  Obviously these are not the real package and method names that the developer uses when he writes.  Since he is in an alpha stage, it seems that the developer should like to be able to make sense of his stack traces, especially if someone is able to provide one for a bug report.  As this stands, it's mostly meaningless. What advantage could one possibly hope to gain by obfuscating his code like this that overcomes the drawbacks of more difficult bug identification? ",
        "Best answer": "We obfuscate our Java code too.... The advantage is that it makes it harder to reverse-engineer (if you are worried about someone stealing your code base and using it as a base to create a similar competing product, for example, etc). You can get the original stack trace back: there are obfuscation tools out there which create special reference files which you can use to run the obfuscated stack traces through, and it comes out with the original source stack trace. These are generated by the obfuscation process itself, so you can't get the original stack trace back unless you have your hands on the reference file that you used to obfuscate the code in the first place. This has no disadvantages really. :) "
    },
    {
        "ID": "18026",
        "Question": "I'm working with a web company that's approaching a point where it will likely need re-think the product as a V2 - due to outgrowing some of its V1 foundations and principles that have been built into virtually everything, from the data model to the user interfaces. For various reasons, this evolution might involve a migration from CakePHP (with which the V1 has been built) to Symfony or Zend. I would like to ask for some experienced views on how people might have managed a transition like this for a website that has significant traffic and generates revenue. I don't want to open up a discussion on the pro's & con's of different PHP frameworks, or why this migration might be needed. Rather, I would be very interested in hearing whether there are some practical alternatives to essentially building a V2 from scratch alongside the V1 for a couple of months - and locking up precious coding time for the duration of this intense period. An example of such an alternative might be migrating an app in parts over a longer period of time. I'd be grateful for any views from people who might have managed or been involved in such transitions. Thanks in advance. ",
        "Best answer": "Learn the new framework very well first, and make sure it is going to meet your needs, and that you really grok the paradigms of the new framework.  You are going to have to throw out a lot of code, and that's ok. The important thing is that you are using the new framework the way it was meant to be used, taking full advantage of it's features, and not being tied into ways of thinking from your old framework. Don't try to use Zend \"the CakePHP way\"* For example, when I moved to using MVC framework from previous non-MVC ones, I didn't really get how models, views, and controllers were supposed to work, and I wrote some terrible looking code because I didn't understand the new way of doing things. You'd be better off sticking with an inferior old framework than having a poorly written app on a better framework. *I'm not familiar with either, so I don't know how similar/comparable they are. "
    },
    {
        "ID": "18055",
        "Question": "I feared programs that require installation a lot. They leave traces behind the registry even after un-installation, plus, some uninstallers are poorly-programmed (that means, some programs cannot be uninstalled). That applies to Windows. The good side of installers, they create shortcuts and file associations. However, the Komodo on Linux uses a shell script to setup and can be safely uninstalled by deleting the folder. Do you think installers should be all abolished? Do you think the effect of installations should be replaced by shell scripts? Do you think all computer games should be portable? Should the \"scan the /Applications/${AppName}/Information folder and list app icons\" design should be used for future desktop environments? ",
        "Best answer": "It depends on your target audience. As a programmer or a power user, I like my software to be inside a compressed file for portability and to be able to just delete the software without having to worry about it leaving any traces in the registry. As a regular person (mostly applies to regular Windows users), however, I just want to click setup.exe and have it install itself for me. I don't want to have to bother looking at a README file or following some other form of instruction just to install the software that I want. Don't make me click any more than I could handle. You should consider who you're making the software for first before deciding whether to make an installer or not. "
    },
    {
        "ID": "18096",
        "Question": "I am the first developer in a large scale web project, in the real estate sector. I am not an expert in any field, I know the basic of all, programming, databases, something about design and a little bit about SEO, and website optimization / caching. And I have some knowledge about other technologies and stuff that could be required in the project. So I am a developer, my boss does some drawing on a paper to present me his ideas and then I start programming and I show him the result. Until now there was no problem, but now the web application is large enough, and it lacks a little bit of database optimization and intuitive user interface. Beside the website, the project also has an offline newspaper, and a desktop application that is a reduced version of the online one, both this things are not managed by me, but by other people or developers that are external to the company. We do not use a collaboration tool for sharing knowledge between the people working on this project, just emails, and we do not use a development methodology, as for the team, we are:  the developer(me),  a designer,  a secretary, and  the boss.  I have the possibility to ask the boss hire the people I want so I can increase the team and have the right person dealing with the right part of the project. This is the story, the real question is, what should be my attitude towards the project and the company? Should I stay a developer and participate in taking decisions and organizing tasks from time to time to help the boss? Or should I get more serious about this and try to learn project management and implement everything I consider it's required to ensure the quality of the work and final results? I am the one who best knows what has been developed until now, should I try to organize all the work and the team? Or should I ask my boss to hire some expert to do that? ",
        "Best answer": "Well it really depends on you.  Now that you're in a position to hire people underneath you, you can take one of two central roles:  You can play the role of the project manager.  You create the time estimates and organize the specifications for each thing your boss wants you to do.  It essentially distants you a little from the technical aspect of the job, and there's a lot of room for growth (perhaps not in the immediate future but down the road).  It also means you're the one who gets fired when your hirelings don't do the work on time, so you have to provide good estimates and reasonable deadlines. Or you can play the role of the technical advisor.  Whenever any questions of a technical nature arise, you're the man to consult.  It also means you must be able to make educated decisions regarding the direction of technologies used.  As for organization, you leave it to your superiors.  You're only responsible for the time estimates that you claim.  The role of your hirelings is to assist you complete the work that needs to be done and explaining how to do it if they don't know how (which is a subtle difference from handing over the work and letting them make more independent decisions).  Hope that helps.  Do whatever sounds more appealing to you for a job.  Though I would encourage you not to follow the path to the dark side and stay true to your humble beginnings as a programmer. "
    },
    {
        "ID": "18131",
        "Question": "I know the general trend against comments explaining how something works, and how it's a good idea to use them only to explain why you're doing what you're doing, but what about using comments as a means of dividing up the code? Assume you have a script that does a preamble, searches through a bunch of records, prints the records, then closes everything: // Preamble ... preamble code, say about 10-15 lines ...  // Find records ... sql query ... ... put records into array ... ... some other stuff ...  // Print records ... printing records, 20-30 lines a record ...  // Close everything ...  More of a way to visually divide the code, make it easier to find a certain section. Like if in a month you need to fix the printing code, rather than reading through a couple hundred lines to try and find the right spot, just look at the comments to see where it is. What actually happens in each section being fairly straightforward and easy to tell what's going on, would this approach to comments be considered good or bad? Edit: I'm working mostly with PHP scripts, where you either can't put code into functions, or it's impractical to do so. However, the same sort of thing would apply to large class files, with several methods that do related things, like getters/setters, database updates, etc. ",
        "Best answer": "They're not out and out bad but I would ask if your routine is so long that it has sections that are significant in their own right and need a comment wouldn't you be better breaking it down into smaller routines? You'd then have a top level routine which was totally readable without comments: Thing.Initialise Thing.PopulateFromDatabase Thing.PrintResults Thing.ShutDown   Plus it's all reusable now. I'd also add that generally things like \"close everything down\" should be obvious from the code and the structure and therefore unnecessary.  If the code is This.Close(), That.CleanUp(), TheOther.Disconnect() then you really don't need a comment explaining.  Where I think these sorts of comments are good is in roughing out your design and structure before you start.  I find it's good to write the thing out in pseudo code in comments and then remove them as I actually code it.  You can then add and amend the design quickly as you are coding without the risk of forgetting what it was you'd decided. "
    },
    {
        "ID": "18176",
        "Question": "I have this idea that I want to build. I want to start the idea as a web-app, then move into mobile-app and possible desktop application. The project should have databases (mostly strings). Also users would have accounts and access to these databases. So the project provides access to the user-resources through the Internet. I want the database easily accessible through different platforms. The project is also to sharpen my IT, so I don't want the easy road, but the better one.  What are my best options of frameworks/skills? Good tools to manage the project? Is starting as a web-app a good choice, or should I build it as a mobile-app first?  I have some knowledge (but not a lot of experience) in: C, C++, Java, PHP, Javascript, MySQL, HTML/CSS. Edit: I know that my question is too general, all I wanted is some ideas and guiding to have a good start. @Nathan Taylor, regarding the web-app, I don't mind any option (should I learn Python, or stick with php in this regard)? I'll probably implement the project in Android, and possibly iPhone. ",
        "Best answer": "The shared part of your code will be the server-side component. I assume you will be building a set of web services, e.g. SOAP services. The various front-ends will all communicate with this same services API. It's very important to think long and hard about what API will be able to cater to all the target platforms and devices. Think about the different user expectations between a desktop and a mobile app, and what this means for the underlying API. Don't bother trying to use the same client-side code for the different target platforms (mobile, web, desktop). You'll never get a solid user experience that way. For the various front-ends, it's probably best to focus on a single technology stack that can stretch to all the platforms you want to aim for. Otherwise you're going to drown in the amount of things to learn. Picking the web stack is a safe choice, because you can adapt it to mobile devices and to desktop devices with various solutions (e.g. Adobe Air, Sencha Touch, ...). If you absolutely have to you can pick up objective c for writing native iphone apps later. If it were me, I would use the following technology choices:  Server-side: PHP or node.js Client-side (web): Ext JS Client-side (mobile): Sencha Touch Client-side (desktop): Adobe Air  I'm not saying those are the best choices for you. They would just be the best choices for me. "
    },
    {
        "ID": "18202",
        "Question": "The project, I have involved, has an architecture-oriented project's file/folder structure: Root |____ Node1     |____ Event Handlers     |         |___ <all event handlers of project>     |____ Events     |         |___ <all events of project>     |____ Request Handlers       |         |___ <all request handlers of project>     |____ Requests     |         |___ <all requests of project>     |____ ...  It is a clear from the architectural point of view of system (has been proposed by development team). It is a feature-oriented structure has been proposed by designer team: Root |____ Feature #1     |____ Event Handlers     |         |___ <all event handlers of Feature #1>     |____ Events     |         |___ <all events of Feature #1>     |____ Request Handlers       |         |___ <all request handlers of Feature #1>     |____ Requests     |         |___ <all requests of Feature #1>     |____ ...  This variant is closer to designers and it describes clearly a feature to be implemented. Our teams have started a holy war: what is the best approach. Could somebody help us and explain cons and pros of the first and second ones. Maybe there is a third one that is more useful and beneficial for both of us. Thank you. ",
        "Best answer": "I would vote for the second one. In the first structure, event handlers for FeatureA are completely unrelated to event handlers for FeatureB. It seems that developers will be working on one feature at a time, and if you're working on a FeatureX request, it's far more likely that you'll need to tweak a FeatureX request handler than, say, a FeatureZ request. By the way, I love how you asked this question from a neutral point of view. "
    },
    {
        "ID": "18349",
        "Question": "Suppose you are a programmer in your mid-20's who has had several years industrial experience working for both big tech and a few internet startups but now you'd like to try contracting in the hope you'll be able to choose more interesting and varied work and be able to have a few months off between jobs to focus on things like open source, travelling and developing your own startup ideas. Now this is possibly one of those 'grass is greener' daydreams.. Even if in the workplace you manage to have the enthusiasm and skill to work equally alongside much more experienced co-workers, how realistic do you think it would be for someone with too few years of experience to appear 'specialist' or 'experienced' to compete for contracts and afford to live this lifestyle? ",
        "Best answer": "There are contract roles at all levels therefore for all levels and I've seen people contract with 12 months commercial experience (though that was in a good market).  That said you'll see more (and better paid) opportunities the more experience you have.  Ideally I'd suggest that you're either at or close to senior developer type experience (5 years +) but in most places it's really not impossible with less so long as the market is OK at the time. In terms of the lifestyle though, some words of warning.   I know a lot of people who went into contracting with the intention of doing a bit of time on a bit of time off and it never worked out that way.   When things are going well the temptation is to keep working while the going is good because there might be a downturn on the way and you should make hay while the sun shines.   When things are going badly you're so nervous about being out of work that you're desperate to get back in a position.  Even if you don't need the money in a bad market contracting is a bit \"you're only as good as your last role\" so gaps on the CV aren't great. You can make it work but you're going to need to be disciplined and budget for plenty of time out of work to give you the assurance that you're not going to leave yourself in trouble when it comes time to go back and can't find something immediately.   That financial security is the thing that's going to make your goal possible so it's critical to understand what your average wage over time working and not working will be rather than get carried away with what you're earning when things are good. "
    },
    {
        "ID": "18357",
        "Question": "This is a specific question relating to C#. However, it can be extrapolated to other languages too.  While one is preparing for an interview of a C# Developer (ASP.NET or WinForms or ), what would be the typical reference material that one should look at?  Are there any good books/interview question collections that one should look at so that they can be better prepared? This is just to know the different scenarios. For example, I might be writing SQL Stored Procedures and Queries, but I might stumble when asked suddenly  Given an Employee Table with the following column(s).  EmployeeId, EmployeeName, ManagerId Write a SQL Query which will get me the Name of Employee and Manager Name? NOTE: I am not asking for a Question Bank so that I can learn by rote what the questions are and reproduce them (which, obviously will NOT work!) ",
        "Best answer": "Take a look at Scott Hanselman's blog post on \"What Great .NET Developers Ought to Know\". From there you should be able to figure out how to search and study based on the concepts he mentions there. "
    },
    {
        "ID": "18371",
        "Question": "The company I'm working for is looking to hire a senior developer with more experience than me, and they expect me to do the technical part of the interview. I've only been programming a few years and am not sure I have the knowledge needed to evaluate the coding skills of someone who has greater understanding/experience than I do. Can anyone recommend some technical interview questions to ask that are a good means for evaluating higher-level programming skills, but still be ones I can understand?  I would say I'm past the jr. programmer level, but nowhere near senior. Most of what I've done is built small apps (web and desktop), some of them fairly complicated, but all of them have been meant to be used by no more then a handful of users. I feel I have a decent understanding of most programming concepts and am capable of learning/teaching myself just about anything, however I lack experience. As my boss is fond of telling me, \"You don't know what you don't know\". In particular, things we'd like the person we hire to have experience with (that I don't have) is: Multi-tier development, multi-user environment, large-scale application development, two-way messaging, shared sessions, and Multi-threading/BackgroundWorkers. UPDATE:  In response to Thor's comment below, we hired someone a few months ago and I think it has been working out great. I am learning a lot, not just about coding but also about things like design patterns, software architecture, documentation, and how other larger programming teams get stuff done. Its not always easy having someone come in and point out better ways to do things you have done, but if you can swallow your pride and be willing to try out new things you can learn a lot. The interview process went better than I expected. I started asking questions about things I was familiar with, then asked some questions about some things I was struggling with. Whenever the interviewee said something I didn't understand, I'd ask them to explain it to me and then write it down so I could look it up later on. Overall, I felt I was able to get a pretty good idea of the applicant's skill level, intelligence, and what they'd be like to work with. ",
        "Best answer": "You can't. Instead, I would suggest you to come up in the interview with a list of problems you have today, and ask him how he would solve them. This a is very interesting method for the following two reasons:  It is free consultancy. Even if you don't hire the guy, he may suggest nice solutions to your problems. If he comes with interesting solutions, he is a problem solver. The kind of guy you want to hire.  "
    },
    {
        "ID": "18379",
        "Question": "Any tips for a group needing to start code reviews? We have no real software engineering processes currently. I have been working as software engineer for 7 years and have never seen any real processes implemented at the various places I have worked, though many books espouse all kinds of engineering methodologies. The management supports the idea of enabling new processes, though there is some resistance from the senior engineer who has over 20 years of software engineering experience.  We are a set of two programmers, with no software engineering manager. We report direct to the managing director of the Company. But we work on numerous software projects. The real reason why we have to begin performing code reviews is because of PA-DSS requirements. Therefore, I am pushing for us to do code reviews all around - not just on the payment application software. ",
        "Best answer": "You want code reviews to be a positive exercise!  Here are some tips:  Code review early by pair programming!  Even if this is only for 30 minutes a day. Use static code analysis tools, style checkers and templates to get rid of the fluff.  You can then concentrate on the meatier issues (SRP, design patterns, exception handling, coding to interfaces etc etc) which is what code reviews should be really about (IMHO). Get an agreed checklist between you on what's important and what's not Some sort of Test Driven approach (forces better upfront design) is recommended further down the track There are some really good code review tools out there - I use Crucible personally Make sure that an attitude of 'improve the code' as opposed to 'crticise the coder'.  This is vital in keeping everyone's egos happy. Do it regularly, don't skimp on it. Either after a group of commits or 1/day or whatever you agree on.  Good luck! "
    },
    {
        "ID": "18391",
        "Question": "Recently, I had a job interview at a big Silicon Valley company for a senior software developer/R&D position. I had several technical phone screens, an all day on-site interview and more technical phone screens for another position later. The interviews went really well, I have a PhD and working experience in the area I was applying for yet no offer was made. So far, so good. It was an interesting experience, I am employed, absolutely no hard feelings about this. Some of the interviewers asked really detailed questions to the point of being suspicious about new technologies I have been working on. These technologies are still in development and have not come to market yet. I know some major hardware/software companies are working on this too. I have had many interviews before and based on my former interviewing experience and the impression some of the interviewers left behind, I know now all this company wanted from me is to extract some ideas about what I did in this field. Remember, I am referring to a R&D position, not the standard software developer stuff. Has anybody encountered this situation so far? And how did you deal with it? I am not so much concerned about \"stealing\" ideas but more about being tricked into showing up for an interview when there is no intension to hire anyway. I am considering refusing technical interviews in the future and instead proposing a trial period in which the company can easily reconsider its hiring decision. ",
        "Best answer": "first, vet the interviewer - are they in a competing field? why are they interested in you? second, refuse to answer any question that goes beyond the general/common knowledge of the field, on non-disclosure-agreement grounds (politely) third, expect some level of giving them 'free advice'; but cut it off after (say) 3-5 minutes. fourth, take control of the interview as soon as you can, to make sure that your questions get answered. Since you are interviewing from a position of strength it is important that you understand how you might fit in the company culture, what would be expected of you, where they are at currently with their R&D efforts, and so on. You're interviewing them as much if not more than they are interviewing you. good luck! "
    },
    {
        "ID": "18421",
        "Question": "I've got substantial J2EE experience, have worked with Grails, and am comfortable with several languages (Perl, PHP, Smalltalk). I've decided to try a new project using Ruby on Rails. For those who've decided to migrate to a new technology stack, how do you make the transition? Do you learn the details of Ruby first (I've done a few tutorials, and feel like I've got a decent enough grasp of the language to start doing some basic work) or do you do a full Ruby on Rails tutorial, and expand your knowledge from there. I'm leaning towards the latter approach because I feel like I can delve into details of the language as I encounter them. For those who work in Ruby on Rails, how did you learn, and which resources do you recommend? ",
        "Best answer": "I learned Rails by going through the Agile Web Development with Rails book and immediately applied everything that I learned to a real web project. I ended up with a rather substantial application (mykidslibrary.com) and a deep understanding of the framework. "
    },
    {
        "ID": "18439",
        "Question": "I work with open source software a lot and keep coming across weird and funny names for executables, variables, constants and etc. Currently I am wondering why CATALINA_HOME environment variable was named that way? I did a quick search on google with but nothing relative came up. My only guess is that it is named after the island of the shore of south California. On the related note, very often I find amusing and clever names used in open source programs. One of my favorites are slapd and slurpd executables of OpenLDAP suite. What are your favorite/noteworthy variable/executable names that you have seen in your programming career? Do you think it is OK to name your variables/executables this way, or would you rather see boring but clear names? PS. This is not really a programming question but I think it somewhat fits in here, according to faq. ",
        "Best answer": "I believe it comes as a result of thinking about your applications home directory, which may in turn have been influenced by Unix home directories before that. Typical examples are  JAVA_HOME for the JDK installation directory MAVEN_HOME for the Maven installation directory ANT_HOME  CATALINA_HOME because the project name for the servlet engine within the overall Tomcat project was called Catalina  As a general convention for naming environment variables, the _HOME suffix has stood the test of time and so has become generally adopted for new projects that require them. In terms of my personal favourite names, here's a short list  grep (global regular expression print) is synonymous with search among *nixers awk (named after the programmers) is great for awkward text manipulation  For more Unix weirdness have a look at this article. "
    },
    {
        "ID": "18444",
        "Question": "There have been many questions with good answers about the role of a Software Architect (SA) on StackOverflow and Programmers SE. I am trying to ask a slightly more focused question than those. The very definition of a SA is broad so for the sake of this question let's define a SA as follows:  A Software Architect guides the   overall design of a project, gets   involved with coding efforts, conducts   code reviews, and selects the   technologies to be used.  In other words, I am not talking about managerial rest and vest at the crest (further rhyming words elided) types of SAs. If I were to pursue any type of SA position I don't want to be away from coding. I might sacrifice some time to interface with clients and Business Analysts etc., but I am still technically involved and I'm not just aware of what's going on through meetings. With these points in mind, what should a SA bring to the table? Should they come in with a mentality of \"laying down the law\" (so to speak) and enforcing the usage of certain tools to fit \"their way,\" i.e., coding guidelines, source control, patterns, UML documentation, etc.? Or should they specify initial direction and strategy then be laid back and jump in as needed to correct the ship's direction? Depending on the organization this might not work. An SA who relies on TFS to enforce everything may struggle to implement their plan at an employer that only uses StarTeam. Similarly, an SA needs to be flexible depending on the stage of the project. If it's a fresh project they have more choices, whereas they might have less for existing projects. Here are some SA stories I have experienced as a way of sharing some background in hopes that answers to my questions might also shed some light on these issues:  I've worked with an SA who code reviewed literally every single line of code of the team. The SA would do this for not just our project but other projects in the organization (imagine the time spent on this). At first it was useful to enforce certain standards, but later it became crippling. FxCop was how the SA would find issues. Don't get me wrong, it was a good way to teach junior developers and force them to think of the consequences of their chosen approach, but for senior developers it was seen as somewhat draconian. One particular SA was against the use of a certain library, claiming it was slow. This forced us to write tons of code to achieve things differently while the other library would've saved us a lot of time. Fast forward to the last month of the project and the clients were complaining about performance. The only solution was to change certain functionality to use the originally ignored approach despite early warnings from the devs. By that point a lot of code was thrown out and not reusable, leading to overtime and stress. Sadly the estimates used for the project were based on the old approach which my project was forbidden from using so it wasn't an appropriate indicator for estimation. I would hear the PM say \"we've done this before,\" when in reality they had not since we were using a new library and the devs working on it were not the same devs used on the old project. The SA who would enforce the usage of DTOs, DOs, BOs, Service layers and so on for all projects. New devs had to learn this architecture and the SA adamantly enforced usage guidelines. Exceptions to usage guidelines were made when it was absolutely difficult to follow the guidelines. The SA was grounded in their approach. Classes for DTOs and all CRUD operations were generated via CodeSmith and database schemas were another similar ball of wax. However, having used this setup everywhere, the SA was not open to new technologies such as LINQ to SQL or Entity Framework.  I am not using this post as a platform for venting. There were positive and negative aspects to my experiences with the SA stories mentioned above. My questions boil down to:  What should an SA bring to the table? How can they strike a balance in their decision making? Should one approach an SA job (as defined earlier) with the mentality that they must enforce certain ground rules? Anything else to consider?  Thanks! I'm sure these job tasks are easily extended to people who are senior devs or technical leads, so feel free to answer at that capacity as well. ",
        "Best answer": "A Systems Architect should:  Specify the high-level architecture Participate in code reviews Approve technologies used Assist the developers in their coding effort Maintain and monitor the development schedule Produce SA artifacts, such as UML diagrams, Gantt charts and the like.  SA's must know how to code, and should participate in some of the coding work, get their hands wet, so to speak.  This keeps them in touch with the gestalt of the development effort. As Uncle Bob Martin once said, the Architect should do some of the coding himself, so that he can see the pain he is inflicting on others with his designs. The SA should have the last word on all design, technology and coding style decisions.  But, like all managers, the job of the SA is to clear the path for his people so they can be productive.  That means, for the most part, that the developers get to decide, at their level, how problems are to be solved.  It means that the SA keeps the pointy-haired bosses out of the developers' cubicles.  And it means that the SA pitches in to help, as needed. Like all human beings, SA's can and do make mistakes.  The good ones learn from those mistakes, and become better SA's. "
    },
    {
        "ID": "18551",
        "Question": "I'm an artist, mostly, though I describe myself as a artist/physicist.  While I can do math, deal with  words, and the \"logical\" stuff considered left-brain, it's an effort and I make mistakes, whereas I do well with and most of the time think in terms of those things associated with right-brain thinking - spatial relations, holistic big-picture context, etc.  Of course all that is fuzzy, as the right-left brain theory is oversimplified and no mental activity is so simple.   Yet I do sense that I fit in fine with artists, video directors, chefs, and other non-verbal thinking, creative types, while most people in \"IT\" or hardcore software engineers have minds that work differently, with attention to detail, holding many details in mind at one time, and strong rational and verbal capabilities.   So here I am in a job being paid to fix fussy and obscure bugs in a massive swarm of C++ software, very heavy on OO, and any one line of code makes no sense unless I hold in mind about twenty other class and method names, relations between them, the flow of execution (very spaghetti-like) and other detail.   Besides that,  I'm also rather strongly against much of contemporary C++ and OO styles.  Those who wrote this code really drank the deep OO and Modern C++ kool-ade.  I find it actually makes code much harder to follow, much harder to decide where to fix or change something.   I don't know if this is part of the left/right differnce (or whatever you want to call it) or not. But work on the C++ I must - people depend on me for my income.  What are tips and techniques to deal with this situation, to be as effective as possible for my employer? ",
        "Best answer": "It doesn't sound (at least to me) like your code is particularly object-oriented, or particularly similar to \"Modern C++\". Rather the contrary, one of the key elements of good object orientation is encapsulation, the primary intent of which is to reduce the number of things you need to keep track of at any given time. Likewise, \"very spaghetti-like ... flow of execution\" sounds neither object oriented nor modern (anything). Now, I suppose if I looked at the code you're maintaining, I might see it differently, and/or you might see my code as being similar to what you're maintaining right now -- that's a bit hard to guess. It is true that if you tried to trace through every detail of how my code works, I suppose you could see it as a rather spaghetti-like flow of control. Just for example, I'm a lot more fond (or at least tolerant) of quite a few implicit conversions than many programmers -- I use things like proxy classes quite a bit. This means there might easily be three or four temporary objects of different types created in the course of calling a single function (and note that I'm not talking about actually executing the function, just calling it). Of course, all of those temporary objects will be destroyed again at the end of the expression containing the function call. If you count it up, you might easily have half a dozen or more separate functions invoked in calling/returning from one function that's \"visibly\" called in the code. The point of doing things that way, however, is to make it easy to ignore most of the trivia involved in (for example) dealing with details like how a particular object is represented, and concentrate solely on what it really is instead. You'd only ever need to deal with most of that code if you saw a bug in that particular part. I try to avoid much of that, however, by creating classes so small and simple, that do so little, that it takes barely more than a glance to realize that it's obviously correct, so it's easy to ignore from then on. "
    },
    {
        "ID": "18564",
        "Question": "Everyone knows of the old adage that the best programmers can be orders of magnitude better than the average. I've personally seen good code and programmers, but never something so absurd. So the questions is, what is the most impressive feat of programming you ever witnessed or heard of?  You can define impressive by:  The scope of the task at hand e.g. John single handedly developed the framework for his company, a work comparable in scope to what the other 200 employed were doing combined. Speed e.g. Stu programmed an entire real time multi-tasking app OS on an weekened including  its own C compiler and shell command line tools Complexity e.g. Jane rearchitected our entire 10 millon LOC app to work in a cluster of servers. And she did it in an afternoon. Quality e.g. Charles's code had a rate of defects per LOC 100 times lesser than the company average. Furthermore he code was clean and understandable by all.  Obviously, the more of these characteristics combined, and the more extreme each of them, the more impressive is the feat. So, let me have it. What's the most absurd feat you can recount? Please provide as much detail as possible and try to avoid urban legends or exaggerations. Post only what you can actually vouch for. Bonus questions:   Was the herculean task a one-of, or did the individual regularly amazed people? How do you explain such impressive performance? How was the programmer recognized for such awesome work?  ",
        "Best answer": "While I can't officially vouch for it, I have always been impressed with Chris Sawyer developing Rollercoaster Tycoon almost entirely in assembly language. "
    },
    {
        "ID": "18585",
        "Question": "Often we as programmers see large organisations wasting huge sums of money on bloated and inefficient solutions to problems. This pains me greatly because I like organisations to benefit from best of breed solutions. However, my abilities as a programmer are limited when it comes to influencing the key decision makers and often my perspective on the matter is constrained to my own little technical world. So, my question is this. After encountering an egregious waste of money on some software and/or hardware that really got your goat, what did you do about it to get it fixed or were you doomed to bite the bullet and mutter forever under your breath? I'm interested in hearing your overall experiences and especially what lessons you learned about how to tackle this sort of thing in the future. Let's not name names, the experience of how to tackle the problem is more important than the actual offending product. ",
        "Best answer": "I've seen too many examples to name a favourite, but I've noticed a few general trends in my main field, web-development:  Vanity Websites. These are websites that serve no useful purpose to anyone outside the small organisation that commissions them and are built around an obsessive compulsion with logos, photos of themselves and self-indulgent waffle. The worst part is these are usually public-sector funded and commissioned by people who have no clue about the web. (For instance, once had a NHS hospital trust who wanted to develop a mini-version of Facebook for their own staff intranet). Paid for is Best. The mindset that insists that paid-for software must intrinsically be better than open-source. After all, it's paid for, right? I've seen so many clients insist on making stupid choices simply because they work in a culture that automatically discounts anything open-source as a matter of policy. Design by committee. This is where a huge group of people have a \"brainstorm\" and then try to incorporate every crack-pot idea there is into the design, inevitably resulting in a ill-thought through mess that compromises on everything in favour of trying to please everyone (and by everyone they mean the committee making the decisions, not the people having to use the application). Consultants. This is where you pay a middle-man (who knows neither business practices nor software development) to get in the way and cream-off money by protracting the development process with confusing techno-babble and business-speak.  "
    },
    {
        "ID": "18649",
        "Question": "First off, not my phrase: http://www.whattofix.com/blog/archives/2010/11/to-code-quickly.php Props to Mr. Markham. BUT, it got me to thinking about a lot of questions I have seen about being able to get things done. The approach advocated (setting a timer for a set period, in this case 50 minutes, but I've seen people talk about breaking procrastination by setting times as short as five minutes on tasks that you just cannot bring yourself to do, and then taking a short break) seems to be common sense, but lots of people advocate getting into the \"zone\" and staying there as long as possible, maybe many hours, rather than break their groove. I keep trying different approaches and find that each has its own strengths and weaknesses. What kind of technique do you use to be more EFFECTIVE (i.e., getting work done to the quality level demanded by your client / boss / etc. in the time frame allowed) in your software development and not just to spend more time at the keyboard? ",
        "Best answer": "I use that technique daily. My timer is 45 minutes of work for 10 minutes of rest. I also maximize the time on the computer to 4 hours per day. I understand this is not possible when your main task is coding. The rest of the time, I do any task that doesn't require a computer. The tool I use is a WorkRave. The author wrote it because he started to have physical problems due to its inability to stop coding for long hours. I'm less stressed and it affected positively my productivity. Since a few weeks, I also try some mindfulness techniques during the pause times. Delicious. Now regarding anti-procrastination techniques, I have one that beat everything I tried before: I manage a single task list, prioritized by importance. I pick the first in the list. I maintain the list (and calendar) with a combination of GTD and 7 Habits. To enhance the list effectiveness, write your tasks as next actions instead of descriptions (see the chapter Actions, Contexts & Projects in this Blog Post) "
    },
    {
        "ID": "18666",
        "Question": "I've got a bit of a challenge.  While I'm not \"in charge\" I'm certainly lead developer on a project and my team will be scattered to high heaven. I've got someone who works on Fridays only, and probably 2 people who will be 3-4 hours away in another state. (One of the lovely side effects of partnering with multiple universities). Anyway, how can I get them all on the same \"team\"?? I've read a bunch of Joel Spolsky's stuff, but I can't create the same kind of environment that he uses.  This is also the first project I'll be lead on, so it's kindof make-or-break. Specifically I'm looking for managing styles (personality), process models, project management  (scrum?), video conferencing tools, and the like.  I just don't have any resources for this kind of thing, and I'm afraid I'm going to be stuck doing too much work. Thanks, ",
        "Best answer": "This one is a tough one!  Have a google on \"distributed agile teams\" when you get the chance, but in short:  Communication is vital.  Find out how each team member prefers to communicate and come up with a medium that everyone is happy with. Make sure all decisions/conversations are open (not private), e.g. Archivable mailing list, commit log, IRC chat is OK if you save off the logs You probably want to adopt some sort of agile methodology, at least in terms of getting small units of work delivered often, that way you can all help each other out, see what others are up to and not 'disappear for weeks on end' See http://www.producingoss.org for some good tips (open source, but many of these can be applied). Use canned hosting (issue tracking, mailing lists, source control, build tool, CI etc), make it as painless as possible for everyone to get each others work and get it ti interoperate. Try to all meet at least once in person or over a video conference, we interact vastly differently with people we've met in the flesh.  Good luck! "
    },
    {
        "ID": "18669",
        "Question": "The two predominant software-development methodologies are waterfall and agile.  When discussing these two, there is often much focus on the particular practices that distinguish them (pair programming, TDD, etc. vs. functional spec, big up-front design, etc.)  But the real differences are far deeper, in that these practices come from a philosophy. Waterfall says:  Change is costly, so it should be minimized. Agile says:  Change is inevitable, so make change cheap. My question is, regardless of what you think of TDD or functional specs, is the waterfall development methodology really viable?   Does anyone really think that minimizing change in software is a viable option for those that desire to deliver valuable software?  Or is the question really about what sort of practices work best in our situations to manage the inevitable change? ",
        "Best answer": "Of course waterfall is viable. It brought us to the moon! And it's a agile coach talking here! Unless you can clearly identify problems related to the way you manage your projects, there is no valid reason to change. As an alternative of Agile and Waterfall methodologies, I will suggest YOUR methodology. Adapted to your specific business, your specific team, your products, you way of working, your company culture... It's why Scrum is called a simple framework instead of a methodology. Wanting to implement a methodology because someone on a blog you like talked about it is as stupid as letting problems going without doing anything. "
    },
    {
        "ID": "18704",
        "Question": "I periodically teach an introduction to programming course using Java. I want to give my students some exciting assignments that they can relate to or find interesting. At the very least, I want assignments that make sense and have an internal cohesion (for example, some very bad programming exercises seem contrived just so that you have to use the most recent programming construct covered). To give you an idea of scope, here's what's being covered:  The assignments must be in Java, using some external library can be done, but it would need to be a simple API and not a full framework Variables, Primitives and Strings Console input and output if, for, while Arithmetic and logical operators Simple graphics with line and shape drawing Static methods One-dimensional arrays  The students will not go into advanced topics (e.g., no recursion, no emphasis on inheritance). Thus, I'm not looking for complex projects: \"Have them write a C compiler. I did it when I was 5 and it was the only way I learned!\" If you have ideas that are more advanced than the scope, please post them on the \"Challenging\" question linked below, and not this one. Instead, what I'm looking for are interesting ideas that can be accomplished in a short program. For example:  Students can write a console version of the \"Star Wars Name\" Generator. This is effectively reading Strings and using substring, but I think it's fun. A similar variation would be a \"Mad Libs\" program. I used this one five years ago, and perhaps it's already \"stale.\" Using some trig supplied to them, students can draw regular polygons, and create interesting spiral shapes. It's also not out of the question to use some simple animation class with most of the code supplied to them. And if you know a Twitter or Facebook service that can be accessed with a simple API, I would be very interested to know.  Please note that this question is different from the \"Challenging Java questions for beginners\" Question. I'm not looking for challenging per se, just interesting. I think if students work on something where they can easily answer \"why would anyone ever want to program that?\" then they will learn better. Even simple problems like computing Miles per Gallon can be good. Although, if there's a way to update it to be more relevant, all the better. If you have an exercise from somewhere else (a book or a website), please cite the source. To help you think about the typical freshman today, check out the Beloit Mindset list, for many examples that will surely make you feel old. ",
        "Best answer": "Given the constraints, I'd suggest implementing a version of Hangman. It would allow for the students to demonstrate all of the techniques you are introducing them to, without being overly complex.   It can also be used as a developing project as the course progresses.  e.g.  once you have covered strings and variables starts out as a text version  e.g. You have 10 guesses left.      * * * E * * T What is your next guess?  then introduce loops to remove the cut and paste element from the code as the 10 guesses count down...  building up to having line graphics and the stick person being hung / saved at the end of the 5 week course. Like most other people who have experience of recruiting and interviewing programmers, it really makes me cringe that this level of tuition is needed at university, but alas, it probably will continue to be needed until schools treat programming as a serious subject on a par with mathematics or sciences "
    },
    {
        "ID": "18737",
        "Question": "I've just finished my master and I'm starting to dig into the laboral world, i.e. learning how programming teams and technology companies work in the real world. I'm starting to design the idea of my own service or product based on free software, and I will require a well coupled, enthusiast and fluid team to build and the idea. My problem is that I'm not sure which would be the best skills to ask for a programming team of 4-5 members. I have many friends and acquaintances, with whom I've worked during my studies. Must of those ones I have in mind are very capable and smart people, with a good logic and programming base, although some of them have some characteristics that I believe that could influtiate negatively in the group: lack of communication, fear to debate ideas, hard to give when debating, lack of structured programming (testing, good commenting, previous design and analysis). Some of them have this negative characteristics, but must of them have a lot of enthusiasm, nice working skills (from an individual point of view), and ability to see the whole picture. The question is: how to pick the best team for a large scale project, with a lot of programming? Which of these negative skills do you think are just too influential? Which can be softened with good leadership? Wich good skills are to be expected? And any other opinion about social and programming skills of a programming team. ",
        "Best answer": " Joel Spolsky said:  In principle, it’s simple. You’re   looking for people who are  Smart, and Get things done.   "
    },
    {
        "ID": "18843",
        "Question": "I am trying to understand the landscape of different approaches, and best practices, around the development of complex client-side JavaScript. I'm not sure what to label this class of application, perhaps heavy AJAX or RIA (but not plugins like Flash/Silverlight).  I'm referring to web apps with these characteristics:  Emulate rich/native desktop UX in JavaScript Contain most/all behavior in client-side JS, using the server as a data-API (JSON/Html-Templates).  This is in contrast to using the web-server for the UI rendering, producing all HTML in a page-refresh model. Some examples are:  Google Docs / Gmail Mindmeister Pivotal Tracker  As we move forward into HTML5, I can see this style of RIA development, with heavy JavaScript, becoming ever more common and necessary to compete. QUESTION: So what are the common approaches emerging around managing these kinds of heavy JS developments? Client-side code, as an app grows in features, is fiendishly complicated.  There are problems scaling a development effort across multiple teams with raw JS (or so I hear, and can well believe it). Google has approached the problem by building GWT that compiles from a higher level language (Java) to JS, leaning on the existing development infrastructure that the higher level language has (Eclipse, strong-typing, refactoring tools), along with abstracting browser compatibility and other issues away from the developer.  There are other tools, like Script# for C# that do something similar.  All this puts JS more in the role of an IL (Intermediate Language). ie. \"You never really write in that 'low level language' anymore.\" But this 'compile to JS' is not the only approach.  It's not apparent that GWT is the dominant approach...or indeed will become it. What are people doing with rich-client JavaScript? Some orienting questions:  Are most shops crafting JS manually (atop libs like jQuery et al)? Or are there many many different approaches, with no clear best-practice emerging? Are most shops avoiding RIA scale development in favor of the simpler to developer server-side/page-redraw model?  If so, will this last? Is compiling to JS perhaps an emerging future trend?  Or is this just wrong headed? How do they manage the complexity and refactoring of client JS? Modularization and distribution of work across teams? The application, enforcement, and testing of client-side patterns like MVC/MVP etc.  So, what are the emerging trends in this heavy-JavaScript and HTML5 future of ours? Thanks! ",
        "Best answer": "Most of the Web apps I see (and Web devs I've talked to) who are moving in this direction are using jQuery as their base. The whole reasoning behind GWT (and similar multi-level languages) is that JavaScript is too flakey/too fragile/too changeable for \"real programmers\" to use. But if you have a framework handling the flakey/fragile/changeable bits for you, then there's no reason to add on that extra layer of complexity. Just my opinion… "
    },
    {
        "ID": "18975",
        "Question": "Even with detailed specifications for exchanging data between computer programs, chances are that data generated by older versions of programs do not conform 100% to the specification, or that they use old obscure features which few people know how to implement them. As a result, companies have to keep a library of \"odd specimens\" for testing their software. In good software design, these quirks can be confined into a small layer of libraries called \"Abstraction Layer\". However, most abstraction layers cut off too many functionalities (in order to prevent the higher level software from touching the unstable parts of the lower level software).  Sometimes it is not possible to completely hide away the unstable parts. What are the strategies for coping with such necessary workarounds? ((Please pardon my poor language skills.) ",
        "Best answer": "Good documentation. Code comments go a long way towards telling the next developer why the hack was placed there, and why it needs to stay.    Hopefully the reason is sound. At some point, if enough technical debt is incurred, a refactoring should be considered. "
    },
    {
        "ID": "19028",
        "Question": "So, I have a web application that I need to build. The application will be done in a LAMP environment. I'm being given the resources to bring in 2 other programmers to work on my team. I have team lead and architecture experience before, so taking charge isn't a problem. The roadblock that I am experience is that I am currently living in a developing country. They have plenty of Software Engineer programs at universities, but no training/classes on really anything relating to LAMP. It is all C++/Java. So, I have a set of resumes I've culled as experience programmers that seem intelligent sitting in front of me. They come from various backgrounds, which include a couple Java developers, an old-school C++/MFC type guy, some C# devs, and a whole bunch of young out of school C++/Java developers. I'm honestly already leaning one way on which skillset I think would mold the easiest to web dev, but as I am not an application developer (exposure in school, but that was a while ago), I though I might open it up and ask you guys: Which skillset of these do you think would lead into the easiest transition to web development? I'm especially curious to hear from anyone who moved from application side over to web development. ",
        "Best answer": "A great developer who is \"smart and gets things done\" (via Joel Spoelsky - credit to @EricBoersma) will likely be more productive than a poor one no matter what language they work in Unless you're on an extremely tight deadline (and you need the specific skill set) then simply hire people who are smart and work hard.  They'll pick up the web development tech quickly enough, just give them some time to bash out a prototype or two first. "
    },
    {
        "ID": "19030",
        "Question": "In a programming interview if I am asked a question I already know the answer for, such as giving an algorithm for a particular problem. Should I disclose that to the interviewer? This issue only makes sense when there's a novelty aspect to the question.  One example is \"How would you reverse each word in a string, but not their order?\". There also seems to be a division between easier stuff, that you are \"supposed\" to know, such as my example and harder more contrived problems. Whats your policy and rationale for dealing with this issue?  If you are already familiar with the question/brainteaser, should you disclose this in addition to answering the question?  Is there any ethical dilemma involved with not disclosing your prior knowledge of the question?   ",
        "Best answer": "I'd just answer it without much hesitation or screwing around. Knowing the answer to a question isn't something evil, it means you've encountered it before and know how to solve it. Fix it, move on to the next. "
    },
    {
        "ID": "19158",
        "Question": "I recently read this article on knowledge sharing and immediately recognized the same problem within my own organization. My main goal now is to 'kill peer-to-peer collaboration' as the default method of communication for non-private, system related discussions. Otherwise you end up with all of the historical knowledge living in the heads of individuals, or lost in a massive email system. My question for the group is as follows:  What methods / software have you used to encourage more 'public' discussions among your developers?  Some initial ideas I had.. any feedback would be great:  Internal news group 'better' wiki software (using Sharepoint now) Message board  (I would love to have an internal instance of StackExchange, but don't think that is an option!) Note: As stated above, we already have a wiki, but I dislike the wiki idea because things are usually only added to the wiki after the fact, if at all. Thanks! ",
        "Best answer": "StackOverFlow for enterprise like explained in the article you mentionned? IMHO it's a terrible idea. It will reinforce competition instead of collaboration. You need cross division/department collaboration, not increasing their competition. Also imagine the extremely high negative impact of being downvoted by your coleague (in front of others) can have on your psychic health. Don't mix everything. However, an idea box much more uservoice.com where employees can post ideas (anonymously) and other employees upvote them (also anynomously) will have a positive impact. I developed a such platform few years ago for a very large banking institution, and it helped executives to identify what to improve in priority. "
    },
    {
        "ID": "19244",
        "Question": "I've come across the idea of Aspect Oriented Programming, and I have some concerns with it. The basic idea seems to be that we want to take cross-cutting concerns which aren't well modularized using object and modularize them. That is all very fine and well. But the implementation of AOP seems to be that of modifying code from outside of the module. So, for example, an aspect could be written that changes what happens when a particular object is passed as a parameter in a function. This seems to go directly against the idea of modules. I should not be able to modify a module's behavior from outside of that module, otherwise the whole point of modules are overturned. But aspects seem to be doing exactly that! Basically, aspects seems to be a form of code patching. It may useful for some quick hacks; but, as a general principle perhaps its not something you want to do. Aspect Oriented Programming seems to me taking a bad practice and raising to a general design principle. Is AOP a good practice?  Are certain programming problems solved more elegantly with AOP? ",
        "Best answer": "Aspect-Oriented Programming makes it possible to do certain types of programming that are difficult to do without unnecessarily scattering code throughout your application or library that is unrelated to the primary functions of your software (i.e. cross-cutting concerns).  Examples include:  Logging and Monitoring Performance analysis Debugging and Tracing Undo Functionality Validation of inputs and outputs Morphing the behavior of existing objects Object Filters Security Implementation Managing transactions  By confining such cross-cutting concerns to a single part of the application, and then referencing these features in the code via attributes, method call interception, or dynamic proxies, you allow the encapsulation of the cross-cutting behavior; this has all of the benefits (i.e. a single modification point) encapsulation would provide anywhere else in your application. The key point here is that AOP encapsulates behaviour that is 1) common throughout the application, and 2) peripheral to the application's primary functionality. "
    },
    {
        "ID": "19278",
        "Question": "(Originally posted on Stack Overflow but closed there and more relevant for here) So we first interviewed a guy for a technical role and he was pretty good. Before the second interview we googled him and found his MySpace page which could, to put it mildly, be regarded as inappropriate.  Just to be clear there was no doubt that it was his page (name, photos, matching biographical information and so on). The content was entirely personal and in no way related to his professional abilities or attitude. Is it fair to consider this when thinking about whether to offer them a job? In most situations my response would be what goes on in someone's private life is their own doing. However for anyone technical who professes (implicitly or explicitly) to understand the Internet and the possibilities it offers, is posting things in a way which can so obviously be discovered a significant error of judgement? EDIT: Clarification - essentially it was a fairly graphic commentary on porn (but of, shall we say, a non-academic nature).  I'm actually more interested in the general concept than the specific incident as it's something we're likely to see more in the future as people put more and more of themselves on-line. My concerns are not primarily about him and how he feels about such things (he's white, straight, male and about the last possible victim of discrimination on the planet in that sense), more how it reflects on the company that a very simple search (basically his name) returns these things and that clients may also do it.  We work in a relatively conservative industry. ",
        "Best answer": "Oh Sex! You would be surprised of the diversity of the thing. Yesterday on TV, there was a documentary on fetishism. That was very instructive. I wasn't aware of all the things people could do... did you knew about zentai???  My opinion is regardless their private practices (as soon as it doesn't hurt someone else), such difference should not be used to pick, or not pick someone, just like his skin color, religion or political orientation. I want also add that if he is happy in his sexuality, this will affect his work at your company positively. On ther other hand, it's perfectly understandable that his behavior outside the company may affect the company. And therefore, some juridictions may approve firing someone that had bad behavior. And it's perfectly understandable that a company won't hire someone telling on his blog that his thing is animals, or that he likes to be nude in parks. "
    },
    {
        "ID": "19292",
        "Question": "We all have tasks that come up from time to time that we think we'd be better off scripting or automating than doing manually. Obviously some tools or languages are better for this than others - no-one (in their right mind) is doing a one off job of cross referencing a bunch of text lists their PM has just given them in assembler for instance.   What one tool or language would you recommend for the sort of general quick and dirty jobs you get asked to do where time (rather than elegance) is of the essence? Background: I'm a former programmer, now development manager PM, looking to learn a new language for fun.  If I'm going to learn something for fun I'd like it to be useful and this sort of use case is the most likely to come up. ",
        "Best answer": "Python The obvious answer (and with good reason) is Python. Its a solid language, available cross platform. As its dynamic you can run it interactively which is great for lashing stuff together and it has a fairly large selection of libraries so its a general purpose language so can be applied to most problems. "
    },
    {
        "ID": "19317",
        "Question": "I keep running into the same problems.  The problem is irrelevant, but the fact that I keep running into is completely frustrating. The problem only happens once every, 3-6 months or so as I stub out a new iteration of the project.  I keep a journal every time, but I spend at least a day or two each iteration trying to get the issue resolved. How do you guys keep from making the same mistakes over and over? I've tried a journal but it apparently doesn't work for me.  [Edit] A few more details about the issue:  Each time I make a new project to hold the files, I import a particular library.  The library is a C++ library which imports glew.h and glx.h GLX redefines BOOL and that's not kosher since BOOL is a keyword for ObjC. I had a fix the last time I went through this.  I #ifndef the header in the library to exclude GLEW and GLX and everything worked hunky-dory. This time, however, I do the same thing, use the same #ifndef block but now it throws a bunch of errors.  I go back to the old project, and it works.  New project no-worky. It seems like it does this every time, and my solution to it is new each time for some reason.  I know #defines and #includes are one of the trickiest areas of C++ (and cross-language with Objective-C), but I had this working and now it's not. ",
        "Best answer": "I'd suggest determining what triggers the issue, and restructuring your development process to avoid that scenario. What 'restructuring' entails is highly dependent on the problem. It ranges from abstracting some behavior into a seperate class to changing the composition of your team. A journal detailing the context of the incident and resolution approaches can certainly help you converge on the root cause and/or a general solution. Once you've determined that there are a few obvious options:  If the cause is avoidable: Try to avoid triggering the root cause next time. If the solution proves to be simple: Implement the general solution whenever the problem occurs. Restructure your development process so that it naturally avoids the issue.  The options available depend on the information about the issue you have, and the amount of control you have over the development process. "
    },
    {
        "ID": "19320",
        "Question": "We are trying to make a decision about if we should create a program in WPF w/ a WCF server, and I was wondering what the current opinions are of WPF. I was looking around online but most of the arguments I saw for/against WPF were dated a year or two ago, and about WPF3.5. I was wondering if those opinions have changed in the past year or two with the release of WPF4.0. Some notes.... The program we are making is for use by a single-company in a controlled enviornment. With the exception of one or two Vista laptops, most users are on Terminal Servers or WindowsXP. It is not media-heavy and doesn't need a pretty UI (although it won't hurt). It needs to be extensible and allow for easy additions later on. Security and Performance are our key concerns. EDIT Primary concern about WPF is Support (MS and community), memory/cpu usage on terminal servers, and finding people to work on the project now and maintain it in the future. Some smaller things of concern are lack of library controls, development time, and learning curve. The primary alternative we were considering is ASP.Net/jQuery. ",
        "Best answer": "I like it. IMO it is good. WPF is just presentation, so it won't affect security, unless there are some security issues I've not heard about. I've not seen any metrics comparing WPF to WinForms performance-wise. I'm guessing they're comparable. This SO question is about the performance of WPF v WinForms: https://stackoverflow.com/questions/368371/performance-comparison-of-winforms-and-wpf I like  WPF because XAML is declarative and makes saying how I want the UI to look and act easier than with WinForms. This is only true after I have learned the basics of MVVM. "
    },
    {
        "ID": "19326",
        "Question": "I've had experience as a developer or team lead on several projects that have been outsourced and have seen less than stellar results in all cases. Most of these projects have employed a waterfall philosophy, with large kickoff meetings, months-long requirements gathering phases, plenty of conference calls, and innumerable emails. One thing that's always frustrated me is an absence of early access to the code. The contracts are setup in a way where the offshore team is responsible for meeting functional requirements, which is what the executives are concerned with. This means, however, that architecture decisions, implementation details, pattern usage, and things that concern other developers are so deeply settled by the time the product is delivered that we're unable to offer feedback or request any real changes.  Has anybody here managed an offshore project that hasn't experienced these problems? Specifically, I'm wondering if there's any way to structure the contracts to compel offshore teams to work in shorter cycles and re-factor or re-design based on feedback from on-shore developers. I haven't had too much experience with agile methodologies (I agree with the general principles, but work in a conservative shop that has it's entrenched methodologies), but wonder whether these could somehow be adapted to help manage offshore development. Overall, I'm looking to minimize the surprises and maintenance nightmares that inevitably arise when a development team is left to work in isolation for months at a time.  ",
        "Best answer": "I have done it and I prefer it. The contract will have to be on a time and materials basis, rather than a fixed bid. The fixed bid theoretically moves some risk to the vendor and may be more competitively priced but in reality it generates a need for so much more work like you describe, not to mention fights over change control definitions etc. So we just hire x number of bodies and work a lot more dynamically with them, assigning work on a task basis. They do not participate in Scrums though - there are onshore leads that will represent them. Essentially the onshore looks like he is doing the work of three people, but he is parceling out tasks to offshore. The onshore/offshore model does cost you some higher rates with the onshore person obviously. "
    },
    {
        "ID": "19344",
        "Question": "Lets suppose that I have a class which represent an image and has a number of methods. class Image {     circle(x,y,radius,color);     square(x,y,w,h,color);     floodfill(x,y,color)     clear(); }  Furthermore, I want to have undo functionality. A simple way of implementing this is to keep a list of all the actions that have been performed. When I undo, I just re-run all of the actions. One solution would be to implement an aspect, something like this: aspect ImageUndo {     on Image::circle, Image::square, Image::floodfill     precall(object, args)     {          object.actions_list.add(args)     } }  Essentially, this aspect has now modified the behavior of Image. That gives me concern. In particular, another programmer unfamiliar with the existence of ImageUndo aspect may run into the following problems:  He adds a method, and it does not work with the undo functionality. Attempting to debug the undo mechanism, it is not clear where the actions_list get added to.  On the other hand we could have class Image {     @undoable_action     circle(x,y,radius,color);      @undoable_action     square(x,y,w,h,color);      @undoable_action     floodfill(x,y,color)      @undoable_action     clear(); }  Which does not bother me as much because it gives an idea of where to look for the undo code and makes is so that the new coder will probably notice it an automatically add it to a new method. To summarize: aspects (at least those like the one I showed) seems to bring \"implicit magic\" into what code does. It seems to me that the implicitness is dangerous and we should really make it explicit. Are there good reasons for the implicitness? Do people who actually use AOP write code that does this sort of modification?  Note: this is a reworking of Are certain problems solved more elegantly with AOP? which was closed because my previous version came across, unintentionally, as ranting. ",
        "Best answer": "Classes that implement cross-cutting concerns have nothing to say about the core functionality of classes being AOP'd.  That's the core idea here. Your example is one of an undo action.  Let's take it a step further: we will make a deep copy of your class and store it somewhere.  If we want to perform an undo, all we have to do is reverse the deep copy.  This will \"roll back\" the class to its original state.  You don't even need to annotate the class members to make it work (although you could annotate the class itself if you wanted the deep copy to operate automatically). Here's my question: Does this deep copy process have anything at all to do with the primary functionality of the class being deep copied?  Does the deep copy even care what the class does? Serializing and deserializing objects are another example of this.  The serialization process has no knowlege of the behavior of the class, and the objects being serialized have no knowledge of the serialization process.  It's a powerful decoupling tool. "
    },
    {
        "ID": "19392",
        "Question": "Okay, I'll cop to it: I'm a better coder than I am at databases, and I'm wondering where thoughts on \"best practices\" lie on the subject of doing \"simple\" calculations in the SQL query vs. in the code, such as this MySQL example (I didn't write it, I just have to maintain it!) -- This returns the username, and the users age as of the last event.  SELECT u.username as user,         IF ((DAY(max(e.date)) - DAY(u.DOB)) &lt; 0 ,           TRUNCATE(((((YEAR(max(e.date))*12)+MONTH(max(e.date)))        -((YEAR(u.DOB)*12)+MONTH(u.DOB)))-1)/12, 0),          TRUNCATE((((YEAR(max(e.date))*12)+MONTH(max(e.date))) -                    ((YEAR(u.DOB)*12)+MONTH(u.DOB)))/12, 0)) AS age    FROM users as u JOIN events as e ON u.id = e.uid ...  Compared to doing the \"heavy\" lifting in code: Query:  SELECT u.username, u.DOB as dob, e.event_date as edate FROM users as u JOIN events as e ON u.id = e.uid  code:  function ageAsOfDate($birth, $aod) {    //expects dates in mysql Y-m-d format...      list($by,$bm,$bd) = explode('-',$birth);      list($ay,$am,$ad) = explode('-',$aod);       //Insert Calculations here       ...      return $Dy; //Difference in years }  echo \"Hey! \". $row['user'] .\" was \". ageAsOfDate($row['dob'], $row['edate']) . \" when we last saw him.\";   I'm pretty sure in a simple case like this it wouldn't make much difference (other than the creeping feeling of horror when I have to make changes to queries like the first one), but I think it makes it clearer what I'm looking for.  Thanks! ",
        "Best answer": "You want to do all set-based operations in the database for performance reasons. So aggregation functions, sorting functions, joins etc.  This age calculation, I'd do in code. The only reason I might ever do something like this in a database query is if it required lots of columns that I wouldn't otherwise select that could actually amount to enough data to meaningfully slow down my query. Selecting a few integer values will not make a meaningful performance difference. And even if it makes a moderate performance difference I will be biased towards keeping this logic in the application code. "
    },
    {
        "ID": "19419",
        "Question": "In web development, a mashup is a web page or application that uses and combines data, presentation or functionality from two or more sources to create new services. There are many sources for getting data (raw information). Governments are good examples. However many of those cost and have complicated licensing. For instance SMHI in Sweden sells temperature data meanwhile our neighbor country Norway gives it away for free. One would say what kind of application are you building and what data do you need?  I would say give me some interesting data and I will do a good mashup of it. I could just plot it on Google maps for instance. So the question is: Where can I get free data? Note: I'm not interested in data mining using a webcrawler! Please share your sources with me. ",
        "Best answer": "For general information, a good place to start might be Wikipedia's Open Data page - specifically the bottom parts of the page (Organisations promoting open data, See also, and External Links) I was going to turn the rest of this post into a list of data sources for country and regional based data ... but the Guardian UK Newspaper has already gone and compiled a World Government Data Store so no point duplicating the effort, instead I'll just point to... guardian.co.uk/world-government-data ...for a catalog of a lot of different country and city level Open Data Initiatives. (It's not UK based, though it does seem to focus on English-speaking countries.) At time of edit, it contains data for UK, US, Australia, Canada, New Zealand, and Basque Spain. One thing not listed on there but worth mentioning is the Ordnance Survey UK mapping data. "
    },
    {
        "ID": "19425",
        "Question": "What are the pros and cons of having static object creation methods over constructors? class Foo {   private Foo(object arg) { }    public static Foo Create(object arg) {     if (!ValidateParam(arg)) { return null; }     return new Foo(arg);   } }  Few that I can think of: Pros:  Return null instead of throwing an exception (name it TryCreate). This can make code more terse and clean on the client side. Clients rarely expect a constructor to fail. Create different kinds of objects with clear semantics, e.g. CreatFromName(String name) and CreateFromCsvLine(String csvLine) Can return a cached object if necessary, or a derived implementation.  Cons:  Less discoverable, more difficult to skim code.  Some patterns, like serialization or reflection are more difficult (e.g. Activator<Foo>.CreateInstance())  ",
        "Best answer": "The biggest drawback with static 'creators' is probably limiting inheritance. If you or the user of your library derives a class from your Foo, then Foo::Create() becomes pretty much useless. All logic defined there will have to be rewritten again in the inherited Create(). I'd suggest a compromise: define a constructor with trivial object initialization logic that never fails/throws, then define creator(s) with caching, alternate construction etc.  That leaves the possibility of deriving and you benefit from having creators for a given class. "
    },
    {
        "ID": "19427",
        "Question": "I've built a ASP.NET MVC 3 (RC at the moment) site that uses OpenID login system. I was still learning about OpenID while implementing this so I commented the code heavily. The result is a site that let's users login/register with OpenID, add other OpenIDs to their account and also remove them. This little project can be then used as a starting point for any new project that would use OpenID login system. It can also be used as a resource for people to learn OpenID with. I decided to release this project as open source. This will be my first open source project and I need to decided what license to use. I want people to be able to use this for any purpose they wish for. They can learn from it, use it for commercial or non-commercial projects and make their own forks of the code. It would also be nice for others to be able to contribute back to the project with stuff like bug fixes on sites like GitHub. But I'd like to be the copyright owner of the code that is under my control. For example the code that is in my GitHub repository (I'll call this the main code base). I've heard that for this I need to get every contributor, that adds code to this code base, to give me the copyright for their contribution. How exactly does this work? I also use other licensed (mostly open source) resources in my projects. Here's their list and their licenses:  DotNetOpenAuth (Ms-PL) T4MVC (part of MvcContrib which is licesned using Apache License 2.0) ASP.NET MVC (Ms-PL) ADO.NET Entity Framework CTP4 (I couldn't find a license)  I of course want to use the main code base for any type of projects I want. Commercial, non-commercial, open source, ... So I have some very important questions here:  Which license should I use? I think GPL or LGPL is not suitable here. I was looking at Apache 2, New BSD, MIT and Ms-PL. Ms-PL seems to be a good fit as, but I'm not sure. What restrictions and/or obligations do I have towards the resources I use in this project? I think I read somewhere that I have to add -LICENSE.txt for Ms-PL resources. Is that true? How does this work for Apache 2 and other licenses? What do I have to do if I modify any of these resources' code and then use that in my project? I'd also really like a \"as-is\" clause in the license, so people can't sue me if something goes wrong while they're using my code. Do I need to add anything to my files to make clear what the license is? If so, how do I format that?  Also one last thing. If I decide to make a Visual Studio template out of this samples how do I license that? ",
        "Best answer": "Since you're using MS-PL and Apache 2.0 components, you're restricted by the MS-PL license and the Apache 2.0 license.  This means you can't use the GPL anyway, since it's incompatible with MS-PL, and version 2 of the GPL is also incompatible with Apache 2.0.  Given that, I would suggest releasing your parts either under MS-PL, Apache 2.0, or a BSD-style, so you're not adding requirements.  You can read the licenses to see what you have to do.  They aren't long.  With MS-PL, you can do pretty much everything as long as everything's released under MS-PL and you include the entire license (which isn't long).  With Apache 2.0, it's pretty much the same, except that you need to include any NOTICE file.  You do have to include all attributions, etc., and that's pretty standard across OS licenses. You really can't add to the licenses.  You are free to put any disclaimers you want.  You can put them in a NOTICE file, which under Apache 2.0 requires them to be preserved. You should list which files are under which licenses, and you do have to include full copies of MS-PL and Apache 2.0 in the package. Be careful about ADO.NET Entity Framework CTP4, as you say you can't find a license.  If you can't find a license, ordinary copyright law applies, and you can't legally use it.  You may want to write to whoever owns that code, and see what license they use. As far as copyright ownership goes, you can never retract open source licenses for the versions you release under them, but if you own all applicable copyrights you can relicense as you wish.  Some companies, like MySQL AB, released what they had under the GPL, and would sell other licenses for money, so that (say) another company could use MySQL as a part of their commercially sold product without having to release under the GPL. Since you're using components owned by other people, you really can't do that with the entire project, but you could with your portions. To do that, you'd have to get everybody who contributes to fill out a copyright assignment form, along with some proof that they do own the copyright (and, for example, that it doesn't count as work for hire for an employer).  This does tend to diminish user contributions, so you may want to skip it entirely.  You might want to look at what the Gnu project does, since they do want complete copyright, and emulate them. "
    },
    {
        "ID": "19449",
        "Question": "I have been developing Windows GUI applications since many years and jumped into .NET in early 2005. .NET is a undoubtedly a remarkable platform and I am still using it, but with a variety of technologies out there, I don't want to remain dedicated to this camp. I want to learn new languages with which I can develop GUI applications. I am learning Ruby and just installed Python. I read about WxRuby, a framework for developing Windows GUI app. in Ruby. I am searching for a similar framework for Python. Apart from that I want to know which language is more suitable for a production-level GUI app. I doubt that Ruby is more focused on Web platform with it's glamor, Ruby on Rails. I know that I may not get those rich .NET classes and that impressive Visual Studio IDE, but still I want to follow the road less traveled. I don't want to go with IronPython and IronRuby, however sometime later, I may dip hands to explore them. ",
        "Best answer": "Check Qt. it's arguably as rich as .NET, and the IDE (QtCreator) is simple yet pretty powerful.  Of course, it's better used on the native C++, but the Python binding is kept complete and up to date. On top of it, it's really crossplatform, and that includes mobile platforms now too :-) "
    },
    {
        "ID": "19451",
        "Question": "I'm in charge of managing mobile application development at my company, and I am currently building a mobile device \"library\" for testing. Essentially, we want to have a representative device in-house for each of the OSes we are developing for, currently iOS (iPhone-only), Blackberry, and Android. Simulators only go so far, but I'm placing into the process a step to test software on the devices themselves. The problem we're finding is with Android. I don't think any of us here ever really understood just how fragmented the whole platform is until we started looking at devices to acquire. We are going to wait until v2.3 of Android is released, but which products to choose? Do we go by the most popular by market share? Do we get a small range of products by specs from least to most powerful overall?  We're trying to avoid having to manage a dozen different devices to test each app, if not because of cost if only for the repeated time sink. How do you manage the testing of your Android software on physical devices? UPDATE: To bring some context to what I am asking, the applications being created are for educational purposes only, and there are no plans to use any of the hardware features of the devices themselves. We are simply displaying content in a variety of ways: eBooks, test questions, flash cards, etc. that may or may not be downloadable content. So, no cameras, GPS, user-driven content management, or anything like that; just as device-agnostic as we can possibly make it. ",
        "Best answer": "Presumably, you won't have to code exceptions for various devices (i.e. all of the Android devices should behave the same).  Accordingly, you should only need one Android device; it should be a relatively popular one.  By the time v2.3 of Android is released, maybe the market will be less fragmented. "
    },
    {
        "ID": "19549",
        "Question": "The focus of this question: Some software performs \"extra work\" in order to increase the chance of a \"eventually successful/satisfactory\" outcome, despite one or more internal errors in the software, which requires a longer execution time when those errors happen. All these happen without the user's knowledge if the outcome was successful. Definition of complex software:  Contains code written by (contributed from) more than 10 developers over its lifetime, and not written in the same time frame Depends on more than 10 external libraries, each with caveats A typical software task (for generating a result wanted by the user) requires 10 or more input parameters, where most of them have default values but are configurable if the user needs control. Most importantly, software that has the appropriate complexity relative to the task being performed, i.e. not unnecessarily complicated.  Edited: What is complex? Please see There is a big difference between Complex and Complicated. (direct link) Definition of Redundancy/Robustness within this question:(Added Robustness based on comments)  If a software task failed when the current set of parameters was used, try different parameters.   Obviously, there must be inside knowledge that those \"different\" parameters use a different code path, possibly resulting in a different (hopefully better) outcome. Sometimes these different code path are chosen based on observations of the external libraries.  At the end, if the actual task performed is slightly different from the user's specification, the user will receive a report detailing the discrepancy. Finally, like the 10-plus configurable parameters, the redundancy and reporting are also configurable.  Example of such software:  Database Migration   Business database Source control database, etc.  Batch converting between a Word document and an OpenOffice document, PowerPoint and OpenOffice Draw, etc. Automatic translation of an entire website Automatic analysis of software package, such as Doxygen, but where the analysis needs to be more dependable (i.e. not just a documentation tool) Network communication, where packets may be lost and a number of retries are expected  This question was originally inspired from How do you deal with intentionally bad code? but is now focused on just one of the causes of software bloat. This question does not address any other causes of software bloat, such as addition of new features. Possibly related:  How to deal with complex codes in (huge) projects How do people manage to write and maintain extremely complex and hard to read code?  ",
        "Best answer": "This is a business question, not a technical one. Sometimes I'm coding with researchers or on a prototype, so we'll build something with very low robustness. If it breaks, we fix it. There's no point in investing in extra magic if we're going to throw away the code soon. But if the users of your system need it to be robust, you should build it that way. And you should make it robust specifically in the ways that you and they need to maximize long-term success, while ignoring the kinds of redundancy/robustness that they don't need. Generally, I start rough and then add robustness over time. I frequently make questions like this part of the normal planning process. I generally work in the Extreme Programming style, where we make a long list of desired features, and I put robustness features in there, too. E.g., \"System survives the failure of any single box,\" gets mixed in with things like \"User can join using Facebook credentials.\" Whichever one comes up first, I build first. "
    },
    {
        "ID": "19584",
        "Question": "In cluster computing, there seems to be two options: task redirection and task splitting. Task redirection seems easy enough, you just have the master dispatch the small calls to other nodes in the cluster for processing (eg webserver clusters (I think)). Task splitting however seems wildly more complex. Since I don't think you can have two threads of the same program run on different machines, meaning you have to split up the work. How though does one split up the work? I can see some stuff like rendering or video encoding just because you can tell each node to work on a different part of the video, but when you want to do things like calculate the 5 trillionth digit of pie, how would you split that up? Or even in science where you need to simulate weather or other resource intensive tasks? In short, how do you split up tasks that aren't really designed for splitting up? ",
        "Best answer": "Not every task is suitable for parallel processing. Factoring is, but long division isn't. We use the term Parallel algorithm to describe tasks which are designed to be executed in parallel (potentially on multiple computers, or using multiple cores of a single computer). "
    },
    {
        "ID": "19770",
        "Question": "I need to quickly train Fortran 90 developers into object oriented design, good coding, and general coding practices to make code maintenance easier and accessible to everybody. Their current style is the usual:   long routines that do too much modules are aggregated in \"family mode\" (routines that have something to do with different kind of objects who share a common use are all in one module, instead of having different modules for different type) huge globals module with hundreds of variables general un-greppability of identifiers  This is just out of my head. I did a course to one of them explaining OO and modularization in terms of a Pen (with methods such as uncap(), and members like inkLevel) and I was very successful in diverting his point of view. I also gave him the exercise to code in OO style a very simple textual adventure, where the player can move around rooms. I also introduced the concept of patterns and antipatterns. I would like to know any hints and suggestions on how to perform this task at best.  Thanks ",
        "Best answer": "experienced procedural programmers - if they're good at it - have internalized models of code construction that tend towards object-orientation, but they don't realize it. The same way good database designers tend to create tables in third normal form, even if they've never heard of database normalization. chances are the big ugly balls of mud you're seeing are ancient artifacts, laden with decades of technical debt and emergency changes. when teaching OOP to procedural programmers (caveat: many years ago) I found that starting with what they know and what they think is good and building up from there into objects and classes makes it \"click\" much faster than lecturing on abstract concepts and coding talking animals. example: a good function uses all of its parameters, and only its parameters. a good data structure contains only elements that are logically related to the 'key'. a good module typically centers on one data structure (or an aggregate) and contains the functions that produce, use, and consume the data structure.  remember that the habits these programmers have developed have served them well (as far as they know!) for a very long time; don't tell them that they need to 'forget all that and start over', that just breeds resentment. Tell them that everything they already know is still correct, it's just reorganized. Then show them how to reorganize it, then show them the benefits of the reorganization. a useful exercise might be to have them code something simple but useful 'the old way', then walk through the refactoring that would make it work 'the new way'. The Socratic method (asking questions instead of showing them directly) is entirely appropriate for this kind of exercise. good luck! "
    },
    {
        "ID": "19783",
        "Question": "My day job is  java/web developer. I have been using eclipse for ~5years. I think its excellent and I also use Webstorm for javascript and html/jsp.   I do on occasion need to ssh into server and mess around with config files; for this I use vi and it pains me. I have to get up a webpage listing the syntax/commands : press escape, then asterix, turn around three times and the text will be entered two lines above your cursor. Its so unintuitive to me, and I imagine anyone who grew up in the late eighties nineties. Here are the main reasons I think eclipse is brilliant(and I assume other IDE's), and do not switch to emacs and/or vim.  Error highlighting with no need to recompile project. Code assist. Refactoring. Opening call hiearchy/Opening declaration. Fully integrated with source control. Debugger is included. availablity of 3rd party plugins - eg findbugs/checkstyle.  One of the arguments I hear is that with emacs/vim you can create your own plugins - well OK, but you can do that in eclipse too. But you don't need to as everything is already there!  Its like saying buy this half built car, you can build the rest yourself. Why are people using emacs/vim ? Do people who use it actually work on complex object-oriented projects in large organisations ? What are the reasons to switch to vim/emacs. How would my productivity increase if I did switch? ",
        "Best answer": "Emacs and Vi still have a place.   They are ubiquitously available in Unix and Unix-like environments, and can be installed on most other popular platforms. They are popular and stable, so learning them once pays off over the long run. They run over a text terminal, so you can use them in telnet and ssh sessions. They provide editing modes and syntax highlighting for a wide variety of languages, including very new and very rare languages. (This is one of my favorite advantages.)  The key to understanding these programs, though, is to know what problems they were originally meant to solve. For Vi this was editing text files over terminal connections as slow as 300 Baud. In that environment you don't want to display menus or radically change the screen contents if you can avoid it.  Emacs was intended to be used in a faster environment. It's strength was that it could be loaded once and never exited. The user could accomplish any other task they needed from Emacs without leaving, and often in a more friendly way than if they had to do it from the command line. People didn't have a graphical desktop environment with an Emacs window open. Emacs let the user accomplish almost any normal task (and many strange ones) with just a few key strokes. Anything not built in could be scripted. Obviously peoples' needs have changed a lot since these programs were introduced, but they still have some real strengths. I've learned the basics of both, and use them on a weekly basis. Still, I think their strengths are often overstated. They have achieved such legendary status that people don't admit to their weaknesses and instead tend to think they are doing something wrong if Emacs/Vi doesn't make them more productive than Eclipse  or Visual Studio. Now to the point. Java is a popular language with excellent support in Eclipse, and odds are you are developing code on a modern operating system that lets you quickly accomplish common tasks and script others without doing it through your IDE. I don't think it would make sense for you to switch. "
    },
    {
        "ID": "19842",
        "Question": "As part of a continuing quest for knowledge, I'm spending some time this week learning the basics of F# as an introduction to functional programming. I think I am understanding the construction of software under this model and some of the proposed benefits, but am still a little fuzzy on the real-world use cases. I was hoping that a F# success stories, or at least a few applications that I'd be familiar with might make it a little clearer about when F# (or any functional programming language) would be appropriate for a project. So back to the question, what are some well known applications that use F# or a similar functional programming language?  BTW: I am also open to hearing about some lesser-known apps, but if you mention one, please give the basics of what the app is about and why the language used was the appropriate choice. ",
        "Best answer": " The entire F# system (compiler, libraries and tools) IronJS (already mentioned) The XBox Live TrueSkill algorithm, as well as the whole Achievements system, the ranking system and the matchmaking system surrounding it  I guess the main reason why there are no \"well-known\" applications written in F#, is because it takes years, even decades to become \"well-known\" and F# simply isn't that old yet. However, the overwhelming majority of applications are not \"well-known\". There is only one PowerPoint, but there are literally millions of hand-written custom little one-off in-house payroll apps. And F# is certainly used in-house in many financial companies, doing scientific and numeric computing in, say, biotech or greentech, doing statistics god knows where and so on. These applications aren't well-known, either because they simply aren't \"sexy\" or often because the companies consider F# their secret weapon which gives them a competitive edge. (OCaml, Smalltalk and APL fill similar roles. Many financial companies use APL, for example, but they don't widely publicize it. Indeed, oftentimes the APL users aren't even programmers, and thus wouldn't even know where and how to publicize it even if they wanted to.) "
    },
    {
        "ID": "19888",
        "Question": "This started as a \"Note to Self,\" so please excuse me if the frustration is all too evident and the writing is less than stellar... Three major subjects I've had at the forefront of my mind lately:  Motivation Learning (Curiosity) Doing (Making)  I've been studying motivation and incentives for months now. It seems there are an infinite number of different motivations that people might have for doing things (I realize that sounds trite but bear with me). I've been really drawn to it because I'm desperate to find out why I do the things I do and why I don't do the things that I want to do but don't do. I'm in the midst of reading Paul Graham's excellent Hackers and Painters book. In it, he makes the case that hackers and painters are very similar because they are both \"makers.\" Painters make paintings. Hackers make software. Painters don't necessarily need to understand the chemical composition of paint to make beautiful paintings. And hackers don't necessarily need to know 1's and 0's to make beautiful software. Graham then draws the distinction between disparate computer science fields:  some people seem to be studying mathematics some people seem to be studying the computers themselves the hackers are making software.  The difference is incredibly important. It seems the motivation for some is to make beautiful things. And the motivation for the others is to learn out of curiosity. Certain motivations seem obvious to me, but curiosity seems a bit less obvious. I would certainly consider myself as a curious person with a seemingly unquenchable thirst to learn just about everything I can. But this is exactly where the problem comes up. The thing that scares me so much is that I desperately want to make things. I desperately want to do things. I want to write a book. I want to paint a painting. I want to compose a song. I want to do things like travel. But the strangeness is that I also want to learn things. I want to learn to play guitar. I want to learn about art history. I want to learn more about philosophy and literature. The key seems to be the balance between learning and doing... between studying and making. While I'm not sure how much one should learn about a given thing before doing it, I know for certain that I find myself constantly on one side rather than the other. As it stands now (and as far as I can tell I've always been this way), I am a learner and not a doer. I've read great books. I've practiced guitar for years. I've spent countless hours studying programming. But I've written 0 books. I've composed 0 songs. I've coded 0 beautiful programs. I've painted 0 beautiful paintings. I've started 0 viable businesses. The scary part of all this is that there are probably countless unfinished works of art in the world. Is this my misanthropic revenge against society and culture to never produce or finish any of the works of art that I start? Perhaps the worst part (aside from this being my natural inclination), is the fact that I f***ing know better. I just finished books like \"Getting Things Done\" and \"Making Ideas Happen.\" I've aggregated and synthesized countless words of wisdom on how to do things and how to make things. Imagine the horror of going through life without being able to do the things you want to do. If this is something you've struggled with (and hopefully overcome), please share. If not... perhaps some delicious pity would make me feel better. [UPDATE: Just wanted to send a quick thanks to everyone that shared their thoughts. I deliberately left the question somewhat open-ended in hopes of encouraging discussion and having others refashion the central problem around their similar experience, and I think it worked out great... there's a lot of amazing insight here to work with and it was really helpful. Thanks again.] ",
        "Best answer": "Here's a motto for you: Anything worth doing is worth doing badly. You speak of painting beautiful paintings and coding beautiful programs.  I suspect you also want to write good novels and compose good songs.  You don't get to do those things, by and large, without working for a long time first and making bad things. So, go out there and do cruddy work.  Write ugly programs, and try to see why they're ugly and what you could do better.  Find other people to tell you what you did wrong.  Try to learn about what you're doing, and remember that lots of learning materials make more sense with a little more experience under your belt  Do that with everything you want to do. If you want to be a maker, make something.  Then make another thing.  Don't sweat the quality.  Then do it again.  Be reasonably proud of having made things, good or bad.  Most people don't even write bad programs or books, or paint something badly.   "
    },
    {
        "ID": "19893",
        "Question": "Our team is divided on this and I wanted to get some third-party opinions. We are building an application and cannot decide if we want to use .Net WPF Desktop Application with a WCF server, or ASP.Net web app using jQuery. I thought I'd ask the question here, with some specs, and see what the pros/cons of using either side would be. I have my own favorite and feel I am biased. Ideally we want to build the initial release of the software as fast as we can, then slow down and take time to build in the additional features/components we want later on. Above all we want the software to be fast. Users go through records all day long and delays in loading records or refreshing screens kills their productivity. Application Details:  I'm estimating around 100 different screens for initial version, with plans for a lot of additional screens being added on later after the initial release. We are looking to use two-way communication for reminder and event systems Currently has to support around 100 users, although we've been told to allow for growth up to 500 users We have multiple locations  Items to consider (maybe not initially in some cases but in future releases):  Room for additional components to be added after initial release (there are a lot of of these... perhaps work here than the initial application) Keyboard navigation Performance is a must Production Speed to initial version Low maintenance overhead Future support Softphone/Scanner integration  Our Developers:  We have 1 programmer who has been learning WPF the past few months and was the one who suggested we use WPF for this. We have a 2nd programmer who is familiar with ASP.Net and who may help with the project in the future, although he will not be working on it much up until the initial release since his time is spent maintaining our current software. There is me, who has worked with both and am comfortable in either We have an outside company doing the project management, and they are an ASP.Net company. We plan on hiring 1-2 others, however we need to know what direction we are going in first  Environment:  General users are on Windows 2003 server with Terminal Services. They connect using WYSE thin-clients over an RDP connection. Admin staff has their own PCs with XP or higher. Users are allowed to specify their own resolution although they are limited to using IE as the web browser. Other locations connects to our network over a MPLS connection  Based on that, what would you choose and why? ",
        "Best answer": "It certainly sounds to me like a WPF app, with lot's of user interaction and potentially interacting with hardware.  You can deliver the app via Click-Once so deployment is mostly a non-issue.  Your WPF app can access a WCF service and deliver the data as binary so the performace will be excellent.  I would start reading up on WPF and getting familiar with it as soon as possible. "
    },
    {
        "ID": "19911",
        "Question": "How are companies working on large projects evaluating an impact of a single modification on an existing code base?  Since my question is probably not very clear, here's an example: Let's take a sample business application which deals with tasks. In the database, each task has a state, 0 being \"Pending\", ... 5 - \"Finished\". A new requirement adds a new state, between 2nd and 3rd one. It means that:  A constraint on the values 1 - 5 in the database must be changed, Business layer and code contracts must be changed to add a new state, Data access layer must be changed to take in account that, for example the state StateReady is now 6 instead of 5, etc. The application must implement a new state visually, add new controls for it, new localized strings for tool-tips, etc.  When an application is written recently by one developer, it's more or less easy to predict every change to do. On the other hand, when an application was written for years by many people, no single person can anticipate every change immediately, without any investigation. So since this situation (such changes in requirements) is very frequent, I imagine there are already some clever techniques and ways to predict the impact. Is there any? Do you know any books which deal about this subject?  Note: my question is not related to How do you deal with changing requirements? question. In fact, I'm not interested in evaluating the cost of a change, but rather the way to predict the parts of an application which will be concerned by the change. What will be those changes and how difficult they are really doesn't matter in my question. ",
        "Best answer": "I don't think you can ever tell just how much a change will affect code. That being said though, if one know the framework of the application well, he can generally predict about how long it will take and what needs to be done. It comes from experience, not metrics. I'm sure you've thought of cases that a user thought a change would be simple ('He's just adding one menu option! how hard can that be?!) but in reality was quite complex. I have two possible solutions. The first I think is critical: Documentation. Being able to read about the general layout of an application will help someone greatly in determining where changes will need to be made. Lower level docs about specific areas will help in actually making the changes. The second would be to have a senior developer look at each request as he probably can tell quickly what it will entail. He then can give an estimate of how long it will take and what needs to happen before it is assigned to a person or team for development. This may not always be practical, so documentation is still extremely critical. "
    },
    {
        "ID": "19934",
        "Question": " This is a chart I whipped together showing the length of active (meaning ongoing bug fixes and service packs) support offered for each version of Delphi. It is based on the published support data obtained from Embarcadero's website. Delphi 2010 and XE are excluded because their active support is still ongoing so they can't really be compared accurately.  Ironically, Delphi 7, which was regarded by many to be the most stable until the release of Delphi 2009, had a support cycle three times as long as Delphi 2009. Granted, this chart spans three different companies with three different agendas. My question is why is Delphi 2009's support cycle so short? I understand Embarcadero has a business to run and they don't make money with service packs but really, 12 months? I would expect that of a $10 shareware title with low profit margins not a $900-$3500 world class development tool. ",
        "Best answer": "Your graph shows pretty clearly the differences in priority between the policies of Emabarcadero vs. Codegear vs. Borland.  They would have different priorities and different expectations from their users.  That might explain your graph.  It's pretty clear from Embarcadero \"All Access\" that their intention is to make an MSDN style Embarcadero empire and sell their software as a service more than as a $900-$3500 dollar a seat behemoth.  They're really hybridizing the software as a service model though and I'm thinking they're trying to strike a balance between a steady release cycle and a steady stream of income. For my part, I hope we start using Delphi 2009 before they pull the plug on rebates for it. "
    },
    {
        "ID": "19941",
        "Question": "From this question, I have another question about... How long and what type of complexity would have been involved in Chris Sawyer writing most of rollercoaster tycoon in assembler? In order to specify and break this question down, I am interested in;    Approximately how many man hours (have a guess) do you estimate it would have taken Chris to write the game by himself? Or alternatively give an approximate percentage of the ratio of assembler coding hours to say, writing the whole thing in C/C++. Do the programmers that know assembler well think of this as an overly complex task for such a low level language abstraction? Apart from performance benefits is this just a freakish natural ability that Chris has, or a skillset worthy of learning to that extent? I'm interested if people think the complexity/performance thing is worth learning assembler that well (in order to write), or is it only \"worth it\" if you have a lot of naturally developed skills in assembler (presumably from working with hardware/hardware drivers/electronics/etc).  ",
        "Best answer": "From reading his rough bio, it looks like two years (early 1997 to late 1998).  Given that he seems to be a 'one project at a time' person and the tool set and timeframe isn't great for 'team development', I would assume a straight 24 programmer-months. At that point he had been a professional games programmer working in assembly since 1983, so I wouldn't equate \"two Chris years\" to \"two me years\" of work. Given that most professional games take around 25-person years for a team to develop, two-person years for an industry leading game is an amazing accomplishment, lending creedence to the sayings:  A great programmer is 10x more productive than a good programmer The programmer is more important than the language for productivity  So in this case how much faster would Rollercoaster Tycoon be written had Chris used C or Java or Javascript or ...?  Probably it wouldn't have mattered other than the fact that he may have been a bit slower using a higher level language that he didn't have 14 years experience with... "
    },
    {
        "ID": "20002",
        "Question": "I work in a company that does web applications for various banks and some smaller e-shops We employ about 20 developers and have 4-5 projects in development at any one time. Our development teams don't interact much and a lot of the same problems are done in varied ways(good to bad).  I was wondering if it would be a good idea for a company to have a team of programmers that do research on current frameworks and continually improve a common library of functions and a common framework to build current and future projects much faster and more efficiently. How large should a team like this be? Also should it have permanent members that train others or should it rotate people? Update: I was thinking about a common project that people can work on for fun that might spark some interest. It seems that when people have job pressures the solutions they come up with are not the best. ",
        "Best answer": "One important point is that it's impossible to develop a good framework in total isolation. Good frameworks are organically grown: when a programmer notices that he needs some specific functionality, he adds it to the framework, and so the framework grows little by little - as opposed to architecting a \"perfect framework\" up front, which never works, because the architect can't be aware of all eventually turning up use cases. Of course, organically growing the framework has the downside that its internal integrity might not be too good, and it turns into spaghetti. If your team keeps up good internal communication, then you might be able to combine the best of both worlds: a separate architect team keeping up the integrity of the framework, but building for real needs of the end users (developers). "
    },
    {
        "ID": "20023",
        "Question": "When I started using Java in the nineties, it was all \"Write once, run anywhere!\" from day one. That was probably all true then and I was a part of the choir as well.  I am not sure what to think about that anymore, considering all the other languages using multi platform runtimes (python, flash, perl, html, php...). But I am still seeing lots of arguments that says you should use Java because it is supposedly better for cross platform development.  So, Is that still true today? Is Java still the language of choice for multi platform development?  Please be specific with focus on cross platform aspects. I am not asking for general language feature comparisons.  Update: Great responses so far! Most answers seems to be favoring Java or web. Any input from the script crowd? ",
        "Best answer": "While Java may not be the or the only viable cross-platform tool, it has some strengths:  It's extremely fast. It's extremely robust. It's extremely portable (e.g. bytecode compiled 10 years ago in Windows 95 runs fine in OS X today).  and some weaknesses:  Core GUI libraries (Swing...) are showing their age (3rd party additions help here). The language itself could be less verbose (e.g. checked exceptions...). Startup time could be snappier (although it's improving all the time).  When talking specifically about Java the platform, there's one point more:  There are quite a few languages that run on the JVM and interoperate with Java.  "
    },
    {
        "ID": "20080",
        "Question": "Over on stackoverflow, I see this issue crop up all the time:    E_NOTICE ?== E_DEBUG, avoiding isset() and @ with more sophisticated error_handler  How to set PHP not to check undefind index for $_GET when E_NOTICE is on?  How to stop PHP from logging PHP Notice errors  How do I turn off such PHP 5.3 Notices ?   Even Pekka (who offers a lot of solid PHP advice) has bumped against the dreaded E_NOTICE monster and hoped for a better solution than using isset(): isset() and empty() make code ugly  Personally, I use isset() and empty() in many places to manage the flow of my applications.  For example: public function do_something($optional_parameter = NULL) {     if (!empty($optional_parameter)) {         // do optional stuff with the contents of $optional_parameter     }     // do mandatory stuff }    Even a simple snippet like this: if (!isset($_REQUEST['form_var'])) {     // something's missing, do something about it. }  seems very logical to me.  It doesn't look like bloat, it looks like stable code.  But a lot of developers fire up their applications with E_NOTICE's enabled, discover a lot of frustrating \"uninitialized array index\" notices, and then grimace at the prospect of checking for defined variables and \"littering\" their code with isset(). I assume other languages handle things differently.  Speaking from experience, JavaScript isn't as polite as PHP.  An undefined variable will typically halt the execution of the script.  Also, (speaking from inexperience) I'm sure languages like C/C++ would simply refuse to compile. So, are PHP devs just lazy?  (not talking about you, Pekka, I know you were refactoring an old application.)  Or do other languages handle undefined variables more gracefully than requiring the programmer to first check if they are defined? (I know there are other E_NOTICE messages besides undefined variables, but those seem to be the ones that cause the most chagrin) Addendum From the answers so far, I'm not the only one who thinks isset() is not code bloat.  So, I'm wondering now, are there issues with programmers in other languages that echo this one?  Or is this solely a PHP culture issue? ",
        "Best answer": "I code to E_STRICT and nothing else.  Using empty and isset checks does not make your code ugly, it makes your code more verbose. In my mind what is the absolute worst thing that can happen from using them? I type a few more characters. Verses the consequences of not using them, at the very least warnings. "
    },
    {
        "ID": "20084",
        "Question": "I know Git is great for open source projects. But I was wondering: for a company with 20 programmers working on a 1 year project, which source control system is desirable? From what I heard Git uses pulling; wouldn't it be less than desirable to need to go through someone else to get your changes in the main trunk? Especially when everyone is working at the same time? That's just of an example I was wondering about. I know how to use SVN but even at my last job we didn't use it on our projects, since everything was done in PHP and those were typically standalone 1 week projects. I just had SVN for my local code and didn't need to use it with others. So what are good source controls, and specifically why is it good for this? ",
        "Best answer": "Use whatever your team is comfortable with. All Version Control systems do roughly the same thing in similar ways; there's no reason to re-invent the wheel because \"it might work better\". If your team isn't comfortable with anything, then pick the option that has the easiest integration with your team's standard IDE. "
    },
    {
        "ID": "20092",
        "Question": "I'm trying to convince the company I work for to contribute towards open-source software, specifically building a library/component for ASP.NET web apps. We have an 'Innovation Day' coming up where we can work on our own personal pet projects at work, similar to Google's one day a week policy, except ours is once a month :-), and I have an idea that I want to open-source. Other companies do this, eg. Headspring Systems (http://automapper.codeplex.com - see banner at top). What do I tell them? What benefits can I convey to them that would be beneficial to the company? I've already mentioned possible exposure for our company and reputation etc. as well as attracting top software developers when we next go on a hiring spree. But what other arguments could I make? Update: The company I work for is a software company that mainly builds web applications in ASP.NET and the MS Stack. Our clients are mainly the NHS (public health sector of the UK). ",
        "Best answer": "You have to make a business case.  I've done this before by making the case that the intellectual property we'd be open sourcing wasn't a core business asset (didn't differentiate us), but by releasing the code we'd be creating a marketing channel to the users of that open source code, who just also happen to be our target audience. You can use this theory: \"Commoditize Your Complements\" "
    },
    {
        "ID": "20109",
        "Question": "Introduction If an error occurs on a website or system, it is of course useful to log it, and show the user a polite message with a reference code for the error. And if you have lots of systems, you don't want this information dotted around - it is good to have a single centralised place for it. At the simplest level, all that's needed is an incrementing id and a serialized dump of the error details. (And possibly the \"centralised place\" being an email inbox.) At the other end of the spectrum is perhaps a fully normalised database that also allows you to press a button and see a graph of errors per day, or identifying what the most common type of error on system X is, whether server A has more database connection errors than server B, and so on. What I'm referring to here is logging code-level errors/exceptions by a remote system - not \"human-based\" issue tracking, such as done with Jira,Trac,etc.  Questions I'm looking for thoughts from developers who have used this type of system, specifically with regards to:  What are essential features you couldn't do without? What are good to have features that really save you time? What features might seem a good idea, but aren't actually that useful?  For example, I'd say a \"show duplicates\" function that identifies multiple occurrence of an error (without worrying about 'unimportant' details that might differ) is pretty essential. A button to \"create an issue in [Jira/etc] for this error\" sounds like a good time-saver. Just to re-iterate, what I'm after is practical experiences from people that have used such systems, preferably backed-up with why a feature is awesome/terrible. (If you're going to theorise anyway, at the very least mark your answer as such.) ",
        "Best answer": "I have been in a project where with logged client errors using Microsoft Enterprise library. All exception where send to our mail box.  In the mail subject we added hash code of serialized error for avoiding duplicated messages. One could of course store serialized messages in database and so on. I recommend you to check out Microsoft Enterprise library and Log4Net. Some Features of Log4Net  Support for multiple frameworks Output to multiple logging targets Hierarchical logging architecture XML Configuration Dynamic Configuration Logging Context Proven architecture Modular and extensible design •   High performance with flexibility  "
    },
    {
        "ID": "20178",
        "Question": "I need a way to filter out resumes of folks who just copy-and-paste code then hope it works, and check it in if it does.  All this happens without having an understanding (or care) to understand the rest of the code in the system. Sure I know that copying and pasting code is part of learning a new object, control, etc... but how can one tell if that accounts for 70% (or more) of their development career? I've come across some senior level guys perhaps whose skills are so outdated or irrelevant for the project, that all they do is google, copy-then-paste some code without thinking about the solution as a whole.  As a result we have a mismash of JSON, AJAX, callbacks, ASMX, WCF and postbacks in the same project.  It is clear there is no consistency or logic behind where each technology is being used. In the worst case, this type of developer creates security issues and vectors for attack. Question How would you recommend I filter out people who have a poor programming background?  Can I do it at the resume level?  If not, how do I do this during the interview. ",
        "Best answer": " I've come across some senior level   guys perhaps whose skills are so   outdated or irrelevant for the   project, that all they do is google,   copy-then-paste some code without   thinking about the solution as a   whole. As a result we have a mismash   of JSON, AJAX, callbacks, ASMX, WCF   and postbacks in the same project. It   is clear there is no consistency or   logic behind where each technology is   being used.  I don't think the skills of your developers are the problem.  Your problem lies elsewhere, perhaps a team leader or architect who doesn't have the self-confidence to \"encourage\" better coding disciplines, or a management team that doesn't understand the importance of managing technical debt, and doesn't give their developers the time and resources to do so.  Does your company hold code reviews? Leadership may be the problem, not copy-paste developers. "
    },
    {
        "ID": "20204",
        "Question": "I came across this article Work for Free that got me thinking.  The goal of every employer is to gain   more value from workers than the firm   pays out in wages; otherwise, there is   no growth, no advance, and no   advantage for the employer.   Conversely, the goal of every employee   should be to contribute more to the   firm than he or she receives in wages,   and thereby provide a solid rationale   for receiving raises and advancement   in the firm. I don't need to tell you that the   refusenik didn't last long in this   job. In contrast, here is a story from last   week. My phone rang. It was the   employment division of a major   university. The man on the phone was   inquiring about the performance of a   person who did some site work on   Mises.org last year. I was able to   tell him about a remarkable young man   who swung into action during a crisis,   and how he worked three 19-hour days,   three days in a row, how he learned   new software with diligence, how he   kept his cool, how he navigated his   way with grace and expertise amidst   some 80 different third-party plug-ins   and databases, how he saw his way   around the inevitable problems, how he   assumed responsibility for the   results, and much more. What I didn't tell the interviewer was   that this person did all this without   asking for any payment. Did that fact   influence my report on his   performance? I'm not entirely sure,   but the interviewer probably sensed in   my voice my sense of awe toward what   this person had done for the Mises   Institute. The interviewer told me   that he had written down 15 different   questions to ask me but that I had   answered them all already in the   course of my monologue, and that he   was thrilled to hear all these   specifics. The person was offered the job. He had   done a very wise thing; he had earned   a devotee for life. The harder the economic times, the   more employers need to know what they   are getting when they hire someone.   The job applications pour in by the   buckets, all padded with degrees and   made to look as impressive as   possible. It's all just paper. What   matters today is what a person can do   for a firm. The resume becomes pro   forma but not decisive under these   conditions. But for a former boss or   manager to rave about you to a   potential employer? That's worth   everything.  What do you think? Has anyone here worked for free? If so, has it benefited you in any way? Why should(nt) you work for free (presuming you have the money from other means to keep you going)? Can you share your experience? Me, I am taking a year out of college and haven't gotten a degree yet so that's probably why most of my job applications are getting ignored. So im thinking about working for free for the experience? ",
        "Best answer": "No. Never work for free for anyone but yourself. You'll get more out of good open-source credentials and personal projects, in the way of job-hunting, than you will out of working for some son of a b__ who thinks that your skills aren't worth paying for. Of course, if no one is willing to pay for your skills, you may need to find another career: software development is wide open right now, so (depending on where you live, of course) it should be possible to get a job where you actually get paid. Pro bono software development is theft. "
    },
    {
        "ID": "20225",
        "Question": "This question is to experienced testers or test leads. This is a scenario from a software project: Say the dev team have completed the first iteration of 10 features and released it to system testing. The test team has created test cases for these 10 features and estimated 5 days for testing. The dev team of course cannot sit idle for 5 days and they start creating 10 new features for next iteration. During this time the test team found defects and raised some bugs. The bugs are prioritised and some of them have to be fixed before next iteration. The catch is that they would not accept the new release with any new features or changes to existing features until all those bugs fixed. The test team says that's how can we guarantee a stable release for testing if we also introduce new features along with the bug fix. They also cannot do regression tests of all their test cases each iteration. Apparently this is proper testing process according to ISQTB. This means the dev team has to create a branch of code solely for bug fixing and another branch where they continue development. There is more merging overhead specially with refactoring and architectural changes. Can you agree if this is a common testing principle. Is the test team's concern valid. Have you encountered this in practice in your project. ",
        "Best answer": "I would say instead that each release of new features should be on a separate branch. This allows development and releases to be decoupled. "
    },
    {
        "ID": "20239",
        "Question": "I guess most people have been in this situation. The initial project planning begins. The requirements are outlined. After architectural review and sorting through APIs/Frameworks the fitting technology is picked. The development starts. And then it starts. As soon as you need to do some supposedly simple supporting things, framework/API start to backfire, and instead of doing any work you end up fighting against the technology. The research time skyrockets, forums are silent, nothing seems to be done, and even when you get something to work, you're not really sure it's done right. How do you manage in these situations? Do you go for hacks, do you research further, what do you say to management? ",
        "Best answer": "Prototype, Prototype, Prototype!! If your team is not familiar with a particular framework then prototype something in it to evaluate where the pain points are. Matt Raible (Java Web framework comparator guy) suggests working with a framework for one week if possible. Prototyping includes investigating the community support behind a framework and other factors "
    },
    {
        "ID": "20246",
        "Question": "A very extensive application began as an Access-based system (for database storage). Forms were written in VB5 and/or VB6. As .Net became a fixture in the development community, certain modules have been rewritten. This seems very awkward and potentially costly just to maintain because of the cross-technologies and extra work to keep the two technologies happy with each other. Of course, the application uses a mix of ODBC OleDb and MySql.   Would you think spending the time and resources to completely re-develop the application under .Net would be more cost effective? In an effort to develop a more stable application, wouldn't it make sense to use .Net? Or continue chasing Access bugs, adding new features in .Net (which may or may not create new bugs between .Net and Access), and rewriting old Access modules into .Net modules under time constraints that prevent proper design and development? Update The application uses OleDb and MySql - I corrected my previous statement.   Also, to lend further support to rewriting: I have since found out that when the \"porting\" to .Net began, the VBA/VB6 code that existed was basically translated to the .Net equivalent. From my understanding, nothing was done to improve performance, or take advantage of new libraries or technologies.   In my opinion, this creates a very fragile and unstable application. With every new update, this becomes more and more visible. As a help desk technician, I have noticed an increase in problems reported. The customers using the software have noticed an increase in problems and are commenting on it. ",
        "Best answer": "A lot of people discourage rewriting an application from scratch and sometimes I agree with the reasoning.  But most of the times I find rewrting the app the least painful solution and anything written in Access needs to be ported to .NET - PERIOD.  Don't get me wrong, Access has its place and can provide alot of functionality to an organization, but if it turns into a full-fledged app that people rely on then it has out grown Access. It would probably not take much time to port the extisting VBA to .NET in a one for one conversion.  But that may not be a great solution if the VBA is not very good to begin with.  A redesign/rewrite will take longer to write but will in the long run be much easier to maintain.   I am almost always in the camp of rewriting it from scratch where Access is concerned and have not regretted it once. "
    },
    {
        "ID": "20255",
        "Question": "You are just starting a new project and you have these two technologies to choose from, Java and .NET. The project you are working doesn't involve having features that would make it easy to choose between the two technologies (e.g. .NET has this that I need and Java does not) and both of them should work just fine for you (though you only need one of course). Take into account:   Performance  Tools available (even 3rd party tools) Cross platform compatibility Libraries (especially 3rd party libraries) Cost (Oracle seems to try and monetize Java) Development process (Easiest/Fastest)  Also keep in mind that Linux is not your main platform but you would like to port your project to Linux/MacOs as well. You should definitely keep in mind the trouble that has been revolving around Oracle and the Java community and the limitations of Mono and Java as well. It would be much appreciated if people with experience in both can give an overview and their own subjective view about which they would choose and why. ",
        "Best answer": "The single most important (edit: technical) decision is:  Will you at this point in time commit 100% to use Windows as your future deployment platform?   If no, then you should go with Java.  The conclusion from Mono is frequently used to say \"Yes, .NET is cross-platform\". How valid is that claim? was that Mono is only an option IFF you develop against it!   You cannot expect .NET-applications to work out of the box.  @Basic said that this was more a comment than an answer.  To be precise I consider it a question to go on top of the list, because this is perhaps the most important technical decision you need to do when dealing with .NET.  As Basic says he will test against Mono, then that is out of the way, and I would consider Java and .NET to be pretty equally well suited.  I have very little experience with .NET, but quite a bit in Java.   Performance - Java runs pretty well, but still has quite a bit of startup time.  This is because a JVM starts from scratch when being initialized, and the random access of the runtime library jar file is rather slow when needing to be read in from disk.  Recent Java 6's have a background process to try to keep the runtime library jar files in the disk cache so access is fast when needed. Tools available.  Plenty of tools exist, and there is a lot available as high quality Open Source.  IBM has some very advanced tooling available, but they also take quite a bit of money for them.  You may want to have a look at MyEclipse who make a living from putting together the best parts in the Java world and make them accessible for a low cost, to see what is available.  Netbeans has a very nice GUI editor.  JDeveloper has a nice Swing debugger.  The Sun 6 JDK has VisualVM which is a nice entry level profiler which can analyze already running programs (which is a killer feature). Cross platform compatibility.  Very good, tending to excellent.  The JVM is very, very reliable and predictable.  Issues only show when operating system differences seep in - like file separators, file name case sensitivity and menu behaviour.  Libraries.  There are many and many of them are freely available and usable, but primarily written in Java as it is rather difficult to pull in code written in non-JVM languages. Cost.  Java is basically freely available.  What Oracle is indicating is that the power tools - most likely coming from JRocket - will be at a cost.  Also note that exended support (\"Java for Business\") also comes for a price.   Non-x86 platforms are a dying breed, but IBM has plenty and IBM provides an excellent Java implementation for them.  This is priced as  being a part of the operating system - most likely for better adoption. Development process.   Much time with Java is spent researching and choosing the appropriate technology and learning it, but when that is done I think there are plenty of technologies which are quite fast to develop with.  The latest version of Java EE provides for writing very powerful web pages using Facelets which can be reloaded at least as fast as PHP pages.  I think that unless you are not skilled in neither Java or .NET, you will save time and money by choosing the technology you and your organization are the most familiar with. "
    },
    {
        "ID": "20275",
        "Question": "In What would you choose for your project between .NET and Java at this point in time? I say that I would consider the \"Will you always deploy to Windows?\" the single most important technical decision to make up front in a new web project, and if the answer is \"no\", I would recommend Java instead of .NET. A very common counter-argument is that \"If we ever want to run on Linux/OS X/Whatever, we'll just run Mono\"1, which is a very compelling argument on the surface, but I don't agree for several reasons.  OpenJDK and all the vendor supplied JVM's have passed the official Sun TCK ensuring things work correctly.  I am not aware of Mono passing a Microsoft TCK. Mono trails the .NET releases.  What .NET-level is currently fully supported? Does all GUI elements (WinForms?) work correctly in Mono? Businesses may not want to depend on Open Source frameworks as the official plan B.  I am aware that with the new governance of Java by Oracle, the future is unsafe, but e.g. IBM provides JDK's for many platforms, including Linux.  They are just not open sourced. So, under which circumstances is Mono a valid business strategy for .NET-applications?  1Mark H summarized it as: \"If the claim is that \"I have a windows application written in .NET, it should run on mono\", then not, it's not a valid claim - but Mono has made efforts to make porting such applications simpler.\" ",
        "Best answer": "Sparkie's answer got it, let me complement a little. \".NET is cross platform\" is too much of an ambiguous statement as both the framework and the world it was originally created for have changed and evolved.    The short answer is:  The underlying engine that powers .NET and its derivatives, the Common Language Infrastructure Standard, is cross-platform and as if you want to make your code go to multiple platforms, you need to plan on using the right APIs on the right platform to deliver the best experience on each platform. The CLI family has not tried the \"Write Once, Run Anywhere\" approach, as the differences from a phone to a mainframe are too big.   Instead a universe of API and runtime features that are platform-specific has emerged to give developers the right tools to create great experiences in each platform. Think of it: programmers no longer target Windows PCs or Unix Servers.   The world, now more than ever is surrounded by fascinating platforms from PCs, to gaming consoles, to powerful phones, to set-top boxes, to big servers and distributed clusters of machines.  A one-size fits on all platform would merely feel bloated on tiny devices, and feel underpowered on large systems. The Microsoft's .NET Framework product is not cross platform, it only runs on Windows.  There are variations of the .NET Framework from Microsoft that run on other systems like the Windows Phone 7, the XBox360 and browsers through Silverlight, but they are all slightly different profiles.    Today you can target every major mainstream OS, phone, mobile device, embedded system and server with .NET-based technologies.   Here is a list that shows which CLI implementation you would use in each case (this list is not comprehensive, but should cover 99% of the cases):  x86 and x86-64 based PC computers:   running Windows -> Typically you run .NET or Silverlight but you can also use full Mono here. running Linux, BSD or Solaris -> You run full Mono or Silverlight running MacOS X -> You run full Mono or Silverlight running Android -> You run Mono/Android subset  ARM computers:   Running Windows Phone 7: you run Compact Framework 2010 Running Windows 6.5 and older: you run the old Compact Framework Android devices: you run Mono/Android  PowerPC computers:   You run full Mono for full Linux, BSD or Unix operating systems You run embedded Mono for PS3, Wii or other embedded systems. On XBox360, you run CompactFramework  S390, S390x, Itanium, SPARC computers:   You run full Mono  Other embedded operating systems:   You run .NET MicroFramework or Mono with the mobile profile.   Depending on your needs the above might be enough or not.   You will hardly get the same source code to run everywhere.   For example, XNA code wont run on every desktop, while .NET Desktop software wont run on XNA or the phone.   You typically need to make changes to your code to run in other profiles of the .NET Framework.   Here are some of the profiles I am aware of:  .NET 4.0 Profile Silverlight Profile  Windows Phone 7 Profile XBox360 Profile Mono core Profile - follows the .NET profile and is available on Linux, MacOS X, Solaris, Windows and BSD. .NET Micro Framework Mono on iPhone profile Mono on Android Profile Mono on PS3 Profile Mono on Wii Profile Moonlight profile (compatible with Silverlight) Moonlight extended profile (Silverlight + full .NET 4 API access)  So each one of those profiles is actually slightly different, and this is not a bad thing.   Each profile is designed to fit on its host platform and expose the APIs that make sense, and remove the ones that do not make sense. For instance, Silverlight's APIs to control the host browser do not make sense on the phone.   And shaders in XNA make no sense on PC hardware that lacks the equivalent support for it. The sooner you realize that .NET is not a solution to isolating the developer from the underlying capabilities of the hardware and the native platform, the better off you will be. That begin said, some APIs and stacks are available in multiple platforms, for example ASP.NET can be used on Windows, on Linux, on Solaris, on MacOS X because those APIs exist both on .NET and Mono.   ASP.NET is not available on some of Microsoft's supported platforms like XBox or Windows Phone 7 and is not supported either on other platforms that Mono supports like the Wii or the iPhone. The following information is only correct as of November 21st, and many things in the Mono world will likely change. The same principles can be applied to other stacks, a full list would require a proper table, which I have no idea of how to present here, but here is a list of technologies that might not be present on a particular platform.   You can assume that anything not listed here is available (feel free to send me edits for things I missed): Core Runtime Engine [everywhere]  Reflection.Emit Support [everywhere, except WP7, CF, Xbox, MonoTouch, PS3] CPU SIMD support [Linux, BSD, Solaris, MacOS X; Soon PS3,  MonoTouch and MonoDroid] Continuations - Mono.Tasklets [Linux, BSD, Solaris, MacOS, PS3, Wii] Assembly Unloading [Windows only] VM Injection [Linux, BSD, MacOS X, Solaris] DLR [Windows, Linux, MacOS X, Solaris, MonoDroid] Generics [some limitations on PS3 and iPhone].  Languages  C# 4 [everywhere] C# Compiler as a Service (Linux, MacOS, Solaris, BSD, Android) IronRuby [everywhere, execpt WP7, CF, Xbox, MonoTouch, PS3] IronPython [everywhere, execpt WP7, CF, Xbox, MonoTouch, PS3] F# [everywhere, execpt WP7, CF, Xbox, MonoTouch, PS3]  Server Stacks  ASP.NET [Windows, Linux, MacOS, BSD, Solaris] ADO.NET [everywhere] LINQ to SQL [everywhere] Entity Framework [everywhere] Core XML stack [everywhere]   XML serialization [everywhere, except WP7, CF, Xbox)  LINQ to XML (everywhere) System.Json [Silverlight, Linux, MacOS, MonoTouch, MonoDroid] System.Messaging [Windows; on Linux, MacOS and Solaris requires RabbitMQ] .NET 1 Enterprise Services [Windows only] WCF [complete on Windows; small subset on Silverlight, Solaris, MacOS, Linux, MonoTouch, MonoDroid] Windows Workflow [Windows only] Cardspace identity [Windows only]  GUI stacks  Silverlight (Windows, Mac, Linux - with Moonlight) WPF (Windows only) Gtk# (Windows, Mac, Linux, BSD) Windows.Forms (Windows, Mac, Linux, BSD) MonoMac - Native Mac Integration (Mac only) MonoTouch - Native iPhone Integration (iPhone/iPad only) MonoDroid - Native Android Integration (Android only) Media Center APIs - Windows only Clutter (Windows and Linux)  Graphic Libraries  GDI+ (Windows, Linux, BSD, MacOS) Quartz (MacOS X, iPhone, iPad) Cairo (Windows, Linux, BSD, MacOS, iPhone, iPad, MacOS X, PS3, Wii)  Mono Libraries - Cross Platform, can be used in .NET but require manually building  C# 4 Compiler as a Service Cecil - CIL Manipulation, workflow, instrumentation of CIL, Linkers RelaxNG libraries Mono.Data.* database providers Full System.Xaml (for use in setups where .NET does not offer the stack)  MonoTouch means Mono running on iPhone;   MonoDroid means Mono running on Android;   PS3 and Wii ports only available to Sony and Nintendo qualified developers. I apologize for the lack of formality. "
    },
    {
        "ID": "20342",
        "Question": "Although questions similar to this one has already been asked and I can perhaps assemble the answer for myself by reading  already posted materials. Still I am going ahead with the question, cause I feel it has elements of new flavor and I would like to see the views of others on how to tackle this. I have developed several Web Applications of various scale. In some of them, I was only Developer, in another I led small teams and was part of a big team in another. Most of these were developed using PHP. One of them used Kohana Framework, the rest were all developed using Raw PHP. The main difficulty that I faced in regards to speedy development was, whenever a small change was  done to fix a bug or modify a feature, I had to refresh my web browser, go through several pages until I reached the page which included the fix.   I would like to know, what should be the development approach so that I can test out my code without having to, or at least minimize the number of refreshes required to test out changes in code. Although, the concepts maybe applicable for developments of any nature,  for this question, let us stick to PHP and it's framework. ",
        "Best answer": "You're going to have to refresh the page: PHP works on page generation. But you can automate the refreshes using a variety of different methods. Check out this article for four different ways: Quick Tip: 4 Ways to Auto-Refresh your Browser when Designing new Sites You'll also want to get into test-driven development. It'll automate the process of testing the same things over and over. For PHP, there's SimpleTest among other frameworks for TDD. "
    },
    {
        "ID": "20344",
        "Question": "Customers need some education because they think different. Customers think:   changes are not a problem in any time of the project details are not important (exceptions even less) time does not cost money (they have a fix price agreed) one sentence in the specification can be extended/read freely to fit the actual needs - and this does not affect the contract. (here we see often the \"common sense\" discussion - example: \"Of course we need a invoice management screen when we talk about accounting managment - this is common sense!\") the list goes on...  The main problem is that customer (no matter if external or internal/department) do not want or can not understand. It took me many years to understand software creation process and I am still learning, so how can they in just a few months. What is your experience, what is the best approach to educate customers? ",
        "Best answer": "This last summer, I have had a very similar conversation with a customer. The customer wanted me to provide a competitive price for the defined work, and then when their needs change, or their understanding of their needs change, they wanted to change the spec without allowing me to change the cost to reflect the change in work. I asked my customer if they had any suggestions for how I could cost for unknown changes as part of the quote.  The solution we worked out was for me to quote including an itemised contingency allowance of 15%, which we would then work with the customer to prioritise their changes to utilise that allowance. In the end the contingency was not completely used I only invoiced for the work done. The end result was that I was happy I'd been paid for the work done, the customer delivered internally under budget, and because I'd raised the issue in a professional way with them up front they chose me over a competitor to actually do the work. I just wish all the potential customers out there are this professional and actually value quality workmanship. "
    },
    {
        "ID": "20369",
        "Question": "I'm planning on moving to NY in 6-12 months tops, so I'm forced to find a new job. When I'm planing to start my life in another city it's also probably a good time to think about career changes. I've found a lot of different opinions about PHP vs .Net vs Java and this is not the topic here. I don't want to start a new fight about which language is better. Knowing a programming language is not the most important thing for being a software developer. To be a really good developer you need to know OOP, design patterns, testing... and a language is just a tool to make things happen.  So back to my question. I have mixed experience in IT - 1 year as an IT support guy (Windows administration and support), around 2 years of experience in embedded programming (VB.Net 2005) and for the last 2 years I'm working with PHP/MySQL. I have worked with Magento web shop, assisted in some projects in Symfony, modified few Drupal sites. My main concerns are the following:  Do I continue to improve my skills in PHP e.g. to start learning some major PHP framework like Zend, Symfony maybe get some PHP certification. Or do I start learning .NET or Java. I'm more familiar to .NET so I'll probably choose it if choice falls between .NET and Java ( or you could convince me to choose Java :).  Career-wise, I don't know what is the best choice. Learning a new framework and language is more time consuming then improving my existing skills in PHP. But with .NET you have a lot of possibilities (Windows 7 Phone development, Silverlight, WPF) and possibly bigger chances to find better jobs. PHP jobs are less well payed then .NET, at least, according to my research (correct me if I'm wrong). But if I start now with .NET I'm just a beginner and my salary will be low. I need at least 2+ years of experience in some language to even try to find some job that is paying higher than $50-60k in NY. My main goal in the next 2-3 years is to try to find a job in the $60-80k category. Don't get me wrong, I'm not just chasing money, but money is an important factor when you're trying to start a family. I'm 27 years old and I feel that there isn't a lot of room for wrong decisions regarding my career, so any advice will be very welcome. Update Thank you all for spending time to help me with my problem. All of the answers and comments have been very helpful. I have decided to stick with PHP but also to learn C# and Silverlight 4. We'll see where the life will take me. ",
        "Best answer": "What on earth does the choice of programming language have anything to do with your career? This question is like asking, \"I have two choices for a place to work. Should I work at the one where the boss has a norwegian accent, or the one where the boss has a spanish accent?\" There are much more important career considerations.  Startup or established company? Product company or company where IT is a support function? Will you be learning new things or rehashing the old? 9 to 5 or \"work any 80 hours you want?\" Nice co-workers or mean co-workers? Smart co-workers or stupid co-workers? Suit and tie or t-shirt?  This list could go on for hours. The choice of a programming language is just about as relevant to a programmer's career as the choice of whether to comb your hair to the left or to the right. It's all software development no matter what programming language dialect you happen to be speaking. "
    },
    {
        "ID": "20427",
        "Question": "I have a question that can be best answered here, given the vast experience some of you guys have! I am going to finish my bachelor's degree in CS and let's face it, I am just comfortable with C++ and Python. C++ - I have no experience to show for and I can't quote the C++ standard like some of the guys on SO do but yet I am comfortable with the language basics and the stuff that mostly matters. With Python, I have demonstrated work experience with a good company, so I can safely put that. I have never touched C, though I have been meaning to do it now. So I cannot write C on my resume because I have not done it ever. Sure I can finish K & R and get a sense of the language in a month, but I don't feel like writing it cause that would be being unfaithful to myself. So the big question is, are two languages on a a resume considered OK or that is usually a bad sign? Most resumes I have seen mention lots of languages and hence my question. Under the language section of my resume, I just mention: C++ and Python and that kinda looks empty! What are your views on this and what do you feel about such a situation? PS: I really don't want to write every single library or API I am familiar with. Or should I? ",
        "Best answer": "As long as you know how to think the problems through, it does not matter how many languages you are proficient in. But since you are proficient with C++, you could invest a few months time to gain some skill in C# or Java (or Ruby, for that matter). "
    },
    {
        "ID": "20552",
        "Question": "I've been wondering about getting contributions to a new open source product my team will be developing.  There's encouragement for us to get as much support from the wider community as we're able, but I can also see this absorbing a lot of time making sure that 3rd parties located outside our office are on track regards things like code quality.  Also at the start of the project we're likely to have a lot of informal discussions within the core team regarding design of the system, spikes etc. and taking these online to allow community involvement will be time consuming and I can imagine could make the discussion less effective. There is a more human side to this which probably needs to be considered: allowing community involvement in the design process may also have its benefits regards perceived ownership of the project, and there's always a chance that early involvement could pick up on problems that the core team haven't noticed. So the question: at what stage of an Open Source project should you invite contributions from the community? ",
        "Best answer": "Right at the very beginning!  You want the community to feel that they have a genuine stake in your project, otherwise they will feel like they are being used as free labour. All communication should be over a public mailing list or forum, again this enhances the idea of the community. You can mitigate the 'design by committee' problem by laying out a clear vision in your initial posts to the mailing list, e.g. \"So we're looking at a domain model to represent our Pet Store (as per JIRA-4).  Does anyone see any major issues with this model?\" In terms of accepting actual physical contributions, you should start by accepting patches and performing public code reviews on them.  That way contributors can already publicly see what sort of coding standards they need to adhere to.  Make sure your commits are available in a commit mailing list as well - you need to be held to the same standards! It also pays to have project standards on a Wiki or some such document. Read http://www.producingoss.org for more details on how to run a successful open source project. "
    },
    {
        "ID": "20564",
        "Question": "There has been a lot of discussion around the excellent question \"Will high reputation in Stack Overflow help to get a good job?\". I immediately agreed with JoshK that basically said \"No\" (I'll will explain why), but Joel chimed in with lot of convincing facts which made me upvote him as well. My question is what other skills (other than being a technical genius) do you require from a developer? To get the job, or to keep it. I believe being a genius is far from being enough. I have met many technical geniuses in various companies I have worked for that impressed me a lot, but sadly in lot of cases, they were simply fired after a few months or put in ivory towers (mainly because of internal mutiny from other developers). I've seen many in personal distress as a result of this which I understand.  That's why I'm a big fan of non-technical questions for technical positions. I like to know how the candidate will interact with others (including non-technical employees), how much consideration he will have for the business, if he will work for the desired outcome, and so on. I would like to know what you require from your developers and WHY it's important (after all, you hire someone to write code, don't you? Why would you want him to be assertive?). Ideally, I'd like you to come up with an example question you would ask during interviews in support of your answer. ",
        "Best answer": "Excellent communication skills.   If your colleagues cannot read your brain directly, you will need to be able to tell them what you think.  Preferrably both verbally and written.  EDIT:  A way to see them at interview time may be by asking them what their favorite framework for doing X is, and then say that they need to work on a project where X could be used, but it is a political decision to use technology Y (which is clearly older and has some limitations that X solves). If this ends up in an argument about why the political decision is wrong, you have a strong indication of this person not doing well with pragmatic decisions. "
    },
    {
        "ID": "20573",
        "Question": "Any time a Perl, Python, C++ or Tcl program halts with an unhandled exception, those language runtimes take care to register a non-zero exit code for the process. Even Eclipse-based programs return 1 if they fail during startup. Programs run by the standard java.exe, however, happily return zero no matter how abruptly they end, unless the program calls System.exit() with an exit value. Even AssertionFailedError or UnsatisfiedLinkError are reported back to the calling program as successful exits. Of course not all systems have program return codes, but Unix and Windows were important enough to warrant java.lang.Process.exitValue() for child processes; don't they also warrant honouring conventions for parent processes? Is this a flaw in the language design or just in the implementation? Is there an argument that it's a good idea? ",
        "Best answer": "If the Java Language Specification does not explicitly define what exit value to expect, then it is undefined and you should not rely on it. You need to catch Throwable in your main method, and call System.exit(1) yourself. "
    },
    {
        "ID": "20586",
        "Question": "I've been programming for years now, working my way through various iterations of Blub (BASIC, Assembler, C, C++, Visual Basic, Java, Ruby in no particular order of \"Blub-ness\") and I'd like to learn Lisp. However, I have a lot of intertia what with limited time (family, full time job etc) and a comfortable happiness with my current Blub (Java).  So my question is this, given that I'm someone who would really like to learn Lisp, what would be the initial steps to get a good result that demonstrates the superiority of Lisp in web development? Maybe I'm missing the point, but that's how I would initially see the application of my Lisp knowledge.  I'm thinking \"use dialect A, use IDE B, follow instructions on page C, question your sanity after monads using counsellor D\". I'd just like to know what people here consider to be an optimal set of values for A, B, C and perhaps D. Also some discussion on the relative merit of learning such a powerful language as opposed to, say, becoming a Rails expert. Just to add some more detail, I'll be developing on MacOS (or a Linux VM) - no Windows based approaches will be necessary, thanks. Notes for those just browsing by I'm going to keep this question open for a while so that I can offer feedback on the suggestions after I've been able to explore them. If you happen to be browsing by and feel you have something to add, please do. I would really welcome your feedback.  Interesting links Assuming you're coming at Lisp from a Java background, this set of links will get you started quickly.  Using Intellij's La Clojure plugin to integrate Lisp (videocast) Lisp for the Web Online version of Practical Common Lisp (c/o Frank Shearar) Land of Lisp a (+ (+ very quirky) game based) way in but makes it all so straightforward Install Clojure and Sublime 2 on MacOS an excellent getting started guide Look at the Clojure in Action book. Worked for me.  ",
        "Best answer": "This is probably counter to most peoples' recommendations, but steer clear of Emacs to start with, unless you already know it. Especially if you're used to modern IDEs/editors. I'm speaking from experience; I tried to start with Emacs, but having to learn the language AND the editor at the same time really frustrated me (especially since Emacs is so different from other tools). I ended up chucking it, getting started with PLT Scheme (now Racket) which comes with a comparatively simple cross-platform IDE, a centralized package repository and fabulous docs (including an intermediate tutorial specifically aimed at web development). After I got a clean grip on Lisp, I ended up going back to Emacs, picking up EL/CL by way of Practical Common Lisp and Xah's Elisp Guide. Basically, as I see it, you need to know a Lisp before you can use Emacs effectively, and you need to know Emacs in order to be very effective with CL/Elisp. This means that you can either pick up Scheme, or learn Common Lisp through some other editor first (LispWorks personal, probably). If you already know Emacs, then yeah Elisp is the natural step (though how you would enjoy Emacs use without knowing Elisp first is beyond me). YMMV of course, this is just what I found helped. Since your current blub is Java, you could probably get a lot of mileage out of Clojure, Armed Bear or Kawa. I haven't tried any of them myself, not being a Java guy, but Clojure specifically comes highly recommended from other Lispers I know. It seems like the ability to keep your current VM and IDE might help with the learning process. "
    },
    {
        "ID": "20653",
        "Question": "This is a dilemma about which I have been thinking for quite a while. I'm a graduate student and my topics of interest are programming language design, code analysis, compilation, etc. So far, this field has been very interesting and rewarding for me, so I was thinking about finding a job in that field and continuing to specialize in it. I feel like it's a relatively solid field which won't \"get out of style\" anytime soon. I've always thought that in such complex fields it's better to be a real expert than just another guy who superficially understand what the experts are talking about. On the other hand, I feel that by specializing this way I really limit my future option. I have always been a strong believer in multidisciplinary approaches to problems. Maybe I should go search for a general programming job in which I could gain experience in other fields, as well as occasionally apply my favorite field for solving problems. Specializing in only one or two fields can prevent me from thinking outside the box and cause stagnation. I would really like to hear more opinions about this choice. The truth is I'm already leaning towards one of the choices, so basic psychology says nothing will change my mind, but I would still love to hear some feedback. ",
        "Best answer": "Specialise if you enjoy it As you are aware, if you specialise you are automatically incurring an opportunity cost in that you won't be immediably eligible for other technologies (e.g. Java programmers don't often immediately get accepted for compiler optimisation jobs). However, you have to balance this with your love of the complexity inherent in your chosen discipline.  You say you want to be an expert - well go ahead and take the time to learn your chosen discipline. We as a community always need new experts. However, my advice is to follow the pragmatic programmer recommendation of \"Learn a new language every year\". That way, while you're engaging in deep lexical analysis of algorithmic encoding, you can also be churning out a little iPhone app that interests you on the side. You never know, the cross pollenation of different paradigms may cause you some insight that will extend your specialisation into new areas. "
    },
    {
        "ID": "20663",
        "Question": "I'm working on a decentralized transaction processing system that needs both authentication and general encryption, and I have a design decision to make: should I use PGP or X.509 certificates? While the world tends to use X.509 for authentication (especially in browsers), they are based on the principle of having a central authority vouch for each certificate, meaning the system would need a CA and anyone who wants to participate in the system would need to have their keys signed by that CA. PGP, on the other hand, is based on a \"Web of Trust\", where parties who want to communicate merely need to have any trusted party in common who will sign both keys. A CA isn't necessary in this case. My overall design goals prefer the Web-of-Trust model, to keep the system decentralized, but I may also have customers who prefer a centralized model, and I don't want to prohibit that possibility. Is there anything stopping the Web-of-Trust principle from working with X.509? Likewise, is it easy and straightforward to mimic CAs in the PGP cryptosystem? Are they interchangeable in practice? (For some reason, this is making me think of the false dichtomy: \"I'm trying to bang in a nail, should I use a beer bottle or the heel of a shoe?\") ",
        "Best answer": "PGP keyservers can act as an authoritative point of trust(similar to a CA).  The thing I like about PGP keys/keyservers is that I choose which sources to trust to build this web.  In most modern OSes and applications using x509/CA/web-browser model there are \"chain of trust\" issues where there are several ways to fool/break the chain.  Alternately, you can run your own CA, but if you do and someone with a signing certificate from an automatically imported one(an isp/foreign government for example) could break your chain of trust trivially.  You wouldn't even know. "
    },
    {
        "ID": "20664",
        "Question": "I don't know if \"In-Memory debugger\" is really what I mean, or if it's even possible to produce, but it's the best name I could come up with...  Here's the tool I'm looking for:  Given a variable name, resolve that name to a reference Given that reference, be able to watch the object referenced Given the watch, be able to visualise it in a useful way, e.g.  browse the properties of the object as you can in Visual Studio be notified when the object referenced is changed.   Here's a use case for this hypothetical tool.  I have spent all of today trying to track down an ObjectDisposedException in my C# web app.  The object being disposed is the WindowsIdentity of the current user, and there would appear to be a race condition causing the WindowsIdentity to be disposed before it is used.  If I could watch a particular memory reference in an intelligent way, rather than a variable in some given scope, I think it would help me pin down the source of this sort of bug more easily.   Does such a tool exist for .NET? ",
        "Best answer": "If I understand correctly you want to know when a reference is changed. Well, just add a break point on the lines where the reference might be changed and you will break the execution when this happens (or before, depending on where you put the breakpoint). About seeing variables in other contexts: you can see any variable as long as it exists in one context. Use the threads drop down to select different threads and the call stack to navigate between different frames. "
    },
    {
        "ID": "20721",
        "Question": "I work at a small Web Dev firm, and have been handling all the PHP/MySQL/etc. for a while. I'm looking at improving our practices to allow for easier collaboration as we grow. Some things I have in mind are:  Implementing a versioning system (source control) Coding standards for the team (unless mandated by a certain framework, etc.) Enforcing a common directory structure for our Desktops (for backup purposes, etc.) Web-based task/project/time/file/password/contact management and collaboration app(we've tried a bunch; I may just create one)  What do more experienced developers view as necessary first steps in this area? Do you recommend any books? One thing to consider is that the bulk of our daily tasks involve maintenance and adding minor functionality rather than new projects, and the team size will be between 3 and 5. I just found a related question about teams that will be expanding from a solo developer. ",
        "Best answer": " Implementing a versioning system  If this means source control and you're not doing it, then do it NOW. Don't wait, don't even finish reading this. Do it. If you mean coming up with some fancy pants scheme for numbering your releases then simple is best. Try this... X.00 = major release that may break compatibility or you want to charge money for. 0.X0 = new features that don't break things. Free update. 0.0X = bug fixes.  Coding standards for the team (unless mandated by a certain framework, etc.)  Again, keep it simple. One A4 page is more than enough.  Enforcing a common directory structure for our Desktops (for backup purposes, etc.)  Your source control system will do this for you. When a dev does a check out they will get the directory structure. Your VCS is a backup, just make sure you backup the machine with the VCS on it. Anything not in VCS isn't important and doesn't need backing up.  Web-based task/project/time/file/password/contact management and collaboration app(we've tried a bunch; I may just create one)  Don't waste your time building one. The free tools on the net are good enough. In addition you want...  a continuous integration system. When a dev checks something in they need to know that they haven't broken anything, ie. it builds in a clean environment, they haven't forgotten to add new dependencies, run tests to show they haven't caused a regression, the system deploys to a clean environment. a deployment system that includes roll back. You should be able to deploy a new system or push an update with a single command and back out a broken version just as simply.  "
    },
    {
        "ID": "20918",
        "Question": "I'm a beginner learning Java and after reading the docs I'm trying to solve some of the problems at codingbat.com. Some I managed to solve pretty quickly in a matter of seconds but some not quite. The problems themselves are easy (example http://codingbat.com/prob/p126212) but I sometimes have a hard time finding the solutions. In part because I think of the best possible solution and automatically discard the ones that might work but aren't so elegant and in part because I can't find the right algorithm. This is either due to my inexperience (I have used PHP for web development for about 2 months so I'm not a total beginner - even worked with one framework to build a website) or due to my inability to think of a good algorithm.  I normally don't look at the solutions. I sometimes get stuck even for 1 1/5 hours at simple problems and some of them I eventually solve but some I can't so I eventually look at the solution.  Is it normal/good/bad to concentrate this long if I find a problem I can't solve? I might overreact but should this be so hard for me? Is there such thing as not being able to think of algorithms even with much learning and reading?  Thank you all in advance.    ",
        "Best answer": "From what you've said in your question, the issue isn't that you can't solve the problems, it's that you can't think of an elegant solution that would work, so you give up. The main problem with this approach is that you're not really learning anything.  A better approach would be to try one of the not so elegant possible solutions. If it doesn't work, it's not the end of the world, just move on to plan B. If it does work, you now have the chance to make it more elegant. Either way, you will have learned something, and this approach will serve you better if and when you write code for a living.  Remember: you can't improve what's not there.  "
    },
    {
        "ID": "20949",
        "Question": " Possible Duplicate: Will high reputation in Stack Overflow help to get a good job?   In reference to this question, do you think that having a high reputation on this site will help to get a good job? Aside silly and humorous questions, on Programmers we can see a lot of high quality theory questions. I think that, if Stack Overflow will eventually evolve in \"strictly programming related\" (which usually is \"strictly coding related\"), the questions on Programmers will be much more interesting and meaningful (\"Stack Overflow\" = \"I have this specific coding/implementation issue\"; \"Programmers\" = \"Best practices, team shaping, paradigms, CS theory\"). So could high reputation on this site help (or at least be a good reference)? And then, more o less than Stack Overflow? ",
        "Best answer": "I think SO rep would be better for getting you a great coding job. Programmers rep more accurately reflects your ability to impress people with subjective BS.  Therefore, programmers rep will be better for getting you a job in management. ;) "
    },
    {
        "ID": "20965",
        "Question": "Hello Over the years have I made a programming journey from C in 1990 to Pascal, C++ with I programmed commercially, java VHDL, C# and now I'm taking a look on F#.   In the spring I will go back to study embedded programming in C in a university course so before I do that I will refresh my knowledge in C. I don't remember much of the C that I learnt and I think I am thankful for that for probably learned some bad habits back then.   I have some questions to get me on the way. --edit-- To clarify: I am thankful for tips that have been given regarding embedded programming but now I am more interested in relearning C in general. Sorry if my question was unclear,  Compiler, Debugger, IDE? As a microslave I was thinking about 2010 C++ Express, any alternatives? Where do you find good libraries with code for C. I'm thinking something similar to Boost, POCO in C++ Source for procedural programming patterns and best practices. Where can you find good code learn from?  Thanks in advance Gorgen ",
        "Best answer": "There is a very good chance for embedded C programming that you will end up with using the GNU C compiler to target the destination platform, so you might as well learn to use gcc along with gdb to debug (perhaps even using Emacs as both a C IDE which is pretty good, and as the gdb frontend). For this you essentially just need any modern Linux distribution which contain all of the above - usually as optional packages just requiring a single command to install. This looks like a good C tutorial : http://www.faqs.org/docs/learnc/ You will naturally need the K&R manual.  Use ANSI C if you can. "
    },
    {
        "ID": "21032",
        "Question": "Not specific code writing practices. Please also include reasoning.  My start:  use GCC or Clang  gcc because it is unchallenged in the amount of static checking it can do (both against standards and general errors) clang cause it has such pretty and meaningful error messages  when compiling C code using GCC use -Wall -Wextra -Wwrite-strings -Werror  in 99,99% the warning is a valid error  when compiling C++ code using GCC use -Wall -Wextra -Weffc++ -Werror  you could skip -Weffc++ (cause it can be confusing)  always code against a standard C (C89, C99), C++ (C++98, C++0x)  while compilers change, standards don't, coding against a standard gives at least some level of assurance that the code will also compile in the next version of the compiler or even a different compiler/platform  make sure that the compiler checks your code against standard (-std=c99 -pedantic for C99, -std=ansi -pedantic for C++98 in GCC)  cause automatic checking always good  use valgrind or a similar tool to check for runtime errors (memory, threads, ...)  free bug catching  never duplicate functionality of the standard libraries (if there is a bug in your compiler, make a temporary patch, wrapper, ...)  there is no chance that your code will be better then the code maintained by hundreds of people and tested by tenths of thousands  make sure that you actually fix all bugs that are reported by automatic tools (GCC, valgrind)  the errors might not cause your program to crash now, but they will  never follow recommendations that include \"never use feature X\"  such recommendations are usually outdated, exaggerated or oversimplified   ",
        "Best answer": "Learn C++ from a book Unfortunately, most freely available C++ resources are complete garbage. Use the \"Resource Acquisition Is Initialization\" idiom (RAII) This takes care of 90% of your memory management problems. The other 10% can be taken care of with smart pointers (which themselves depend on RAII). Even though the language is not garbage-collected, I've never had to use a delete statement or some kind of DestroyXXX() or ReleaseXXX() or CloseXXX() function in application code - they're always somewhere deep in library/wrapper code. It's the reason why std::vector allows for dynamic arrays without new or delete and fstream allows for manipulation of files without needing fopen() or fclose() in application code - it's all been taken care of. Compile with aggressive optimization when you test (e.g. GCC's -O3 switch).  This will often uncover bugs arising from subtle things like violation of strict aliasing rules.  By doing so, you become aware of such issues, and your program will work properly in the presence of such optimizations. Test on a PowerPC (or other big-endian machine) from time to time Better yet, test on a 64-bit PowerPC if you can get your hands on one.  Things you can learn by doing so:  When reading a binary file, you have to pack/unpack 16-bit, 32-bit, 64-bit, etc. words a byte at a time, or use some sort of endian-aware byte-swapping mechanism. char is not always signed.  On PowerPC Linux, GCC defaults to unsigned char.  This isn't an endianness issue, but it's a subtlety I picked up on while testing on both x86 and PowerPC. Big endian won't let you get away with long n = ...; printf(\"%d\", n); 64-bit big endian won't let you get away with: curl_easy_setopt(handle, CURLOPT_TIMEOUT, 1);  See if you can spot the bug.  Always pay attention to recommendations that say \"never use feature X\"  Such recommendations are typically based on the experience of lots of skilled people over a significant period of time. If you choose to disregard such recommendations, make sure that you really understand them and the rationale behind them, before you disregard them. If you choose to ignore them out of hand, don't be surprised if people criticize your code.  "
    },
    {
        "ID": "21084",
        "Question": "I'm an Object Oriented Programming fanatic. I have always believed in modelling solutions in terms of objects. It is something that comes to me naturally. I work with a services start up that essentially works on application development using OOP languages. So I tend to test understanding of OOP in the candidate being interviewed. To my shock, I found very very few developers who really understood OOP. Most candidates brainlessly spit out definitions they mugged up from some academic book on object oriented programming but they don't know squat about what they are saying. Needless to say I reject these candidates. However, over the course of time, I ended up rejecting almost 98% of the candidates. Now this gets me thinking if I'm being over critical about their OOP skills. I still believe OOP is fundamental and every programmer MUST GET it. Language knowledge and experience is secondary. Do you think I'm being over critical or do I just get to interview bad programmers unfortunately? EDIT: I usually interview programmers with 2 to 5 years of experience. The position that I usually interview for is Ruby/Ruby on Rails application developer. ",
        "Best answer": "haven't you imagined that OOP might not be the pinnacle of all knowledge?  there are other ways to think, after all.  Even more, there are lots problems out there where OOP isn't the best answer. "
    },
    {
        "ID": "21133",
        "Question": "Triggered by this thread, I (again) am thinking about finally using unit tests in my projects. A few posters there say something like \"Tests are cool, if they are good tests\". My question now: What are \"good\" tests? In my applications, the main part often is some kind of numerical analysis, depending on large amounts of observed data, and resulting in a fit function that can be used to model this data. I found it especially hard to construct tests for these methods, since the number of possible inputs and results are too large to just test every case, and the methods themselves are often quite longish and can not be easily be refactored without sacrificing performance. I am especially interested in \"good\" tests for this kind of method. ",
        "Best answer": "The Art of Unit Testing has the following to say about unit tests:  A unit test should have the following   properties:  It should be automated and repeatable.  It should be easy to implement. Once it’s written, it should remain for future use. Anyone should be able to run it.  It should run at the push of a button. It should run quickly.   and then later adds it should be fully automated, trustworthy, readable, and maintainable. I would strongly recommend reading this book if you haven't already. In my opinion, all these are very important, but the last three (trustworthy, readable, and maintainable) especially, as if your tests have these three properties then your code usually has them as well. "
    },
    {
        "ID": "21209",
        "Question": "The main language that I use at the moment is C# and I am the most comfortable with it. However, I have started dabbling in F# and Haskell and really enjoy those langauges. I would love to improve my skills in either of those languages over time since it truly is fun for me to use them (as opposed to Ruby, which is hyped as \"fun\", I just don't get where the fun is, but I digress...). My question is directed at those who have hired/interviewed for programming positions (junior/mid-level): if you see a functional programming language on a resume, does it affect your opinion (whether positive or negatively) of that candidate? My rationale for knowledge of functional programming affecting the view of a candidate is because it can show that the candidate can adapt to different methodologies and take a mulit-dimensional approach to problems rather than the \"same old OO approach\". (This may be off-base, please let me know if this assumption is as well!) ",
        "Best answer": "Knowing the functional paradigm makes you a better programmer. Dabbling in languages like Haskell shows that you're interested in having more than one instrument in your mental toolbox. That should have a concrete boost in your attractiveness as a hire. (If it doesn't, I wouldn't be interested in working for that company.) "
    },
    {
        "ID": "21304",
        "Question": "A while ago the company I work for had outsourced a development project to a third party. They employed agile practices in developing the solution. However when asked for documentation they would just say it was required as it was incorporated in a wiki or as part of their sprints. They did leave at the completion of the project, with all but one of the project team. The project wiki site has now been closed down once the yearly subscription was due. When they left they took most of the knowledge and understanding of what was developed with them. So I do have 2 main questions;  Is this normal for agile or just an excuse for not wanting to write it? What are the industry norm for documentation in agile projects to record development  requirements, designs, key decisions and context?  ",
        "Best answer": "Did they leave you a set of TDD Tests, Acceptance Tests, or other Unit Tests?  They do a good job of documenting how an application works and is expected to work, as well as providing a sample implementation of how to use what was developed. "
    },
    {
        "ID": "21339",
        "Question": "I have been using the http:BL to block bad IP's from accessing my site. If a malicious IP (comment spammer) trys to hit the site I just exit the web script which implicitly returns a 200 OK response.  Other responses I could return: 404 - Not found? If I return a 404 maybe the robots will think \"this is a waste of time, lets move on to attack another site\" which would reduce the load on the site (I currently get about 2 spam-hits per second). However  I'm reluctant to return 404's on urls that, under normal circumstances, can be found.  I'm not sure if spam robots can 'waste time'. i.e Why would a bot writer be bothered to code for 404's when they just blitz the web anyway?  401 Unauthorized? Blocking a bad IP is not quite the same as \"resource requires user authentication 1) which has not yet been provided or 2) which has been provided but failed authorization tests\"  In general I feel that 'responding to the bad-bots according to proper http protocol' gives the bad guys the upper hand. In the sense that I play by the rules while they do not. On some days I feel like I should do something clever to divert these bot's away. On other days I just think that I should not take it personally and just ignore them. Accepting it as par for the course of running a web site.  I dunno - what are your thoughts? How do you respond when you know its a bad IP? ",
        "Best answer": "If you want to play by the rules, 403 Forbidden, or 403.6 IP address rejected (IIS specific) would be the correct response. Giving a 200 response (and ignoring the comment) may just increase the load on the server, as the spam bot will presumably continue submitting spam on future occasions, unaware that it is having no effect. A 4XX response at least says \"go away you need to check your facts\" and is likely to diminish future attempts. In the unlikely event you have firewall access, then a block of blacklisted IP addresses at the firewall would minimize server load / make it appear that your server didn't exist to the spammer.  I was going to suggest using a 302 Temporary Redirect to the spammer's own IP address  - but this would probably have no effect as there would be no reason for the bot to follow the redirect. If dealing with manually submitted spam, making the spam only visible by the IP address that submitted it is a good tactic. The spammer goes away happy and contented (and does not vary his approach to work around your defences), and the other users never see the spam.  "
    },
    {
        "ID": "21400",
        "Question": "I always see abstraction is a very useful feature the OO provides for managing the code-base. But how are large non-OO code bases are managed? Or do those just become a \"Big Ball of Mud\" eventually? Update: It seemed everyone is thinking 'abstraction' is just modularization or data-hiding. But IMHO, it also means the use of 'Abstract Classes' or 'Interfaces' which is a must for dependency-injection and thus testing. How non-OO code bases manage this? And also, other than abstraction, the encapsulation also helps a lot to manage large code bases as it define and restrict the relation between data and functions. With C, it is very much possible to write pseudo-OO code. I don't know much about other non-OO languages. So, is it THE way to manage large C code bases? ",
        "Best answer": "You seem to think that OOP is the only means of achieving abstraction. While OOP is certainly very good at doing that, it’s by no means the only way. Large projects can also be kept manageable by uncompromising modularization (just look at Perl or Python, both of which have excelled at that, and so do functional languages like ML and Haskell), and by using mechanisms such as templates (in C++). "
    },
    {
        "ID": "21456",
        "Question": "In an attempt to reach at least beginner status with such a legendary editor, I have been coding in Emacs for the last two months.  I try to keep an open heart, but I find myself continally disagreeing with one core design choice: that Emacs allow its users to never have to leave.  In a 2010 world, I just think that every side feature of Emacs is hopelessly behind dedicated software:  I would never use its built-in browser; Chrome is years ahead. I would never use its dired feature; Path Finder (Mac OS X) suits my needs. I would never use its built-in email; the Gmail web interface has more relevant features like Priority Inbox. Etc.  Sure, I might occasionally dip into Emacs to use regexps, etc. for one of the above tasks, but other than regexps, I really see no reason to ever touch those side features.  I'm a completely newbie, yet I have a strong gut feeling that Emacs-as-an-OS is obsolete. Emacs experts, do you think that Emacs' choice to be a comprehensive environment is the right choice for 2010 and the future?  Are there particular peripheral features that are still at or ahead of their time compared to alternatives? ",
        "Best answer": "Choose the right tool for the job.  Try running Chrome or Path Finder through an ssh connection - here you will need alternative toolings and Emacs was designed to run in a terminal. "
    },
    {
        "ID": "21463",
        "Question": "When doing TDD and writing a unit test, how does one resist the urge to \"cheat\" when writing the first iteration of \"implementation\" code that you're testing? For example: Let's I need to calculate the Factorial of a number.  I start with a unit test (using MSTest) something like: [TestClass] public class CalculateFactorialTests {     [TestMethod]     public void CalculateFactorial_5_input_returns_120()     {         // Arrange         var myMath = new MyMath();         // Act         long output = myMath.CalculateFactorial(5);         // Assert         Assert.AreEqual(120, output);     } }  I run this code, and it fails since the CalculateFactorial method doesn't even exist.  So, I now write the first iteration of the code to implement the method under test, writing the minimum code required to pass the test.   The thing is, I'm continually tempted to write the following: public class MyMath {     public long CalculateFactorial(long input)     {         return 120;     } }  This is, technically, correct in that it really is the minimum code required to make that specific test pass (go green), although it's clearly a \"cheat\" since it really doesn't even attempt to perform the function of calculating a factorial.  Of course, now the refactoring part becomes an exercise in \"writing the correct functionality\" rather than a true refactoring of the implementation.  Obviously, adding additional tests with different parameters will fail and force a refactoring, but you have to start with that one test. So, my question is, how do you get that balance between \"writing the minimum code to pass the test\" whilst still keeping it functional and in the spirit of what you're actually trying to achieve? ",
        "Best answer": "It's perfectly legit.  Red, Green, Refactor. The first test passes.   Add the second test, with a new input.   Now quickly get to green, you could add an if-else, which works fine.  It passes, but you are not done yet. The third part of Red, Green, Refactor is the most important.  Refactor to remove duplication.  You WILL have duplication in your code now.  Two statements returning integers.  And the only way to remove that duplication is to code the function correctly. I'm not saying don't write it correctly the first time.  I'm just saying it's not cheating if you don't. "
    },
    {
        "ID": "21467",
        "Question": "I just saw this lecture by Spolsky, where he questions the need for choices and confirmation dialogs. At some point he has a MacOS settings window and he mentions that \"now some are getting rid of the OK button\". The window indeed has no OK (or cancel) button. Changing a setting makes it change, when you're done configuring, you close that window, period. Being a long time Windows user and a recent Mac owner, the difference is noticeable at first. I looked for the OK button for a while, only to find out, quite naturally and painlessly, that there was none. I expressed satisfaction and went on my merry way. However, I'm curious to know if this UI design pattern would succeed in the Windows-based world. Granted that if Microsoft brought it out with say Windows-8 (fat chance, I know), people would get used to it eventually. But is there some experience out there of such an approach, of changing the \"confirmation paradigm\" on a platform where it's so prevalent? Did it leave users (especially the non-technical ones) confused, frustrated, scared, or happy? TL;DR: Remove OK/cancel confirmation, what happens? EDIT: Mac GUI for appearance settings.  ",
        "Best answer": "If there's no OK/Cancel, there better be Undo. "
    },
    {
        "ID": "21480",
        "Question": "As part of a research I'm working on, I'm looking for public APIs that only work correctly when you apply a certain sequence of actions on them. For example, the java.nio.channels.SocketChannel class, from the Java standard library, only works correctly with sequences such as open() -> connect() -> read() -> read() -> close(). A more complete demonstration if how it may be used may be represented in the following graph:  Additional examples of Java standard library APIs that require certain sequences are java.io.PrintStream (very similar to the one above) and java.util.Iterator (which requires a next() call between every two remove() calls, thus enforcing a certain sequence). So, does you favorite API for doing X also behave that way? I would very much like to know about additional APIs that require a certain method sequence for correct usage; especially classes that are not part of the Java standard library. The more complex the sequence(s) required, the better.  Some APIs require a sequence that spans across multiple classes, for example: X x = new X(); x.setup(); Y y = x.createNewY(); Z z = new Z(y); z.doSomething();  These examples are also interesting, but I'm mostly looking for sequences that all appear in the same class.  EDIT added bounty for greater visibility. I'm sure many of you have encountered many APIs that will match this description - I would really appreciate some good examples. ",
        "Best answer": "From the Spring framework The Lifecycle interface forces the following action sequence: start (isRunning)* stop  which is used in just about all the principal components that make up the framework. Fortunately, this lifecycle is managed by the container. From the Hibernate framework The Lifecycle interface supports the following action sequence: (onDelete, onSave, onUpdate)* onLoad  From the Servlet API My all-time favourite - the lifecycle of a servlet: init service destroy  with service delegating to the doGet, doPost operations etc.  "
    },
    {
        "ID": "21496",
        "Question": "Let's say that I have a project that depends on 10 libraries, and within my project's trunk I'm free to use any versions of those libraries. So I start with the most recent versions. Then,  each of those libraries gets an update once a month (on average). Now, keeping my trunk completely up to date would require updating a library reference every three days. This is obviously too much. Even though usually version 1.2.3 is a drop-in replacement for version 1.2.2, you never know without testing. Unit tests aren't enough; if it's a DB / file engine, you have to ensure that it works properly with files that were created with older versions, and maybe vice versa. If it has something to do with GUI, you have to visually inspect everything. And so on. How do you handle this? Some possible approaches:  If it ain't broke, don't fix it. Stay with your current version of the library as long as you don't notice anything wrong with it when used in your application, no matter how often the library vendor publishes updates. Small incremental changes are just waste. Update frequently in order to keep change small. Since you'll have to update some day in any case, it's better to update often so that you notice any problems early when they're easy to fix, instead of jumping over several versions and letting potential problems to accumulate. Something in between. Is there a sweet spot?  ",
        "Best answer": "I'm shocked - and indeed appalled - at the number of answers here saying \"don't update unless you have to\". I've done that, and whilst it's easier in the short term, it burns like hell in the long run. More frequent, smaller updates are much, much easier to manage than occasional big ones, and you get the benefit of new features, bug fixes, and so on sooner. I don't buy this idea that library changes are somehow more difficult to test than code changes. It's just the same - you're making a change to the codebase, and you need to validate it before you commit, and more deeply before you release. But you must already have processes to do this, since you're making code changes! If you're working in iterations, of two to four weeks length, i would suggest making updating libraries a once per iteration task, to be done as soon as possible after the start, when things are a little more relaxed than just before an iteration deadline, and the project has more capacity to absorb change. Get someone (or a pair if you do pair programming) to sit down, look at which libraries have been updated, and try bringing each one in and running a rebuild and test. Budget half a day to a day for it each iteration, perhaps. If things work, check in the changes (i'm assuming you keep libraries in source control, as we do; i'm not sure how you'd propagate the change in a controlled way if not). This will obviously be a lot easier if you have automated tests than if testing is entirely manual. Now, the question is what you do if an update breaks things - do you spend time fixing it, or leave it out? I'd suggest leaning towards the latter; if it can be fixed in an hour, do it, but if an update is going to take significant work to integrate, then raise it as its own development task, to be estimated, prioritised, and scheduled just like any other. The chances are that unless it brings in some very crucial fix or improvement, the priority will be low, and you'll never get round to it. But you never know, by the time the next iterationly update day rolls round, the problem might have fixed itself; even if not, at least now you know that there's a roadblock on the update path, and it won't catch you by surprise. If you're not doing iterations of that length, i would set up some kind of standalone schedule for updates - no longer than monthly. Is there some other project rhythm you could tie it to, like a monthly status review, or an architecture board meeting? Payday? Pizza night? Full moon? Whatever, you need to find something a lot shorter than a traditional release cycle, because trying to update everything in one go every 6-18 months is going to be painful and demoralising. Needless to say, if you do stabilisation branches before releases, you wouldn't apply this policy to them. There, you'd only update libraries to get critical fixes. "
    },
    {
        "ID": "21526",
        "Question": "First off,  thanks for reading the question.  I understand that this would theoretically be best asked in the SQA stack area - but - seeing as how that doesn't officially exist yet, maybe this was the most appropriate place for it for now. I'm looking into tracking down a certification in Software Quality, however, there are so many different awarding bodies and certifications that telling the difference between them is difficult. So far I've found: Six Sigma - a QA process, originally founded by Motorola & not limited to software. (in fact I'm not sure if it even covers software development).  Based on a series of 'belts' (black being the best, I assume it starts at green? or white?) ASQ - American Society of Quality - offers numerous certifications in quality  CQA - Quality Auditor CQE - Quality Engineer CQIA - Quality Improvement Associate CQI - Quality Inspector CQPA - Quality Process Analyst CQT - Quality Technician CRE - Reliability Engineer  QAI Global  Software Testing Software Quality Assurance Process Engineering Process Management   With all the different designations and levels, I'm having trouble understanding the differences and the 'start point' and progression between the levels.  (Are they a hierarchy or are they in parallell?) ",
        "Best answer": "QAI, ASQ, and ISTQB (another big one you didn't mention) are all parallel (six sigma is a bit of a different beast, but not a gradiation of any of the others). There's a bit of a controversy in the testing / qa world these days on the value of certifications - one front calls them a money making machine with little value and the other cites the need for a common baseline of testing terms and knowledge. I won't hijack your question to go any deeper :} "
    },
    {
        "ID": "21601",
        "Question": "Two of the functions our company provides our customers are:  Our Development team of ~10 employees creates software products for businesses in a particular industry. The software is used on hundreds of our customers' computers. Although the product is, for most customers, off-the-shell, larger customers drive its development, so it's somewhere between shrinkwrap software and custom software. Our Support team of ~25 employees provides technical support for both our own software and a few other software products for which we are the vendor.  Our Development team recently switched to Scrum for their development methodology, and while I support this change, I worry that our Support team is going to have more trouble in the future.  With a quicker release cycle, our customers will have features and products in production which Support isn't even aware of until the customer calls. We've never been particularly good at getting documentation and information from Development to Support, but I fear Scrum will just exacerbate the problem. What are good references on how companies reconcile Development's desire to release frequently with Support's desire to have full documentation and support information before customers see the product?  How should a company structure their release and change management when using agile methods? EDIT: What I'm interested are specifics on how companies structure themselves to deal with these issues. Does the scrum team push releases to customers? Does the software have to go through a release/change management team who will not deploy until Support has been given documentation and brought up to speed?  How do you get releases to customers quickly yet still keep everyone informed of changes and new features? ",
        "Best answer": "First thing to know is that at the end of the iteration, you don't have to release. The objective is to have a potentially shippable increment of the software, not necessarily to release it. Product owner should decide when to release. Increase Collaboration When you decide to release, it is quite obvious that the support is being informed and/or trained with the new changes. The support should be involved in the process. They should also be informed which bug has been fixed so they can inform the customers in the support ticket. Depending on your situation it may be a good idea to invite one or more support team member to the spring planning meeting. It's also a good idea to invite them to the sprint review meeting as well. Try it to see if it works for you. Write a Definition of Done Each feature your developers build should comply to a Definition of Done you write and maintain that matches your organization & product specificities. Here is an example of DoD:  Code build, committed in the repos Unit test coverage 80% Technical documentation completed (just enough) End user documentation completed (just enough) Reviewed & approved by another developer Fully tested What's new file updated  The concept of Definition of Done alone is a strong company anti-procrastination technique. It forces you to advance, ... to ship. Once a feature is \"done\", you have everything to release it already. Including what is needed by your support team. Support Is Useful For Developers I personnaly love support. It's the best source of strategic information for your software. It is better than any market study. This is why I think having developers in the support helps you to build on quality. Remember the expression Throw it over the wall? I also think the product owner should be involved. "
    },
    {
        "ID": "21617",
        "Question": "Is it ok to put a GPL notice inside a small script or a snippet?   This program is free software: you can redistribute it and/or modify   it under the terms of the GNU General Public License as published by   the Free Software Foundation, either version 3 of the License, or   (at your option) any later version. This program is distributed in the hope that it will be useful,   but WITHOUT ANY WARRANTY; without even the implied warranty of   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the   GNU General Public License for more details. You should have received a copy of the GNU General Public License   along with this program.  If not, see http://www.gnu.org/licenses/.  Or will just the Copyright notice be enough? ",
        "Best answer": "You are correct that you should:  Assert Copyright Grant people explicit rights to use, modify and distribute your code.  In many places, the idea of public domain is not recognized. In order for someone to be able to (legally) use your code, there must be something that says that they can, and under what conditions. For snippets, I highly recommend the 3 clause BSD style license. It asserts your copyright, tells people that they are free to do whatever they like with the code, so long as they preserve your copyright when distributing it via source and tells them that they can't use your name to endorse derived works of the code without your permission. The GPL has very specific definitions of what should happen when you combine GPL covered work with something else. The GPL has to prevail as the dominate license in the code base, which means someone using your snippets inherits the GPL. The LGPL (known as the library GPL, or lesser GPL) is more suited for that, because it allows linking without inheriting the license. Still, if you're going to take the teeth out of the GPL, you might as well use a much simpler license. Another excellent choice would be the MIT license, which is very similar to the three clause BSD style license.  Finally, if you must use the GPL, please indicate \"Version 2 or later\" if at all possible. This ensures that your code can work in places where a tree might be using \"Version 2 Only\", which is quite common. A substantial portion of the Linux kernel specifies version 2 only, GPL3 (while widely accepted) was met with a fair amount of criticism and rejection. I'm not discounting the importance, significance or usefulness of the GPL. I'm just suggesting that it might not be the best choice for snippets and libraries.  "
    },
    {
        "ID": "21676",
        "Question": "I've read quite a bit about the positive effects of micro finance in developing countries. Organizations exist that solicit capital from donations and then make micro loans to people for the purpose of setting up some kind of business. The loans are usually quite small, with little to no interest and are well under ten thousand US Dollars, typically reaching only hundreds of dollars. The pay back rate is also excellent. The programs that exist seem to focus on retail and agricultural endeavours, I have yet to see an organization with a focus on technology sectors like software development. Do you know of any established / credible micro finance organizations that supply micro loans to individuals in order to set up software related businesses? Have the loans been paid back as well as other sectors, such as agriculture and retail? I'm looking globally, so organizations in any country would be appreciated. ",
        "Best answer": "Community Driven Funding The closest thing I know is called Kickstarter.   What is Kickstarter? We’re the largest funding platform for creative projects in the world  It is not specifically bound to software development. My understanding is that it is only available to the US market.  An alternative if you don't like Kickstarter: Ulule.com that is global. Both of them are community driven. Money comes from people like you and me, and not funds or banks. National or Regional Fundings In many countries (especially europe) there is local help by governments. In Belgium there is called \"Bourse de préactivité\". They give up to 12.500 Euros to any individual that will come up with an innovative idea and want to create a business with it. It's a subsidy. There is Awex that promotes exportation and refund you up to 50% of your charges for your presence in international trade shows or the fact you build a multi-language e-commerce website. I'm sure every country has his own programs. Microsoft has a lot of Microsoft Innovation Centers in the world. Many of them are joint ventures with the local government. They are aware of all financial tools you could use in your local country, and they will certainly help, even if you are not in Microsoft eco-system. "
    },
    {
        "ID": "21696",
        "Question": "I'm 21 and didn't go to University and hence completely self-taught from tutorials, screencasts, books etc, but when it came to the decision of what languages to specifically learn, I thought, what's going to the most beneficial to learn, ie. for money, exposure, etc...  So I did the Sun Certified Java Programmer course, but have never used it since, I learned Objective C to make iPhone apps, I have used it lots, I used PHP for web stuff, I've used it lots.  So my question is, if you chose to learn a specific language, was it for money or for a certain job or because your employer needed you to get up to speed on a language for a specific project? I cant really think of any other motivations to be a programmer? Is that shallow?? ",
        "Best answer": "Motivations range from availability to curiosity to best-choice-for-job to no-choice-for-job:  I learned BASIC out of curiosity and because that's all that was available to me at the time (DG Nova 2) [1976] I learned Pascal because that's what the university taught (Cyber). [1982] I learned DataFlex because it was available and was sooooo much better than BASIC for PC business applications (IBM PC) [1983] I learned Assembly Language because BASIC wasn't fast enough to get the job done (Apple ][). [1983] I learned C because it had the best libraries to get the job done for the platform (80286) [1984] I learned Prolog because it was there (curiosity) [1985] I learned Lisp because academics kept using it to do cool things (curiosity) [1985] I learned SmallTalk because it looked like fun (curiosity) [1985] I learned C++ because I read Stroustrup's book [1985] I learned Java because it was the best cross-platform solution available at the time for the job [1999] I learned VB because that was the IT shop standard [2002] I learned VB.NET because it was so much better than VB [2002] I learned C# because VB.NET was too #@$% verbose [2003]  Addendum: Javascript, HTML, vbscript, powerbuilder, and several others all learned because that's what the job required "
    },
    {
        "ID": "21735",
        "Question": "I'm currently working on a project, and there are some \"code growth\" problems, that need a solution for. The deadline is not very far, and it would be advisable to stay within allocated time-frame. Still, the code needs some restructuring, and here lies the question. Should we try to use a new framework to fix the current problem, or not. My colleague is very open towards external frameworks and offers to restructure the code by finding an existing library something along the lines of WTL/boost/TinyXML/ATL. I, on the other hand, am very cautions concerning new frameworks for existing code bases. The language is C++, and I have a fair amount of experience where libraries like MFC backfire with a huge flamethrower. I am not saying my way is the right one, as I guess the truth is somewhere between, but I need to decide how to proceed. The deadline is pretty close and the changes would touch a large part of the code-base, app logic layer. Restructuring the existing code base would require some peaceful thinking and some extra coding. But the code is not TDWTF kind, rather it has grown past the original requirements and needs new clothes. The app is heavily asynchronous, and uses multiple threads, so it's not the trivial kind. Can anyone tell me what is the general rule when you have to decide whether to use the premade wheel vs. reinventing a new one? What would be the acceptance criteria for new code? ",
        "Best answer": "If you have a deadline which you need to meet, then don't introduce new, unknown stuff in your environment.   If the thing you need to fix is more important than the deadline, then strongly consider officially pushing the deadline if you are in doubt you will make it. Note that for asynchronous applications with multiple threads Executors are your friends to tame the beasts.  Will they do, or do you need more than that? "
    },
    {
        "ID": "21771",
        "Question": "I've heard the argument that you should use the most generic interface available so that you're not tied to a particular implementation of that interface. Does this logic apply to interfaces like java.util.Collection? I would much rather see something like the following: List<Foo> getFoos()  or Set<Foo> getFoos()  instead of  Collection<Foo> getFoos()  In the last case, I don't know what kind of data set I'm dealing with, whereas in the first two instances I can make some assumptions about ordering and uniqueness. Does java.util.Collection have a usefulness outside of being a logical parent for both sets and lists?  If you came across code that employed Collection when doing a code review, how would you determine whether its usage is justified, and what suggestions would you make for its replacement with a more specific interface? ",
        "Best answer": "Abstractions live longer than implementations In general the more abstract your design the longer it is likely to remain useful. So, since Collection is more abstract that it's sub-interfaces then an API design based on Collection is more likely to remain useful than one based on List. However, the overarching principle is to use the most appropriate abstraction. So if your collection must support ordered elements then mandate a List, if there are to be no duplicates then mandate a Set, and so on. A note on generic interface design Since you're interested in using the Collection interface with generics you may the following helpful. Effective Java by Joshua Bloch recommends the following approach when designing an interface that will rely on generics: Producers Extend, Consumers Super This is also known as the PECS rule. Essentially, if generic collections that produce data are passed to your class the signature should look like this: public void pushAll(Collection<? extends E> producerCollection) {}  Thus the input type can be E or any subclass of E (E is defined as both a super- and sub-class of itself in the Java language). Conversely, a generic collection that is passed in to consume data should have a signature like this: public void popAll(Collection<? super E> consumerCollection) {}  The method will correctly deal with any superclass of E. Overall, using this approach will make your interface less surprising to your users because you'll be able to pass in Collection<Number> and Collection<Integer> and have them treated correctly.  "
    },
    {
        "ID": "21802",
        "Question": "Getters and setters are often criticized as being not proper OO. On the other hand, most OO code I've seen has extensive getters and setters. When are getters and setters justified? Do you try to avoid using them? Are they overused in general? If your favorite language has properties (mine does) then such things are also considered getters and setters for this question. They are the same thing from an OO methodology perspective. They just have nicer syntax. To state the criticism simply: Getters and Setters allow you to manipulate the internal state of objects from outside of the object. This violates encapsulation. Only the object itself should care about its internal state. And an example: Procedural version of code: struct Fridge {     int cheese; }  void go_shopping(Fridge fridge) {      fridge.cheese += 5; }  Mutator version of code: class Fridge {      int cheese;             void set_cheese(int _cheese) { cheese = _cheese; }      int get_cheese() { return cheese; }  }  void go_shopping(Fridge fridge) {      fridge.set_cheese(fridge.get_cheese() + 5);         }  The getters and setters made the code much more complicated without affording proper encapsulation. Because the internal state is accessible to other objects we don't gain a whole lot by adding these getters and setters. ",
        "Best answer": "Having getters and setters does not in itself break encapsulation.  What does break encapsulation is automatically adding a getter and a setter for every data member (every field, in java lingo), without giving it any thought. While this is better than making all data members public, it is only a small step away. The point of encapsulation is not that you should not be able to know or to change the object's state from outside the object, but that you should have a reasonable policy for doing it.   Some data members may be entirely internal to the object, and should have neither getters nor setters. Some data members should be read-only, so they may need getters but not setters.  Some data members may need to be kept consistent with each other. In such a case you would not provide a setter for each one, but a single method for setting them at the same time, so that you can check the values for consistency. Some data members may only need to be changed in a certain way, such as incremented or decremented by a fixed amount. In this case, you would provide an increment() and/or decrement() method, rather than a setter. Yet others may actually need to be read-write, and would have both a getter and a setter.  Consider an example of a class Person.  Let's say a person has a name, a social security number, and an age.  Let's say that we do not allow people to ever change their names or social security numbers.  However, the person's age should be incremented by 1 every year.  In this case, you would provide a constructor that would initialize the name and the SSN to the given values, and which would initialize the age to 0.  You would also provide a method incrementAge(), which would increase the age by 1.  You would also provide getters for all three.  No setters are required in this case. In this design you allow the state of the object to be inspected from outside the class, and you allow it to be changed from outside the class.  However, you do not allow the state to be changed arbitrarily.  There is a policy, which effectively states that the name and the SSN cannot be changed at all, and that the age can be incremented by 1 year at a time.   Now let's say a person also has a salary.  And people can change jobs at will, which means their salary will also change.  To model this situation we have no other way but to provide a setSalary() method!  Allowing the salary to be changed at will is a perfectly reasonable policy in this case.   By the way, in your example, I would give the class Fridge the putCheese() and takeCheese() methods, instead of get_cheese() and set_cheese().  Then you would still have encapsulation.  public class Fridge {   private List objects;   private Date warranty;    /** How the warranty is stored internally is a detail. */   public Fridge( Date warranty ) {     // The Fridge can set its internal warranty, but it is not re-exposed.     setWarranty( warranty );   }    /** Doesn't expose how the fridge knows it is empty. */   public boolean isEmpty() {     return getObjects().isEmpty();   }    /** When the fridge has no more room... */   public boolean isFull() {   }    /** Answers whether the given object will fit. */   public boolean canStore( Object o ) {     boolean result = false;      // Clients may not ask how much room remains in the fridge.     if( o instanceof PhysicalObject ) {       PhysicalObject po = (PhysicalObject)o;        // How the fridge determines its remaining usable volume is a detail.       // How a physical object determines whether it fits within a specified       // volume is also a detail.       result = po.isEnclosedBy( getUsableVolume() );     }       return result;   }    /** Doesn't expose how the fridge knows its warranty has expired. */   public boolean isPastWarranty() {     return getWarranty().before( new Date() );   }    /** Doesn't expose how objects are stored in the fridge. */   public synchronized void store( Object o ) {     validateExpiration( o );      // Can the object fit?     if( canStore( o ) ) {       getObjects().add( o );     }     else {       throw FridgeFullException( o );     }   }    /** Doesn't expose how objects are removed from the fridge. */   public synchronized void remove( Object o ) {     if( !getObjects().contains( o ) ) {       throw new ObjectNotFoundException( o );     }      getObjects().remove( o );      validateExpiration( o );   }    /** Lazily initialized list, an implementation detail. */   private synchronized List getObjects() {     if( this.list == null ) { this.list = new List(); }     return this.list;   }    /** How object expiration is determined is also a detail. */   private void validateExpiration( Object o ) {     // Objects can answer whether they have gone past a given     // expiration date. How each object \"knows\" it has expired     // is a detail. The Fridge might use a scanner and     // items might have embedded RFID chips. It's a detail hidden     // by proper encapsulation.     if( o implements Expires && ((Expires)o).expiresBefore( today ) ) {       throw new ExpiredObjectException( o );     }   }    /** This creates a copy of the warranty for immutability purposes. */   private void setWarranty( Date warranty ) {     assert warranty != null;     this.warranty = new Date( warranty.getTime() )   } }  "
    },
    {
        "ID": "21831",
        "Question": "I believe it is.  Why?  I've encountered many Software Engineers who believe they are somehow superior to QA engineers.  I think it may help quench this belief if they do the job of a QA engineer for some time, and realize that it is a unique and valuable skill-set of its own. The better a Software Engineer is at testing their own programs, the less cost in time their code incurs when making its way through the rest of the software development life-cycle. The more time a Software Engineer spends thinking about how a program can break, the more often they are to consider these cases as they are developing them, thus reducing bugs in the end product. A Software Engineer's definition of \"complete\" is always interesting...if they have spent time as a QA engineer maybe this definition will more closely match the designer of the software's.  Note I make above suggestion with a small time-frame in mind as I am aware having someone work in a position that is not the position they are hired for is definitely a recipe for losing that developer.  What do you all think? ",
        "Best answer": " 1. I've encountered many Software Engineers who believe they are somehow superior to QA engineers. I think it may help quench this belief if they do the job of a QA engineer for some time, and realize that it is a unique and valuable skill-set of its own.  A good software engineering has a background in quality, including testing, metrics, and statistics. Anyone doing any kind of software development should be aware (if not familiar with) maintaining at quality source code and producing/maintaining effective test cases. Over time, I would suspect that any software developer would gain an understanding of the different facets of quality - code quality, portability, maintainability, testability, usability, reliability, efficiency, and security. Software engineers might focus on a particular aspect of the lifecycle - requirements engineering, architecture and design, construction, testing, and maintenance. However, regardless of your focus (either as a job or at the current phase of the project), it's important to remember quality.  2. The better a Software Engineer is at testing their own programs, the less cost in time their code incurs when making its way through the rest of the software development life-cycle.  That might be true. But some issues are best seen later in development. For example, performance and efficiency issues might not be seen until integration. Having good, solid code and effective unit tests are just the beginning. Quality needs to begin with requirements, and follow all the way through maintenance activities.  3. The more time a Software Engineer spends thinking about how a program can break, the more often they are to consider these cases as they are developing them, thus reducing bugs in the end product.  That's a totally true statement. But then again, it's also up to the requirements engineers to verify that there are no conflicts in requirements, architects to ensure that the design actually addresses the requirements, and so on. Everyone should be trying to poke holes in their work and then working with the appropriate people to seal them up nice and tight.  4. A Software Engineer's definition of \"complete\" is always interesting...if they have spent time as a QA engineer maybe this definition will more closely match the designer of the software's.  \"Complete\" can only be measured against requirements. Either the requirements are satisfied and the project is complete, or there are incomplete requirements and the project is not complete. Any other measure of complete is useless. "
    },
    {
        "ID": "21891",
        "Question": "Dr Bjarne Stroustrup in his book D&E says  Several reviewers asked me to compare C++ to other languages. This I have decided against doing. Thereby, I have reaffirmed a long-standing and strongly held view: \"Language comparisons are rarely meaningful and even less often fair\" . A good comparison of major programming languages requires more effort than most people are willing to spend, experience in a wide range of application areas, a rigid maintenance of a detached and impartial point of view, and a sense of fairness. I do not have the time, and as the designer of C++, my impartiality would never be fully credible.  -- The Design and Evolution of C++(Bjarne Stroustrup)  Do you people agree with his this statement \"Language comparisons are rarely meaningful and even less often fair\"?  Personally I think that comparing a language X with Y makes sense because it gives many more reasons to love/despise X/Y :-P What do you people think? ",
        "Best answer": "I think Stroustrup is entirely correct. Adequately comparing two languages on their technical merits requires enough familiarity with both to write idiomatic code and use the same design patterns normally used by programmers who are very productive in both languages. Someone who doesn't have that level of knowledge of both languages may see things that aren't explicitly provided for by the language that he's not as familiar with, and assume there would be problems as a result.  For example, someone who doesn't use Python on a regular basis may assume that Python users regularly have trouble because of indentation. Or someone not familiar with Common Lisp may look at the lack of polished libraries, but not know that the FFI is powerful enough to write wrappers for C libraries with nominal effort. Someone not familiar with Ruby may see the lack of static typing and assume type errors would be a major problem. Finally, someone not familiar with Haskell may see the lack of assignment, and assume it can't handle state. Now all of this assumes that languages actually are compared only on their technical merits. "
    },
    {
        "ID": "21907",
        "Question": "The definition of \"Free Software\" from the Free Software Foundation:  “Free software” is a matter of   liberty, not price. To understand the   concept, you should think of “free” as   in “free speech,” not as in “free   beer.” Free software is a matter of the   users' freedom to run, copy,   distribute, study, change and improve   the software. More precisely, it means   that the program's users have the four   essential freedoms:  The freedom to run the program, for   any purpose (freedom 0). The freedom   to study how the program works, and   change it to make it do what you wish   (freedom 1). Access to the source   code is a precondition for this. The   freedom to redistribute copies so you   can help your neighbor (freedom 2). The freedom to distribute copies of   your modified versions to others   (freedom 3). By doing this you can   give the whole community a chance to   benefit from your changes. Access to   the source code is a precondition for   this.  A program is free software if users   have all of these freedoms. Thus, you   should be free to redistribute copies,   either with or without modifications,   either gratis or charging a fee for   distribution, to anyone anywhere.   Being free to do these things means   (among other things) that you do not   have to ask or pay for permission to   do so.  The definition of \"Open Source Software\" from the Open Source Initiative:  Open source doesn't just mean access   to the source code. The distribution   terms of open-source software must   comply with the following criteria:  Free Redistribution The license shall not restrict any party from   selling or giving away the software as   a component of an aggregate software   distribution containing programs from   several different sources. The license   shall not require a royalty or other   fee for such sale. Source Code The program must include source code, and must allow   distribution in source code as well as   compiled form. Where some form of a   product is not distributed with source   code, there must be a well-publicized   means of obtaining the source code for   no more than a reasonable reproduction   cost preferably, downloading via the   Internet without charge. The source   code must be the preferred form in   which a programmer would modify the   program. Deliberately obfuscated   source code is not allowed.   Intermediate forms such as the output   of a preprocessor or translator are   not allowed. Derived Works The license must allow modifications and derived works,   and must allow them to be distributed   under the same terms as the license of   the original software. Integrity of The Author's Source Code The license may restrict   source-code from being distributed in   modified form only if the license   allows the distribution of \"patch   files\" with the source code for the   purpose of modifying the program at   build time. The license must   explicitly permit distribution of   software built from modified source   code. The license may require derived   works to carry a different name or   version number from the original   software. No Discrimination Against Persons or Groups The license must not   discriminate against any person or   group of persons. No Discrimination Against Fields of Endeavor The license must not restrict   anyone from making use of the program   in a specific field of endeavor. For   example, it may not restrict the   program from being used in a business,   or from being used for genetic   research. Distribution of License The rights attached to the program must apply to   all to whom the program is   redistributed without the need for   execution of an additional license by   those parties. License Must Not Be Specific to a Product The rights attached to the   program must not depend on the   program's being part of a particular   software distribution. If the program   is extracted from that distribution   and used or distributed within the   terms of the program's license, all   parties to whom the program is   redistributed should have the same   rights as those that are granted in   conjunction with the original software   distribution. License Must Not Restrict Other Software The license must not place   restrictions on other software that is   distributed along with the licensed   software. For example, the license   must not insist that all other   programs distributed on the same   medium must be open-source software. License Must Be Technology-Neutral No provision of the license may be   predicated on any individual   technology or style of interface.   These definitions, although they derive from very different ideologies, are broadly compatible, and most Free Software is also Open Source Software and vice versa. I believe, however, that it is possible for this not to be the case: It is possible for software to be Open Source without being Free, or to be Free without being Open Source. Questions  Is my belief correct? Is it possible for software to fall into one camp and not the other? Does any such software actually exist? Please give examples.  Clarification I've already accepted an answer now, but I seem to have confused a lot of people, so perhaps a clarification is in order. I was not asking about the difference between copyleft (or \"viral\", though I don't like that term) and non-copyleft (\"permissive\") licenses. Nor was I asking about your personal idiosyncratic definitions of \"Free\" and \"Open\". I was asking about \"Free Software as defined by the FSF\" and \"Open Source Software as defined by the OSI\". Are the two always the same? Is it possible to be one without being the other? And the answer, it seems, is that it's impossible to be Free without being Open, but possible to be Open without being Free. Thank you everyone who actually answered the question. ",
        "Best answer": "According to Wikipedia, any software licensed under the NASA Open Source Agreement is open source, but not free, so that would be one example. "
    },
    {
        "ID": "21943",
        "Question": "We're all familiar with the Java package name convention of turning the domain name around. I.e. www.evilcorp.com would, by convention, chose to have their java packages com.evilcorp.stuff. Increasingly I'm getting fed up with this. As a commercial programmer, I encounter time and again that the software package name is completely irrelevant due to some rebrand, acquisition or similar. In the opensource world there's less name changes so there it makes sense. However it seems to me the shelf life of many pieces of (commercial/internal) software are much longer than that of the organisation making them. The problem is often made worse by software projects taking the marketing department's lead to use the name du jour they use refer to a certain project. A name that will, without fail, change 3 months down the line to make the emperor's new clothes feel fresh and new. Because of this, I've mostly stopped using the reverse domain as package name. Granted, if this is done on a large scale, there's risk of name collisions, but surely this is mitigated by either using \"unique\" software names, avoiding generic words, or use the reverse domain for projects intended to be sold/released as libraries. Other thoughts? ",
        "Best answer": "I'm going to quote the advice Microsoft gives for namespaces (.NET's packages), which doesn't have the domain name convention. I think it's good advice for Java packages too, since I don't believe that a domain name represents a solid and stable identity.  The general format for a namespace name is as follows: <Company>.(<Product>|<Technology>)[.<Feature>][.<Subnamespace>] For example, Microsoft.WindowsMobile.DirectX.  Do prefix namespace names with a company name to prevent namespaces from different companies from having the same name and prefix. Do use a stable, version-independent product name at the second level of a namespace name. Do not use organizational hierarchies as the basis for names in namespace hierarchies, because group names within corporations tend to be short-lived. The namespace name is a long-lived and unchanging identifier. As organizations evolve, changes should not make the namespace name obsolete.  If even your company name is unstable, you might want to just start with the product name. "
    },
    {
        "ID": "21977",
        "Question": "Back in the \"good ol' days,\" when we would copy shareware onto floppies for friends, we also used a fair bit of assembly. There was a common practice of \"micro-optimization,\" where you would stare and stare at lines of assembly until you figured out a way to express it in one fewer instruction. There was even a saying, which was mathematically impossible, that \"You can always remove one more instruction.\" Given that changing runtime performance by small constant factors isn't a major issue for (most) programming today, are programmers transferring these micro-optimization efforts elsewhere? In other words, Can a best-practice be taken to an extreme state where it's no longer adding anything of value? And instead is wasting time? For example: Do programmers waste time generalizing private methods that are only called from one place? Is time wasted reducing test case data? Are programmers (still) overly concerned about reducing lines of code? There are two great examples of what I'm looking for below: (1) Spending time finding the right variable names, even renaming everything; and (2) Removing even minor and tenuous code duplication.  Note that this is different from the question \"What do you optimize for?\", because I'm asking what other programmers seem to maximize, with the stigma of these being \"micro\" optimizations, and thus not a productive use of time. ",
        "Best answer": "Code Formatting Don't     get    me   wrong             , code      should be   consistent        &  readable                                ,  but       some   take it         too far.  "
    },
    {
        "ID": "21987",
        "Question": "I know there have been questions like What is your favorite editor/IDE?, but none of them have answered this question: Why spend the money  on IntelliJ when Eclipse is free? I'm personally a big IntelliJ fan, but I haven't really tried Eclipse. I've used IntelliJ for projects that were Java, JSP, HTML/CSS, Javascript, PHP, and Actionscript, and the latest version, 9, has been excellent for all of them. Many coworkers in the past have told me that they believe Eclipse to be \"pretty much the same\" as IntelliJ, but, to counter that point, I've occasionally sat behind a developer using Eclipse who's seemed  comparably inefficient (to accomplish roughly the same task), and I haven't experienced this with IntelliJ. They may be on par feature-by-feature but features can be ruined by a poor user experience, and I wonder if it's possible that IntelliJ is easier to pick up and discover time-saving features. For users who are already familiar with Eclipse, on top of the real cost of IntelliJ, there is also the cost of time spent learning the new app. Eclipse gets a lot of users who simply don't want to spend $250 on an IDE. If IntelliJ really could help my team be more productive, how could I sell it to them? For those users who've tried both, I'd be very interested in specific pros or cons either way. ",
        "Best answer": "I work with Intellij (9.0.4 Ultimate) and Eclipse (Helios) every day and Intellij beats Eclipse every time.  How? Because Intellij indexes the world and everything just works intuitively. I can navigate around my code base much, much faster in Intellij. F3 (type definition) works on everything - Java, JavaScript, XML, XSD, Android, Spring contexts. Refactoring works everywhere and is totally reliable (I've had issues with Eclipse messing up my source in strange ways). CTRL+G (where used) works everywhere. CTRL+T (implementations) keeps track of the most common instances that I use and shows them first.  Code completion and renaming suggestions are so clever that it's only when you go back to Eclipse that you realise how much it was doing for you. For example, consider reading a resource from the classpath by typing getResourceAsStream(\"/ at this point Intellij will be showing you a list of possible files that are currently available on the classpath and you can quickly drill down to the one you want. Eclipse - nope. The (out of the box) Spring plugin for Intellij is vastly superior to SpringIDE mainly due to their code inspections. If I've missed out classes or spelled something wrong then I'm getting a red block in the corner and red ink just where the problem lies. Eclipse - a bit, sort of. Overall, Intellij builds up a lot of knowledge about your application and then uses that knowledge to help you write better code, faster. Don't get me wrong, I love Eclipse to bits. For the price, there is no substitute and I recommend it to my clients in the absence of Intellij. But once I'd trialled Intellij, it paid for itself within a week so I bought it, and each of the major upgrades since. I've never looked back. "
    },
    {
        "ID": "22019",
        "Question": "What's the best place to put your hobby web projects(the web app itself, not the code) ? Typically, the projects are such that: a) I just want to test out an interesting idea without exploring the business angle to it, just to see how people take it. b) I don't expect a lot of traffic c) I don't want to scale immediately d) I don't want to be tied down to one technology(I want to do different projects to get familiar with various web stacks, langs and libs) Google app engine seems very restrictive for such exploratory stuff ... Restrictions like no outbound request can go beyond 10 seconds and every request has to return with 30 seconds, etc. piss me off, I know they are needed for scale, but I would like them to be optional. Amazon EC2 micro nodes are free for a year. But they ask for credit card information which I am not sure if I want to give away when I'm not paying initially. What other free/cheap alternatives do I have? ",
        "Best answer": "I'd just get a cheap VPS at any reputable provider. This gives you full control over the server / language stack, allows you to work with any kind of storage or DB and can easily be re-loaded if you want to completely switch technologies to work on something else (for instance, going from Linux to Windows or back). Something in the range of $15 could satisfy the needs of multiple hobby projects .. and I think most people would have no problem spending that on a hobby that they found enjoyable and worth while. This also ensures that you have full control of your data, as well as support if you need it. "
    },
    {
        "ID": "22070",
        "Question": "Similar question was closed on SO. Sometimes when we're programming, we find that some particular control structure would be very useful to us, but is not directly available in our programming language. What alternative control structures do you think are a useful way of organizing computation? The goal here is to get new ways of thinking about structuring code, in order to improve chunking and reasoning. You can create a wishful syntax/semantic not available now or cite a less known control structure on an existent programming language. Answers should give ideas for a new programming language or enhancing an actual language. Think of this as brainstorming, so post something you think is a crazy idea but it can be viable in some scenario. It's about imperative programming. ",
        "Best answer": "OK, this is a fun question. I would also like to have a general else for while and for loops, for when the condition isn't true on the first test: while (condition) {     // process } else {     // condition was never true }  This avoids the awkward re-computation of the condition or storing it in a variable. "
    },
    {
        "ID": "22073",
        "Question": " Possible Duplicate: What are some well known applications written in F#?   I see a lot of people talking about how cool functional programming is, how awesome Lisp and Haskell and F# are, etc, but what I don't see is them actually being used.  Just about every program on my computer is written in either something in the C family, Delphi, or Python.  I see tutorials talking about how easy it is to use functional languages to do complicated math problems, but no one talking about using them to do things most people actually care about using computers for, like business applications and games. Does anyone have any examples of actual programs that people have heard of and are using, written in a functional language?  The only one I can think of off the top of my head is Abuse, from almost 15 years ago.  (Things like Emacs, where the core program is written in C with a functional language scripting layer on top, don't count.) ",
        "Best answer": "I don't know of a lot of games or end-user applications written in functional languages, though I'm sure there are some. Games and end-user applications, though an important and visible part of the software market, are only a part of it. Remember, there is huge amounts of software out there that you never see because it is orchestrating processes that don't have end-user application or game interfaces.  For example, F# is popular at Credit Suisse for quantitative analysis: http://blogs.msdn.com/b/dsyme/archive/2010/07/15/f-jobs-at-credit-suisse-london.aspx Unless you work there, you're probably not going to ever see the user interface to that software. Or, Erlang is popular for writing the software that controls mobile phone switches: http://en.wikipedia.org/wiki/Erlang_(programming_language) You probably don't think of making a phone call as an application, but someone had to write the software that controls the switch. "
    },
    {
        "ID": "22183",
        "Question": "I have a thought that I tried asking at SO, but didnt seem like the appropriate place. I think that source sites like Google Code, GitHub, SourceForge... have played a major role in the history of programming. However, I found that there is another bad thing to these kind of sites and that is you may just \"copy\" code from almost anyone, not knowing if it is good(tested) source or not. This line of thought has taken me to believe that source code websites tend to lead many developers (most likely unexperienced) to copy/paste massive amounts of code, which I find just wrong. I really dont know how to focus the question well, but basic thought would be: Is this ok? Is Open Source contributing to that or I'm just seeing ghosts... Hope people get interested because I think this is an important theme. ",
        "Best answer": "Correlation doesn't imply causation. Developers copy/paste code they don't understand because they're bad developers. The availability of such code doesn't turn good developers bad. If there were no open source projects, there would still be forum posts with code snippets or programming books with examples. So we're back to my first paragraph: bad developers will find a way to be bad at writing code. The blame for copying and pasting code lies with the developers who do it, not with the source code repositories. "
    },
    {
        "ID": "22234",
        "Question": "Hypothetically speaking imagine that there exists a coworker that has a very shallow understanding of computing. To avoid stereotyping lets give this coworker the gender neutral name Chris.  Now Chris' diagnostic ability is low and can't figure out the correct IP addresses to set his/her virtual machines to. Chris also fails to merge code properly and overwrites a commit I made fixing something, thereby re-introducing a bug. I let this slide, refix the bug and do not make a sound about it to management.  Given a task Chris either 1) complains that there isn't sufficient information resulting in 2) you provide Chris with inordinately detailed instructions to satisfy 1). The more detail you provide in a list of steps to carry out, the more chance of an error being present in your instructions. Chris gets these instructions, tries to execute them, fails and it becomes your fault because your instructions aren't good enough. How do you deal with this?  ",
        "Best answer": " Anyway given a task Chris either 1)   complains that there isn't sufficient   information resulting in 2) you   provide Chris with inordinately   detailed instructions to satisfy 1).   The more detail you provide in a list   of steps to carry out, the more chance   of an error being present in your   instructions.  Having been in both your position and Chris's, I might be able to explain things a bit.  I hear you saying that you're giving Chris tasks, but you don't mention involving him in coming up with those instructions.  You're probably trying to help him do the right thing, but that's probably not how he sees it.  When you're in Chris's place, it's difficult not to think of what you're trying to do as saying \"OK, here's your work.  Now do your job, drone.\" In other words, the solution isn't to give Chris more instructions.  In fact, you should give him no instructions.  Instead, you should help him come up with a course of action.  Once Chris sees his role in the process, he might very well turn into a different person altogether. "
    },
    {
        "ID": "22249",
        "Question": "Even though I haven't been in a TDD or BDD project, or I have been in some that say they are doing TDD but are pretty far from it, these are things that I think about and really try to read as much as I can about.  Back to the question. When you're doing BDD you should write your \"test\" first and make it fail, right? And then implement that feature or what you call it. But if you take this to the extreme couldn't this be some kind of top-down development? You're looking at your UI and says, \"I would like to have this feature/behavior here\". Then you fix your UI to implement that feature and the code that supports the UI. At this point you haven't implemented any business logic or data access logic, you have just implemented your behavior. What I'm aiming at instead of writing the test first you write your UI code first. In some cases it should result in the same code for the data access and business layer, since you use your UI code to define what your business needs to support.  Of course you should complement this with tests that is used to make sure that the feature is working as it should in the feature. Any thoughts? ",
        "Best answer": "Yes! Otherwise, what you get is development-driven testing. Realistically speaking, however, there are problems that are hard to approach by using \"pure\" TDD.  You could be more productive writing by writing some uncovered production code up front and covering it with tests later (and learning how to approach similar problems with TDD in the future). Look at this technique, which its author called rinse-and-repeat TDD for want of a better term. "
    },
    {
        "ID": "22255",
        "Question": "Let's say I am working on an open source project and want to re-use a trivial utility function from another open source project (for example, a file search/replace function). Is it legal to copy the function and simply write a small copyright notice in top of the file? Should I include their name as copyright holders of the entire project in the license? Similarly, let's say I fork an open source project. Where and how do I specify that the copyright is shared between both the original copyright holder and myself?  I guess the answer must somewhat vary according to the open source license but I'd like a general answer as much as possible. PS: I'm mostly concerned about the legal aspect, but feel free to include your ethical point of view.  ",
        "Best answer": "I'm trying to make this answer as 'meta' applicable as possible. Using snippets / bits from other projects Clearly mark the code with the original author's copyright. Make sure that your license of choice is fully compatible with the license of the code you are using. You will need permission of the author to move the code to a different license (unless they specifically allow you to do so, I.e. \"GPL 2 or any later version\") Your program should have an AUTHORS file (or similar), where you list all contributors and things that you used from other projects. Forking a project For each module that you substantially change, add your copyright under the original author's. The same thing goes for licensing, you are bound by the terms of the license that was in effect when you forked it. If the project says \"GPL2 only\", you must respect that, you can't go to GPL3 without their permission. This varies, greatly, depending on the license at hand. The QPL says you can only distribute changes in patch format, for instance, so make sure you understand the terms that allow you to distribute modified versions of the software. Beyond that, always preserve copyright. If adding your own copyright to what exists, be sure that you clearly mark exactly what you are claiming.  "
    },
    {
        "ID": "22265",
        "Question": "I'm still at school, and I know I have problems dealing with other people. I'm not an angry or shy or different, I just like to work my way and with my opinions while respecting other's, I have a big curiosity and hunger for knowledge, but I lack practice and I guess people don't want to work with me because they might fear I would talk some kind of morale. (For example I started to learn programming using linux instead of windows, even if I use windows a lot. And I have a mac). What happens to programmers who lack teamwork? Where does the problems begin? Does being a good programmer compensate at least a little? Is it normal for a programmer to have a vision about his work instead of just doing what he is told? ",
        "Best answer": "Your behavior is pretty common at your age. I was like you. The good news is that most of the time, it evolves in the good direction. You will learn how to integrate yourself within a team. You will love it! But I met some people that weren't able to make it and are now stuck is depression. Depending on the style of management of your company, you will either be rejected by your team or simply fired after a while. So you must be prepared to face some difficulties. France's most common style of management is based on fear and punishment. This is not a good news for you since it will encourage your individualism. So it will encourage your behavior. That said, you already know there is a problem with you, so it's a pretty good indication that you have all you need to evolve without external help. The first step is being aware. The second one, the most difficult, is acting on it. "
    },
    {
        "ID": "22305",
        "Question": "I'm working on a site for my company which, up until a certain point, was an Internet Explorer-only site for various reasons, mainly that when the site was originally conceived IE had a 90%+ market share so the work to get it working in other browsers just didn't make sense. Now that we live in a more diverse browser economy we're doing the work to get the site working everywhere, and as luck would have it a decent chunk of it just happens to already work.  However, one issue we're struggling with is the issue of what to support and what not to support. For starters, non-IE browsers release much more frequently than IE did, and you don't know which versions are still in the wild. There's been basically three versions of IE released in the last decade, and IE6 is still supported until 2014. But there's an update for Firefox every other day, Apple updates Safari more or less annually. And then there's Chrome. Chrome has gone from 0.2 to 9.0 in a little over two years. 7.0.517 was released a month and a half after 6.0.472. There's three different versions out right now, a stable, a beta, and a dev. And the dev version of 9.0.587 was actually released before the latest beta version of 8.0.552. With IE we've had the situation arise where we have to support an old version because the IT department of the company in question does not allow the employees to upgrade. With non-IE browsers I'm thinking we'll adopt the line of \"update to the latest or we can't help you\" but I'm not sure how effective that is. Also, my company does some amount of artificial limitations. For example we have a product aimed at companies so we don't support \"Home\" versions of Windows (i.e., XP Home, 7 Home Premium) even though there's no technical reason we couldn't.  When my company starts asking \"what version or versions of Chrome do we support\", how should I answer? ",
        "Best answer": "Chrome's new version adoption rate is really fast because of their automatic upgrade. Way faster than IE and even quite a bit faster than Firefox. Generally, if you are supporting the latest stable build of Chrome, you should be fine. It is literally only a matter of days or weeks before a new stable version almost totally replaces the old.  Edit: Graph from How Google Keeps Chrome Fresh "
    },
    {
        "ID": "22347",
        "Question": "One thing that I've learnt in my short dev career is that there are best practices (like TDD) and then there are actual practices that occur under pressure, or through budgetary/time constraints.  I'm working in a small PHP dev team on a project using Zend framework. The project has the potential to demand a lot of scalability, and so we are trying to establish a good development environment to promote ease of development long term. Architectural patterns/practices aside, I'm looking at trying to automate deployment, checking code quality through svn hooks, possibly even setting up a CI environment.  So I'm interested in hearing people's experience of deployment practices. I've checked other examples as they pertain to other languages - but I'm ideally looking to hear from people who have worked in a PHP/Zend environment To make the question less vague: What are people's experiences of using deployment systems like phpUnderControl and rSync? ",
        "Best answer": "Basically it needs to be easy (if everything is acceptable) to deploy.  This usually means, that after a checkin the system rebuilds automatically.  If ok, the user can at a single click cause the test server to update to the new version. You should be able to roll-back as easy. The easier to use you can keep it, the happier people will be.  If at all possible set up a CI.  Hudson Jenkins is nice, but there are plenty others.  You WILL appreciate it quickly. "
    },
    {
        "ID": "22363",
        "Question": "For a large enterprise app, everyone knows that being able to adjust to change is one of the most important aspects of design. I use a rule-based approach a lot of the time to deal with changing business logic, with each rule being stored in a DB. This allows for easy changes to be made without diving into nasty details. Now since C# cannot Eval(\"foo(bar);\") this is accomplished by using formatted strings stored in rows that are then processed in JavaScript at runtime. This works fine, however, it is less than elegant, and would not be the most enjoyable for anyone else to pick up on once it becomes legacy. Is there a more elegant solution to this? When you get into thousands of rules that change fairly frequently it becomes a real bear, but this cannot be that uncommon of a problem that someone has not thought of a better way to do this. Any suggestions? Is this current method defensible? What are the alternatives? Edit: Just to clarify, this is a large enterprise app, so no matter which solution works, there will be plenty of people constantly maintaining its rules and data (around 10).  Also, The data changes frequently enough to say that some sort of centralized server system is basically a must. ",
        "Best answer": "I would use WF or Drools if you're trying to create an abstraction that non-programmers could work with to develop business rules.  However, if you're dealing with programmers than the abstraction of WF isn't worth the time it takes to develop a solution, I fail to see the added value for your investment. A database is a good way to maintain rules that might change a lot, but let me suggest a possible alternative (not necessarily better, but an alternative).   1) Separate out the business logic into its own layer (dll) -- abstracting how you're calling it with some interfaces (look at the strategy and observer patterns) i.e IRuleEvaluator.Evaluate(string myParams).   2) Now you can create discrete classes for each rule (if necessary) and accompanying unit tests. 3) Now for the Pièce de résistance, wire up everything behind the scenes in an IOC container -- the dll and the rule evaluator instance itself.  This way your rule evaluator is constructed through the configuration at runtime -- nothing is hard-coded together. This approach is very dynamic (maybe too dynamic for some tastes) -- it will allow you to have actual discrete unit tests for your all of your rules.  If you need to re-deploy or change a rule, you can drop a new DLL, change your IOC configuration file, restart and verify -- or roll back the change if something is wrong -- without modifying your core code (just like a database).  And unlike a database, it will keep the logic and application code under one umbrella, instead of half written in C# and half in SQL. "
    },
    {
        "ID": "22395",
        "Question": "We have a trac server setup that works with the svn commits, so we can do things like \"fixed #183\" in the commit messages, and reference the changes involved.  Right now I have eclipse with subclipse + trac plugins, and it works pretty well. But I don't have internet all the time and it becomes very difficult to commit to an inaccessible svn server. I would like to use some type of local repository for commits, and then push commits all at once, but individually to server.  Changing svn / trac isn't an option at this time.  Other developers can touch the svn server directly. Is there a way to cache the commits locally, and then send them when I have internet again?  Take in mind that I can't just do one big commit of all my changes because it makes it a nightmare since I can't selectively merge. Basically is there a way to do this with git/bzr/mercurialand still use svn/trac as intended? I'd be willing to do some scripting, but I don't know where to start. ",
        "Best answer": "git-svn does pretty much exactly what you want. I used to do this at a former place; they had SVN installed as source control, but the connection was unreliable so I tracked my changes locally in Git and only committed to the main repo when I had a hard line into the office server. It was really, REALLY useful having local branches, and merging my code in got much easier once I switched over. The link up there is basically an online man page. If you decide to go this route, also check out the tutorials available online. The only real complaint I had was that the initial checkout took a good 40 minutes; I went to grab a sandwich and a tea, and it had just finished when I got back. Committing and pushing thereafter was a breeze (actually much faster than the SVN process), but don't be surprised if that first pull takes a while. "
    },
    {
        "ID": "22416",
        "Question": "I am being approached with a job for writing embedded C on micro controllers. At first I would have thought that embedding programming is too low on the software stack for me, but maybe I am thinking about it wrong. Normally I would have shrugged off an opportunity to write embedded code, as I don't consider myself an electrical engineer. Is this a bad assumption? Am I able to write interesting and useful software for embedded systems, or will I kick myself for dropping too low on the software stack? I went to school for computer science and really enjoyed writing a compiler, thinking about concurrent algorithms, designing data structures, and developing frameworks. However, I am currently employed as a web developer, which doesn't scream the interesting things I just described. (I currently deal with issues like: \"this check box needs to be 4 pixels to the left\" and \"this date is formatted wrong\".) I appreciate everyone's input. I know I have to make the decision for myself, I just would like some clarification on what it means to be a embedded programmer, and if it fits what I find to be interesting. ",
        "Best answer": "If you want to be good at working on embedded systems, then yes, you need to think like a EE some of the time.  That is generally when you are writing code to interface with the various peripherals (serial busses like UART, SPI, I2C or USB), 8 and 16-bit timers, clock generators, and ADCs and DACs.  \"Datasheets\" for microcontrollers often run into the hundreds of pages as they describe every bit of every register.  It helps to be able to read a schematic so you can probe a board with an oscilloscope or logic analyzer. At other times, it is just writing software.  But under tight constraints: often you won't have a formal OS or other framework, and you might have only a few KB of RAM, and maybe 64 KB of program memory.  (These limits are assuming you are programming on smaller 8 or 16-bit micros; if you are working with embedded Linux on a 32-bit processor, you won't have the same memory constraints but you will still have to deal with any custom peripheral hardware that your Linux distro doesn't provide drivers for.) I have a background in both EE and CS so I enjoy both sides of the coin.  I also do some web programming (mostly PHP), and desktop apps (C# and Delphi), but I've always enjoyed working on embedded projects the most. "
    },
    {
        "ID": "22525",
        "Question": "For years I have considered digging into what I consider \"low level\" languages. For me this means C and assembly. However I had no time for this yet, nor has it EVER been neccessary. Now because I don't see any neccessity arising, I feel like I should either just schedule some point in time when I will study the subject or drop the plan forever. My Position For the past 4 years I have focused on \"web technologies\", which may change, and I am an application developer, which is unlikely to change. In application development, I think usability is the most important thing. You write applications to be \"consumed\" by users. The more usable those applications are, the more value you have produced. In order to achieve good usability, I believe the following things are viable  Good design: Well-thought-out features accessible through a well-thought-out user interface. Correctness: The best design isn't worth anything, if not implemented correctly. Flexibility: An application A should constantly evolve, so that its users need not switch to a different application B, that has new features, that A could implement. Applications addressing the same problem should not differ in features but in philosophy. Performance: Performance contributes to a good user experience. An application is ideally always responsive and performs its tasks reasonably fast (based on their frequency). The value of performance optimization beyond the point where it is noticeable by the user is questionable.  I think low level programming is not going to help me with that, except for performance. But writing a whole app in a low level language for the sake of performance is premature optimization to me. My Question What could low level programming teach me, what other languages wouldn't teach me? Am I missing something, or is it just a skill, that is of very little use for application development? Please understand, that I am not questioning the value of C and assembly. It's just that in my everyday life, I am quite happy that all the intricacies of that world are abstracted away and managed for me (mostly by layers written in C/C++ and assembly themselves). I just don't see any concepts, that could be new to me, only details I would have to stuff my head with. So what's in it for me? My Conclusion Thanks to everyone for their answers. I must say, nobody really surprised me, but at least now I am quite sure I will drop this area of interest until any need for it arises. To my understanding, writing assembly these days for processors as they are in use in today's CPUs is not only unneccesarily complicated, but risks to result in poorer runtime performance than a C counterpart. Optimizing by hand is nearly impossible due to OOE, while you do not get all kinds of optimizations a compiler can do automatically. Also, the code is either portable, because it uses a small subset of available commands, or it is optimized, but then it probably works on one architecture only. Writing C is not nearly as neccessary anymore, as it was in the past. If I were to write an application in C, I would just as much use tested and established libraries and frameworks, that would spare me implementing string copy routines, sorting algorithms and other kind of stuff serving as exercise at university. My own code would execute faster at the cost of type safety. I am neither keen on reeinventing the wheel in the course of normal app development, nor trying to debug by looking at core dumps :D I am currently experimenting with languages and interpreters, so if there is anything I would like to publish, I suppose I'd port a working concept to C, although C++ might just as well do the trick. Again, thanks to everyone for your answers and your insight. ",
        "Best answer": "I was just thinking this recently. I'd currently consider myself as a C# developer - which is perfectly fine for my career. However, every so often I miss out on the really low level things (essentially 'getting my hands dirty' by doing assembler or device drivers in C). I just miss the programming. I don't expect that to help me in my career massively. If device drivers or embedded systems are your thing, then it could help a lot. The more I program in the abstracted languages, the more I miss what got me into computers in the first place: poking around the computer and seeing what twitches. Assembler and C are very much suited for poking :) By using the older languages, I think you're forced to do pretty much everything yourself. In C# I can do something like myArray.SortBy(x=>x.Name). No way I'd be able to do that in C. I'm accepting that the language will do the best sorting for me. If I was to do it in C, I'd be able to go back to the days of my university modules and revise my different sort and search algorithms. So, I think the lower level languages would help you to revise any of the long forgotten bits that have all being abstracted away. More of a personal challenge than a career progressing one. "
    },
    {
        "ID": "22528",
        "Question": "Sometimes I come across these message-hub-style APIs, for example the Cocoa NSNotificationCenter: http://developer.apple.com/library/mac/#documentation/Cocoa/Reference/Foundation/Classes/NSNotificationCenter_Class/Reference/Reference.html Usually these APIs provide a global access point on which you subscribe to or broadcast messages/events. I'm thinking this is a problem because it encourages a flat and unstructured program architecture, where dependencies are not explicit in the API, but hidden in the source code. You are not forced to think about object ownership and hierarchies, but can rather make any object in your program result in any code anywhere being called. But maybe this is a good thing? Does this pattern generally encourage good or bad program design, and why so? Does it make the code harder or easier to test? Forgive me if this question is too vague or broad. I'm trying to wrap my head around the potential consequences of extensive use of an API like this, and the different ways you could use it. Edit: I guess my biggest issue with this pattern is that the API \"lies\" about dependencies and object couplings, and can be illustrated with this example: myObj = new Foo(); myOtherObj = new Bar(); print myOtherObj.someValue; // prints 0 myObj.doSomething(); print myOtherObj.someValue; // prints 1, unexpectedly, because I never indicated that these objects had anything to do with each other  ",
        "Best answer": "Asynchronous messaging is a good architectural principal for large systems that must scale The Java equivalent of this is JMS and generally considered to be a good thing. This is because it promotes decoupling of your client code from the code that actually services the message. Client code merely has to know where to post their message. Service code merely has to know where to pick up messages. Client and service know nothing of each other and therefore can change independently of each other as required.  You can easily externalise the URI of the message hub to make it configurable and not embedded in the source code.  "
    },
    {
        "ID": "22542",
        "Question": "I want to know in which applications/programming domain are most suitable for Smalltalk. Could anyone please provide me some useful links that could answer my query? Through googling I learned that some companies use it for: logistics and foreign trade application desktop, server and script development data processing and logistics, scripts and presentations but I cant find documents/research papers that can tell me which programming domain Smalltalk-80 (or Smalltalk) is best suited. Some of the programming domains are: - Artificial intelligence reasoning - General purpose applications - Financial time series analysis - Natural language processing - Relational database querying - Application scripting - Internet - Symbolic mathematics - Numerical mathematics - Statistical applications - Text processing - Matrix algorithms I hope you guys can help me. I am doing this for my case study. Thanks in advance. ",
        "Best answer": "Smalltalk is a general-purpose programming language that is used for all sorts of stuff. A lot of consulting companies treat Smalltalk as their \"secret weapon\" that allows them to outbid and outperform their competitors, for example. Some of them even go so far as to develop their own Smalltalk implementations. E.g. Smalltalk/X is the in-house platform for eXept Software. In a similar vein, Lesser Vision-Smalltalk is the secret weapon of Lesser Software. They actually use it for Windows desktop applications. It is widely used in the financial industry for modeling complex financial products such as derivatives, swaps, etc. So widely, in fact, that some Smalltalk community leaders have jokingly accepted full responsibility for the 2007 meltdown. Smalltalk excels the more complex the domain is. Trendly uses an interesting mix of languages and its lead developer, Avi Bryant, gave a good description of what the particular languages are useful for. He said something like \"we use Smalltalk for the thinking, Java for the muscle, ECMAScript for the pretty face and Ruby to glue it all together\". So, the complex statistical models and algorithms are in Smalltalk, the actual brute-force calculations are done in Java and the results are rendered dynamically on the client-side in ECMASCript. "
    },
    {
        "ID": "22568",
        "Question": "The situation is that a client requested a number of changes about 9 months ago which they then put on hold with them half done.  They've now requested more changes without having made up their mind to proceed with the first set of changes.  The two sets of changes will require alterations to the same code modules. I've been tasked with explaining why them not making a decision about the first set of changes (either finish them or bin them) may incur additional costs (essentially because the changes would need to be made to a branch then if they proceed with the first set of changes we'd have to merge them to the trunk - which will be messy - and retest them). The question I have is this: How best to explain branching of code to a non-technical client? ",
        "Best answer": "It probably not that important to explain branching. What is important is that you explain the impact of their non-decision.  In this case the impact is if they decide they want the first set of changes down the road it will increase the cost then if you implement the change now. One nice way they will get the message is if you do an estimate for both.  If they don't understand why the estimates are different you can explain as you already have. Testing will have to be done twice and incompatibilities will need to be resolved, etc. You can also use a building metaphors. Personally I don't like them but it wouldn't be that hard to do. One example that springs to mind replacing a bathtub and the plumbing together is cheaper than replacing the bathtub and plumbing separately, since you only have to rip out the tub once and re-caulk once and so on.  "
    },
    {
        "ID": "22597",
        "Question": "This came out of some of the answers and comments on another question (this one). I've worked primarily with waterfall projects and while I've worked on ad-hoc projects that have taken on agile behaviours and have read a fair bit about agile, I'd say I've never worked on a \"proper\" agile project. My question is does the concept of \"late\" have any meaning in agile, if so then what? My reasoning is that with agile you have no upfront plan and you have no detailed requirements at the outset.  You may have a high level goal in mind and a notional date attached to it but both may change (potentially massively) and neither are certain. So if you don't know exactly what you're going to deliver basically until you deliver it and the user accepts it, and if you don't have a schedule beyond the next sprint, how could you ever be late in any way that actually has meaning? (Obviously I understand that a sprint might overrun but I'm talking about beyond that.) Just to be clear I'm (personally) happy with the assumption that on time waterfall projects (even relatively large ones) are possible based on the fact I've seen them and been involved in them - they're not easy or common even but they are possible.   This isn't about knocking agile, it's about me understanding it.  I've always seen the benefit of agile as nothing to do with deadlines or budgets (or rather only indirectly), it's to do with scope - agile delivers closer to what is really important rather than what the project team think is important before they've seen anything. ",
        "Best answer": "I disagree that an Agile project has no upfront plan.  My experience is that the business analysts have spent a fair amount of time working in design meetings with customers and developers to come up with a detailed list of achievable requirements that are presented as user stories. These are then broken down into tasks with suitable estimates attached by experienced developers. Once the most important tasks have been identified at the start of the sprint/iteration then coding can begin. This selection process determines the meaning of the iteration in the overall project (\"We're building the login process\"). Various memebers of the team get on with the various tasks necessary to make that user story happen.  At the end of the iteration all the user stories for that iteration should be complete, or you're late. Equally, development should be able to stop at the end of each iteration and the product released. It may not be complete in terms of all user stories, but those user stories that were requested in the iteration are complete and the product can work to those limits. "
    },
    {
        "ID": "22598",
        "Question": "Preface This is not code golf. I'm looking at an interesting problem and hoping to solicit comments and suggestions from my peers. This question is not about card counting (exclusively), rather, it is about determining the best table to engage based on observation. Assume if you will some kind of brain implant that makes worst case time / space complexity (on any given architecture) portable to the human mind. Yes, this is quite subjective. Assume a French deck without the use of wild cards. Background I recently visited a casino and saw more bystanders than players per table, and wondered what selection process turned bystanders into betting players, given that most bystanders had funds to play (chips in hand). Scenario You enter a casino. You see n tables playing a variant of Blackjack, with y of them playing Pontoon. Each table plays with an indeterminate amount of card decks, in an effort to obfuscate the house advantage. Each table has a varying minimum bet. You have Z currency on your person. You want to find the table where:  The least amount of card decks are in use The minimum bet is higher than a table using more decks, but you want to maximize the amount of games you can play with Z. Net losses, per player are lowest (I realize that this is, in most answers, considered to be  incidental noise, but it could illustrate a broken shuffler)  Problem You can magically observe every table. You have X rounds to sample, in order to base your decision. For this purpose, every player takes no more than 30 seconds to play. What algorithm(s) would you use to solve this problem, and what is their worst case complexity? Do you:  Play Pontoon or Blackjack ? What table do you select ? How many rounds do you need to observe (what is the value of X), given that the casino can use no more than 8 decks of cards for either game? Each table has between 2 and 6 players. How long did you stand around while finding a table?  I'm calling this the \"standing gambler problem\" for lack of a better term. Please feel free to refine it. Additional Where would this be useful if not in a casino? Final I'm not looking for a magic gambling bullet. I just noticed a problem which became a bone that my brain simply won't stop chewing. I'm especially interested in applications way beyond visiting a casino. ",
        "Best answer": "If I can concurrently examine all tables even while playing then: Given that all games progress at the same speed for simplicity...(i could adjust for this)    while(not satisfied with winnings)  keepcurrenttable == 1 || Wait for positive count threshold on any one table     if Z/(min bet)>%chance of win*payout       next     else       Play hand according to standard counting rules.     endif if (%chance of win on another table*payout on another table > same for current) && Z/(min bet)>%chance of win*payout   change tables    elseif currenttable %chance of win*payout >   keepcurrenttable=1  else   keepcurrenttable=0  endif next   still some bugs in there and doesn't account for EVERYTHING, but you get where I'm going   there's a lot more to %chance of win   number of players should be minimally pertinent(more players == faster burning through decks)   does less players mean I can play more hands concurrently?(that would require heavier weight on player count)   additionally, count threshold could be defined with a risk appetite meter. "
    },
    {
        "ID": "22643",
        "Question": "It's not that this doesn't make sense, but it just works out awkward 99% of the time. Often in 2D graphics rectangles are initialized, stored and manipulated as a pair of points.  In no particular language, class Rect:    p1, p2: point  It makes more sense to define a rectangle as two x values and two y values, like this: class Rect    xleft, xright: int    ytop, ybottom: int  With two points, if at some place in the source code you want to make use of the y value of the top, you'd have to say rect.p1.y  (hmmm, stop and think, is it p1 or p2) but with the four values as plain data members, it's clear and direct:  rect.ytop (no thinking required!)     The use of two points means that in dealing with the vertical, you have to tangle the horizontal; there's an extraneous relation between indepenent elements. How did this two-point idea come about and why does it persists?  Does it have some benefit over bare x and y coordinates? ADDED NOTE: This question is in the context of X-Y aligned rectangles, such as in windows managers and GUI toolkits, not in the context of arbitrary shapes in drawing and painting app. ",
        "Best answer": "I always liked defining a rectangle as a point + width and height, where the point is the upper-left corner of the rectangle. class Rect {   float x, y;   float width, height; }  And then add whatever methods you need to fetch the other metrics. Like the Java version "
    },
    {
        "ID": "22721",
        "Question": "My question relates to how I should share the fees paid by clients to me and a fellow programmer, both of whom are freelancers. I've thought of a few options, but I'm in a dilemma as to which one would be the most motivating for both of us. 1) The first option I've thought is a fifty-fifty share. However, more brainwork is going to be done by my colleague, while initially at least, I will be handling the communication with customers. 2) The second option is a 60-40 share, where the colleague exerting more efforts gets a bigger share. This is the option I feel most inclined to adopt, but I'm not sure how it's going to feel in the long run. 3) The third option is calculating each one's contribution in terms of the number of hours spent, and sharing the revenue accordingly. It will be wonderful to hear everybody's thoughts on this! ",
        "Best answer": "Money is one of those things that can leave people feeling bitter and drive them apart very quickly. Regardless of the actual percentages, make sure you and your fellow programmer have a clear understanding of the profit sharing scheme. After you had a chance to discuss profit sharing, put it in writing with each of you having a signed copy. Also, agree to revisit the approach after you had a chance to work together and get an understanding of your working dynamic. As long as both of you are satisfied with the profit sharing, you can put your thoughts to solving the problems you are hired to do and not be distracted by the financial side. "
    },
    {
        "ID": "22762",
        "Question": "I'm an experienced Java developer who is just starting up a project for an NGO. I will be working on the project for at least 6 months, following which the NGO will have to pay or find a volunteer to maintain the project. Seeing as they already have people working on their website in PHP, I figured PHP was the obvious choice to make sure the skills are still available (it is webby) - eliminated Java because Java devs are typically expensive. Unfortunately I have next to zero experience with proper PHP development (just a few months spending a small percentage of my time on a Drupal project without any real coding). What are some things I can do to ensure that the code I leave behind is maintainable by a relatively low-skilled PHP developer (eg a teenager wanting to make some holiday cash)? Do I go with a CMS? Are Drupal developers cheap? Any other CMS / Framework I should look at?  Background: the project is a website that people will search for educational information, with some simple user-management to only allow some users to create content, restrictions to specific content-types etc.  The CMS vs write myself question is not the only thing I'm interested in hearing. I'm also interested in any tips about code style, anything you think my Java experience would push me towards that is going to make it difficult for the hypothetical volunteer etc. There's probably things about this scenario that I haven't thought through - so anything related to keeping maintenance costs low would be appreciated. ",
        "Best answer": "Definitely go with a well-known CMS like Drupal, Wordpress or Joomla.  They all have very large communities and therefore plenty of developers and developer resources. Trying to build your own will lead to a good deal of pain :-) "
    },
    {
        "ID": "22769",
        "Question": "What language, in your opinion, allows the average programmer to output features with the least amount of hard-to-find bugs? This is of course, a very broad question, and I'm interested in very broad and general answers and wisdoms. Personally I find that I spend very little time looking for strange bugs in Java and C# programs, while C++ code has its distinct set of recurring bugs, and Python/similar has its own set of common and silly bugs that would be detected by the compiler in other languages. Also I find it hard to consider functional languages in this regard, because I've never seen a big and complex program written in entirely functional code. Your input please. Edit: Completely arbitrary clarification of hard-to-find bug: Takes more than 15 minutes to reproduce, or more than 1 hour to find cause of and fix. Forgive me if this is a duplicate, but I didn't find anything on this specific topic. ",
        "Best answer": "The more powerful the type system of the language, the more bugs will be caught at the compile time itself. The following figure compares some of the well known programming languages in terms of the power, simplicity, and safety of their type systems. [ Source ]  *Factoring in the ability to use unsafe constructs.  C# gets stuffed into the unsafe row because of the \"unsafe\" keyword and associated pointer machinery. But if you want to think of these as a kind of inline foreign function mechanism feel free to bump C# skyward. I've marked Haskell '98 as pure but GHC Haskell as not pure due to the unsafe* family of functions. If you disable unsafe* then jump GHC Haskell up accordingly.  "
    },
    {
        "ID": "22819",
        "Question": "Suppose I have some code void some_block(void) {     some_statement_1();     some_statement_2();     some_statement_3(); }  Now I wish to out-comment statement #2, but here I never can find a way where it feels right. Usually I will do the following: void some_block(void) {     some_statement_1(); //    some_statement_2();     some_statement_3(); }  But now we got what looks like extra indentation of statement #2, yet it is arguable whether the indentation is still \"correct\". Alternatively we could do: void some_block(void) {     some_statement_1();     //some_statement_2();     some_statement_3(); }  This looks a bit better, yet still somehow also wrong. However the following just looks misplaced: void some_block(void) {     some_statement_1();   //some_statement_2();     some_statement_3(); }  How do you outcomment code and keep \"correct\" indentation? Update It seems most people who have provided answers did not answer my question, but simply state that it is bad practice to leave dead code around. While I completely agree with that, that was not the point of the question. Right now I am facing a case of example where I have 5 helper functions, that manage 5 similar but slightly different structs for me. I use these helper functions to make my code easier to read. But, at the moment (and for the next month probably) one of the structs are not going to be used. And I cannot commit anything which causes compiler warnings, so I have to outcomment that part of the code and commit it like that. Also I forgot this last variant, which I think just looks plain wrong: void some_block(void) {     some_statement_1(); //  some_statement_2();     some_statement_3(); }  ",
        "Best answer": "Format it so it sticks out and makes your code look so ugly that you'll be compelled to delete it. Comments are for comments and not for dealing with unnecessary code; that's the job of version control.  "
    },
    {
        "ID": "23121",
        "Question": "How is a Ninja programmer defined? If it is just an experienced developer, then is there really no better way to describe that? Ninja sounds like a childish marketing ploy designed to attract developers with delusions of self-grandeur. Or am I missing something? In particular I want to know what qualities of a Ninja are desired in programmers that makes the comparison valid (besides the coolness factor)? I did find this article which makes some good comparisons between Ninja characteristics and Agile development. However, I would like to hear from people who have used the word Ninja for hiring purposes and what their motivation was behind it. Update: There were several good points raised on both sides of the argument. I've tried to summarize these in my blog post. I've chosen JB's answer as the accepted one because it summarizes all the valid reasons for making the comparison. ",
        "Best answer": "It's pretty much as you say - an attempt to imply coolness and eliteness (frequently among those who are neither) by making highly tenuous comparisons. Personally I hate the phrase (along with \"Rockstar Programmer\" which makes we want to batter people to death - you can be a rockstar programmer when you've got a drug habit an alcohol problem and have trashed at least one car and one hotel room).   You're a programmer, if you have issues with being a programmer because you want to do something you think is cooler then deal with them (or quit to do something you think is cooler).  If you don't like the way others view programmers then understand that's their issue not yours. You're certainly not going to change their minds with phrases that would make the average 12 year old think you were trying a bit hard to be impressive. /rant Edit:  To be clear, the rant is not aimed at the questioner... "
    },
    {
        "ID": "23142",
        "Question": "I've been thinking a lot about the problem of standardization across large companies.  For me, the answer is pretty clear: do not arbitrarily standardize on tools just because that's what everyone else does.  It's counter-productive.   I'm thinking about source control, bug tracking and testing tools primarily.  It seems so obvious to me that I can't really think of many good counter-arguments.  Some I've thought of :  Easier sharing between teams Easier for IT to manage  (1) is definitely true if different teams are working on the same codebase/product.  But in cases where one team doesn't work directly with the code I don't think is a valid reason. (2) is just kind of pisses me off if that's really an argument.  The limitations placed on programmers, I believe, is more burdensome than any extra work piled on to IT.  But I'm not an IT guy. I'm hoping for some better arguments than those. ",
        "Best answer": "Yes, but it needs to be done in a manner which is sensible and doesn't affect the productivity of developers for the sake of process. There is definitely a case for standardisation of bug tracking/source control as a lot of enterprise IT is somewhat isolated and distributed. For instance, consultants might be hired to create something for one business unit, in-house staff might be maintaining a legacy system and over in the Republic of Elbonia HQ, another in-house team is working something else.  Common source control and bug tracking systems, place everyone on a level playing field and make it easier to deal with multiple systems. Imagine your support people having to learn 3 different bug tracking systems because each team preferred a different solution. Without standardisation you can also end up with the situation of the consultants using source control system X and in-house using source control system Y (or worse, no SC at all), which will lead to all sorts of fun when the software is completed and the consultancy either moves on or goes bust. This doesn't always mean you'll end up standardising on the best software available though, often the software which is chosen is based upon compromise and practicalities rather than having all of the latest and greatest features. Standardisation also requires common sense. There is a level where you just have to let the developer get on with the job without undue interference. Where that line lies depends on the culture of the organisation, the kind of products being worked upon and regulatory factors. There is more to software than just code. "
    },
    {
        "ID": "23146",
        "Question": "I am having a disagreement with a client about the user authentication process for a system. The nub of it is that they want each user to have a globally unique password (i.e. no two users can have the same password). I have wheeled out all the obvious arguments against this:  It's a security vulnerability. It confuses identification with authentication. It's pointless. etc.  Still they are insisting that there is nothing wrong with this approach. I have done various Google searches looking for authoritative (or semi-authoritative , or even just independent) opinions on this, but can't find any (mainly it's just such an obvious faux pas that it doesn't seem worth warning against, as far as I can tell).  Can anybody point me towards any such independent opinion, please?  EDIT: Thanks for all your answers, but I already understand the problems with this proposed approach/requirement, and can even explain them to the client, but the client won't accept them, hence my request for independent and/or authoritative sources. I'd also found the Daily WTF article, but it suffers from the problem that Jon Hopkins has pointed out - that this is such a self-evident WTF that it doesn't seem worth explaining why. And yes, the passwords are going to be salted and hashed. In which case global uniqueness might well be difficult to ensure, but that doesn't solve my problem - it just means that I have a requirement that the client won't budge on, that's not only ill-advised, but is also difficult to implement. And if I was in a position to say \"I'm not budging on salting and hashing\", then I'd be in a position to say \"I'm not implementing globally unique passwords\". Any pointers to independent and/or authoritative sources for why this is a bad idea still gratefully received... ",
        "Best answer": "Whenever a client tries to create a password that already exists, it receives feedback that some user already uses that password, usually a violation of the privacy agreement. Next to that, usernames are much easier to guess (and if there is a forum, you could just find alot of usernames there) and you're hinting the user ways to hack the website. There should be some page on the internet somewhere that describes the privacy agreement violation part, other than that it's just common sense: they'd basically be giving someone a key and a list of home addresses. EDIT: Not close to authorative, but perhaps helpful after you explain them what WTF means: http://thedailywtf.com/Articles/Really_Unique_Passwords.aspx "
    },
    {
        "ID": "23178",
        "Question": "Unit: A multithreaded unit reads data of a queue and processes it, sending it off to another queue. When the last item is processed (null value found) the process stops. Test: Place a set of known data in the queue and run. Check that data placed on the output queue is correct. If the unit is incorrectly programmed, it may continue trying to read and process nulls of the queue. This may or may not result in an exception (using Java as the example language). So this test could conceivably cause an infinite loop. Should the test keep track of the number of items read off the queue and throw an error if more than the specified number of elements is read? or should the test continue to run, knowing that the runner(human) will see that it is not ending? ",
        "Best answer": "Tests should always terminate (ideally quickly!). If you have a test that doesn't terminate, your continuous integration server will fail its test run, which is great, but the failure will likely be \"your test suite took too long so I killed it\", which isn't very helpful. In this case, if you have N items in the list and you've read N + 2 items already, then your test should fail, and explain why (say, a message like \"test failed because it was about to loop infinitely\"). "
    },
    {
        "ID": "23215",
        "Question": "So I went through an initial interview on school campus, and an on-site interview(one day, mostly technical interviews) with this company, and then a week later, I am told that there will be an additional phone interview with one of their hiring managers, called \"Professional Fit Interview\", which I have no idea what it is. Before I thought the on-site interview was gonna be the last round, at least that's the impression I got from them.  So what this \"professional fit interview\" might be? How should I prepare for it? Thanks a lot! A follow-up: Thank everyone for the replies. As everybody has mentioned, it's a HR type interview. The interviewer did some introduction of the team, then asked if I have some questions, and asked me a couple of questions like \"where do you see yourself in 2 years\". Anyway, a rejection email came about one week later. ",
        "Best answer": "Typically, a cultural fit assessment is something to gauge your personality (and how it'll mesh with the existing employees there). Given that your day of interviews was mostly technical, it's possible that this is what they were referring to. Just try not to be too much of a richard and you'll be fine. It's typically HR equine-feces and tea-leaves; you really can't study for it short of reading up on Myer-Briggs and playing the meta-answer game. "
    },
    {
        "ID": "23240",
        "Question": "I can't find this now but I've read it before on various blogs discussing the topic. I'll give an example and hopefully it's clear (albeit it may not clarify anything at all). Given this piece of markup (arbitrary on purpose): <div class=\"myWrapper\">   <a href=\"somepage.url\" class=\"img\">     <img url=\"someimage.url\">   </a> </div> <div class=\"yourWrapper\">   <a href=\"somepage.url\" class=\"img\">     <img url=\"someimage.url\">   </a> </div>  While the CSS could read like this: .myWrapper .img img {border: thin black solid;} .myWrapper .img {margin:10px;} .yourWrapper .img img {border: thick red dashed;} .yourWrapper .img {margin:20px;}  It could also be written like this: .myWrapper {   .img {      margin:10px;     img {         border: thin black solid;     }   } } .yourWrapper {   .img {      margin:10px;     img {         border: thick red dashed;     }   } }  But I can't remember seeing where that was discussed or if it's something in the works. Anybody know what the hell I'm talking about? And I don't think this is an SO question or I would've put it on SO. ",
        "Best answer": "LESS CSS  dynamic stylesheet language designed by Alexis Sellier. It is influenced by Sass and has influenced the newer \"SCSS\" syntax of Sass, which adapted its CSS-like block formatting syntax. LESS is open-source. Its first version was written in Ruby, however in the later versions, use of Ruby has been deprecated and replaced by JavaScript. The indented syntax of LESS is a nested metalanguage, as valid CSS is valid LESS code with the same semantics. LESS provides the following mechanisms: variables, nesting, mixins, operators and functions; the main difference between LESS and other CSS precompilers being that LESS allows real-time compilation via LESS.js by the browser.] LESS can run on the client-side and server-side, or can be compiled into plain CSS...  LESS uses those nested styles. #header {   color: red;   a {     font-weight: bold;     text-decoration: none;   } }  "
    },
    {
        "ID": "23364",
        "Question": "To me it seems these are two different platforms that address similar enterprise(but not only) problems. I'm interested however in problems that Java can solve better than C#/.NET or C#/.NET can solve better than Java. Leaving aside the 'cost' factor, since that is somehow not an issue for big enterprise companies (the developers probably cost more than the licenses from Microsoft - it's just a thought) and even for small projects there are free/open-source tools for .NET so the cost can be zero for the .NET world as well. With Mono, you can port .NET to Linux and MacOS as well so the benefit that Java had(cross platform) isn't THE decision factor to use Java nowadays.. What real world problems can you think of where .NET would be a much better solution than Java, and vice-versa? I'm only interested in solutions to problems. ",
        "Best answer": "There are some language features in C# (4/5) such as LINQ, using predicate and some functional aspects that many would argue place it ahead of Java 6 in terms of productivity/flexibility (YMMV on that).  Others would argue that C# is becoming a big cesspit of every brand new idea that happens to be popular and it's getting harder to wield effectively (YMMV on that). Some will also argue that the Java eco-system is far more flexible and that the JVM is probably the most capable and flexible runtime around (if you look at the various languages that run on the JVM adn their interoperability with Java).  Again YMMV. What it really boils down to is the quality of the developer(s) who are designing and writing the code.  .NET and the Java eco-system are so close in capabilities that its usually the developer that makes a difference. From a systems integration flexibility aspect, yes Java still has an edge there.  Cross platform is still an issue - many enterprises will see using Mono as risky (again YMMV here) and there are some systems that Mono won't run on that have JVMs (Some older big IBM machines for example).  Java has also been around a bit longer and has that established 'trust' in enterprises, it's not easy for .NET to gain that trust at say a mega-bank. Meh - I conclude with no real conclusion, which I think mirrors the fact that Java and .NET are neck and neck these days. "
    },
    {
        "ID": "23386",
        "Question": "The most common discussion I've seen regarding the pros and cons of REST tends to frame that discussion relative to SOAP. I have no experience in either. I am currently faced with a decision which my lack of experience is making hard for me to evaluate.  I am beginning to develop an application that has several components - primarily an administrative aspect that allows the owner to administer several sites - and a public facing user interface that allows the user to interact with data held on the host. I need to evaluate the implications of allowing the latter part to be hosted anywhere and communicate with the former via a RESTful architecture - or demanding that both components reside on the same host.  What are the key implications of developing RESTful architecture, particularly with regards to it's capacity in the following areas: 1: Security 2: Performance 3: Interface complexity EDIT: Looking at some of the answers to this question - I should clarify. I'm not looking for a comparison to SOAP - rather an overview of REST applications vs applications where all components reside on one host. (thanks for those answers though!) ",
        "Best answer": "Given those areas, I can give a rough overview, but I can't draw your conclusions for you.  There are two chief areas where the two protocols differ:  Message format Service discovery  Message format is easiest to understand.  The SOAP packaging for both requests and responses is fairly heavy weight.  There's the SOAP envelope that contains both a header and a body section.  The header can be used by several filters in the request chain to perform some sort of identification, authorization, etc.  However, XML is expensive to parse, which yields a certain penalty to the scalability of your system.  Just how much depends on the SOAP processing layer in your stack. Service discovery is where you probably will have the most contention.  REST by its very nature provides predictable end points, and the content of the request is a simple HTTP request.  The benefit is that there is no additional overhead, and end users can pretty much guess how to do what they need once they understand the URL structure of your site.  Of course, naive security conscious people will see that as a weakness.  Afterall, with SOAP, you have to consume a WSDL to know what the endpoints are.  Of course, with SOAP you were given the entire message format so you can make more targeted attacks. Broken down by the categories you gave: Security Neither is inherently more secure than the other.  Use good security principles:  Encrypt communications Make sure you authenticate and authorize users before processing Good coding habits to avoid direct attacks And that's just the short list.  Remember obscurity != security. Performance Both raw performance and scalability will go to REST due to the request following simple HTTP protocols.  Most SOAP stacks use SAX parsing (event based parsing) which greatly improves the scalability of SOAP stacks, but there is a measurable impact to the overhead.  SOAP has the normal HTTP processing overhead in addition to the XML parsing overhead.  REST just has the HTTP processing overhead. Complexity From the system's perspective, REST wins.  There's fewer moving parts, a leaner request chain, etc.  That means it's easier to make reliable. From the programmer's perspective, SOAP can win if the IDE or framework you are using provides good support for it.  Essentially, with REST the onus is on you to perform the preprocessing work (authentication/authorization/etc) while with SOAP much of that can be accomplished with a pluggable processing chain. My Preference I'm very comfortable with HTTP requests, and I know how the web works.  As a result, the REST approach is more preferable for me.  However, I do know that some of my clients are uncomfortable with that.  They've read some industry article denouncing the security of REST vs. SOAP, etc.  Bottom line is that neither approach guarantees security.  It's on you to make sure the application is as secure as it needs to be.  Obviously, a social web application doesn't demand (or desire) as much security as a bank or government system.  Many SOAP stacks include processors that you can plug in to provide some semblance of security, but it is still your responsibility to search them out and put them in place. "
    },
    {
        "ID": "23404",
        "Question": "I completed this website 4 months ago. (I would like to keep the website anonymous - if this post comes up on google search by customers, it will have a negative impact on their business). We did several iterations, the client was very demanding (and nosy), I complied with every single thing that was wanted, including the font-size of the footer which destroyed the aesthetic looks by quite a bit; (and they messed with the colours too, making it look dangerously childish and unprofessional).  Anyway, the client was very friendly during the whole process. After getting all the requirements I accepted to build the website for £950 and I charged him £150 in advance so he wouldn't want to bail out after I started the work. The requirements kept changing quite a lot, I made adjustments as and completed the website just 5 days late, w.r.t. the agreed date, in-spite all the changes in requirements. That was four months ago. They have never returned my calls nor replied to my emails since then. How do I get the money back from them? I really need some advice on this, this is the second time someone has not paid me. Points to be noted:  We did not sign any contract - the client was a friend of a friend.   I still hold the passwords for ftp, cpanel and everything. I don't want to bring the site down; somehow it doesn't seem ethical to me.    I posted this on Stack Overflow and it was closed, I was redirected here.  EDIT Thank you everyone, for your enormous support. I thought I should include some more details: The client is hosting the site; as in, I made them register and use their own debit card to buy the domain name, webspace and everything, I just have the passwords for his cpanel, and ftp (well, I have access to cpanel that pretty much gives me access to many things).  Now, if I give them a warning, he might change the password to cpanel, but I am fairly sure they doesn't know about ftp's. UPDATE I followed Anna Lear's advice and sent the final invoice, on Saturday, with a warning that I will take the site down if he doesn't respond to my email before this Tuesday. They havn't replied to my emails yet, but I will let you know how this goes. Thanks much for your support. I plan to put up a holding page as Darknight suggested after Tuesday. Points I don't have a server space of my own, however I have few web spaces of my clients but I don't want to use them for this kind of purpose. RESULTS After my follow up of final invoice and warning, the client refused to anything. workman's advice seemed to be the most sensible to me. I mirrored the site to my own webspace, logged into his CPanel and forwarded the website to my site. Once that was done, I put up a holding page on my site so anyone going to the website would see a holdup page (nothing fishy, just an Under Construction page). Of course, this was all very weak in the sense that if my client knew about CPanel he could just remove the redirection. But, I had this second chance to get back to him and I was relying on him not knowing anything.   Two hours later, I got a call from the said client, (and  acting as if they did not get any of my voice mails or emails) and asked me to get the site back up. I, of-course, said I had to be paid and two days after that I received a bank transfer. The site's back up and running now.  Just wanted to say thank you all very much, I was very desperate when I had started this question. You are the best, not only you told me to learn the lesson (which I did) you also gave me a very very good solution that seemed to be very ethical (almost) to me.    A BIG thank you to workman!    If I could choose more than once answer, I would have chosen Anna Lear  too, but since I followed workman's advice I had to pick that up. People who made a point about not taking the site down without consulting a lawyer, thanks a lot.  ",
        "Best answer": "Take it down 1 It's your work and it's unpaid. What happens if you take a car off the lot and fail to pay? They take it back. Better do it now while you can then regret it later. Why would it not be ethical to take back something that is yours?  Hopefully a down site will motivate them to pay you. Otherwise you are in a bit of a bind. No matter who you work for get a contract or something in writing. Friends, family, the only exclusion would be your mother because honestly you probably won't be charging her anyways.  I'm opposed to warning him only because that will prompt him to remove your access, and prevent you from taking any action. What good is the warning if you can't follow through?   1: Please be aware of the possible legal consequences that may present themselves in following this course of action. This information is provided \"as-is\". The author cannot not be held liable for the results of following any part of the above stated recommendation. By reading this you are affixing an assumed electronic signature that you agree to all terms, as stated, without reservation. If any part of this agreement is found unlawful, I don't really care. Just adding fine print for fine prints sake.  "
    },
    {
        "ID": "23453",
        "Question": "In pricing a service or product it is general practice to not merely charge for the effort spent and the costs involved + margin but from the value delivered down. As an independent consultant, how do you set the price of your work?  What is the process of determining your hourly rate as an independent software developer? If you have one, what is your hourly rate and how did you arrive at that figure?  What factors would you take into account.  Is the process of setting a hourly rate only based on balancing demand with supply so that supply (you and your time) doesn't get overwhelmed?  When are the good times to raise your rates?  Are there projects in which you have had to charge higher than other projects? If so, please cite examples. Do you change your hourly rate based on what kind of development you are doing? (for example, .net programming more, and lesser for php programming) Do you set hourly rates based on what type of business the client is? If so how? If you know of any relevant articles or material on the topic of charging for programming services, please post the same.  ",
        "Best answer": " In pricing a service or product it is   general practice to not merely charge   for the effort spent and the costs   involved + margin but from the value   delivered down. As an independent   consultant, how do you set the price   of your work?  A combination of the above + what the market will take.  Some further answers below.  What is the process of determining   your hourly rate as an independent   software developer? If you have one,   what is your hourly rate and how did   you arrive at that figure?  I actually have a daily rate.  I start at the market rate for a senior Java developer and then added 'chunks' to that rate based on my unique selling points for a particular client if they are relevant.  So for one client my experience in open source community management is a bonus, another client values my contacts in the Java community as a whole (I co-lead the JUG in London) etc.  I also factor who the client is and what sector they are in (financials will pay more if you have relevant business experience for example). Supply and demand also come into it, as does the length of contract.  What factors would you take into   account. Is the process of setting a   hourly rate only based on balancing   demand with supply so that supply (you   and your time) doesn't get   overwhelmed?  No it's not just based on that one factor, it's one of many factors.  When is it a good time to raise your   rates?  When you have demonstrated over a length of time that you are worth more than your original rate and the market/client can afford it.  Remember as a contractor/consultant they expect to see real value for money, it helps to be able to track your work to a tangible saving/revenue/profit.  Do you change your hourly rate based   on what kind of development you are   doing? (for example, .net programming   more, and lesser for php programming)  Yes, the market will pay differently for different skill sets.  Do you set hourly rates based on what   type of business the client is? If so   how?  Yes, as above, investment banks and the like do pay more - but they expect more as well, if you don't deliver you're soon out the door :) HTH! "
    },
    {
        "ID": "23472",
        "Question": "What do you when you're working with someone who tends to write stylistically bad code? The code I'm talking about is usually technically correct, reasonably structured, and may even be algorithmically elegant, but it just looks ugly. We've got:  Mixture of different naming conventions and titles (underscore_style and camelCase and UpperCamel and CAPS all applied more or less at random to different variables in the same function) Bizarre and inconsistent spacing, e.g. Functioncall  (arg1 ,arg2,arg3 ); Lots of misspelled words in comments and variable names  We have a good code review system where I work, so we do get to look over and fix the worst stuff. However, it feels really petty to send a code review that consists of 50 lines of \"Add a space here. Spell 'itarator' correctly. Change this capitalization. etc.\" How would you encourage this person to be more careful and consistent with these kinds of details? ",
        "Best answer": "I think you just have to keep doing what you are doing.  Have a clear set of coding guidelines, and enforce them during code reviews.  If a developer gets 50 or 100 lines of \"Add a space here\" and \"Spell 'iterator' correctly\" every time he tries to check something in, and he is actually not allowed to check in before all of those get fixed, eventually he'll have to start writing cleaner code just to avoid the hassle. I think if you fix these things yourself, like NimChimpsky suggested, you will be cleaning up after this person forever. "
    },
    {
        "ID": "23535",
        "Question": "Should web developers continue to spend effort progressively enhancing our web applications with JavaScript, ensuring that features gracefully degrade, thereby ensuring accessibility?  Or should we spend that time focused on new features or other areas of development? The subtext of that question would be:  How many of our customers/clients/users utilize our websites or applications with JavaScript disabled?  Do you have any projects with requirements that specifically demand JavaScript functionality (almost all of mine do), and do those requirements also demand graceful degradation? For the sake of asking this question, I pulled up programmers.stackexchange.com without JavaScript enabled, and I was greeted with this message: \"Programmers - Stack Exchange works best with JavaScript enabled\".  It was difficult to log in, albeit the site seemed to generally work okay.  (I wasn't able to vote up any questions.) I think this is a satisfactory approach to development.  Imagine the effort involved in making all of the site's features work with plain old HTML and server-side logic. On the other hand, I wonder how many users have been alienated by this approach. We've all been trained (at least the good developers among us) to use progressive enhancement and to ensure our web applications' dynamic features degrade gracefully.  Is this progressive enhancement just pissing into the wind, or do some of our customers actually utilize certain web services without JavaScript enabled? ",
        "Best answer": "I guess the percentage of people without JavaScript turned on is negligible. But be careful with search engine bots! They usually don't use JavaScript, but you wouldn't want them to skip some important content of yours because of that, right? "
    },
    {
        "ID": "23542",
        "Question": "I really enjoy programming language design. Sometimes I think my language projects and their potential users would benefit from a comprehensive standards document. I've looked at many language standards, ranging from the very formal (C++) to the rather informal (ECMAScript), but I can't really get a handle on how I should break things down and organise such a document, even though I think I'm pretty good at technical writing in general. Should I write it like a long tutorial, or more like a formal math paper? How do I keep it up to date if I'm developing it alongside a reference implementation? Should I just give up and treat the implementation and documentation as the de facto standard? Further, is there really any significant benefit to having a standard? Does requiring a standard mean that the language is needlessly complex? ",
        "Best answer": "Read lots and keep it simple Designing a new language is hard. Really hard. But ultimately very satisfying if it becomes popular and really solves a problem that people are experiencing in an elegant manner. As I mentioned in the comments, I'd advise you to read Domain Specific Languages by Martin Fowler for the following reasons:  He goes into a lot of practical depth about why you should design a language There are details on how to do it (parsers, lexical analysers, language workbenches etc) There are detailed implementation instructions about how your chosen syntax can be made to handle concepts like closures, annotations, literal lists, dynamic reception etc  As for how to go about writing your specification, think about your audience. Obviously, before putting finger to keyboard to design your language you will have thought carefully about what it is intended to do.  If it's a new, interpreted language to replace JavaScript then you'll be wanting a very laissez faire approach in order to reach web developers with a limited attention span and a desire for immediate results - or quicker if possible. If it's going to be used on the next mission to Titan, then extremely detailed specifications showing exact formal proofs of the behaviour of each component will be the minimal entry level. So, it's not a straightforward thing. To approach the specification, you would probably be better off gaining a lot of experience in creating your languages and also working with those who actually use them on a day to day basis. If you have willing victims... er... developers, at work who can take some time to learn your language then they can give you feedback on what is needed to get them to use it. In short, keep it simple and more people will use it.  "
    },
    {
        "ID": "23548",
        "Question": "I don't work at a software company, and I'm one of a small handful of people in the company that know anything about programming. I spend a lot of time automating other programs that are used in the office through public APIs, and I've also created a few stand alone applications. I work almost entirely in C#.NET as every application we seem to use in the office seems to have some form of .NET API. I've had a few people here ask me about learning \"how to program\", and where they should start. I think it makes a lot more sense to learn a .NET language as nearly all the programs they would want to automate have a .NET API, and it sounds like VBA is on it's way out and being replaced by VSTA. However, I'm trying to figure out how to explain what .NET is and why they should learn it to a someone that doesn't know anything about programming. It's not really a language, as there are a number of languages that are considered .NET languages. Plus I think there is a distinction between \".NET\" and \"The .NET framework\" as the latter is more about the libraries provided by Microsoft.  ",
        "Best answer": ".NET for the Non-programmer Programming - Basically telling a computer what to do and how to do it. Source File - This is a document written in a programming language that tells the computer what you want it to do. Programming Language - This is a language that (usually) resembles a mixture of English and math. It is both simple and strict enough for a compiler to understand. Compiler - This translates a programming language that you can understand into a language the computer can understand, you can call it Computerese. Library - A collection of useful code that has already been translated into Computerese that you can use in the programs you write. .NET Platform - A large collection of tools, languages and libraries for writing programs with a heavy emphasis on productivity. Sure, there's a lot more to it than that. You could tell them about IL and JIT compiling or garbage collection but these details aren't very relevant to a non-programmer. "
    },
    {
        "ID": "23683",
        "Question": "I would like to ask you some questions about dirty code. There are some beginners who coded on a medium project. The code is a very huge ball of mud. They are not advanced programmers. They just know how to use keyboard an a little about java. They just wrote code with 12 000 lines in their main class, though, 6 000 lines belongs to NetBeans itself. My job is to analyze the code and suggest a good way to maintain the code. My idea is to scrap the project and start a new one with OOP methodology. Recently I collected some notes and ideas about the problem, from this site and some others. Now, I have the followings questions:  Should we repair the code, and change it to a OOP? We are now debugging it. The code has no comments, no documentation, no particular style of programming, and so forth. Changing it is really expensive and time consuming. What do we can do about this? How can I teach them to follow all the rules (commenting, OOP, good code quality, etc.)? The code is erroneous and error prone. What can we do? Testing? We almost write two or three A4 papers for correction, but it seems endless.  I should have to say that I am new with them. I think I have broken the rules about adding people too late to the project, as well. Do you think I have to leave them? ",
        "Best answer": "Step 0: Backup to SCM Because, as hinted to by JBRWilkinson in the comments, version control is your first line of defense against (irreversible) disaster. Do also backup software configuration details, procedures to create deliverables, etc... Step 1: Test First Then start by writing tests:  for what works, and for what fails.  No matter what you decide to do, you're covered. You can now either:  start from scratch and re-write, or fix it.  My advice would be to start the general architecture from scratch, but extract from the mess the parts that validate checkpoints and to refactor these as you see fit. Step 2: Verify and Monitor Set up a Continuous Integration system (to complement step 0 and step 1) AND a Continuous Inspection system (to prepare for step 4). Step 3: Stand on the Shoulders of Giants (as you always should...)  Refactoring to Patterns Refactoring: Improve the Design of Existing Code. Working Effectively with Legacy Code  (as recommended by Jason Baker) Refactoring  Step 4: Clean That sort of goes without saying, but instead of skimming though the code yourself, you may want to simply run linters / static analyzers and other tools on the broken codebase to find errors in the design and in the implementation. Then you might also want to run a code formatter, that will already help a bit with the housekeeping. Step 5: Review It's easy to introduce tiny bugs by refactoring or cleaning things up. It only takes a wrong selection and quick hit on a key, and you might delete something fairly important without realizing at first. And sometimes the effect will appear only months later. Of course, the above steps help you to avoid this (especially by implementing a strong test harness), but you never know what can and will slip through. So make sure to have your refactorings reviewed by at least one other dedicated pair of eye-balls (and preferably more than that). Step 6: Future-Proof your Development Process Take all of the above, and make it an inherent part of your usual development process, if it already isn't. Don't let this happen again on your watch, and work together with your team to implement safeguards in your process and enforce this (if that's even possible) in your policies. Make producing Clean Code a priority.  But really, test. A lot. "
    },
    {
        "ID": "23760",
        "Question": "I'm graduating in a couple of weeks, and my resume (as expected) lists the languages that I've had experience with. Previously I've put \"C/C++\", however back then I didn't have that much experience with these two languages as I do now. Now that I've formally learned these two languages, it has become evident to me (and anyone who really knows these languages) that they are similar, and completely disimilar at the same time. Sure, most C code is compilable C++ code, but syntax and incorporation of library functions is pretty much where these similarities end. In most non-trivial problems, chances are that the desirable C++ solution will be different from the desirable C solution.  My question:  Will recruiters take note or care about whether you put \"C/C++\" as opposed to \"C, C++\"? Will they assume a lack of knowledge of the workings of either because of the inclusion of the first form, or perhaps see the inclusion of the second form as a potential \"resume beefer\" (listing them as 2 languages, instead of \"one\")? Furthermore, for jobs that you've applied to that were particularly interested in these two langauges, did the interview process include questions about the differences between C programming and C++ programming (so, about actual programming techniques, not only the extra paradigms in the latter)? ",
        "Best answer": "C, C++  I don't like C/C++, because though C++ is technically a superset of C, to do it right, you have to do things differently. C/C++ makes you look like someone who knows C and knows that a C++-compiler accepts C, too. "
    },
    {
        "ID": "23792",
        "Question": "I work with a lot of legacy Java and RPG code on an internal company application. As you might expect, a lot of the code is written in many different styles, and often is difficult to read because of poorly named variables, inconsistent formatting, and contradictory comments (if they're there at all). Also, a good amount of code is not robust. Many times code is pushed to production quickly by the more experienced programmers, while code by newer programmers is held back by \"code reviews\" that IMO are unsatisfactory. (They usually take the form of, \"It works, must be ok,\" than a serious critique of the code.) We have a fair number of production issues, which I feel could be lessened by giving more thought to the original design and testing. I have been working for this company for about 4 months, and have been complimented on my coding style a couple of times. My manager is also a fan of cleaner coding than is the norm. Is it my place to try to push for better style and better defensive coding, or should I simply code in the best way I can, and hope that my example will help others see how cleaner, more robust code (as well as aggressive refactoring) will result in less debugging and change time? ",
        "Best answer": "You didn't mention your level of expertise on the subject. If you are not a senior programmer however, I think the best thing you can do is to do your own work well. You also mentioned that your manager likes clean work - that's great. If you can talk to him about it (maybe in a semi-professional environment) you should share your concerns about the issue with him. HE is in the position to change workflow. "
    },
    {
        "ID": "23810",
        "Question": "Well I've been hitting the books wherever I can. I have an interview coming up, first one via phone, for a software engineer position. I've read all the blog posts, I've read all the accounts of interviews (some pretty old), and Google itself even suggested a reading list of books, none of which would surprise anyone here. Still, after some time preparing, I can't shake that feeling that there is such a large ground to cover, and I'm never sure whether to go with depth or breadth. I've found myself re-learning a whole area of compsci, only to forget most of the nitty details as I move on to another. So, I don't know that there's a good answer to this question, but I'm looking for any practical advice on how to tackle the remaining weeks in advance of the interview. Part of my brain is tired from cramming, and of course the rest of it has to be utilized for some tough problems at my current place of employment. ",
        "Best answer": "Things you should know  Google wants to hire you! The life-blood of any software company is its employees and Google is no different. It's looking to hire the best and the brightest and the people conducting the interview(s) want you to succede just as much as you do. Google will do it's best to evaluate you as accurately as possible. It's their job. Google is a data-driven company. Hiring decisions are not decided by manager fiat. Instead, each interviewer takes extensive notes during the interview which gets combined into a packet. That packet will then get reviewed by a separate committee, which will ultimately make the decision. So if you just weren't 'gelling' with one of your interviewers don't worry! What matters is how well you perform on the interview.  Skills you should have Be sure to brush up on the following skills/techniques before your interview. Even if you don't get asked a question on these directly, reviewing them can certainly get your head into the right mindset.  Data structures What is the difference between an Array and a Linked List? A Tree and a Graph? When would you use one over the other? How would that impact speed/memory tradeoffs?  An interview question doesn't end at a working solution. Be able to explain the runtime of your approach and what sorts of trade offs you could make. For example, \"if I cached everything it would take X gigs of RAM but would perform faster because...\". Or, \"if I kept the binary tree sorted while I performed the operations X would be slower, Y would be faster, etc.\" Algorithms Basic graph traversal algorithms, tree traversal algorithms, and two good approaches for sorting numbers. Make sure to practice solving a non-trivial problem using Dynamic Programming. That is your ace in the hole when it comes to tough interview questions! Hash tables This is huge. Know everything there is to know about hash tables, from being able to implement one yourself, to knowing about hashing functions, to why the number of buckets should be a prime number. The concepts involved with hash tables are relevant to just about every area of Computer Science. Talking points about yourself That first few minutes of chit-chat with the interviewer is an important time to explain any sort of experience which sets you apart. Relevant projects, significant technical accomplishments, and the like. Remember, the person conducting the interview has interviewed dozens if not hundreds of smart people just like you.  So what can you say that would surprise them? For example, in an interview I spoke to the interviewer about a program I wrote to play the game of Go in college. It is very difficult to write an AI for the game of Go, and I have a horrible Go-bot to prove it! The bottom line is be yourself, and not just some smart person who knows how to program.  Don't stress out too much, it's just an interview like any other. Rest assured that nobody will ask you stupid questions about manhole covers or Mt. Fuji.  "
    },
    {
        "ID": "23816",
        "Question": "Back in what must've been the mid-80s, when every microcomputer included BASIC and Choose Your Own Adventure Books were really popular, there were some novels that had BASIC programs listed in the text, for you to type in and further enjoy.   For example, as I recall, in one of these books, the adventurers were infiltrating an undersea base (and eventually wiped the bad guy's data storage [I do believe it was a hard drive, even though they were uncommon then]).  One of the programs involved a game were you piloted a submarine and had to avoid or shoot sharks.  The code was in BASIC; minor changes (such as 'CLS' -> 'HOME') had to be made for your specific computer, and it used good old 40-column text-mode to display the action.  IIRC, the plot never depended upon the programs. My question is, does anyone else recall these sorts of books?  Do you know any titles that I could look for, or of any online?  I am toying with the idea of writing a story like this (no, not in BASIC!), and would really like to see how it was done, back in the day. ",
        "Best answer": "Read Gateway by Fred Pohl.  A good part of the novel is conversations between the protagonist and his therapist, the therapist being a very sophisticated program.  Every so often, there will be a page in the book that is some sort of BASIC-ish computer code that, if you're a techie and read it, it gives a bit of insight on how the AI therapist is thinking. "
    },
    {
        "ID": "23845",
        "Question": "I was watching Bob Ross paint some \"happy trees\" tonight, and I've figured out what's been stressing me out about my code lately. The community of folks here and on Stack Overflow seem to reject any whiff of imperfection.  My goal is to write respectable (and therefore maintainable and functioning) code, by improving my skills.  Yet, I code creatively. Let me explain what I mean by \"coding creatively\":  My first steps in a project are often to sit down and bash out some code.  For bigger things, I plan a bit out here and there, but mostly I just dive in. I don't diagram any of my classes, unless I'm working with others who are creating other pieces in the project.  Even then, it certainly isn't the first thing I do.  I don't typically work on huge projects, and I don't find the visual very useful. The first round of code I write will get rewritten many, many times as I test, simplify, redo, and transform the original hack into something reusable, logical, and efficient.  During this process, I am always cleaning.  I remove unused code, and comment anything that isn't obvious.  I test constantly. My process seems to go against the grain of what is acceptable in the professional developer community, and I would like to understand why.  I know that most of the griping about bad code is that someone got stuck with a former employee's mess, and it cost a lot of time and money to fix.  That I understand.  What I don't understand is how my process is wrong, given that the end result is similar to what you would get with planning everything from the start.  (Or at least, that's what I have found.) My anxiety over the issue has been so bad lately that I have stopped coding until I know everything there is about every method for solving the particular problem I am working on.  In other words, I have mostly stopped coding altogether. I sincerely appreciate your input, no matter what your opinions are on the issue. Edit:  Thank you all for your answers.  I have learned something from each of them.  You have all been most helpful. ",
        "Best answer": "There's nothing wrong with code-test-refactor-repeat, just tell people you're prototyping. On the other hand, for larger projects you will find that some thought given to the design up-front will save you a lot of time in the oh-crap-now-what loop! P.S.: Diagramming techniques help you to learn visual thinking skills, which are valuable even if no one but you ever sees your diagrams. "
    },
    {
        "ID": "23851",
        "Question": "As a college student do you write unit-test for the project code you write?  Do you know how to properly use TDD?  Have you been taught to use testing frameworks (jUnit for example)? I've just recently heard that a major company's project team was delivering software to User Acceptance Testing group that had only 7% line code coverage for unit testing.  While that shocks me, it just tells me that this company's software engineering practice are immature.  However I wonder if they could just hire a few recent graduates and raise the bar on the teams practices - assuming that these newly minted programmers were trained in the best practices. Are you 2008 - 2010 CS graduates using best practices of unit testing your code? ",
        "Best answer": "I don't write unit-tests for the software I write for assignments, I usually just do some of my own testing and debugging.  Honestly, CS at my university is fairly broken, half the students are diehard passionate programmers who have learned nearly everything on their own. The other half can barely write a hello world in Java, and know no other languages and have extreme problems with even trivial programs. Students barely get much programming experience, most manage to squeak by copy & pasting code form books and the internet without really understanding it. As part of the degree plan, you have to take courses on architecture and management that involve agile and the concepts of software testing and test driven development. However, these courses are taken pretty late (senior year) and are certainly not utilized in other courses. If I had more time, I'd probably unit-test my code properly; unfortunately I'm working and taking 19 hours, so I've got alot of assignments and I can usually bang them out pretty quick and test them enough. I have gotten dinged a couple points for a bug here or there though, and unit-testing my code would certainly help get better grades on the assignments. Sadly, my experience is that any REAL software development, any type of real-world systems programming or coding standards or best practice is done by those who write code on the weekends, and read about it on their own, and look for it on their own. The students who just go to class, I can't imagine how they're going to get hired. You get shown what all exists and that you need to know it in class, but not enough implementation.  Professors can't grade 50 students' programming assignments every week, so if they want to have lots of actual practice, they have us do group work. This results in the one or two good programmers doing almost all the work, and the rest follow along and try to read and modify their code. Maybe other Universities are better, and I think they claim to be; but I think in practice they aren't really any different. "
    },
    {
        "ID": "24047",
        "Question": "I am about to start my first professional career position as a software developer, and I'm likely going to be writing a lot of Java code.  I don't know that much about what specifically I'll be writing, and besides, I have fairly broad interests when it comes to code anyway. For reasons not worth discussing, my last internship had me coding only for the Java 1.3 and 1.4 SDK's.  Also, my university education has focused mainly on algorithms, data structures, and other theory, not language features or libraries. So, with the above in mind, what features, classes, etc. in the current version of Java (what number are we on now, anyway?) would be worth my time to read up on and why?  What have I missed since 1.4.2 that makes your life as a programmer easier? Examples and stories of how you discovered a particular package or some such and put it to use would also be great. ",
        "Best answer": "The changes that I consider most important are:   Generics (e.g. typed collections, like Set)  Enhanced for loop (for (String s : set) {...})  Autoboxing/unboxing (automatically convert between types like Integer to int and vice versa)  Typesafe enums (enum is now a keyword, types can be created out of enums)  Varargs (for printf() function, allows variable number of arguments)  Static import (can now import static methods of a class, such as java.lang.Math)  Annotations  java.util.concurrent (Demonstrates java's concurrency)    Also read What Java are you aiming for?, to get a better understanding of each of the three versions. "
    },
    {
        "ID": "24077",
        "Question": "Are short identifiers bad? How does identifier length correlate with code comprehension? What other factors (besides code comprehension) might be of consideration when it comes to naming identifiers? Just to try to keep the quality of the answers up, please note that there is some research on the subject already! Edit Curious that everyone either doesn't think length is relevant or tend to prefer larger identifiers, when both links I provided indicate large identifiers are harmful! Broken Link The link below pointed to a research on the subject, but it's now broken, I don't seem to have a copy of the paper with me, and I don't recall what it was. I'm leaving it here in case someone else figure it out.  http://evergreen.loyola.edu/chm/www/Papers/SCP2009.pdf  ",
        "Best answer": "The best \"rule\" I've heard is that name lengths should be proportional to the length of the scope of the variable. So an index i is fine if the body of the loop is a few lines long, but I like to use something a little more descriptive if it gets to be longer than 15ish lines. "
    },
    {
        "ID": "24079",
        "Question": "What apsects of Java are the most diificult to learn when coming from such a background?  What common mistakes do people make ? What are the top timesaving and produtivtity increasing tricks ? If you had a room of C/PHP coders who were about to start development using Java what advise would you give ? This is my list of topics so far (in no particular order):  Use jodatime instead of the standard library , and also less importantly the guava library. Arrays are zero indexed I'd also highlight the pass-by-value/reference aspects of Java, and the fact that  String s1 = new String(\"test\");  String s2 = new String(\"test\");  if(s1 == s2) // will be false  if(s1.equals(s2)) // will be true   Introduce the concept of design patterns and give a quick overview. Introduce Spring (it will be used) and the concept of dependency injection  Is there anything obvious I am missing. ",
        "Best answer": "The automatic garbage collecting doesn't mean that you should give up memory usage optimization. "
    },
    {
        "ID": "24107",
        "Question": "Recently and quite often, people with no programming background come and say they have this great idea that can be the next big thing and that the idea(s) is worth a fortune by itself. Then as they know I'm a programmer, they ask me if I'm willing to \"code it up\" for them or find someone willing to do it for next to nothing.  Judging from the enthusiasm, it's like they're drunk on their idea and that that by itself is the most important thing, but they just need a programmer. My response to them, depending on my mood and their general attitude towards what we do, is something along the lines of: \"Having the core of an idea is one thing. Developing it to the point that it becomes a platform that changes the world in which it lives is another, and you're going to be willing to pay proportionately to how big you think your idea is worth.\" Have you been approached by these business type entrepreneurs (with no technical/developer's knowledge) with such a proposal and how do you react to them? ",
        "Best answer": "If they are only willing to pay next to nothing, that means they really don't believe in the idea.  They just want to try a long shot and see if they get lucky. I usually respond with something along the lines of \"you get what you pay for\" for development services, and if they don't want to pay anything, they aren't likely to get much. If they say they'll give you stake in the company, that's a bit different, then you have to evaluate how you feel it will do as the next big thing. "
    },
    {
        "ID": "24157",
        "Question": "My younger brother is looking to start programming. He's 14, and technically-inclined, but no real experience programming. He's looking to me for guidance, and I don't feel as if my experience is enough, so I figured I'd ask here. He's more interested in web programming, but also has an interest in desktop/mobile/server applications. What would be a good learning path for him to take? I'm going to buy him a bunch of books for Christmas to get him started; the question is, what should he learn, and in which order? The way I see it, he needs to learn theory and code. I'd like to start him off with Python or Ruby or PHP. If he wants to get in to web, he's also going to need to learn HTML, CSS, Javascript, etc. Out of those three domains (Languages, Theory, Markup/Etc.), what is the best order do you think to learn in? Also, am I missing anything? Thanks! ",
        "Best answer": "Register him an account for StackOverflow.com and Programmers.StackExchange.com and get him into the habit of browsing different questions when he is bored. Start with the hot/most popular questions.  Also Help him come up with a goal of something he wants to create that is slightly beyond his reach, a simple game, an app that can send a tweet? It has to be something that is exciting. This will help guide the topics he exposes himself to and provide him motivation through the tangible output he creates. And Where possible don't buy books in physical form if he works well with digital print, I am sure one of you has an android or iphone or blackberry or ipod touch? Get the ibooks or kindle app and buy digital versions. Having to deal with a physical book slows the process of knowledge acquisition; the tools built into digital readers provide many benefits to technical reading. Note: as mentioned in the comments, there are drawbacks to digital vs printed books, so take this point with a grain of salt "
    },
    {
        "ID": "24170",
        "Question": "Long time back, when I was reading my introductory programming books on (Basic, Pascal, C/C++), one thing that was emphasized is you can't become a professional programmer over night or over few weeks. It takes time to grasp programming and to apply it for solving problems and developing applications. Now, once someone has the grasp of basic programming, the person can quickly adapt to new technologies. In recent times, the use of frameworks in application development is prevalent. Though the learning curve for framework is way smaller than that of programming, even so they too would require some time to learn. Of course, different frameworks have different complexity, hence the learning curve would vary greatly. My question is, should one start to do commercial projects while they are learning a particular framework, or should a demo/learning project be done first to get the hang the framework and then proceed onto real projects? ",
        "Best answer": "When one understands the need of a framework. "
    },
    {
        "ID": "24378",
        "Question": "PHP, as most of us know, has weak typing.  For those who don't, PHP.net says:   PHP does not require (or support) explicit type definition in variable declaration; a variable's type is determined by the context in which the variable is used.    Love it or hate it, PHP re-casts variables on-the-fly.  So, the following code is valid: $var = \"10\"; $value = 10 + $var; var_dump($value); // int(20)  PHP also allows you to explicitly cast a variable, like so: $var = \"10\"; $value = 10 + $var; $value = (string)$value; var_dump($value); // string(2) \"20\"  That's all cool...  but, for the life of me, I cannot conceive of a practical reason for doing this. I don't have a problem with strong typing in languages that support it, like Java.  That's fine, and I completely understand it.  Also, I'm aware of - and fully understand the usefulness of - type hinting in function parameters. The problem I have with type casting is explained by the above quote.  If PHP can swap types at-will, it can do so even after you force cast a type; and it can do so on-the-fly when you need a certain type in an operation.  That makes the following valid: $var = \"10\"; $value = (int)$var; $value = $value . ' TaDa!'; var_dump($value); // string(8) \"10 TaDa!\"  So what's the point?  Take this theoretical example of a world where user-defined type casting makes sense in PHP:  You force cast variable $foo as int → (int)$foo. You attempt to store a string value in the variable $foo. PHP throws an exception!! ← That would make sense.  Suddenly the reason for user defined type casting exists!  The fact that PHP will switch things around as needed makes the point of user defined type casting vague.  For example, the following two code samples are equivalent: // example 1 $foo = 0; $foo = (string)$foo; $foo = '# of Reasons for the programmer to type cast $foo as a string: ' . $foo;  // example 2 $foo = 0; $foo = (int)$foo; $foo = '# of Reasons for the programmer to type cast $foo as a string: ' . $foo;   A year after originally asking this question, guess who found himself using typecasting in a practical environment?  Yours Truly. The requirement was to display money values on a website for a restaurant menu.  The design of the site required that trailing zeros be trimmed, so that the display looked something like the following: Menu Item 1 .............. $ 4 Menu Item 2 .............. $ 7.5 Menu Item 3 .............. $ 3  The best way I found to do that wast to cast the variable as a float: $price = '7.50'; // a string from the database layer. echo 'Menu Item 2 .............. $ ' . (float)$price;  PHP trims the float's trailing zeros, and then recasts the float as a string for concatenation. ",
        "Best answer": "In a weakly-typed language, type-casting exists to remove ambiguity in typed operations, when otherwise the compiler/interpreter would use order or other rules to make an assumption of which operation to use. Normally I would say PHP follows this pattern, but of the cases I've checked, PHP has behaved counter-intuitively in each. Here are those cases, using JavaScript as a comparison language. String Concatentation Obviously this is not a problem in PHP because there are separate string concatenation (.) and addition (+) operators. JavaScript var a = 5; var b = \"10\" var incorrect = a + b; // \"510\" var correct = a + Number(b); // 15  String Comparison Often in computer systems \"5\" is greater than \"10\" because it doesn't interpret it as a number.  Not so in PHP, which, even if both are strings, realizes they are numbers and removes the need for a cast): JavaScript console.log(\"5\" > \"10\" ? \"true\" : \"false\"); // true  PHP echo \"5\" > \"10\" ? \"true\" : \"false\";  // false!  Function signature typing PHP implements a bare-bones type-checking on function signatures, but unfortunately it's so flawed it's probably rarely usable. I thought I might be doing something wrong, but a comment on the docs confirms that built-in types other than array cannot be used in PHP function signatures - though the error message is misleading. PHP function testprint(string $a) {     echo $a; }  $test = 5; testprint((string)5); // \"Catchable fatal error: Argument 1 passed to testprint()                       //  must be an instance of string, string given\" WTF?  And unlike any other language I know, even if you use a type it understands, null can no longer be passed to that argument (must be an instance of array, null given).  How stupid. Boolean interpretation [Edit]: This one is new. I thought of another case, and again the logic is reversed from JavaScript. JavaScript console.log(\"0\" ? \"true\" : \"false\"); // True, as expected. Non-empty string.  PHP echo \"0\" ? \"true\" : \"false\"; // False! This one probably causes a lot of bugs.   So in conclusion, the only useful case I can think of is... (drumroll) Type truncation In other words, when you have a value of one type (say string) and you want to interpret it as another type (int) and you want to force it to become one of the valid set of values in that type: $val = \"test\"; $val2 = \"10\"; $intval = (int)$val; // 0 $intval2 = (int)$val2; // 10 $boolval = (bool)$intval // false $boolval2 = (bool)$intval2 // true $props = (array)$myobject // associative array of $myobject's properties  I can't see what upcasting (to a type that encompasses more values) would really ever gain you. So while I disagree with your proposed use of typing (you essentially are proposing static typing, but with the ambiguity that only if it was force-cast into a type would it throw an error — which would cause confusion), I think it's a good question, because apparently casting has very little purpose in PHP. "
    },
    {
        "ID": "24398",
        "Question": "I'm a year away from graduating from university, and I'm really looking forward to solving practical problems. Especially non-trivial ones which require a bit of research and a lot of thinking. But at the same time, that is also my greatest fear - being faced with a problem that I'm unable to solve, no matter how hard I try. And with pressure to deliver code on impending deadlines just around the corner, it does look a bit scary when viewing it from the safe playgrounds on uni (where the worst thing that can happen is that you have to redo a course or exam). So for those who have been in industry for any longer length of time, what would happen if you were told to solve a problem that you couldn't? Has it happened, and if so, what did happen? Did they just drop it and said \"Oh well, guess we can make do with something else\"? Were there consequences? Were you reprimanded, or even fired? ",
        "Best answer": "Two things to remember if you're stuck with a seemingly unsolvable problem:  Let other folks know you're stuck as soon as possible. It will help them to adjust the estimate in time before it's too late. If you see one way of solving a problem doesn't work - drop it before you've wasted too much time. Ask for help or try out a different approach. It's not about proving yourself hard and smart, it's about getting things done.  "
    },
    {
        "ID": "24412",
        "Question": "I know that many of us maintain our own little personal library with tools and utilities that we use often. I've had mine since I was 16 years old so it has grown to quite a considerable size. Some of the stuff I've written has since been added to the framework. I wrote my own little implementation of expression trees for use with genetic algorithms long before LINQ, which I quite like and was proud of at the time - of course its pretty useless now. But recently I have been going through it and upgrading to .NET 4.0 and re-kindled an interest.  So I'm curious as to what you use your library for. Maybe we could get some cool ideas going for useful little snippets and share them amongst ourselves. So my questions are:  Do you have a miscellaneous utility library? Which part are you most proud of and why?  Give an example of code if you like :-) ",
        "Best answer": "No.  I've seen some nightmarish effects of a dozen developers all adding their own little \"util.h\" style libraries to projects, and have it turn into a giant mess of inconsistent function naming and behaviors. Much like PHP. So for that reason I avoid doing it.  I avoid needing to do that by using programming environments that give me nearly all the tools and libraries I need up front whenever possible, such as C# and python. "
    },
    {
        "ID": "24414",
        "Question": "I use mainly PHP for web development, but recently, I started thinking about using Google App Engine. It doesn't use PHP which I am already familiar with, so there will be a steeper learning curve. Probably using Python/Django. But I think it maybe worthwhile. Some advantages I see:  Focus on App/Development. No need to setup/maintain server ... no more server configs Scales automatically Pay for what you use. Free for low usage Reliable, it's Google after all  Some concerns though:  Does database with no joins pose a problem for those who used App Engine before? Do I have to upload to Google just to test? Will it be slow compared to testing locally?  What are your thoughts and opinions? Why would you use or not use App Engine? ",
        "Best answer": "Be careful around thinking that it's reliable just because it's from Google. Computer systems do go down and GAE is no exception. One particular case is described here. The reason I post this article and not any of the other ones on the same subject is that it's describing recent problems, not something from a year or two ago. Automatic scaling and pay for what you use models can also be problematic if your app gets popular quickly. Admittedly, for many that's not really an issue, but it is something to be aware of. You can't really tell people to not come to your site because you don't have the money to pay the bill that month. Personally, I'd probably stay away from GAE (and cloud services in general) at this point. I haven't found server configuration to be particularly cumbersome in the past (and on a shared hosting account it's not really my problem anyway) and I like staying in control of my data. If I ever hit the point where I need the scaling capabilities of the cloud providers, then I'll look into it again. Till then... cheap common hosting is better. "
    },
    {
        "ID": "24466",
        "Question": "I have written many database (MySQL) web apps so far but I always think my structure is kinda clumsy. I want to improve the programming/design pattern I use, hoping for some advice here. Particularly, I cannot find a structure that complements an OOP approach that encapsulates the implementation of the database (schema). I  Think my question can be best explained by example. There are 2 approaches I use now say I have an Invoice object/class: first is to use static member functions class Invoice {    int id;    string ref;    int customer_id;    date created;    date due;     static id create();    static bool update(id, field1, field2, ...);    static bool delete(id);    static bool get(id); };  The second approach is to put every thing in a database object: class Database extends ProprietaryDBConnecter, Singleton {    id createInvoice();    bool updateInvoice(id, field1, field2, ...);    bool deleteInvoice(id);    bool getInvoice(id);     id createCustomer();    bool updateCustomer(id, field1, field2, ...);    bool deleteCustomer(id);    bool getCustomer(id);     // etc... }  I find that both ways the (SQL) member functions are very much unseparable from the \"view\", in that the \"view\" determines what the classes needs to have and hence seems to break the document/view architecture. Also, it seems kind of inefficient for example a SELECT statement should only select what is needed, but the presence of member variables in Invoice seems to imply \"guaranteed data\". Don't know if I explained the question clearly, What are some other best approaches to this architecture/design pattern/what-it-is-known-as? Thanks for advices ",
        "Best answer": "Well I suppose that you could use an ORM.  But really, database design should NOT follow OOP priciples, it should follow database design priciples such as normalization.  And it should be designed at the database not in the application. And the data integrity rules should be enforced at the database level not by the application.  I would suggest you read some database design books and, then, read about performance tuning the database of your choice.  "
    },
    {
        "ID": "24538",
        "Question": "I'll preface my question with a disclaimer - I am not an engineer, I'm just someone who works with them full-time, in a learning and development capacity. Given that, one of the comments I get regularly from the engineers I'm tasked with developing is that they feel that they're having solutions (both technical and non-technical) for development \"pushed\" at them vs. anyone from my field consulting with them to determine what they really need. So my question is - if you could give your company a list of the top 3-5 things they could do - in a classroom, or elsewhere - to develop meaningful skills that would help you be a better engineer, a better employee, and one more likely to STAY with the company for the long haul, what would make the cut and why? Thanks in advance for your responses. ",
        "Best answer": " Subsidize/sponsor conference attendance.  Make sure devs have the opportunity for paid time off and expenses to attend tech conferences in their field -- a minimum of once per year. Community development time Let devs spend a few work hours per month on open source projects, so they can stay in touch with the development community and on top of the latest tech. Learning lunches This is when the company buys food and sets aside an extra-long lunch period for devs to eat together and take turns presenting on tech topics.  Once or twice per month is ideal. Build a tech library.  A corporate Safari subscription is a good start, but don't fail to also include classics like The Art of Computer Programming.  The thing about being a dev is that if you are really good at your job, the lag time between relevant tech being created, and that tech making it in to standard corporate learning avenues, makes those traditional methods fairly useless. It's more effective to provide excellent reference resources, and most importantly, opportunities to learn directly from other devs. "
    },
    {
        "ID": "24542",
        "Question": "I have read a lot of threads about functional programming languages lately (almost in the past year, in fact). I would really like to pick one and learn it thoroughly. Last [course] semester, I have been introduced to Scheme. I loved it. Loved the extreme simplicity of syntax, the homoiconicity principle, the macros (hygienic and non-hygienic), the n-arity of procedures, etc. The problem with Scheme is it's an academic language. I don't think it is really used in production environments. I don't believe either that it is particularly good to have on our resume. So, I have been looking around for alternatives. There are many of them and they somehow all seem to have a similar level of popularity. Some thoughts about some other functional languages I have considered yet:  Clojure: It sounds great because it can access the Java world, it is oriented towards scalability and concurrency, but isn't the Java world on an edge right now? I already know Java pretty well, but would it be wise to add even more energy on depending on the JVM? Haskell: Looks like a very appreciated language, but from what I have read, it's also more of an academic language. Lisp: It's been around since forever. It seems to have most of what I like from Scheme. It has a big community. For what I [think I] know, it is probably the most widely used functional programming language in industry(?). F#: Didn't really consider it. I'm not a big fan of MS stuff. I don't have the money to pay for their softwares (I could have them free from university alliances, but I'm more inclined to go with community-driven solutions). Though... I guess it would be the best career-oriented choice.  Tonight, I'm leaning towards Lisp. One week ago, it was Haskell. Before that it was Clojure. In the past year, I was doing some Scheme for fun, not pushing it for the reason you know. Now I would like to get serious (about learning one, about doing real projects with it, about maybe eventually professionally working with it). My problem is I would need to learn them all in depth before being able to choose one. ",
        "Best answer": "Since you want a practical   language:  Notice that Haskell and Lisp are used more than the others in industry, although there has been some recent interest in Clojure and F#. But look what happens when we add Scheme to the mix:  Hmm, doesn't look so much like an academic language now, does it? Actually, the above graph is probably a lie; the word \"scheme\" can appear in help wanted ads in other contexts besides programming languages. :)  So here is another graph that is probably (a little) more representative:  If you want to explore a really kick-ass dialect of Scheme, have a look at Racket.  "
    },
    {
        "ID": "24558",
        "Question": "This is just a wondering I had while reading about interpreted and compiled languages.    Ruby is no doubt an interpreted language since the source code is processed by an interpreter at the point of execution. On the contrary C is a compiled language, as one have to compile the source code first according to the machine and then execute. This results is much faster execution. Now coming to Python:    A python code (somefile.py) when imported creates a file (somefile.pyc) in the same directory. Let us say the import is done in a python shell or django module. After the import I change the code a bit and execute the imported functions again to find that it is still running the old code. This suggests that *.pyc files are compiled python files similar to executable created after compilation of a C file, though I can't execute *.pyc file directly. When the python file (somefile.py) is executed directly ( ./somefile.py or python somefile.py ) no .pyc file is created and the code is executed as is indicating interpreted behavior.   These suggest that a python code is compiled every time it is imported in a new process to create a .pyc while it is interpreted when directly executed.  So which type of language should I consider it as? Interpreted or Compiled? And how does its efficiency compare to interpreted and compiled languages? According to wiki's Interpreted Languages page, it is listed as a language compiled to Virtual Machine Code, what is meant by that? ",
        "Best answer": "Python will fall under byte code interpreted. .py source code is first compiled to byte code as .pyc. This byte code can be interpreted (official CPython), or JIT compiled (PyPy). Python source code (.py) can be compiled to different byte code also like IronPython (.Net) or Jython (JVM). There are multiple implementations of Python language. The official one is a byte code interpreted one. There are byte code JIT compiled implementations too. For speed comparisons of various implementations of languages you can try here. "
    },
    {
        "ID": "24578",
        "Question": "There are some issues about how to manage our program designs and programming styles. I was assigned to find a solution to write reusable code - though, the programming team does not follow the rules. I would rather use MVC to perform a well structured programming style. I found out that a blueprint for next works requires a bunch of experts. The thing is that I have to do it all myself. And the worse part is that I have to use a general MVC platform. I need your helps and suggestions for:  Is there a way that I can write a document for MVC - to use it in our design in Java? How can I represent it? How much work does it need? How can I connect the Model, the View, and the controller parts together?  ",
        "Best answer": " MVC is just a design pattern - it's not a silver bullet. Ask yourself:  Are you actually designing a UI? There are lots of MVC frameworks out there, don't re-invent the wheel! UML is typically used to represent the MVC structure - there's a million generic diagrams out there already  "
    }
]