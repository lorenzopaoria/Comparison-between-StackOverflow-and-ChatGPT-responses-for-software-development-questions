[
    {
        "ID": "16",
        "Question": "I have read a few articles on Internet about programming language choice in the enterprise. Recently many dynamic typed languages have been popular, i.e. Ruby, Python, PHP and Erlang. But many enterprises still stay with static typed languages like C, C++, C# and Java. And yes, one of the benefits of static typed languages is that programming errors are caught earlier, at compile time, rather than at run time. But there are also advantages with dynamic typed languages. (more on Wikipedia) The main reason why enterprises don't start to use languages like Erlang, Ruby and Python, seem to be the fact that they are dynamic typed. That also seem to be the main reason why people on StackOverflow decide against Erlang. See Why did you decide \"against\" Erlang. However, there seem to be a strong criticism against dynamic typing in the enterprises, but I don't really get it why it is that strong. Really, why is there so much criticism against dynamic typing in the enterprises? Does it really affect the cost of projects that much, or what? But maybe I'm wrong. ",
        "Best answer": "Yes, I believe that they do. There are a few reasons that need to be considered in the selection of a language for a new project:  Run-time speed. Compared to C/C++/Fortran, Perl and Python are so slow it's funny. Initialization speed. Compared to the above fast languages, Java falls over and cries as the JVM keeps loading and loading and...while(1).... Prototype-ability. Exhaustively going through and doing the declaration/definition work required for C++ or Java increases the LOC, which is the only known metric that reliably correlates with bugcounts. It also takes a lot of time. It also requires a bit more thinking about types and connections. Internal fiddlability. Dynamically messing around with your internals is great until you begin to debug your self-modifying code. (Python, Lisp, Perl) Correctness verification. A compiler can provide a quick once-over pass of semi-correctness of your code in C++, and this can be really nice.  Static analysis details. C and Java have pretty good static analysis. Perl is not completely statically analyzable at a theoretical level (Possibly Python too). I'm reasonably sure Lisp isn't either.  Weird platforms only take C, in general.  Support chain. If you can have a contract that you will get your bugs looked at and worked on, that's huge.   If you can presume that the organization you are working with has a principle of \"Going forward\"(There's an accounting term for this), and won't just randomly decide to not work on the software, then you have a much better case for using the software. Since there's no Major Business selling (carrying implication of taking responsibility of maintaining it) Python/Perl/$dynamic_language, it considerably reduces risk.  In my experience, open source maintainers often have an issue with fully taking responsibility for bugfixes and releasing updates. \"It's free, YOU work on it!\" is not an answer that is acceptable to most businesses (not their core compentencies, among other things).  Of course, I'm not talking about the webapp/startup world, which tends to play by high risk/high reward rules and be very open to staying on the frothing edge of tech. "
    },
    {
        "ID": "49",
        "Question": "During my four years at university we have been using much functional programming in several functional programming languages. But I have also used much object oriented programming to, and in fact I use object oriented languages more when doing my own small project to prepare for my first job. But I often wish that I was coding in a functional programming language when doing these projects. However, when looking for a job, it is very rare to see a job where knowledge of a functional programming language is required. Why isn't functional programming languages used more in the industry? There is quite much news about functional programming languages these days, so I wonder if functional programming is catching on in the industry now? ",
        "Best answer": "I was a professor and, just like programmers, professors are always looking for the Next Big Thing. When they think they've found one, they make it a bandwagon, and everyone piles on. Since they are preaching to students who think professors must be really smart, else why would they be professors, they get no resistance. Functional programming is such a bandwagon. Sure it's got lots of nice interesting questions to investigate, and lots of sort-of-interesting conference articles to write. It's not a particularly new idea, and you can do it in just about any modern language, and ideas don't have to be new to be interesting. It's also a good skill to have. Given that, functional programming is just one arrow to have in your quiver, not the only one, just as OOP is not the only one. My beef with computer science academia is lack of practical interplay with industry to determine what actually makes real-world sense, i.e. quality control. If that quality control were there, there might be a different emphasis, on classifying problems and the ranges of solutions to them, with tradeoffs, rather than just the latest bandwagons. "
    },
    {
        "ID": "188",
        "Question": "For the longest time in places like Java's IRC channel, SO, and other places I've been told something along the lines of \"Worry about how the code looks and its readability/understandability now, and performance later if absolutely necessary\". So for the longest time, I haven't really been OCD about performance for my small desktop or web apps, just removing the obviously inefficient. Most responses are \"What about scalability?\". Thats a legitimate point, but if my app was only built to parse, say, files 10,000 lines long, should I make my code a mess for the small percentage of people that are going to shove in a 1,000,000 line file? My main question is when should I trade the easy but somewhat inefficient ways of doing tasks for big giant complicated beasts that do things extremely quickly but destroy any possible ways of upgrading and make the code excessively difficult and prone to rewriting anyway by the next developer?  ",
        "Best answer": "Worry about performance when it becomes a problem. If you write a small app to process 10,000 line files and you get a 1,000,000 line file every 100th file, it probably doesn't matter that it takes longer to process that one file. However, if you are regularly getting files that are 5-10 times larger than initially and your application is taking too long to do its job, then you start profiling and optimizing. Now, I said \"too long to do its job\". That is up to the user or sponsoring organization to decide. If I'm doing a task and it takes me 5 minutes to do something when it took me 3 without the software or with a different tool, I'd probably file a bug report or maintenance request to have that improved. If you are the user, how long you want your software to take to do its job is up to you - only you can decide if you want it done faster or if you are willing to wait longer to have more readable code. "
    },
    {
        "ID": "370",
        "Question": "I've been told that to be taken seriously as a job applicant, I should drop years of relevant experience off my résumé, remove the year I got my degree, or both. Or not even bother applying, because no one wants to hire programmers older than them.1 Or that I should found a company, not because I want to, or because I have a product I care about, but because that way I can get a job if/when my company is acquired. Or that I should focus more on management jobs (which I've successfully done in the past) because… well, they couldn't really explain this one, except the implication was that over a certain age you're a loser if you're still writing code. But I like writing code. Have you seen this? Is this only a local (Northern California) issue? If you've ever hired programmers:2  Of the résumés you've received, how old was the eldest applicant? What was the age of the oldest person you've interviewed? How old (when hired) was the oldest person you hired?  How old is \"too old\" to employed as a programmer? 1 I'm assuming all applicants have equivalent applicable experience. This isn't about someone with three decades of COBOL applying for a Java guru job. 2 Yes, I know that (at least in the US) you aren't supposed to ask how old an applicant is. In my experience, though, you can get a general idea from a résumé. ",
        "Best answer": "Having just got a new job at nearly 50 in the UK I can say that it's possible and you're never too old. There are two approaches - both rely on your skills being relevant to the job.  Stick with what you know and become a guru. This is risky as the number of jobs requiring \"old\" technologies are becoming fewer and further between as each year passes. However, as people retire from such jobs there will be openings. Keep refreshing your skills. I moved into Silverlight last year, which is what got me this job. That and my previous team leadership roles which my new employer saw as relevant.  "
    },
    {
        "ID": "408",
        "Question": "\"Regular\" golf vs. code golf: Both are competitions.  Both have a well-defined set of rules, which I'll leave out for simplicity.  Both have well-defined goals; in short, \"use fewer hits/characters than your competitors.\" To win matches, athletic golfers rely on  equipment  Some situations call for a sand wedge; others, a 9-iron.   techniques  The drive works better when your feet are about shoulder width apart and your arms are relaxed.   and strategies  Sure, you could take that direct shortcut to the hole... but do you really want to risk the water hazard or sand bunker when those trees are in the way and the wind is so strong?  It might be better to go around the long way.    What do code golfers have that's analagous to athletic golfers' equipment, techniques and strategies? Sample answer to get this started: use the right club!  Choose GolfScript instead of C#. ",
        "Best answer": "I'd say that thorough knowledge of the syntactical oddities of your language help. Here is one I found in Ruby when doing a bit of code golf: Instead of require \"sequel\" require \"nokogiri\" require \"chronic\"  You can do something like this:  body_of_your_program if %w{sequel nokogiri chronic}.each{|i| require i}  With this kind of thing, you too can write incredibly elaborate Ruby one-liners! In Ruby and Perl, you also get the magic variables like \"$_\" which can be used to do all sorts of magic with strings and regexes. Is your data not strings? Well, you might want to turn it into strings. Obviously, in C, the preprocessor is your friend. "
    },
    {
        "ID": "568",
        "Question": "Managed OSes like Microsoft Singularity and JNode are quite an interesting concept. Essentially, the OS is bootstrapped with code written in a low-level language (C/C++/Assembly), which essentially implements a virtual machine. The rest of the OS (and all userland apps) run on the virtual machine. There are some great things about this. For example, you suddenly make arbitrary pointers obsolete. And if well written, you get rid of a ton of legacy crud that most modern OSes currently have.  However, as a disadvantage, you're that much farther away from the hardware, and as a developer, you lose the ability to drop down to a lower level of abstraction and get your hands dirty.  What are your opinions on this? ",
        "Best answer": "I think that this is another case where \"it depends\". If you're writing applications such as web browsers, word processors etc. where lightning fast performance is not necessarily an issue then this approach has it's merits. By using this approach you can offer your customers a safer, more controlled experience. Not only are you limiting the damage that can be done by malware, but you are also running in a more consistent environment. It's like the difference between console games and PC games. The former know exactly what hardware they need to work with so can make use of that knowledge whereas the latter have to be able to cope with a wider variety of graphics cards, sound cards, hard disk speeds etc. However, there will be applications (such as games!) that require the low level access and will still need to be run \"natively\". Like managed languages you will have to use the appropriate tool for the job. "
    },
    {
        "ID": "648",
        "Question": "We, as programmers, are constantly being asked 'How long will it take'? And you know, the situation is almost always like this:  The requirements are unclear. Nobody has done an in depth analysis of all the implications. The new feature will probably break some assumptions you made in your code and you start thinking immediately of all the things you might have to refactor.  You have other things to do from past assignments and you will have to come up with an estimate that takes that other work into account. The 'done' definition is probably unclear: When will it be done? 'Done' as in just finished coding it, or 'done' as in \"the users are using it\"? No matter how conscious you are of all these things, sometimes your \"programmer's pride\" makes you give/accept shorter times than you originally suppose it might take. Specially when you feel the pressure of deadlines and management expectations.  Many of these are organizational or cultural issues that are not simple and easy to solve, but in the end the reality is that you are being asked for an estimate and they expect you to give a reasonable answer. It's part of your job. You cannot simply say: I don't know.  As a result, I always end up giving estimates that I later realize I cannot fulfill. It has happened countless of times, and I always promise it won't happen again. But it does. What is your personal process for deciding and delivering an estimate? What techniques have you found useful? ",
        "Best answer": "From The Pragmatic Programmer: From Journeyman to Master:  What to Say When Asked for an Estimate You say \"I'll get back to you.\" You almost always get better results if you slow the process down and spend some time going through the steps we describe in this section. Estimates given at the coffee machine will (like the coffee) come back to haunt you.  In the section, the authors recommend the following process:  Determine the accuracy that you need. Based on the duration, you can quote the estimate in different precision. Saying \"5 to 6 months\" is different than saying \"150 days\". If you slip a little into the 7th month, you're still pretty accurate. But if you slip into the 180th or 210th day, not so much. Make sure you understand what is being asked. Determine the scope of the problem. Model the system. A model might be a mental model, diagrams, or existing data records. Decompose this model and build estimates from the components. Assign values and error ranges (+/-) to each value. Calculate the estimate based on your model. Track your estimates. Record information about the problem you are estimating, your estimate, and the actual values. Other things to include in your estimate are developing and documenting requirements or changes to requirements specifications, creating or updating design documents and specifications, testing (unit, integration, and acceptance), creating or updating user's manuals or READMEs with the changes. If 2 or more people working together, there's overhead of communication (phone calls, emails, meetings) and merging source code. If it's a long task, account for things like other work, time off (holidays, vacation, sick time), meetings, and other overhead tasks when picking a delivery date.  "
    },
    {
        "ID": "724",
        "Question": "When learning a new programming language you sometimes come across a language feature which  makes you wish you had it in your other programming languages that you know. What are some language feature which were at the time of learning very new to you and that you wish your other programming languages had.   An example of this is generators in Python or C#. Other examples may include list comprehensions in Python, template in C++ or LINQ in .NET or lazy evaluation in Haskell. What other semi-unique language features have you come across which were completely new and enlightening to you?  Are there other features of older programming languages which were unique and have fallen out of fashion? ",
        "Best answer": "Practically anything in Haskell  Monads. Yes - the big scary word that makes increadibly easy parsers, IO, operations on Lists and other things so easy (once you notice common pattern) Arrows. The same for advanced users ;) Standard stuff like lambdas etc.  Currying functions Algebraic data types Pattern matching  And many more. PS. Yes. I am Haskell fanboy if anyone asked. "
    },
    {
        "ID": "775",
        "Question": "On any team, you are going to have the need for more grizzled and grey developers and some young pups. Some reasons include:  Money. There are often tasks that don't require the same level of experience to deliver, so it makes sense not to pay top dollar to have those tasks fulfilled. Energy. There's an energy and enthusiasm that new people can bring to a team that stops it from getting too stale and set in its ways. There's also calmness and wisdom that the more senior people can bring. Knowledge transfer and career growth. Both in terms of the project and skills, it's useful and often fun to teach people and to learn new stuff. It's satisfying to help \"bring on\" new team members.  I realise there are some cutting edge projects where it may be important for there to be more senior people than junior, but in general, is there an ideal mix of experiences on a team, or is it totally dependent on the project? ",
        "Best answer": "I really like what Eric Brechner has to say on this subject  Think of your team as a river instead of a lake. A lake stagnates. There’s no energy or impetus to change. The same is true of groups that stagnate. They cultivate mediocrity and complacency; they abhor risk. A river is always running and changing with lots of great energy. You want a river. A river depends on the flow of water, and your team depends on the flow of people and information. You can think of the people divided into three groups: new blood, new leaders, and elders ready for a new challenge. Here’s how those groups should balance and flow:  The largest group should be the new blood. Not all of them will become technical or organizational leaders.  Sometimes you’ll have more new leaders than elders, sometimes the reverse, but ideally you should maintain a balance.  For flow, you want a steady stream of new blood becoming your new leaders, and new leaders becoming elders.  The key to flow is new blood coming in and elders moving out. For this to work, you WANT your elders to transfer before they clog the stream and disrupt the flow of opportunitiesfor others.   Not all technologies flow at the same rate. Central engines, like the Windows kernel, flow slowly, while web-based services, like MSN Search, flow quickly. You need to adjust for your situation, but even the most conservative technologies do change and flow. How do you successfully encourage and maintain a healthy flow?  Keep a constant supply of new people.  Instill information sharing as a way of life.  Shape the organization and roles to create growth opportunities.  Find new challenges for your elders.    "
    },
    {
        "ID": "812",
        "Question": "I originally tried asking this on StackOverflow, but it was too subjective :-(. I am interested in methods of defining the power of programming languages. Turing completeness is one, but it is almost universally satisfied. What would be nice is to define a measure of power that discriminates among programming languages that are actually in used. For example, can anyone propose a non-subjective method that would discriminate between assembly and Java? Turing completeness means that a language is maximally powerful in what it can output (which pretty much means it can do anything non-time based in the real world). So if we want to define a stronger measure of power, we need to take another approach. Shortness was suggested in the original question, but this is not easy to define at all. Does anyone have any other suggestions? ",
        "Best answer": "If I understand your question correctly, you are nonetheless looking for something that is relatively measurable and not just a subjective judgement call. If so, I would personally favour the amount of time taken to solve any particular problem (averaged over all problems and all programmers). In this measure, you might need to consider not just the language itself but also the framework/API used with it. Succinct syntax is a very small factor: a much more important one is that the most commonly required functionality is easily accessible. If you are looking for something more subjective, I’d say how fun it is. Programmers tend to be people who want problems solved, so a programming language that is fun for programmers to use is inevitably going to be the one that will solve the most problems. This measure takes into account that different people have different preferences on how to use things, so the “best” programming language will be the one that is most appealing to the widest range of programmers. However, you might need to consider not just the programming language and API here, but also the environment (IDE), which is of course what the programmer actually interacts with. "
    },
    {
        "ID": "827",
        "Question": "For decades, the accepted degree to get to become a software developer was \"Compter Science.\" We've had a few questions already about whether that degree really prepares students to develop software. Some schools have, in the last 8 years or so, started offering multiple distinct majors in programming.  Using the curriculum from my school:  Computer Science, which starts out with some intro programming courses in the first year, and then focuses on theoretical computer science, algorithms, and a bit of OS stuff.  Most classes involve several smaller projects and homeworks, done solo or in pairs. Software Engineering, which starts out with the same intro programming courses, does a few classes of theory, and then goes into software development practices (testing, process methodologies, sofware metrics, requirements gathering) and software design (distributed system design, info system design, real-time/embedded design, subsystem design, etc)  Different schools do it differently, so the above is just a real-world example I'm familiar with.  What I ask is: Is there a need for distinct majors in programming? ",
        "Best answer": "Yes, they should be. The relationship between computer science and software engineering is the same as the relationship between physics and mechanical engineering. One provides the theoretical background while the other takes those theories, along with good engineering principles, and applies them to the design and implementation of software. You need both in order to produce new and better software. A good computer science education trains people to produce new and better algorithms, data structures, programming languages and paradigms, compilers, and a number of other things that can be used to enhance software systems. A good software engineering education, on the other hand, trains you to take these tools and knowledge obtained through a scientific study of computation, along with a knowledge of the software development lifecycle and process models to actually build the system that a customer wants and needs. "
    },
    {
        "ID": "874",
        "Question": "In my current job it feels like we have a lot requirement changes. We are an \"Agile\" shop, so I get that we are supposed to adjust and what not, but sometime the change is large and nothing trivial.  My question is, how do you effectively communicate the cost of the change? Because of being agile, if a change is big enough something will get dropped from the current sprint, but it usually just get added next time around. Since our model is SaaS, the end customer is effectively the business itself, and they know they will get the cut feature n weeks later. I guess what I am trying to get at is the removal of a feature really isn't anything to use for communication as it was only delayed by n weeks. What other ways do you have to get the business to understand what a change costs? ",
        "Best answer": "From what you described, you don't have a problem. They ask for a change and are either willing to wait until you say it can be done or are willing to postpone another feature. Seems like a balance between: time, resources and requirements.  "
    },
    {
        "ID": "1070",
        "Question": "Dcpromo.exe is famous among MCSEs for being they only way to create a Windows Domain Controller (in other words an Active Directory Domain) ... which in turn is often used by the ASP.NET Membership system. I'm trying to determine if I should put content on \"www.dcpromo.com\" geared for developers or more towards sysadmins. To me, a logical extension of this \"domain controller on the internet\" is to use WIF for the ASP.NET Membership system.  We'd then have a website that will serve the developer's interests in the SSO, SAML, user managment and identity areas we all struggle with. So my question is:  Q:  Do programmers see a connection between the utility dcpromo.exe and ASP.NET Membership?  If so does it make sense to have a purpose of http://www.dcpromo.com  help developers in the areas of membership and cloud computing?   ",
        "Best answer": "I've been a Microsoft developer for 11 years, mostly as a web developer.  Been coding since 1.1 and I've used .NET membership in many of my applications, and I've never heard of dcpromo. "
    },
    {
        "ID": "1106",
        "Question": "This goes back to a conversation I've had with my girlfriend. I tried to tell her that I simply don't feel adequate enough in my programming language (C++) to call myself good. She then asked me, \"Well, when do you consider yourself good enough?\" That's an interesting question. I didn't know what to tell her. So I'm asking you. For any programming language, framework or the like, when do you reach a point were you sit back, look at what you've done and say, \"Hey, I'm actually pretty good at this.\"? How do you define \"good\" so that you can tell others, honestly, \"Yeah, I'm good at X\". Additionally, do you reach these conclusions by comparing what others can do? Additional Info I have read the canonical paper on how it takes ten-thousand hours before you are an expert on the field. (Props to anybody that knows what this paper is called again) I have also read various articles from Coding Horror about interviewing people. Some people, it was said, \"Cannot function outside of a framework.\" So they may be \"good\" for that framework, but not otherwise in the language. Is this true?  ",
        "Best answer": "You can't call yourself good at X.  Only other people can. "
    },
    {
        "ID": "1135",
        "Question": "Planning Poker Summary, in case you don't want to read the wiki article:  Get a list of tasks you want to do for the upcoming iteration For each task: 2.1 Discuss with the group what it entails 2.2 Everyone writes down / selects an estimation of how much effort is required for the task 2.3 Everyone reveals their estimation 2.4 The highest and lowest outliers explain their reasoning 2.5 Repeat until a consensus is reached  Usually something similar to numbers from the Fibonacci sequence like 0, ½, 1, 2, 3, 5, 8, 13, 20, 40, 100 are the allowed values, so you don't get long arguments over close values like 23 vs 27. Further, the numbers represent a unit-less value of effort, whose value is determined by a baseline task that everyone agrees equals about a 1, and all else is relative to that. Ultimately, the goal is to get a good feel for a given team's \"velocity\", which is the number of these points that can be completed in a given iteration.  With that, it's possible to make reasonably accurate estimates of how long any given feature will take.  We did this at iteration planning meetings at one company I worked at, and I thought it was one of the few good things about that particular company.  So, what I'm wondering is, has anyone used this?  Do you think it's a useful tool for estimation?  Does it work in all situations, or does it lend itself to certain teams, projects, etc? ",
        "Best answer": "We use it in our company for the project I'm involved in.  Some notes about planning poker are expressed in my recent blog post, and here's a bigger list of why it's cool:  It makes everyone agree.  People are not forced to accept any result; instead they're forced to make their own estimate!  The time to defend their own estimates is also allocated, if it's necessary. It keeps everyone busy.  You can't slack during the meeting, while trying to show that you're so involved. Also, necessity of moving your hands constitutes a good physical exercise to keep you off of sleeping. However, a downside of this is that sometimes you do need to do something else (for example, take some notes and write down the details of the agreement you've just reached).   It keeps meetings faster.  There's no need for a constant involvement of a meeting leader to keep everything on pace.  The game with clear rules is way better for that.  Yes, you need to make some extra moves to put cards on, reveal them, et cetera, but these pay their way. A lot of people just like to play cards, especially poker :-)  This increases motivation.  A company that sells decks of such cards accompanied their site with an article about Planning Poker, which is also worth reading. "
    },
    {
        "ID": "1371",
        "Question": "Having worked on a failed project is one of the few things that most programmers have in common, regardless of language used, industry or experience. These projects can be great learning experiences, soul-crushing disasters (or both!), and can occur for a multitude of reasons:  upper management change of heart under-skilled / under-resourced team emergence of superior competitor during dev cycle over/under management  Once you've worked on a couple of such projects, is it possible to recognise at an early stage exactly when a project is doomed to fail?  For me, a big sign is having a hard & fast external deadline combined with feature creep. I've seen projects which were well planned out and proceeding right on schedule go horribly off the rails once the late feature requests started to roll in and get added to the final \"deliverable\". The proposers of these requests earned the nickname of Columbo, due to rarely leaving the room without asking for \"just one more thing\". What are the warning signs you look out for that set off the alarm bells of impending doom in your head? ",
        "Best answer": "Heroic Coding Coding late into the night, working long hours, and clocking lots of overtime are a sure sign that something went wrong.  Further, my experience is that if you see someone working late at any point in the project, it only ever gets worse.  He might be doing it just to get his one feature back on schedule, and he might succeed; however, cowboy coding like that is almost always the result of a planning failure that will inevitably cause more of it soon.  So, the earlier in the project you see it, the worse it will eventually become. "
    },
    {
        "ID": "1386",
        "Question": "Sometimes I feel like a musician who can't play live shows. Programming is a pretty cool skill, and a very broad world, but a lot of it happens \"off camera\"- in your head, in your office, away from spectators.  You can of course talk about programming with other programmers, and there is peer programming, and you do get to create something that you can show to people, but when it comes to explaining to non programmers what is it that you do, or how was your day at work, it's sort of tricky.  How do you get the non programmers in your life to understand what is it that you do?  NOTE: this is not a repeat of Getting non-programmers to understand the development process, because that question was about managing client expectations.  ",
        "Best answer": "I don't even try. If they aren't tech oriented enough to have at least a basic understanding of programming, I am only going to bore them with the details. Usually I just go with something very high level like \"I create web sites\" or \"I write computer programs to do X\" "
    },
    {
        "ID": "1516",
        "Question": "A few years ago I considered myself somewhat of a web developer, knowing the basic 3 languages (HTML, CSS, JS) and lots of PHP. Moving on from simple text to actual websites was a pain because of the so called \"standards\" out there, which at the time were ridiculously complicated for me. It pretty much boiled down to this (minus the IE related stuff):  Standards are there to replace old ways of doing things in a simpler way. However when trying to actually implement some of the stuff (Entirely CSS based layout for example), it took me 10x longer to actually do it then if I did the simpler and still working solution. If it rendered the same, then why should I use the more complicated example that takes 10x longer and breaks once you change browsers? This sparked many long religious debates in ##php, ##css, and ##js in Freenode IRC and actually got me banned from ##css because I messed with their little world over there. My question: Should I follow every single standard and coding conventions even if they take me 10x longer but get me the same result as the simple one?  For the poll tag, those of you who have websites of any size (huge or small), do you follow all the standards?  ",
        "Best answer": "The standards writers have thought of things that haven't occurred to you, such as accessibility concerns. Standards exist for a reason. And, with HTML5, standards are fairly easy to follow. There may, occasionally, be reason to not follow the standard, but following it should be your default behaviour. "
    },
    {
        "ID": "1674",
        "Question": "i wonder apart from those very simple hello world apps, what other programs should i try developing for learning. i am entering uni next year and wonder what kind of programs will be developed in that environment.  this is not really a language specific thing, but if you want to know what i use currently mainly  windows: C#/WPF - i studied java/vb etc in poly but i think C# is the way to go, maybe even F# web: PHP/MySQL, Zend Framework/Doctrine, + HTML/CSS/JS(jQuery) of course. looking to try google app engine with python too.   for now, i am thinking of   todo lists apps that integrate with api eg. twitter/tumblr which i use text editor - i am currently trying to develop a text editor that uses markdown and generates html files for viewing (link to blog entry). not very pratical after i started developing it. cos when editing theres no formatting and in HTML format, i cannot edit directly  blog software (for web)  ",
        "Best answer": "Have a look at Project Euler.  There's nothing else like it for sharpening your foundational programming skills. "
    },
    {
        "ID": "1719",
        "Question": "In chapter one of \"The Pragmatic Programmer\" the first learning goal is:  Learn at least one new language every year. Different languages solve the same problems in different ways. By learning several different approaches, you can help broaden your thinking and avoid getter struck in a rut. [...]  To achieve this over a career, the list of languages is likely to get quite long (particularly if you do not want to \"progress\" into management). Clearly the education of a programmer (or whatever form) is going to get you started with a core of commercially useful languages (the usual list from job posting: C, C++, Ruby, Python, JavaScript, C#, Java, VB, ...). Additionally a formal or informal learning programme is likely to have covered functional approaches (via something like Haskell, LISP or an ML derived language) But once a reasonable subset of that list is learned- what's next, and why? ",
        "Best answer": "Make it interesting and spend each year writing an interpreter or compiler for your own programming language that fills up a niche you've never used a programming language for. Each year, write your next compiler/interpreter using the language you wrote the previous year. "
    },
    {
        "ID": "1750",
        "Question": "I work for a company that supports several languages: COBOL, VB6, C#, and Java. I use those languages for my primary work, but I often find myself coding some minor programs (e.g. scripts) in Python because I find it to be the best tool for that type of task. For example: an analyst gives me a complex CSV file to populate some DB tables, so I use Python to parse it and create a DB script. What's the problem? The main problem I see is that a few parts of these quick and dirty scripts are slowly gaining importance and:  My company does not support Python They're not version controlled (I back them up in another way) My coworkers do not know Python  The analysts have even started to reference them in emails (\"launch the script that exports...\"), so they are needed more often than I initially thought. I should add that these scripts are just utilities that are not part of the main project; they simply help to get trivial tasks done in less time. For my own small tasks they help a lot. In short, if I were a lottery winner to be in a accident, my coworkers would need to keep the project alive without those scripts; they would spend more time fixing CSV errors by hand, for example. Is this a common scenario? Am I doing something wrong? What should I do? ",
        "Best answer": "You need to get the situation formalised as it shouldn't really have got to this point. However, these things happen so you need to explain to your boss that you created these scripts for personal use, but they've \"escaped\" into wider circulation. Admit (if necessary) that you were at fault for not bringing this to his attention sooner. At the very least the scripts should be put under source control \"just in case\" - then at least if you aren't available (for what ever reason) your co-workers will have access to the scripts. Then you either need to convince your boss that Python is the way to go for these or accept that you are going to have to re-write them in a supported language. If the cost of documenting the scripts and educating your co-workers in Python is lower than that of the re-write you might even win the argument. "
    },
    {
        "ID": "1969",
        "Question": "I'm posting this here since programmers write viruses, and AV software.  They also have the best knowledge of heuristics and how AV systems work (cloaking etc). The EICAR test file was used to functionally test an antivirus system.  As it stands today almost every AV system will flag EICAR as being a \"test\" virus.  For more information on this historic test virus please click here. Currently the EICAR test file is only good for testing the presence of an AV solution, but it doesn't check for engine file or DAT file up-to-dateness. In other words, why do a functional test of a system that could have definition files that are more than 10 years old.  With the increase of zero day threats it doesn't make much sense to functionally test your system using EICAR. That being said, I think EICAR needs to be updated/modified to be effective test that works in conjunction with an AV management solution. This question is about real world testing, without using live viruses... which is the intent of the original EICAR. That being said I'm proposing a new EICAR file format with the appendage of an XML blob that will conditionally cause the Antivirus engine to respond. X5O!P%@AP[4\\PZX54(P^)7CC)7}$EICAR-EXTENDED-ANTIVIRUS-TEST-FILE!$H+H* <?xml version=\"1.0\"?> <engine-valid-from>2010-1-1Z</engine-valid-from> <signature-valid-from>2010-1-1Z</signature-valid-from> <authkey>MyTestKeyHere</authkey>   In this sample, the antivirus engine would only alert on the EICAR file if both the signature  or engine file is equal to or newer than the valid-from date. Also there is a passcode that will protect the usage of EICAR to the system administrator. If you have a backgound in \"Test Driven Design\" TDD for software you may get that all I'm doing is applying the principals of TDD to my infrastructure.   Based on your experience and contacts how can I make this idea happen? ",
        "Best answer": "As you said in the question, it would have to work in conjunction with an AV solution.  In order for that to happen you would either need to write an AV engine, or become involved with an existing AV vendor. If such a thing did exist... Where does the benefit come in?  Just thinking devil's advocate here..  Couldn't the AV engine just report when it's database was updated? "
    },
    {
        "ID": "2042",
        "Question": "I've been working in the enterprise space for the past 4½ years and have noticed that generally speaking, enterprises are not conducive environments for the test-first style of development. Projects are usually fixed-cost, fixed-timeline and waterfall style. Any unit testing, if done at all, usually comes after development in the QA phase and done by another team. Prior to working for an enterprise, I consulted for many small to medium sized companies, and none of them were willing to pay for a test-first style of development project. They usually wanted development started immediately, or after a short design stint: i.e., something more akin to Agile, though some clients wanted everything mapped out similar to waterfall. With what types of shops, companies, and clients does test-driven development work best? What types of projects tend to be conducive to TDD? ",
        "Best answer": "Every line of code I write is using test driven development.  If management isn't on board with writing tests first then I don't tell management about it.  I feel that strongly that test driven development is a better process. "
    },
    {
        "ID": "2164",
        "Question": "So I'm sure everyone has run into this person at one point or another, someone catches wind of your project or idea and initially shows some interest. You get to talking about some of your methods and usually around this time they interject stating how you should use method X instead, or just use library Y. But not as a friendly suggestion, but bordering on a commandment. Often repeating the same advice over and over like a overzealous parrot. Personally, I like to reinvent the wheel when I'm learning, or even just for fun, even if it turns out worse than what's been done before. But this person apparently cannot fathom recreating ANY utility for such purposes, or possibly try something that doesn't strictly follow traditional OOP practices, and will settle for nothing except their sense of perfection, and thus naturally heave their criticism sludge down my ears full force. To top it off, they eventually start justifying their advice (retardation) by listing all the incredibly complex things they've coded single-handedly (usually along the lines of \"trust me, I've made/used program X for a long time, blah blah blah\"). Now, I'm far from being a programming master, I'm probably not even that good, and as such I value advice and critique, but I think advice/critique has a time and place. There is also a big difference between being helpful and being narcissistic. In the past I probably would have used a somewhat stronger George Carlin style dismissal, but I don't think burning bridges is the best approach anymore. Do you have any advice on how to deal with this kind of verbal flogging? ",
        "Best answer": "Don't just let them talk. Get them in front of a keyboard. The phrase \"ok, show me\" should do it. My experience is most blow hards aren't that great, and when they actually try to do what they say it doesn't work and things get real quiet. "
    },
    {
        "ID": "2252",
        "Question": "I've been asked this in a few interviews. And it always catches me off guard.My professional and academic background are already in the resumé, which the interviewer has obviously looked at. What more to tell him/her? Should I start with my hobbies? I like gardening, or looking at NSFW pictures on reddit in my free time? What and how do you answer to this specific question? Do you have a prepared answer for it? Am I wrong if I think this question is a bit silly? UPDATE There have been a lot of great answers to this question. I'm in pickle which to choose as the 'correct' answer, because most of them are very insightful. I found a great writing on this subject matter. It's a bit crazy for my taste, but it's interesting: How To Introduce Yourself… I Mean Practically ",
        "Best answer": "Don't assume the interviewer knows your resumé inside out. Often, they'll be interviewing several people for the position and may have just had a cursory glance over your resumé before starting the interview. With that in mind, and assuming this question comes early on in the interview, use this question as an opportunity to give a brief history of your career and why you are applying for the job, as well as what your stand-out skills or attributes are.  Your answer can effectively steer the course of the interview, giving the interviewer some \"jumping off\" points that could change what questions you get asked next. Focusing on your strengths with this answer means that it will be more natural to talk about what makes you great in answers to subsequent questions and not as something you have to try to shoehorn in to some other answer. "
    },
    {
        "ID": "2410",
        "Question": "I am referring to explaining to the non-programmer what programming is. I made sure to search for similar questions before creating this one, but the few ones I did find seemed to dodge the question, and I specifically would like to see some metaphors or analogies. I personally find it easier to explain something technical to someone through the use of metaphors or analogies. The reason I'm interested in this is because many people encounter the work of a programmer on a daily basis, but if you ask the average person what a programmer is or does, they don't really know. This leads to certain situations of misunderstanding (ex. \"[...] but I thought you were good with computers!\") I really would like to find the best one out there. I would like to be able to easily explain to someone what my career choice is about. Of course, at least the general idea. I personally don't have a solid one, but I have long thought about it and I have usually gravitated towards the 'language' metaphor, where we happen to know a language that computers understand, and therefore we are able to tell computers what to do, or \"teach\" them, to solve our problems. For example:  Imagine that in an alternate reality, humanoid robots with artificial intelligence exist, and some people are able to communicate with them through a common language, which is a variation of English. These people who can communicate with the robots are able to teach them how to solve certain problems or do certain tasks, like doing our chores. Well, although robots like that don't exist yet, programmers of our time are like those people, but instead of communicating with the robots, they communicate with computers. Programmers \"teach\" the computers how to perform certain tasks or solve certain problems by means of software which they create by using this \"common language\". Programmers and this \"common language\" are what give us things like email, websites, video games, word processors, smart phones (to put it simply), and many other things which we use on a daily basis.  I don't mean to put programming on the throne or anything, it's just the best metaphor I could come up with. I'm sure someone will find some issue with this one, it's probably a bit contrived, but then again that's why I'm asking this question. ",
        "Best answer": "It's like having to write detailed step by step directions for how to drive somewhere. But  you usually have to add contingency plans for things like 'what if there is a traffic jam', or 'what if a truck breaks down in the turn lane'.  And sometimes you have to dive even deeper and explain the rules of the road like which side to drive on or what to do at a red light.  And sometimes you even need to explain precisely how the steering wheel or the gas pedal works.   And usually, once you've got that all described in exacting detail, the customer says \"that's perfect, except it needs to work for someone driving a hovercraft\" "
    },
    {
        "ID": "2682",
        "Question": "We’re working on a .Net framework which ultimately amounts to a single DLL.  We intend to charge for commercial use of the framework, but make it free for open source/non-commercial use.  The rough plan at the moment is to administer this through some form of fairly simple licence which will be issued whether you’re using it for free or paying. We’re debating whether to make the source code available.  It’s our perception (and our own preference) that it’s far more appealing to use something where you have access to the source code. I’m interested in whether people think making the source code available will damage our ability to make money from the framework, or whether it will encourage more usage and enough “good” people will arrange to pay for the correct licence if using it commercially. My feeling is that, generally, commercial operations don’t mess about on the licencing front and so making the source code available will only encourage usage and therefore ultimately generate more revenue, but I’d be interested in others views/experience. ",
        "Best answer": "You should definitely make the source available.  Whether it's freely available or only available to those who buy a license is up to you, but I would never use a third-party library with no source.  Unlike Robert Harvey, I emphatically do not \"know that I will probably never need it.\"  Any library of non-trivial complexity is almost certain to have bugs in there somewhere, have missing or poorly-implemented features that could benefit from customization/extension, or most likely both.  (Yes, even yours.)  I've used a lot of different libraries, from different people and written in different languages, and I can't think of any that I've never needed the source from at one point or another. If you want to do it right, add a provision in the license like what the GPL and MPL have, that if they make changes to the code and end up publishing a product using it, they have to publish the changes they made. That way you get free bugfixes and (potential) features just by letting other people use your code. "
    },
    {
        "ID": "2699",
        "Question": "This is a \"Share the Knowledge\" question. I am interested in learning from your successes and/or failures. Information that might be helpful... Background:  Context: Language, Application, Environment, etc. How was the bug identified ? Who or what identified the bug ? How complex was reproducing the bug ?   The Hunting.  What was your plan  ? What difficulties did you encounter ? How was the offending code finally found ?  The Killing.  How complex was the fix ? How did you determine the scope of the fix ? How much code was involved in the fix ?  Postmortem.  What was the root cause technically ? buffer overrun, etc. What was the root cause from 30,000 ft ? How long did the process ultimately take ? Were there any features adversely effected by the fix ? What methods, tools, motivations did you find particularly helpful ? ...horribly useless ? If you could do it all again ?............  These examples are general, not applicable in every situation and possibly useless. Please season as needed. ",
        "Best answer": "It was actually in a 3rd party image viewer sub-component of our application.  We found that there were 2-3 of the users of our application would frequently have the image viewer component throw an exception and die horribly. However, we had dozens of other users who never saw the issue despite using the application for the same task for most of the work day. Also there was one user in particular who got it a lot more frequently than the rest of them. We tried the usual steps:  (1) Had them switch computers with another user who never had the problem to rule out the computer/configuration. - The problem followed them. (2) Had them log into the application and work as a user that never saw the problem. - The problem STILL followed them. (3) Had the user report which image they were viewing and set up a test harness to repeat viewing that image thousands of times in quick succession. The problem did not present itself in the harness.   (4) Had a developer sit with the users and watch them all day. They saw the errors, but didn't notice them doing anything out of the ordinary to cause them. We struggled with this for weeks trying to figure out what the \"Error Users\" had in common that the other users didn't. I have no idea how, but the developer in step (4) had a eureka moment on the drive in to work one day worthy of Encyclopedia Brown. He realized that all the \"Error Users\" were left handed, and confirmed this fact. Only left-handed users got the errors, never Righties. But how could being left handed cause a bug? We had him sit down and watch the left-handers again specifically paying attention to anything they might be doing differently, and that's how we found it. It turned out that the bug only happened if you moved the mouse to rightmost column of pixels in the image viewer while it was loading a new image (overflow error because the vendor had a 1-off calculation for mouseover event).  Apparently, while waiting for the next image to load, the users all naturally moved their hand (and thus the mouse)  towards the keyboard.  The one user who happened to get the error most frequently was one of those ADD types that compulsively moved her mouse around a lot impatiently while waiting for the next page to load, thus she was moving the mouse to the right much more quickly and hitting the timing just right so she did it when the load event happened. Until we got a fix from the vendor, we told her just to let go of the mouse after clicking (next document) and not touch it until it loaded.  It was henceforth known in legend on the dev team as \"The Left Handed Bug\" "
    },
    {
        "ID": "2806",
        "Question": "There's around a zillion \"PHP frameworks\". And most of them bill themselves as following the MVC pattern. While it's welcome to overcome osCommerce coding style (processing logic heavily intermixed with SQL and HTML), there are certainly simpler and easier to follow approaches to get a maintainable application design. The original MVC concept was targetted at GUI applications. And for Gtk/Python it seems feasible to follow it accordingly. But PHP web apps don't operate on live Views (GUI elements) and a persistent Controller runtime. It's quite certainly a misnomer if it just describes the used code + directory grouping or class naming. \"MVC\" seems to be used like a buzzword for PHP frameworks. And I've actually seen one or two mature PHP frameworks admit it, but redefining the phrase anyway to match interna. So is it generally snake oil? Why isn't better terminology used, and a more sensible concept for maintainable PHP propagated? Some elaborative reasoning Why I suspect that PHP implementations don't follow the real MVC pattern: Models: in theory, Models should be fat and contain business logic, and controllers should be thin handlers (input->output). In reality the PHP frameworks advocate shallow Models. CI and Symfony for example equate Model == ORM. Even HTTP input is handled by the controller, isn't treated as model. Views: workarounds with AJAX discounted, there can't be Views on web pages. PHP frameworks still pump out pages. The interface still effectively follows the ordinary HTTP model, there's no advantage over non-MVC applications. (And lastly, none of the widespread php frameworks can factually output to GUI Views instead of HTML. I've seen a PHP library that can operate Gtk/Console/Web, but the frameworks don't.) Controller: I'm unsure. Controllers probably don't need to be long-running and persistently active in the MVC model. In PHP framework context, they're however mostly request handlers. Not really something to get argumentative about, but it just feels slightly buzzwordish. Would there be better descriptors? I've seen acronyms like PMVC or HMVC thrown around. Though descriptions get more ambigous there, maybe these would describe the current web frameworks less hokey? ",
        "Best answer": "I think you are looking at this in completely the wrong way. A GUI app and a web page are worlds apart so the exact same definition of MVC will never work for both. MVC is more about the ideal: separating certain parts of the app like display and logic. In PHP (or the web in general), a View is the web page itself: the HTML output. It's not \"live\" as per your definition, but you simply click links to go back to the controller (i.e. another page request). The Controller and Model is where things do differ, like you explained. In PHP the model tends to be the data layer, interacting with the database and so on. But it is still modelling the situation, and the controller still controls the application flow, if only once per page load. So the name \"Model-View-Controller\" is perfectly logical, albeit a different implementation in GUI apps vs web apps. "
    },
    {
        "ID": "3033",
        "Question": "When writing software for yourself, your company or third parties, do you always consider certain principles, values, standards, or rules of behavior that guide the decisions, procedures and systems in a way that contributes to the welfare of its key stakeholders, and respects the rights of all constituents affected by its operations?  And can this code of conduct sometimes be overruled by business requirements, lack of technical skills or other friction during the development process?  Some random examples in order of severity. (yes that is controversial) :  Do you accept known bugs as a risk for the end-user? When writing applications, do you always give the end user the chance for a complete de-install? Do you always secure and encrypt private data delivered end-users in your web application? Do you alwask ask the end-user before submitting his entered data to the server? Did you ever wrote an application that sends unwanted e-mails? Did you ever work on harvesting or scraping projects only for the benefit of the business? Did you ever write software that is legal but moraly controversial, like  for weapons industry. Did you ever wrote software that ( can intentionally) or is be used for criminal activities  It would be nice if you can get a good case with explanation on your moral and ethical decisions.  note: Since ethics and law are quite local and cultural dependent, it would be interesting if you add the location of the \"crime scene\" with it. ",
        "Best answer": "Overall, I always keep the Software Engineering Code of Ethics in mind. However, to address some of your particular points:  Do you accept known bugs as a risk for the end-user?  It depends. If it's a mission critical system and the bug is a showstopper, that's unacceptable. However, if it's a minor flaw that has workarounds in a non-critical system, that's acceptable. I always consider the impact of the problem on the system and to the users (and people affected by) the system.  Do you always secure and encrypt private data delivered end-users in your web application?  If I was on a project where this applied, I would consult any applicable laws and guidelines and follow them. If there were no applicable guidelines, I would err on the side of caution and use some form of security. Of course, you have to weigh a number of factors, ranging from how the system is deployed (physical locations, connections between nodes) and performance of any algorithms or techniques used.  Did you ever write software that is legal but morally controversial, like for weapons industry.  All of my jobs (as you can see in my profile) have been in the defense industry (and I'm also planning on working in the defense or intelligence industries after graduation), including work on ISR systems and Command and Control systems. I don't understand anyone, especially really good software engineers with skills that these projects need, who says that they won't build such systems. The way I see it, by bringing the best software engineering practices to these systems, you are only making them safer and more reliable. And these systems that involve life and death need to be as safe and reliable as possible. "
    },
    {
        "ID": "3139",
        "Question": " Possible Duplicate: At which point do you “know” a technology enough to list it on a resume   I'm having trouble selecting exactly what to put in the computer skills section of my resume. I feel the need to list a lot of languages and the IDEs I work with, and perhaps mention that I use Mercurial too. But this seems, well, kinda fake; after all, where do I draw the line in the list of languages? Sure, I learned a little C in a class, I can conquer some simple printf and getchar projects, but I don't really think that counts as being able to list it on my resume. I seem to recall Joel or Jeff addressing this but I can't find it now. But I'm pretty sure they said something along the lines of don't put it on your resume if you don't want to be drilled on it. Well, I sure wouldn't want to be drilled on C... But is there no justification in my listing languages like C# that I don't work with daily but could pick back up after a short refresher? I mean, I wouldn't want to be drilled on the internals of .NET either, but I think I am justified in listing it in a list of languages I have used... How do you decide? What do you have in your 'Computer Skills' section of your resume? (and can you please find the Joel/Jeff posts I'm thinking of, if they exist?) ",
        "Best answer": "As little as possible, and only those relevant to the position I'm applying for. As someone who reads resumes on occasion, nothing is more annoying than going through a list of every single computer related piece of equipment, software, and skill the applicant has ever touched, read about, or has actual experience with. You applying for a job writing code?  Why the $*@( are you telling me you have experience with Outlook?  Seriously? Only include the skills relevant to the position you are applying for on your resume.   You are retooling your resume for each position you are applying for, aren't you? Aren't you? "
    },
    {
        "ID": "3233",
        "Question": "As an entrepreneur/programmer who makes a good living from writing and selling software, I'm dumbfounded as to why developers write applications and then put them up on the Internet for free.  You've found yourself in one of the most lucrative fields in the world.  A business with 99% profit margin, where you have no physical product but can name your price; a business where you can ship a buggy product and the customer will still buy it. Occasionally some of our software will get a free competitor, and I think, this guy is crazy.  He could be making a good living off of this but instead chose to make it free.    Do you not like giant piles of money? Are you not confident that people would pay for it?   Are you afraid of having to support it?  It's bad for the business of programming because now customers expect to be able to find a free solution to every problem. (I see tweets like \"is there any good FREE software for XYZ? or do I need to pay $20 for that\".) It's also bad for customers because the free solutions eventually break (because of a new OS or what have you) and since it's free, the developer has no reason to fix it. Customers end up with free but stale software that no longer works and never gets updated. Customer cries. Developer still working day job cries in their cubicle. What gives? PS: I'm not looking to start an open-source/software should be free kind of debate. I'm talking about when developers make a closed source application and make it free. ",
        "Best answer": "Because I don't want to feel obligated to provide technical support or offer refunds. "
    },
    {
        "ID": "3241",
        "Question": "This question is a little abstract but I'm hoping someone can point me in the right direction. My question is what amount of time can one expect to devote to a software project's bugs in relation to the original development time. I realize there are a huge number of determining factors that go into but I was hoping for a typical or average breakdown.  For example, if Project A takes 40 hours to complete and an additional 10 fixing bugs then this project would have a 4:1 ratio. If another Project (B) takes 10 hours to complete but another 8 on bugs then it would have a 5:4 ratio.  Is this a documented/researched concept? UPDATE Thanks for all the informative answers. I understand that it's impossible to put a standard to this kind of metric due to all the variables and environmental factors involved. Before I assign an answer I'd like to know if this metric has an agreed-upon name so I can do further research. I would like to get to a point where I can understand the measurements necessary to generate the metrics myself and eventually come up with a baseline standard for my project. ",
        "Best answer": "The equilibrium percentage of total capacity allocated to defect-fixing is equal to the defect injection rate. Many factors can affect this rate, among them, of course: what kind of product the team is developing, what technologies and technical practices they use, the team's skill level, the company culture, etc. Considering Team B, if they create on average 8 units of rework for every 10 units of work they complete, then working those 8 units will create new 6.4 units of rework.  We can estimate the total effort they will eventually have to expend as the sum of a geometric progression: 10 + 8 + 6.4 + 5.12 + ... The number of bugs will decrease exponentially with time, but Team B has such a coefficient in their exponent that it will go to zero very slowly.  Actually, the sum of the first three terms in the above series is only 24.4; of the first five, 33.6; of the first 10, 45; of the entire series, 50.  So, Team B summary: defect injection rate, 0.8; feature development, 10/50 = 20%; defect-fixing, 80%.  20/80 is their sustainable capacity allocation. By contrast, Team A is in much better shape.  Their progression looks like this: 40 + 10 + 2.5 + 0.625 + ... The sum of this series is 53 1/3, so Team A's feature development allocation is 40/(53 1/3) = 75% and defect-fixing allocation is 25%, which matches their defect injection rate of 10/40 = 0.25. Actually, all terms in Team A's series after the first three are negligibly small.  What this means in practical terms is that Team A can probably squash all their bugs with a couple of maintenance releases, the second release being pretty small in scope.  This also creates an illusion that any team can do that.  But not Team B. I thought about this equivalence while reading David Anderson's new book, \"Kanban\".  (The book is on a different subject, but addresses quality concerns, too.)  When discussing software quality, Anderson quotes this book, by Capers Jones, \"Software Assessments, Benchmarks, and Best Practices\": \"...in 2000... measured software quality for North American teams... ranged from 6 defects per function point down to less than 3 per 100 function points, a range of 200 to 1.  The midpoint is approximately 1 defect per 0.6 to 1.0 function points.  This implies that it is common for teams to spend more than 90 percent of their effort fixing defects.\"  He cites an example provided by one of his colleagues of a company that spends 90% of the time fixing their bugs. The fluency with which Anderson goes from the defect injection rate to the defext-fixing capacity allocation (failure demand is the term for it) suggests that the equivalence of the two things is well known to software quality researchers and has probably been known for some time. The key words in the line of reasoning that I'm trying to present here are \"equlibrium\" and \"sustainable\".  If we take away sustainability, then there's an obvious way to cheat these numbers: you do the initial coding, then move on to code somewhere else, and leave maintenance to others.  Or you run up the technical debt and unload it on a new owner. Obviously, no particular allocation will suit all teams.  If we decreed that 20% must be spent on bugs, then, if a team has an ultra-low defect injection rate, they will simply not have enough bugs to fill the time, and if a team had a very high rate, their bugs will continue to accumulate. The math I used here is way simplified.  I neglected things like transaction costs (planning and estimation meetings, post-mortems, etc.), which would affect the percentages somewhat.  I also omitted equations simulating sustaining one product and developing another one concurrently.  But the conclusion still stands.  Do what you can, in terms of technical practices, like unit-testing, continuous integration, code reviews, etc., to reduce your defect injection rate and, consequently, your failure demand.  If you can create only one bug for every 10 features, you will have a lot of free time to develop new features and satisfy your customers. "
    },
    {
        "ID": "3450",
        "Question": "Long winding if conditions should be avoided if at all possible, yet sometimes we all end up writing them. Even if it's a very simple condition, the involved statements are sometimes simply very wordy, so the whole condition ends up being very lengthy. What's the most readable way to format those? if (FoobarBaz::quxQuux(corge, grault) || !garply(waldo) || fred(plugh) !== xyzzy) {     thud(); }  or if (     FoobarBaz::quxQuux(corge, grault)  || !garply(waldo)  || fred(plugh) !== xyzzy ) {     thud(); }  or if (FoobarBaz::quxQuux(corge, grault)     || !garply(waldo)     || fred(plugh) !== xyzzy) {     thud(); }  or thudable = FoobarBaz::quxQuux(corge, grault); thudable ||= !garply(waldo); thudable ||= fred(plugh) !== xyzzy;  if (thudable) {     thud(); }  or any other preferences? ",
        "Best answer": "Often, a long if condition is the sign of code that needs refactoring, but sometimes you can't avoid it. In those cases, I prefer the first:  if (bar || baz || quux) { ... }  Because you're able to tell what's going on with one line. However, I'd much rather do something like this, when possible: function foo() {   return bar || baz || quux; }  if (foo()) { ... }  "
    },
    {
        "ID": "3482",
        "Question": "You've shipped, asserts are turned off, you receive a rare crash report indicating that a null pointer violation occurred in your code.  In a development environment, the problem would have been caught by an assert. All you have is a crash report, so reproducing the issue is near impossible.  Following the backtrace doesn't give any clues as to why the crash happened in the first place. Options: - Add pointer checking to prevent the crash.  This will prevent the crash, but you probably won't even find out why it happened in the first place. - let it fly, hope that it happens again with a repro scenario Let's say the application isn't intended for a guided missle or automatic braking system... Which would you choose? ",
        "Best answer": "I chose the second approach. There is no point in hiding the crash if the NULL pointer was unexpected at the point where crash has occured. This NULL pointer in most cases would just be one of the symptom of something else is wrong. If we hide it with a NULL pointer check it is almost certain that something else will break. I feel you have a better chance catching the scenario if you know the point where it crashes everytime instead at some random place. "
    },
    {
        "ID": "3622",
        "Question": "I often develop an application entirely myself. But did I really? I feel strange about that claim and never know when that is true. I mean I designed it, coded it, but I used XYZ plug-in. Can I still claim that I did it all myself even though I didn't create the plugin I used? Consider this conversation?  ME: I designed and developed this app entirely myself. Other: Cool, how did you program XYZ part? ME: I didn't program that part I used XYZ plugin. Other: So you didn't really program it ALL yourself than did you?  I mean if I must give them credit for the plug-in I used, then do I have to give the language authors credit for the language I used and the IDE authors credit as well? Where do I draw the line? This is just something that always crosses my mine as soon as I am about to take full credit for a project, and was wondering others opinions on the matter. ",
        "Best answer": "That depends on what \"XYZ part\" does.  Most code libraries are pretty modular things that take care of one simple task.  Even if someone else designed that, you're the one who integrated it into the overall vision of what your program is supposed to do. Unless the third-party code takes care of a major part of the application logic itself--for example, building a video game on top of an existing engine--it's just a component.  Saying that you didn't really build the program yourself because you used third-party controls is like saying that you didn't really build the house yourself because you bought premade nails, boards, drywall, wiring, etc... "
    },
    {
        "ID": "3713",
        "Question": "With parallel algorithms knocking at the door, it might be a good time to think about error handling. So at first there were error codes. Those sucked. It was free to ignore them, so you could fail late and produce hard-to-debug code. Then came exceptions. Those were made impossible to ignore once they occur, and most people (except Joel) like them better. And now we got libraries that help parallel code. Problem is, you can't handle exceptions in parallel code as easily as you could with non-parallel code. If you asynchronously launch a task and it throws an exception, there's no stack trace past it to unwind; best you can do is capture it and register it on the task object, if there's such an object. However, it defeats the primary strength of exceptions: you have to check for them and you can ignore them without any additional effort, whereas in single-threaded code an exception will necessarily trigger the appropriate actions (even if it means terminating your program). How should language implementations or libraries support errors in parallel code? ",
        "Best answer": "I'm rather fond of callbacks for errors that can be handled. And they can be made to work just fine asynchronously... But for errors that can't be handled, truly exceptional errors, I'd rather see the relevant information saved and the program terminated. Since this is usually accomplished via some sort of global error-handler anyway, I see no need to twist exceptions into something that works for this - but better platform support for detecting critical errors and producing memory dumps, etc. would be nice. "
    },
    {
        "ID": "3824",
        "Question": "I've been involved with many projects in several companies because I've been a developer for a long time and I'm a contractor. I estimate that less than 20% of projects are methodically tested. With methodically tested I mean any testing beyond ad-hoc no plan testing. I also estimate that less than 10% of projects are thoroughly methodically tested where they have dedicated testers as part of the team, test plan document, where developers write automated tests and then they also track test coverage and measure results.  Two questions  What are your percentage estimates about this issue? What's your professional experience regarding software testing?  Additional note Since methodical testing question may get quite biased answers (people like to brag about being superior to others) I encourage other developers (those that're not exposed to methodical testing) to provide their answer as well, because otherwise it will look like testing is being done everywhere... except at your company. ",
        "Best answer": "The pattern I have seen with testing over my career shows a strong correspondence with the risk of failure in a project. Big projects are more likely to be tested than small ones, mission critical applications are more likely to be tested than one off marketing web sites, in house systems are less likely to be tested than public facing ones. That said there are still projects that have been excessively tested and those that have not been tested enough, but these are the minority. "
    },
    {
        "ID": "3853",
        "Question": "I'm doing my first big project and I don't have a lot of experience in a professional programming environment. While researching anything programming-related I often see references to Unit Testing, but I am still unclear as to how to set those up or even if it would be beneficial to me.  Can someone explain unit testing to me, and how to set it up in a Visual Studio 2010 solution that has multiple projects? Is it something that occurs within your project's solution, or is it a separate solution? And is it something you'd recommend for a small development team or is it just a waste of time to setup?   Right now I just run the entire program to test whatever I am currently working on, but occasionally I have run into problems that are not easy to debug and it would be useful to run subsets of the code elsewhere... Sometimes I do setup another project with some of the libraries referenced to test a small part of the program, but I feel more time is wasted setting that up then just running the entire program because of all the dependencies involved ",
        "Best answer": "Setting up unit testing in VS2010 is fairly easy.   Add a new project to your solution and select the Test Project template from the New Project dialog box.  Usually you create one test project for project you want to test in your solution (ie. BusinessLogic.proj would have a mate called BusinessLogicTest.proj, just as an example). Once you have the project added, the easiest way (not necessarily the best way to do this, especially if you're goal is to get to TDD) to get started is to right-click on a function that you want to test and select \"Create Unit Tests\".  Make sure the Output Project is the Test Project you just created and click OK.  VS creates a new Test Class for you with a Test Method stubbed out. That's the basics of getting setup.  There's a lot to learn to test effectively, but you're at least on the right path by wanting to unit test.  I'll see if I can find a demo or article to get you a bit further. EDIT Found one.  This article is by Deborah Kurata (an MS MVP) and she has both C# and VB code samples.  She is using VS2008, but the basics are the same for VS2010 "
    },
    {
        "ID": "4107",
        "Question": "I was browsing some old code that I wrote (first year in the university) and noticed that I used to write comment titles preceeding various parts of the code. Stuff like (this is from a Monopoly game):  /*Board initialization*/ ...code...  /*Player initialization*/ ...code...  /*Game logic starts here*/ /*Displaying current situation*/ ...code...  /*Executing move*/ ...code...  /*Handle special event*/ ...code...  /*Commit changes, switch to next player*/ ...code...  This might be redundant, and arguably unnecessary if the code is really super clear, but as I scanned through the file it surprised me how strongly I felt like I know what's going on even though I hardly looked at the actual code. I can definitely see this as being fitting in certain circumstances, so I wonder- do you do this? Do you think it's a good idea? Or is it too much?  ",
        "Best answer": "This is a code smell. This says what and not why. If this is necessary, split the code in small functions. "
    },
    {
        "ID": "4112",
        "Question": "If you were to run across code like this in your travels would the short names bother you? let setPropertyAssignment =      parser {         let! a = setIdentifierName         let! b = propertyName         let! c = openParenthesis          let! d = propertySetParameterList         let! e = closeParenthesis          let! f = openCurlyBracket          let! g = functionBody         let! h = closeCurlyBracket         return Node(NodeType.PropertyAssignment, [a;b;c;d;e;f;g;h])     }  Here is the largest parser in the project: let iterationStatement =      parser {         let! a = doKeyWord         let! b = statement         let! c = whileKeyWord         let! d = openParenthesis         let! e = expression         let! f = closeParenthesis         let! g = statementTerminator         let symbols = [a; b; c; d; e; f; g]         return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = whileKeyWord         let! b = openParenthesis         let! c = expression         let! d = closeParenthesis         let! e = statement         let symbols = [a; b; c; d; e]         return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! c = maybeOne expressionNoIn         let! d = semiColon         let! e = maybeOne expression         let! f = semiColon         let! g = maybeOne expression         let! h = closeParenthesis         let! i = statement         let symbols = [a;b] @ c @ [d] @ e @ [f] @ g @ [h;i]                       return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! c = varKeyWord         let! d = variableDeclarationListNoIn         let! e = semiColon         let! f = maybeOne expression         let! g = semiColon         let! h = maybeOne expression         let! i = closeParenthesis         let! j = statement         let symbols = [a;b;c;d;e] @ f @ [g] @ h @ [i;j]         return Node(NodeType.IterationStatement, symbols)     } +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! c = leftHandSideExpression         let! d = inKeyWord         let! e = expression         let! f = closeParenthesis         let! g = statement         let symbols = [a;b;c;d;e;f;g]                        return Node(NodeType.IterationStatement, symbols)     }  +++ parser {         let! a = forKeyWord         let! b = openParenthesis         let! a = varKeyWord         let! c = variableDeclarationNoIn         let! a = inKeyWord         let! c = expression         let! d = closeParenthesis         let! e = statement         let symbols = [a; b; c; d; e]         return Node(NodeType.IterationStatement, symbols)     }  ",
        "Best answer": "Depends on the complexity.  With something this size, it's pretty obvious what you're doing and why.  Make it much bigger--a page or so--and it'll get very confusing very quickly. "
    },
    {
        "ID": "4250",
        "Question": "I'm sure many of us have seen method names like this at one point or another:  UploadTheFileToTheServerPlease CreateATemporaryFile WriteTheRecordToTheDatabase ResetTheSystemClock  That is, method names that are also grammatically-correct English sentences, and include extra words purely to make them read like prose. Personally, I'm not a huge fan of such \"literal\" method names, and prefer to be succint, while still being as clear as possible. To me, words like \"a\", \"an\", and \"the\" just look plain awkward in method names, and it makes method names needlessly long without really adding anything useful. I would prefer the following method names for the previous examples:  UploadFileToServer CreateTemporaryFile WriteOutRecord ResetSystemClock  In my experience, this is far more common than the other approach of writing out the lengthier names, but I have seen both styles and was curious to see what other people's thoughts were on these two approaches. So, are you in the \"method names that read like prose\" camp or the \"method names that say what I mean but read out loud like a bad foreign-language-to-English translation\" camp? ",
        "Best answer": "I'll agree that prose methods suck with one exception: Unit Test Cases These are generally never called in your code and show up in test reports.  As such, it's handy to have readouts with a bit more prose:  AddingACustomerOrderFailWhenCustomersIdIsInvalid : Failed OutOfBoundsPriceReturnsAnError : Passed CanDeleteAnEventFromASeason : Passed  Even this should be done sparingly, but I can see it as at least one case where grammatical additions can make it a little easier to express what passed and what failed.  This is, of course, unless your language/framework provides a good mechanism for test-descriptions in the test readout other than method names, in which case ignore this one too. "
    },
    {
        "ID": "4267",
        "Question": " Basically I am looking for what is it that you goof up and you are out from the remaining process ? Are elimination rounds a fair way to judge a person ? Anyone can have a bad hour :-(  Should you code the best possible or should you get the Algorithm right ? I generally first code a workable solution and then work on it till a level I think it looks beautiful to me. Is this a wrong approach ?  Recently I had a telephonic interview in which I was asked to write a variation of Level Order traversal in 20 minutes. I could get the Algorithm and working code in 20 minutes but couldn't get the Exception handling and the coding convention right, after which I didn't hear back from them :-( All the other questions in the interview went very well this was the only thing which was 'not upto the mark'. Needless to say I like the company and will apply again but want to get it right the next time :-) Please share your experiences and valuable suggestions. ",
        "Best answer": "When doing technical interviews, I'm honestly looking for people to hit a home run. If the candidate doesn't look like they know their stuff, they're not going to be effective in their role (I'm talking senior developers positions here). Look at it this way: Would you rather have a tough interview where you don't get the job (because you're not a good fit), or an easy interview where you do get the job, but then get let go after 90 days because you're in over your head? I've seen far too many developers in the latter camp. If you didn't get the job because you flubbed the technical part, consider it a blessing in disguise. If you don't like rejection, bone up on your technical skills. "
    },
    {
        "ID": "4274",
        "Question": "I'm an ASP.Net/C# programmer using SQL Server as a back end. I am the Technical Director of the company, I'm extremely happy in everything I do and consider the languages and system we use to be perfect for what we do. In the back of my mind though I know that over time programmers can become stale. I remember as a wee youngster that all those \"old\" developers were past it and couldn't keep up with the youngsters. So considering I'm happy in everything I'm doing. What options are there for keeping up with everything and avoiding becoming stale. One particular idea that I use is to let all the new developers use and showcase the things that they think are cool. If anything catches my eye then absolutely it will be something we all use going forward. Thoughts? ",
        "Best answer": "Learning many languages gives you different insights - different tools - into problems. I think it's very important to learn several very different languages. Maybe a functional language (Scheme, Haskell), a object-oriented one (Smalltalk, Ruby), a concurrency-oriented one (Erlang), a logic programming one (Prolog) and and and. The important thing here is that the languages shouldn't be more of the same. If you know C#, learning Java isn't going to teach you that much. If you know C, learning Pascal isn't going to expand your intellectual horizons. "
    },
    {
        "ID": "4399",
        "Question": "How much should programmers help testers in designing tests?   I don't think they should help at all.  My worry is that if they help testers in designing tests for their own code, they'll 'infect' the testers with their own prejudices and blind spots about that code.   I feel that the requirements should be sufficient for giving the information needed for testers to create their tests.  If there is some part of the implementation that the programmers find worrisome, then I think it's their duty to implement unit tests to test that part or even run their own informal system tests to test that part. Not everyone I know agrees with this though (and I understand some of their points to a certain extent).  What do others think about this?  Is this discussed in the literature anywhere? ",
        "Best answer": "I agree. Programmers can help the testers to understand the functional specs, to find resources for research but should not pollute the testers' minds with their own ideas about how to approach testing. "
    },
    {
        "ID": "4418",
        "Question": "While hacking on some static analysis tool, I realized I could make the task at hand (irrelevant for this question) much easier by saving the source files on-the-fly, while they were being edited. This seemed weird at first, but being a lazy cat I actually did it, and it turns out I find this pretty useful :     no need to check if all files were saved before running a compiler/interpreter forces you to make granular commits much more regularly  My editor has been behaving this way for a couple of days, I never had such a clean commit history, and didn't get burned yet.  In the days of DVCS when by saving we actually mean commit, do you think that manually saving files is still a relevant practice ? ",
        "Best answer": "I often use the opportunity to keep editing the file while the build is in progress.  If they were auto-saved, the build would break while I'm editing (a broken code would be complied), and I'd just have to wait while the code is compiling.  This is unproductive and boring. "
    },
    {
        "ID": "4475",
        "Question": "I have several projects coming up soon for public release, both commercial and open source. The projects are all downloadable, not web apps. I've worked on them alone and would like to get feedback during a beta period, but I don't currently have a large audience (though the markets are large). What are the best ways to get participation in the betas? Are there any existing sites or communities that specialize in software testing that I can reach out to? At this point, I'm specifically looking for technical testers who aren't intimidated diving into the code and can help spot security bugs, logical errors, etc. Edit: I'm looking for websites or communities similar to Invite Share. Invite Share itself would be perfect, but there doesn't seem to be any public information about how to submit a beta. Bounty Explanation: While Joel's article on running a beta is helpful, I wonder if there isn't an existing community available for beta testing of any sort, technical or user. As a self-taught and sole developer, I don't have a lot of technical contacts that would be appropriate approaching for testing. I did propose a Beta Testing site in Area 51 a few months ago, but it seems as if it either got buried, there wasn't a whole lot of interest, or it's a poor fit for StackExchange. If you know of existing testing communities, sites like InviteShare, or other ways to get testers, please share. ",
        "Best answer": " What are the best ways to get participation in the betas?  Joel (on Software) has an excellent article on this: Top Twelve Tips for Running a Beta.   Are there any existing sites or communities that specialize in software testing that I can reach out to?  I don't think there will be much \"We Test Your Code\" sites so you will have to start a webpage yourself and advertise it to the right audience...  At this point, I'm specifically looking for technical testers who aren't intimidated diving into the code and can help spot security bugs, logical errors, etc.  This seems more like a job description that you need technical testers in your company and is less like beta testing, perhaps it can still be alpha testing... But those aren't always technical either. Test-Driven Development helps you prevent bugs and errors, thinking about the security can help too... "
    },
    {
        "ID": "4614",
        "Question": "You know who they are.  They are the rock stars of programming:   They code 10X faster. Their code just works. They not only know their primary language inside and out, but they also know how it works under the hood. They know the answer to most any question before you ask it. A few of them invented the programming principles we all use. And they tend to be  uncharacteristically humble, as well.  What is it about these folks?  Is there something about their thought process that is fundamentally different from the above-average programmer?  Or are they simply very talented people that work hard? To put it another way: How can I be like them?  I know what I think I need to learn to be that good, but it seems like it will take me the next ten years to learn it, and then my knowledge will be obsolete. ",
        "Best answer": " Humble: An exceptional programmer will never claim their code is the best, in fact they will always be looking for a better way (Every chance they get.). Patient: An exceptional programmer will have boundless patience (This does not mean they will waste days on a problem. See: Troubleshooter). Troubleshooter: An exceptional programmer will be able to solve a problem in minutes that may take days for your average programmer. Curious: An exceptional programmer will be unable to resist trying to figure out why something occurs. Engineer: An exceptional programmer will engineer systems rather than hobble together a mishmash of frameworks (This does not mean they won't use frameworks.).  "
    },
    {
        "ID": "4654",
        "Question": "I'm tired of how luggish my developments PC is. It's Core2 Duo, 2GB RAM, Seagate ST3500320AS HDD - not the top model, but quite a decent one. Typically I open several copies of Visual Studio 2008, lots of tabs in Firefox, Outlook, MSDN, plus the programs I debug are quite huge, plus whatever Windows thinks it can't live without so I end up with Task Manager showing something like 2,5 GB pagefile usage. All the software above becomes luggish to such extent that it's really annoying. Something like I click on a menubar in Visual Studio - and instead of just opening the menu it works the harddisk for say 10 seconds. I'd like to have some magic \"don't make me think\" solution - so that it is installed once and then the lugs disappear or at least decrease significantly. It should not be very expensive - something like the current price of a hybrid drive. Will a hybrid drive magically help overcome my problem once and for all? Do you have experience using hybrid drives for similar purposes? ",
        "Best answer": "It sounds more to me like getting more RAM in your machine would be the best thing you can do.  "
    },
    {
        "ID": "4714",
        "Question": "It seems to me that rapid-development web platforms are going to radically change the world of web applications. It has been five years since Rails 1.0 was released for Ruby, and since that time we have seen Grails for Groovy, Django for Python, and Roo for Java. But to my knowledge (which is probably limited, being a Java/Groovy progammer) there is no similar framework for C#. Does such a thing exist?  If not, why not? Edit:  It's quite possible I'm not using the right words when I say \"rapid-development,\" but I'm talking about frameworks that can conceivably allow you to build a working blog engine in 30 minutes.  You couldn't reasonably do this with, say, Java, Spring, and Hibernate, given the various configuration needed to allow your controllers to be found, and both configuration and code necessary for your entities to persist and be retrieved.   So I'm talking about frameworks that handle all of the CRUD with a convention-over-configuration mentality.  If someone has the right words for what I'm talking about, let me know. ",
        "Best answer": "I don't know what you mean by \"rapid development web platforms\". The definition of \"rapid development\" that I'm familiar with has nothing to do with languages, paradigms, or frameworks, but rather the use of rapid prototyping and iterative development to produce a system. Any language or framework can be used equally well. I've never used Grails or Roo before, but Django and Rails are both MVC frameworks, so their counterpart in .NET would be ASP.NET MVC. "
    },
    {
        "ID": "5119",
        "Question": "I'm freelancing on a project where I'm the only programmer, and find myself at the end of a line of four middlemen, who stand between me and the actual customer, each passing my work as internal to their own company. Communication is terrible and the requirements, made by an advertising company, are flimsy. I've managed to communicate with people upper the ladder by keeping asking questions that made people face their ignorance, but they won't let me contact the end client since, from his end, it's pretty much a done deal.  The project will soon be over though, and I've decided it's the last time I'll be working under these conditions. The middlemen, are pretty much useless from the perspective of shipping a product, but still necessary to me since they are the ones bringing the contracts in. Hence I'm not thinking about crossing them altogether, which would probably end badly. Rather I'm looking for a way to make them understand I need to be part of the requirements and design process, meet the clients, and shouldn't have to go through a whole channel of clueless people each time I require some information. Sorry for the venting :) Any ideas ? ",
        "Best answer": "Sell the middlemen on giving some progress demos to the client then lead the client into some of the issues that you are facing during the demo. "
    },
    {
        "ID": "5120",
        "Question": "Let me explain this a little. In a previous job, I had a coworker that has a good reputation with the management. He always finished on time. And the bosses were happy with his progress so het got certain privileges. The problem was that the other programmers knew his secret. He has optimized the 80/20 rule, so he worked his 20 percent time to finish 80 percent of the code. The other (hard) 20% was left to the maintenance programmers. Who (not surprisingly) got penalized because of their lack of progress. But because this programmer had a good reputation with the management, it was almost imposible to shift the blame to him. (Fortunately he left the company). My question is, what to do as programming team if you have such a programmer within your team. Do you try to warn management with risk of ruining your own chances? Do you accept the fact? Or are there other options. ",
        "Best answer": "Try to implement a code review team. It sounds like this programmer was working solo on a project with no team interaction. I'd try to encourage a more team-based workflow so that he can't just stomp over everything and then leave it on your door. "
    },
    {
        "ID": "5225",
        "Question": "Suppose I develop a useful library and decide to publish it as open source. Some time later I have a business need to do something that wouldn't comply with the open source licence. Am I allowed to do that? How should I publish the software in a way that I keep ownership and don't block myself from using the library in the future in any way? Keep in mind that at least in theory, other developers may decide to contribute to my open-source project. Can I specify in a licence that I as the original developer get ownership of their contributions as well? Don't get me wrong here, I'm not trying to be evil and get ownership of other's work - I just want to keep ownership of mine, and if someone posts an important bugfix I could be rendered unable to use the original code unless I use his work as well. ",
        "Best answer": "You always keep ownership under open-source licenses.  The work you created is your property, and you can do whatever you want to with it, (within legal limits, of course,) including allowing other people to use it under the terms of an open-source license.  If you want to use it for a proprietary project, you're welcome to do so, unless you have completely turned over the rights to someone else by contract.  But this is not what open-source licenses do.  They're about sharing usefulness, not about giving up ownership. Things get a bit sticker once other people start contributing.  It's their work, then, not yours, and you need to get their permission.  One thing you can do is publish your library under a dual license.  That's what Sam Lantinga, the primary creator and maintainer of SDL, does.  Because Apple doesn't like dynamic link libraries for iOS, and complying with the LGPL in a statically linked app is more trouble than it's worth, he publishes SDL under both the LGPL and a commercial license for static iPhone apps.  When anyone submits a patch, he explicitly asks them for permission to deploy their patch in the library under both licenses, and if they don't like that, he doesn't add it to the codebase. EDIT: My example is no longer accurate.  A while back Sam changed the model (not sure why; maybe he just got tired of the administration hassles) and now licenses SDL under a highly permissive zlib-style license.  But he used to do it this way. "
    },
    {
        "ID": "5297",
        "Question": "I've only been a year in the industry and I've had some problems making estimates for specific tasks. Before you close this, yes, I've already read this: How to respond when you are asked for an estimate? and that's about the same problem I'm having. But I'm looking for a more specific gauge of experiences, something quantifiable or probably other programmer's average performances which I should aim for and base my estimates.  The answers range from weeks, and I was looking more for an answer on the level of a task assigned for a day or so. (Note that this doesn't include submitting for QA or documentations, just the actual development time from writing tests if I used TDD, to making the page, before having it submitted to testing) My current rate right now is as follows (on ASP.NET webforms):  Right now, I'm able to develop a simple data entry page with a grid listing (no complex logic, just Creating and Reading) on an already built architecture, given one full day's (8 hours) time.   Adding complex functionality, and Update and Delete pages add another full day to the task. If I have to start the page from scratch (no solution, no existing website) it takes me another full day. (Not always) but if I encounter something new or haven't done yet it takes me another full day.  Whenever I make an estimate that's longer than the expected I feel that others think that I'm lagging a lot behind everyone else. I'm just concerned as there have been expectations that when it's just one page it should take me no more than a full day. Yes, there definitely is more room for improvement.  There always is. I have a lot to learn. But I would like to know if my current rate is way too slow, just average, or average for someone no longer than a year in the industry.  ",
        "Best answer": "If you're programming for a job, and your superiors are happy with the rate you're turning stuff out at, then I'd say you're doing fine.  As you've lasted a year, they're clearly not outraged with your output.  Also, you've only been there a year, and assuming they've been managing people for more than a day, they know that there's a learning curve when you're still green. As for estimates... I've been in the industry for 5 years now (certainly not veteran territory, I know!), and my personal estimates still suck.  I overestimate almost as often as I underestimate, and I do both far more than I get it right.  Something will come up, somewhere, and bite you.  Sometimes you'll find a library that does everything you thought you had to do yourself, and a week's work disappears in half a day.  Other times a stupid bug will stretch a day's work out to 2, 3, 4... If you're repeating a lot of the same work over and over, and you feel like you've maxed out your throughput on it, maybe you should ask to be moved to another task. 'Cross-pollination' and other PHB-friendly terms are definitely of benefit to devs.  If you spend a month or more on something else, maybe you'll find something you're better suited to.  If not, or you're not able to stay away from webforms, the change won't do you any harm, and you might come back with a bit more knowledge and experience that will help you. "
    },
    {
        "ID": "5331",
        "Question": "So you take a contract where you have solid experience with 75% of the technology necessary.  How do you handle your time to learn the other 25%? Work it into the billing time?  Expose the 25% in the contract as 'research'?  Do the learning on my own time (not billed)?  Not take the contract (too large of an unknown for me and the customer)? On the extreme end of this, I keep hearing a story about Mark Cuban (Dallas billionaire who started broadcast.com and sold it to Yahoo!) when he was at Indiana University.  Someone asked him if he could build a business app for them and he immediately said \"Yes\"... he had no idea how.  So he bought a book, stayed up nights, studied and coded... He finished it (I'm sure it was ugly), it worked and he kept going. I'm not suggesting doing contracts this way (the stress!), but there's a middle ground.  What is it, and how would you (or would you?) bill for the unknown? ",
        "Best answer": "If I'm learning something that I'll take away with me (like say a mainstream new API, or a new feature of .NET or a language that's somewhat useful) then I don't bill, I consider that time time spent sharpening my saw, and it's not the client's fault I didn't know that stuff yet. Now, if it's something obscure, I bill for it at my normal rate.  Some examples:  APIs and protocols which are not mainstream (industry specific, small 3rd party or just niche products); internal tools, configuration formats and services inside the client organization; a non-standard database schema, database query language or security model; etc. I've never had any objections about the way I do this, and I'm very transparent about it in my proposals. "
    },
    {
        "ID": "5356",
        "Question": "When you are setting up your system landscape for large and/or multiple application deployments, do you consider mainframe?  If not, why not?  If so, what factors are you considering. If you take a real TCO look at large ERP and/or consolidated application landscapes, mainframe is actually quite cost-effective. My own consultations have included recommendations for scale-up/mainframe/mid-size systems on some specific needs.  Honestly, I've never had a customer take said recommendation, rather defaulting to countless scale-out VMs on Intel boxen (in non-trivial cost) and yet still to this day have system management and performance issues. Curious your take on this.  We need to remember that the virtual machines we manage (and apparently love in IT departments) today have been done for decades on mainframe.  Most mid-size and mainframe shops have small fractions of support persons managing larger and more complex applications. Your thoughts appreciated. ",
        "Best answer": "It seems to me that you're doing more to express your opinion (\"If you take a real TCO look at large ERP and/or consolidated application landscapes, mainframe is actually quite cost-effective.\") than really ask a question. On that basis, I'm tempted to vote to close, but won't. As for the question (to the extent there is one), I'm going to assume that by \"mainframe\", you mean something like an IBM z series machine, rather than (for example) one of the big Fujitsu SPARC boxes. I think for many people, it's hard to recommend mainframes for a couple of reasons.   Every vendor has TCO numbers to \"prove\" their product is the best. Why should somebody trust numbers from IBM more than from Microsoft, Oracle, etc? Even if a mainframe really would make sense, such a recommendation is unlikely to be taken seriously, and whoever made such a recommendation will often be treated as a pariah. Even if the TCO would theoretically work out better in some respects, introducing machines with which existing staff are thoroughly unfamiliar would often be a mistake anyway. Using a mainframe would often lose versatility in other ways. Just for example, an Intel box can easily run Windows Terminal Server to make Word and Excel available, which is a bit tougher to do with a mainframe.  "
    },
    {
        "ID": "5415",
        "Question": "Frequently, I have been finding myself overloaded with contracts.  Most of the time, I find myself juggling with at least 2 projects, in addition to the numerous websites I have to upkeep and perform maintenance on.  Unfortunately, many of my clients will expect updates constantly - are constantly adding more to the to-do list than any one programmer could keep up with, and freaking out because the deadline was already overdue when I started on a project.  I constantly run into the fact most clients do not really understand the amount of work that can be involved behind the scenes, especially if it is non-visually-impacting. Does anyone know of good ways to handle these situations I might be overlooking? ",
        "Best answer": "Charge more. Learn to say no. Get some help. They're freaking out because they don't know what they are doing and are trying to motivate you. Every feature 'must' be included. They're all #1 priority. And they were due yesterday. Basically their boss is on their case. Take control of the project and bring some sanity to the situation. Start with a small piece of the project. Make a plan. Set a time-frame for your development and for their part (reviews, testing, approval, etc.). When they want to change the plan, just ask them, \"What other part should I remove or how far should I backup the due date?\" Remind them that this is what you agreed on and you don't mind making changes, but something has to give. This should help create a history of what they should expect from you in the future. So far, you haven't mentioned that anyone is trying to dump you. You must be doing something right or you found clients no on else wants. Maybe you could dump the clients and stick it to your competition ;) Edit: The art of saying no is in your mind you're saying no, but don't actually use the word. Features, Time, and resources are a constant compromise. It is important to let the client know the problems and don't just assume they will expect the form to take longer to load when you add 50 more fields. "
    },
    {
        "ID": "5452",
        "Question": "I've enjoyed a number of (fiction/non-fiction books) about hacker culture and running a software business in the 80's, 90's. For some reason things seemed so much more exciting back then. Examples are:  Microserfs (Douglas Coupland) Accidental Empires (Robert X. Cringely Almost Pefect (W.E. Peterson, online!) Coders at Work (Peter Seibel)  Today I'm an entrepeneur and programmer. Back in the 80's a I was a young geek hacking DOS TSR's and coding GWBasic / QBasic. In the 90's I was a C.S. university student, experiencing the rise of the Internet world wide. When reading these books running a software business seemed so much more fun than it is nowadays. Things used to be so much simpler, opportunities seemed to be everywhere and the startups seemed to work with much more real problems (inventing spreadsheets, writing word processors in assembly on 6 different platforms) than all our current web 2.0 social networking toys. Does anyone share these feelings? Does anyone have any good (personal) stories from back then or know of other good books to read? ",
        "Best answer": " In no particular order:  The Fugitive Game: Online with Kevin Mitnick The Cuckoo's Egg: Tracking a Spy Through the Maze of Computer Espionage  Insanely Great: The Life and Times of Macintosh, the Computer That Changed Everything Where Wizards Stay Up Late: The Origins Of The Internet  CYBERPUNK: Outlaws and Hackers on the Computer Frontier The Watchman: The Twisted Life and Crimes of Serial Hacker Kevin Poulsen  Takedown: The Pursuit and Capture of Kevin Mitnick, America's Most Wanted Computer Outlaw-By the Man Who Did It  Geeks: How Two Lost Boys Rode the Internet Out of Idaho  Soul of a New Machine - about DEC and one of their products in development. The Hacker Crackdown: Law and Disorder on the Electronic Frontier by Bruce Sterling tells the story of the 'meeting' of law enforcement and the cracker/phreaker subculture of the 1990s. Also, it describes in detail the Secret Service raid on Steve Jackson Games. That little incident almost put SJG out of business, all for a role-playing supplement not, as the Secret Service described it, a \"hacker's manual\".  Turns out that the Secret Service were actually after copies of a leaked Bell South E911 document. Programmers at Work - This likely inspired the Founders at Work and \"Coders at Work\" books. Free as in Freedom: Richard Stallman's Crusade for Free Software by Sam Williams.  It is a free biography of Richard Stallman, and contains lots of stories of programming culture at MIT.  If you're interested in the FSF and how the whole free software movement started this is worth a read. Hackers: Heroes of the Computer Revolution by Steven Levy (Wikipedia page on Hackers)  \"describes the people, the machines, and the events that defined the Hacker Culture and the Hacker Ethic, from the early mainframe hackers at MIT, to the self-made hardware hackers and game hackers.\"  Show Stoppers  Startup Founders at Work - interviews with startup founders, starting from the early 80's. It's more about how the founders built up their companies, but it has interesting insights into the programming culture prevalent then as well. The case of IBM 386 PC - A detective story for techies.  "
    },
    {
        "ID": "5494",
        "Question": "We're developing a new project that is to be deployed on a large number of client sites. The project contains a web-based GUI as one of it's \"access points\". Speed of the web interface is a priority for this project, second only to security. In the past, we've always created \"web sites\" in Visual Studio, which, when published, results in one dll and one aspx file for each page within the system. However, I am aware that you can actually create a \"web application\" and have it compile everything down to a single dll. To me (based on no real data, just gut feeling), compiling the site as a single dll sounds like it would be better for both security and speed (if only marginally). What are the considerations we should look at, and are there any obvious pitfalls we should be aware of when choosing the method that's right for us? ",
        "Best answer": "If it is a large application, there should be natural areas of division in your business logic (software tiers, if you will) which can go in their own DLL's.   It would be nice if the core website functionality could go into a single DLL.  It eases deployment concerns, and is a natural unit anyway. One DLL per page seems excessively granular. "
    },
    {
        "ID": "5749",
        "Question": "I work as the back-end developer, front-end developer, systems admin, help desk and all-around 'guy who knows computers' at a small marketing company of about 15 people.  I was wondering if others could share their experiences flying solo at companies that aren't necessarily inclined toward the technology industry.  I originally took the job in order to transition from front-end developer/designer to full-time coder. It's been a good experience to a point. I definitely get to occupy the role of 'rock star' programmer - because frankly, no one really understands my job. Lately, it feels like a very solitary position. I rarely get to bounce ideas off of people, and everyone looks to me like I have magic powers that will make all the computers work and land us first on Google searches. I've also felt a strong disconnect versus between what we say we want (projects with large, months-long development schedules) versus what we actually do (copy-edit our sites over and over). So who else finds themselves being the 'tech guy' in a company that thinks technology is all a bit magical, and what is your take on your situation? ",
        "Best answer": "Take advantage of the situation you have - to a certain extent, I think you have a little bit of \"grassisgreeneritis\".  Sorry, I'm not trying to be funny.  What I am saying is every position at every company has short-comings.  Yours are starting to get to you more because they are very familiar.  But, at tech companies, schedules and time commitments become an issue.  At larger non-tech companies, overcoming political stupidity and procedure can be big issues. So take advantage of what you have now; learn what you can.  Once you believe you can't really learn more, it is probably time to move on.  There is no harm in that; it sounds like you are one of those people that have to grow to be happy with a job. Your current company should understand that when you reach that point and honestly, if they don't, leaving is definitely the right thing to do. Having said all that, there is more you can do in your current position.   If you are feeling solitary, make some changes to eliminate that feeling.    Use on-line communities to bounce ideas off of people (StackOverflow is great for this). Do some research with Google to find out what it would take to land your company first and then put a proposal together to get it to happen.   When going through projects, take the initiative and change how things happen.  Don't go for the impractical, long projects. Instead, propose month long incremental improvements.  Over a year, those add up and can really help you feel like you've accomplished something.   "
    },
    {
        "ID": "5757",
        "Question": "I have found that there are only 3 ways to unit test (mock/stub) dependencies that are static in C#.NET:  Moles TypeMock JustMock  Given that two of these are not free and one has not hit release 1.0, mocking static stuff is not too easy. Does that make static methods and such \"evil\" (in the unit testing sense)?  And if so, why does resharper want me to make anything that can be static, static? (Assuming resharper is not also \"evil\".) Clarification: I am talking about the scenario when you want to unit test a method and that method calls a static method in a different unit/class.  By most definitions of unit testing, if you just let the method under test call the static method in the other unit/class then you are not unit testing, you are integration testing.  (Useful, but not a unit test.) ",
        "Best answer": "Looking at the other answers here, I think there might be some confusion between static methods that hold static state or cause side-effects (which sounds to me like a really bad idea), and static methods that merely return a value.   Static methods which hold no state and cause no side effects should be easily unit testable.  In fact, I consider such methods a \"poor-man's\" form of functional programming; you hand the method an object or value, and it returns an object or value.  Nothing more.  I don't see how such methods would negatively affect unit testing at all. "
    },
    {
        "ID": "5951",
        "Question": "A well-tested codebase has a number of benefits, but testing certain aspects of the system results in a codebase that is resistant to some types of change. An example is testing for specific output--e.g., text or HTML. Tests are often (naively?) written to expect a particular block of text as output for some input parameters, or to search for specific sections in a block.  Changing the behavior of the code, to meet new requirements or because usability testing has resulted in change to the interface, requires changing the tests as well--perhaps even tests that are not specifically unit tests for the code being changed.  How do you manage the work of finding and rewriting these tests? What if you can't just \"run 'em all and let the framework sort them out\"? What other sorts of code-under-test result in habitually fragile tests?  ",
        "Best answer": "I know the TDD folks will hate this answer, but a large part of it for me is to choose carefully where to test something. If I go too crazy with unit tests in the lower tiers then no meaningful change can be made without altering the unit tests.  If the interface is never exposed and not intended to be reused outside the app then this is just needless overhead to what might have been a quick change otherwise. Conversely if what you are trying to change is exposed or re-used every one of those tests you are going to have to change is evidence of something you might be breaking elsewhere. In some projects this may amount to designing your tests from the acceptance tier down rather than from the unit tests up. and having fewer unit tests and more integration style tests. It does not mean that you cannot still identify a single feature and code until that feature meets its acceptance criteria.  It simply means that in some cases you do not end up measuring the acceptance criteria with unit tests. "
    },
    {
        "ID": "5972",
        "Question": "This has nothing to do with having a favourite editor or anything like that. I was just wondering, per language, what are the most popular Integrated Development Environments? Maybe a top 2-3 if there is some contention. (Perceived popularity is enough) Thus Far: C# - Visual Studio, SharpDevelop Java - Eclipse, NetBeans, IDEA Objective-C - Xcode Delphi - RAD Studio Object Pascal - Delphi, Lazarus C, C++ - Visual Studio, Vim PL/SQL - RapidSQL, Oracle SQLDeveloper PHP - Eclipse, NetBeans, Nusphere PHPed Actionscript (AS2, AS3) - FlashDevelop Flex - Flash Builder 4 Python - Eclipse, IDLE Perl - Padre Common Lisp - Lispworks, Emacs Ruby - TextMate Haskell - Vim Fortran - Vim Visual Basic - Visual Studio ",
        "Best answer": "All languages - VIM I don't like IDE's.  If I'm on OSX I'll use TextMate at time, but mostly I do everything (JavaScript, Java, Python, PHP) in VIM. I'm also quicker then several colleagues who use an IntelliJ. "
    },
    {
        "ID": "6005",
        "Question": "To the outside world, programmers, computer scientists, software engineers, and developers may all seem alike, but that's far from the case for the people who create software for a living.  Any single programmer's ability and knowledge can range very widely, as well as their tools (OS, language, and yes, preferred editor), and that diversity spawns many sub-cultures in software - like programmers who actively use Stack Overflow and this site, versus many more who don't. I'm curious to hear from others what software sub-cultures they've encountered, belonged to, admired, disliked, or even created.  For starters, I've encountered:  Microsoft-driven companies and developers: their entire stack is from Redmond, WA.  E-mail is Outlook is e-mail.  The web is IE and IIS.  They have large binders of their MS Developer Network subscription full of multiple versions of VB, .net, Visual Studio, etc.  Avoids working with a shell/command-line.  Don't see what the fuss with open-source and such is all about.  MS-centric companies tend to be 9-5 and quite corporate (driven by business managers, not software people). Nowadays (given the wide availability of non-MS tools), this is the antithesis of hacker culture. Old-school CS people: they often know Lisp and Unix extremely well; sometimes, they may have written a semi-popular Lisp themselves, or a system utility. Few, if any, \"software engineering\" things are new to them, nor are they impressed by such.  Know the references, history, and higher-level implications of programming languages like Lisp, C, Prolog, and Smalltalk.  Can be bitter about AI outcomes of the 80's and 90's. Tend to be Emacs users.  Can type out multi-line shell commands without blinking an eye.  Their advice can by cryptic, but contains gold once understood. New-school web developers: played with computers and video games growing up, but often only really started programming in the late '90s or early '00's.  Comfortable with 1 to 1.5 scripting/dynamic languages; think C and languages outside of Ruby/Perl/Python are unnecessary/magical.  May have considered HTML as programming initially.  Tend to get a Mac and be fanatical/irrational about it.  Use frameworks more than build them.  Often overly-enthusiastic about NoSQL and/or Ruby On Rails. New-school CS: lots of training in statistics, Bayesian models and inference; don't say \"AI,\" say \"machine learning.\" More Java than Lisp, but could also be expert Haskell programmers.  Seeing major real-world successes by experts in their field (Google, finance/quants) often makes them (over) confident.  But big data, and the distributed processing of such, really are changing the world.  The examples above are by no means complete, correct, orthogonal, or objective. :)  Just what I've seen personally, and provided to spark some discussion and outline of the broader question.  Feel free to disagree! ",
        "Best answer": "I'd consider myself part of the Real-Time Systems group.   There are some 'Old School' characteristics but with less focus on CS, more on hardware.   The archetype:  Has expert knowledge of 'C'  Has an original copy of K&R Writes in other languages as if they were just an alternate syntax for 'C'  Can predict the assembler output from their code. Can read a circuit diagram Doesn't know how to write code without doing 'premature optimization'. Is quite comfortable with the command line.  "
    },
    {
        "ID": "6014",
        "Question": "A lot of us started seeing this phenomenon with jQuery about a year ago when people started asking how to do absolutely insane things like retrieve the query string with jQuery.  The difference between the library (jQuery) and the language (JavaScript) is apparently lost on many programmers, and results in a lot of inappropriate, convoluted code being written where it is not necessary. Maybe it's just my imagination, but I swear I'm starting to see an uptick in the number of questions where people are asking to do similarly insane things with Linq, like find ranges in a sorted array.  I can't get over how thoroughly inappropriate the Linq extensions are for solving that problem, but more importantly the fact that the author just assumed that the ideal solution would involve Linq without actually thinking about it (as far as I can tell).  It seems that we are repeating history, breeding a new generation of .NET programmers who can't tell the difference between the language (C#/VB.NET) and the library (Linq). What is responsible for this phenomenon?  Is it just hype?  Magpie tendencies?  Has Linq picked up a reputation as a form of magic, where instead of actually writing code you just have to utter the right incantation?  I'm hardly satisfied with those explanations but I can't really think of anything else. More importantly, is it really a problem, and if so, what's the best way to help enlighten these people? ",
        "Best answer": "It's basically because programming is fundamentally difficult.  It requires a lot of logical, structured thought in a way that a lot of people just don't know how to do.  (Or simply can't do, depending on who you listen to.) Stuff like LINQ and jQuery makes certain common data-manipulation tasks a whole lot easier.  That's great for those of us who know what we're doing, but the unfortunate side effect is that it lowers the bar. It makes it easier for people who have no idea what they're doing to start writing code and make things work.  And then when they run into reality, and find something fundamentally difficult that their simple, high-abstraction-level techniques are not well suited to, they're lost, because they don't understand the platform that their library is built upon. Your question is sort of on the right track, but much like the perennial controversy about violent video games \"turning kids violent,\" it has the direction of the link backwards.  Easy programming techniques don't make programmers stupid; they just attract stupid people to programming.  And there's really not much you can do about it. "
    },
    {
        "ID": "6133",
        "Question": "There are some really common usability errors in everyday software we used; errors that result from the ways the particular programmer has learned without learning of all the other ways there are. For example, talking about Windows software in particular, the following common flaws come to mind:  Failure to support multiple screens. For example, windows centered in the virtual desktop (instead of a specific screen) and hence displayed spanning the monitor boundary in a dual monitor setup. Failure to support serious keyboard users. For example, utterly messed up tab order; duplicate or completely missing accelerator keys. Alt+Tab order mess-ups. For example, a window that doesn't go to the end of the tab order when minimized. Subtle breakage of common controls that were reimplemented for one reason or another. E.g. failure to implement Ctrl+Left/Right on a textbox; failure to add an Alt+Space window menu to a skinnable window, failure to make Ctrl+Insert copy to clipboard, etc, etc. This one is a huge category in its own right.  There are a gazillion of things like this. How can we ever make sure we don't break a large proportion of these? After all they aren't all written down anywhere... or are they? ",
        "Best answer": "I think one thing to keep in mind is to remember the source reason for most software usability problems — usability is a human issue, and as such, is difficult to define with a set of rules.  This is totally at odds with the rules-world that most programmers want to live in. Because of that I think you need to throw out the belief that a checklist of usability problems could ever be helpful.  Believing that is thinking like a programmer and will only result in more usability problems that you simply hadn't thought of (or that are the result of sticking to a usability \"rule\" that never really should have been a rule).  One of the biggest differences can be made by designing first (read Alan Cooper's The Inmates are Running the Asylum). Second, make sure your software goes through usability testing with real users. Third, don't think like a programmer. The possible idea in your question is a perfect example of why this is important to remember.  Even good ideas (avoiding non-standard controls) are not always going to hold true.  Those controls may be faulty themselves or may be used for something they shouldn't.  The perfect solution for your form or user input may not have been invented yet, or may simply be not widely used or implemented (the iPhone is a great case study for this).  As another illustration of the problem with \"usability checklists\", the list you presented may well be common to you, and I agree they are problems, but I hadn't really thought of most of them prior to reading your list.  On the other hand, I've experienced tons of annoyances with Windows since being given a PC for my day-job:  (Windows 7) Mousing over a task bar button and then over a Window thumbnail drops all other windows to just outlines.  I've done this by accident a number of times and had what I was reading simply disappear. The Alt+tab pane can now be moused over to temporarily bring the window to the front.  However, when you let go you think it's going to stay there, and it doesn't. (MS Outlook - I know, sort of a cheap shot) If I open a new email and then close it, even if I haven't added any text it asks me if I want to save the draft.  Then if I say no, it moves it to my deleted items folder.  As an unread email, leaving me with a big blue \"(1)\" until I go delete it or \"read\" it.  "
    },
    {
        "ID": "6146",
        "Question": "I've recently been frequented by erroneous error messages from mod_security. Its filter sets cover outdated PHP exploits, and I have to rewrite my stuff because Wordpress&Co had bugs years ago.   Does this happen to anyone else?  Apache mod_security blocks possibly   dangerous HTTP requests before they   reach applications (PHP specifically).   It uses various filter sets, mostly   regex based.  So I have a nice shared hosting provider, technically apt and stuff. But this bugged me: Just last week I had to change a parameter name &src= in one of my apps because mod_security blocks ANY requests with that. I didn't look up its details, but this filter rule was preventing the exploitability of another app which I don't use and probably never had heard about. Still I had to rewrite my code (renaming parameter often suffices to trick mod_security) which had nothing to do or in common with that! And today, a silly regex blocks form submissions, because I wanted to submit php sample code. Given, this is the simple stuff that mod_security is there to protect against. But I don't believe mod_security can detect seriously obfuscated code, and just goes off at obvious (and in this case totally trivial) php snippets. Basically I'm getting penalized by mod_security because other people released bug-prone apps. (Not saying my apps are ultra secure - I'm pretty security wary, but make no hyperbolic claims.) I've already asked my provider to disable it anyway, the benefits are too minuscle IMO and for my apps.  What do you think? Does mod_security make much sense outside of WP hosting? Or is it really just a bunch of blacklists of long passed security bugs? Which of its rules are actually helpful? Is there an application level equivalent? ",
        "Best answer": "I personally see mod_security as a patch. I use it on some of our servers where we can't control the code that's uploaded (shared hosting servers, for example), but it's never really felt like a good solution to me. Based on it's wide and very general blacklist approach, it's more of a patch to cover up security holes than a good security policy. It can also provide a false sense of security. mod_security can reveal some common attacks but can by no means prevent any attack. Again, it's a blacklist of common known attacks. If you simply install mod_security and think that you're magically secure, you're horribly mistaken. I have found a much better policy to me managed servers where my team reviews all code that is placed on them, combined with lots of logs, logfile analysis, reporting systems, and intrusion detection/intrusion prevention systems (IPS). Everytime third-party or open-source software is installed (I'm looking at you, WordPress!) we keep a log of where it was installed, and when new versions are released we update every copy that was installed. Again, you're more likely to find mod_security on a shared hosting server, as you're experiencing now. As you grow you can move to a VPS or clod based hosting provider where you get your own managed environment and can more tightly control the available software. "
    },
    {
        "ID": "6166",
        "Question": "To quote Arthur C. Clarke:  Any sufficiently advanced technology is indistinguishable from magic.  Used to be I looked on technology with wonder and amazement. I wanted to take it apart, understand how it worked, figure it all out. Technology was magical. I'm older, I know more and I spend my days creating stuff that, hopefully, fills other people with that kind of wonder. But lately I've found my own awe for technology has been seriously curtailed. More often I'm just annoyed that it isn't as elegant or seamless or as polished or perfectly delivered as it seemed to be in my youth. It all looks broken and awkward, or cobbled together and poorly tested. Has programming ruined your ability to enjoy technology? Have you stopped wondering in awe and just started saying, \"They could have done this better\" every time you pick up a bit of technology? ",
        "Best answer": "It has ruined my ability to enjoy technology in fiction. I can suspend my disbelief whilst the hero of the [book / film / drama] can withstand numerous karate kicks, fire an infinite number of bullets, leap across a 50ft gap between two buildings, fall from a great height onto a pile of conveniently stacked boxes etc. What makes me shout at the screen in disbelief is when the hero then steps up to a computer, and:   performs a search with some application that has more apparent power than google. hacks into a supposedly secure system with a few key presses and a wink. copies the entire hard disk to a memory stick in a matter of seconds with a convenient \"% complete\" window (which just happens to work with the operating system of the computer he's copying) does anything that involves zooming an image from a CCTV camera to get a high resolution print out of the suspects face.  AAAARHG!!!!  "
    },
    {
        "ID": "6246",
        "Question": "Most software developers want to keep application logic in the application layer, and it probably feels natural for us to keep it here. Database developers seem to want to put application logic in the database layer, as triggers and stored procedures. Personally I would prefer to keep as much as possible in the application layer to make it easier to debug and keep the responsibilities of the layers separate. What are your thoughts on this, and what should or should not be ok to implement in the database layer? Edit This question is also covered on dba.se, from the DBAs perspective. As programmers.se & dba.se have different audiences and biases, future readers might want to review both sets of answers before deciding what works best for them. ",
        "Best answer": "Off the top of my head, advantages of putting logic in the application layer.  Testability.  This should be a good enough reason on it's own actually. Better code structure.  It's very difficult to follow proper OO-architecture with SQL.  This usually also makes the code easier to maintain. Easier to code.  Due to all the different language features available in whatever language you're using it's usually easier to code in the application layer. Code re-use.  It's a whole lot easier to share code with libraries than sharing code in the database.  "
    },
    {
        "ID": "6268",
        "Question": "Just read the question about the Big Rewrites and I remembered a question that I've been wanting answered myself. I have a horrible project passed down to me, written in old Java, using Struts 1.0, tables with inconsistent relationships, or no relationships at all and even tables without primary keys or fields meant to be primary keys but aren't unique at all. Somehow most of the app \"just works\". Most of the pages are reused (copy pasted code) and hard-coded. Everyone who's ever worked on the project has cursed it in one form or the other. Now I had long considered to propose to upper management a total rewrite of this horrendous application.  I'm slowly attempting to on personal time but I really feel that this deserves some dedicated resources to make it happen. Having read the articles on big rewrites I'm having second thoughts. And that's not good when I want to convince my superiors to support my rewrite. (I work in a fairly small company so the proposal has the possibility of being approved) TL;DR When is a big rewrite the answer and what arguments can you use to support it? ",
        "Best answer": "Sorry, this is going to be long, but it's based on personal experience as both architect and developer on multiple rewrite projects. The following conditions should cause you to consider some sort of rewrite.  I'll talk about how to decide which one to do after that.  Developer ramp-up time is very high.  If it takes any longer than below (by experience level) to ramp up a new developer, then the system needs to be redesigned.  By ramp-up time, I mean the amount of time before the new developer is ready to do their first commit (on a small feature)   Fresh out of college - 1.5 months Still green, but have worked on other projects before - 1 month Mid level - 2 weeks Experienced - 1 week Senior level - 1 day  Deployment cannot be automated, because of the complexity of the existing architecture Even simple bug fixes take too long because of the complexity of existing code New features take too long, and cost too much because of the interdependence of the codebase (new features cannot be isolated, and therefore affect existing features) The formal testing cycle takes too long because of the interdependence of the existing codebase. Too many use cases are executed on too few screens. This causes training issues for the users and developers. The technology that the current system is in demands it   Quality developers with experience in the technology are too hard to find It is deprecated (It can't be upgraded to support newer platforms/features) There is simply a much more expressive higher-level technology available The cost of maintaining the infrastructure of the older technology is too high   These things are pretty self-evident. When to decide on a complete rewrite versus an incremental rebuild is more subjective, and therefore more politically charged. What I can say with conviction is that to categorically state that it is never a good idea is wrong. If a system can be incrementally redesigned, and you have the full support of project sponsorship for such a thing, then you should do it.  Here's the problem, though. Many systems cannot be incrementally redesigned. Here are some of the reasons I have encountered that prevent this (both technical and political).  Technical   The coupling of components is so high that changes to a single component cannot be isolated from other components.  A redesign of a single component results in a cascade of changes not only to adjacent components, but indirectly to all components. The technology stack is so complicated that future state design necessitates multiple infrastructure changes. This would be necessary in a complete rewrite as well, but if it's required in an incremental redesign, then you lose that advantage. Redesigning a component results in a complete rewrite of that component anyway, because the existing design is so fubar that there's nothing worth saving. Again, you lose the advantage if this is the case.  Political   The sponsors cannot be made to understand that an incremental redesign requires a long-term commitment to the project. Inevitably, most organizations lose the appetite for the continuing budget drain that an incremental redesign creates. This loss of appetite is inevitable for a rewrite as well, but the sponsors will be more inclined to continue, because they don't want to be split between a partially complete new system and a partially obsolete old system. The users of the system are too attached with their \"current screens.\" If this is the case, you won't have the license to improve a vital part of the system (the front-end). A redesign lets you circumvent this problem, since they're starting with something new. They'll still insist on getting \"the same screens,\" but you have a little more ammunition to push back.   Keep in mind that the total cost of redesiging incrementally is always higher than doing a complete rewrite, but the impact to the organization is usually smaller.  In my opinion, if you can justify a rewrite, and you have superstar developers, then do it. Only do it if you can be certain that there is the political will to see it through to completion.  This means both executive and end user buy-in.  Without it, you will fail. I'm assuming that this is why Joel says it's a bad idea.  Executive and end-user buy-in looks like a two-headed unicorn to many architects. You have to sell it aggressively, and campaign for its continuation continuously until it's complete.  That's difficult, and you're talking about staking your reputation on something that some will not want to see succeed. Some strategies for success:  If you do, however, do not try to convert existing code. Design the system from scratch. Otherwise you're wasting your time. I have never seen or heard of a \"conversion\" project that didn't end up miserably. Migrate users to the new system one team at a time. Identify the teams that have the MOST pain with the existing system, and migrate them first. Let them spread the good news by word of mouth. This way your new system will be sold from within. Design your framework as you need it. Don't start with some I-spent-6-months-building-this framework that has never seen real code. Keep your technology stack as small as possible. Don't over-design. You can add technologies as needed, but taking them out is difficult. Additionally, the more layers you have, the more work it is for developers to do things. Don't make it difficult from the get-go. Involve the users directly in the design process, but don't let them dictate how to do it. Earn their trust by showing them that you can give them what they want better if you follow good design principles.  "
    },
    {
        "ID": "6442",
        "Question": "Assuming the language provides it, what are some things to do or libraries to use that every programmer should know? My list would be  Regular Expressions Named Pipes standard IO (std in/out/error) Executing outside executables (like imagemagik and your own scripts) How to grab an HTTP page as a string (mostly for updates and grabbing configs from servers)  I have a similar questions about tools. Also I am looking for specific answers. I don't want answers that can be done in different ways (such as learn how to synchronize threads). I know how to do the above in 3+ languages. There always seems to be things programmers don't know how to do which can make their lives easier. For a long time I didn't know what regular expression was (although I did hear of it) and I was surprised how many people who tried to make a toolchain or complex app and did not know how to (or that you could) get the stdout from an exe they launched (they checked for errors in a very weird way or just didn't and hope it produce the expected results). What do you think is useful, not well known and should be something every senior (or starting out) programmer should know? ",
        "Best answer": "how about basic graphics operations? DrawLine, DrawRectangle, DrawPixel?  "
    },
    {
        "ID": "6476",
        "Question": "Lately I've been experimenting with using a collaborative text editor (or plugin such as NppNetNote for Notepad++) for two or more partners to edit source code. I've been met with unexpected success, and the workflow differs from anything I've ever experienced. My findings:  It's fun to fight over code, but also very satisfying to collaborate on it in real time. Two people can work collaboratively or separately, and be constantly aware of each other's changes. The comments end up becoming a free-form IM conversation about the code around them. The medium is enormously less restrictive than trying to work back and forth over IM proper. It's like pair programming, but with no overhead to switch roles between driver and navigator.  Has anyone tried this before? What were some of the advantages and problems that you encountered? For those that haven't tried it, I strongly encourage you to find a friend (or two, or more!) and make an attempt. Edit: See Wikipedia for something like more explanation, though in the context of pair programming specifically. ",
        "Best answer": "I often use GNU screen to share terminals (and terminal based editors) for pair programming and remote collaboration. I think one of the most important things that can make this go smoothly is a voice connection (phone, VoIP, etc.) with the other collaborators. Without a voice connection, you incur a lot of overhead and clunkiness as you have to IM (preferably in a separate window) at the same time. Short sharing the same terminal, each sharing a separate terminal (her read/my write, her write, my read). This allows for simultaneous use but also prevents you from working on exactly the same file. I've also been using tmux recently (a program similar to GNU screen) and while I find it better in some aspects I find other aspects less desirable. "
    },
    {
        "ID": "6526",
        "Question": "Prevalence is a simple technique to provide ACID properties to an in-memory object model based on binary serialization and write-ahead logging.  It works like this:  Start with a snapshot.  Serialize the object model and write it to a file. Create a journal file.  For every call into the object model, serialize the call and its arguments. When the journal gets too big, you're shutting down, or it's otherwise convenient, perform a checkpoint: write a new snapshot and truncate the journal. To roll back or recover from a crash or power hit, load the last snapshot and re-execute all the calls recorded in the journal.  The precautions needed to make this work are:  Don't let mutable object references escape or enter the prevalence layer.  You need some sort of proxy or OID scheme, as if you were doing RPC.  (This is such a common newbie mistake it's been nicknamed the 'baptism problem'.) All the logic reachable from a call must be completely deterministic, and must not perform business-logic-meaningful I/O or OS calls.  Writing to a diagnostic log is probably OK, but getting the system time or launching an asynchronous delegate is generally not.  This is so that the journal replays identically even if it's restored on a different machine or at a different time.  (Most prevalence code provides an alternate time call to get the transaction timestamp.) Writer concurrency introduces ambiguity in journal interpretation, so it is prohibited.  Is it because ...  people developed a bad taste for them after trying to use one on a project that wasn't well suited* to it?   Klaus Wuestefeld's strident advocacy turned people off? people who like the imperative programming model dislike separating I/O from calculation, preferring instead to interleave computation with I/O and threading calls? prevalence layers are so conceptually simple and so intimately bound to the characteristics of the framework they inhabit that they're usually custom-rolled for the project, which makes them too alien/nonstandard/risky? it's just too hard to keep straight what you have to be careful not to do? newbies' heads just seem to explode when faced with something that isn't the same kind of two-tier database-driven app they learned to write in school? ;)   *The entire data set fits in RAM, you don't need writer concurrency, and you don't need to do ad-hoc queries, reporting, or export to a data warehouse.  With apologies to SQLite, prevalence is an improvement on save-files, not a replacement for Oracle. ",
        "Best answer": "I think you first need to demonstrate that so many developers absolutely hate them. I don't think that's the case. Consider that fowler, a while back, formalized a pattern of sorts for this here. "
    },
    {
        "ID": "6543",
        "Question": "I'm starting to use the ORM recommended by the framework I choose, and though I like the idea of the added layer of abstraction the ORM provides, I'm starting to realize what this really means. It means I'm no longer working with my database (mysql) and any mysql-specific features are gone, out the window, like they don't exist. The idea the ORM has is that it's trying to help me by making everything database agnostic. This sounds great, but often there is a reason why I choose a specific database system. But by going the database agnostic route, the ORM takes the lowest common denominator, which means I end up with the smallest set of features (the ones supported by all databases).  What if I know that for the long term I won't be switching the underlying database? Why not access the database-specific features, too? ",
        "Best answer": "I see it the same way.  Any abstraction that doesn't allow you to get underneath it when necessary is evil, because it leads to all sorts of ugly abstraction inversions in your code. At work we've got a homegrown ORM that works fairly well, but for the times when we need something that its features don't explicitly provide for, there's a method that takes a string and drops it directly into the query it's generating, allowing for the use of raw SQL when it's necessary. IMO any ORM that doesn't have this feature isn't worth the bits it's compiled from. "
    },
    {
        "ID": "6591",
        "Question": "I'm trying to understand how I should evaluate writing a book, article, or conference presentation. Writing a book is a lot of work. Same for writing an article in a magazine or presenting in a conference. They need time and you may even make a mistake here and there that back fires (bad reviews, people calling you an idiot..) Also you do it for free (at least for magazine articles and conference presentations. For books you get something like a $5K deposit and you rarely get any additional sales royalties after that). So how should I evaluate the benefits? I would appreciate answers that call out if you have done this before. I may not write a book because it's way beyond what I'd like to commit time-wise, but should I bother giving conference presentations or writing shorter articles in magazines? ",
        "Best answer": "It all depends: what are your goals? ** [Note: my background is as a programmer, but I've been making a living as a tech writer/speaker for the last 12 years. After 15 titles, dozens of magazine articles, and speaking internationally, I think I'm at least as qualified as anyone else here.] ** If your goal is to make money, don't bother. Really. I know a lot of people in this business, and very few make a decent hourly wage from writing. Of the ones who do make a living at it, all of them write for beginners (tip: there are always more beginners than  intermediate or advanced users). However… IF you're currently working as a consultant and if you want more consulting gigs with bigger companies at a higher price and if you've been offered a book contract and/or speaking gigs … then go for it. Don't think of it in terms of work with low compensation; instead, think of it as just part of the training and prep you already do in order to get those consulting jobs. Screw writing articles for magazines/sites that don't pay — or say you'll write for them, on the condition that they run your article without ads. If they're making money, you should be too. However, if the magazine helps you get those high-profile consulting gigs, see the advice in the previous paragraph. ** Speaking gigs, though, are almost always worth it. At a minimum, you'll meet other presenters, which is how I've met some truly amazing people. Networking opportunities abound. ** On the other hand… IF you have an amazing idea for a great book that no one else has written and if you can't rest until you see that book in print … then go for it. In this case, it's about love, not money. If you can handle a life where this book doesn't exist, then don't write it. ** But it's really all about where you want your career to go. If a book helps you get to that place, then see if works for you. "
    },
    {
        "ID": "6595",
        "Question": "As part of being a programmer, you often are asked to provide estimates/ make slideware / do technical demos for Sales teams to present to end-clients. Sometimes we go along for the 'technical' discussions or 'strategic capability planning' or some similar mumbo-jumbo. Sometimes, you kind of know which ones are totally going to fail and are not worth pursuing but the Sales guys present fake optimism and extract 'few more slides' out of you or the 'last conference call'. These don't lead to anywhere and are just a waste of time from other tasks for the week. My question is how do you get out of these situations without coming across as non-cooperative. Updated after Kate Gregory's answer: The problem is related to projects we know are doomed (from the technical feedback we've received)  But Sales ain't convinced since they've just had a call higher up the management chain - so it's definitely going ahead ! ",
        "Best answer": "One-by-one, you can't. You're at your desk, sales calls and says excitedly that they've set up another meeting with Mr Big, and are you free at 2? And can you make another demo by then? It's just never going to be an appropriate response for you to say \"dude, Mr Big is totally not buying this software, that meeting would waste my time, no, I'm not going.\" Whether Mr Big is buying or not is something the sales guy is supposed to know more about than you. (In fact, he might even be right.) So for that call, you have to suck it up and say sure. And do a good job on the slides! Longer term, a chat with your boss about this \"supporting the sales team\" role would probably be helpful. Is your issue the time spent? Or that you object to investing time in something that is doomed? In my experience, it is highly unusual for the number of technical people in the room to shrink as the sales process goes on - normally the tech people only show up once things are getting warm, and more of them come to each meeting until the client is landed. So you wanting to bail from the process before the sale is closed will appear strange to management and sales. Discussing this with management may help you to understand how your presence increases the chance of a sale. I think it's unlikely management will grant you a pass to bail on selected client pitches based on your opinion of whether they are going to buy or not, but you could talk about it. Finally, you clearly are expected to produce slides and demos as part of your job, so approach them with the same dignity as \"real code\". Reuse, have a repository, be modular, don't repeat yourself, figure out how to waste as little time as possible giving sales what they need. Remember, when Mr Big does write that cheque, he's funding your salary. You want him to buy. You want to help sales. You just want to do so as quickly and efficiently as possible. "
    },
    {
        "ID": "6723",
        "Question": "Here is a theoretical problem.  It could apply to many systems.  Let's assume RESTful Web services for example.  You want to find out something but as it happens you can't get it in a single query.  You can make multiple queries.  For example, you have a joint bank account (or a company bank account) and you want to find out how much money is in there.  Meanwhile someone else transfers money from checking to savings. Checking:  $5,000   Savings:  $3,000   Me:  Query Checking.    Response: $5,000     Joe:  Transfer $4,000 from Checking to Savings  Checking: $1,000   Savings:  $7,000  Me:  Query Savings.  Response:  $7,000 Total $12,000. How do you avoid anomalies like this?  The example shows two accounts at one bank, but it can also happen with two accounts at two different banks.  ",
        "Best answer": "You need some form of concurrency control to deal with problems like this. Some possible solutions in your example:  Ensure that the service can return both Savings and Checking figures in a single query, perhaps as a Total. Implement some kind of session so that the user requesting values can lock the data until she has finished reading all the values in which she is interested. This approach is often called \"pessimistic concurrency control\". Design the service so that you can pass your previous Checking figure back when you request a Savings figure. If the Checking figure no longer matches its previous value, the service should indicate an error rather than return a value for Savings. This approach is a variation of \"optomistic concurrency control\".  "
    },
    {
        "ID": "6832",
        "Question": "Both in Code Complete by Steve McConnell and The Pragmatic Programmer by Andrew Hunt & David Thomas, they allege that most programmers don't read programming-related books as a habit, or at least not often enough. Does this still hold? Are programmers who do read such books still rare? I must admit that only quite recently did I start reading (and now I can't stop; it's burning a whole in my wallet!). Before I was actually against it and argued that it's better to write programs than to waste time reading about it. Now I realize that the best outcome, for me at least, is to do both. EDIT: I actually meant any kind of books that on way or another was related to programming; be it either on a particular language, technology, pattern, paradigm - anything as long as it may benefit you as a programmer. I was just referring to Code Complete and The Pragmatic Programmer because that's where I read that programmers don't usually read such books. It would have looked stranged if the title just read \"Are programmers who read books still rare?\" ",
        "Best answer": "Everyone seems to be answering this question personally, as in \"I do read such-and-such.\"  However, as a person hanging out at this site, you are already a cut above your \"average\" programmer IMO.  So this skews the numbers dramatically.  To answer the question directly: yes, programmers that read books are a rare breed.  I've worked with dozens of programmers, and only know two or three that would pick up a book to solve a problem.  Most of them would search Google for a particular program, or just stare blankly at the screen hoping for divine inspiration. :-) And the \"theory\" books such as Pragmatic Programmer and Code Complete are even more rarely read by average programmers.  If a programmer is going to read a book, it is more likely to be a technical book on the specific language, framework, or technology they are working on.  The \"soft\" books are not even on most programmers radar. "
    },
    {
        "ID": "6929",
        "Question": "I've got an itch and it needs to be scratched. I've got nothing to program these days. Normally I'd just pick my flavour of the month technology and fiddle with it, but I want to make something and take a break from serious research/experimentation for a while and just write something for fun (ie. not necessarily purely educational/professional development). Problem is,   I can't think of software that I need that I don't already have. Sifting through the open-source projects on various sites to help out on is excruciating (5 hrs down the tubes so far - not worth the trouble)  I'm not looking for a specific suggestion for what to work on because that would be a terribly hard task. I don't think that here knows nearly enough about eachother to effectively give good suggestions. So I ask, is there any mental exercise that you use to decide what to make when you're not \"fiddlin'\"? ",
        "Best answer": "When I run out of applications or utilities I want to write, I usually go solve math problems on Project Euler (I also often use those problems when I'm learning a new language).  Solving math-based problems programatically may not be the most rounded use for programming skills, but I've had a lot of fun and learned some things as well (ymmv). "
    },
    {
        "ID": "7126",
        "Question": "Note: this question is an edited excerpt from a blog posting I wrote a few months ago.  After placing a link to the blog in a comment on Programmers.SE someone requested that I post a question here so that they could answer it.  This posting is my most popular, as people seem to type \"I don't get object-oriented programming\" into Google a lot.  Feel free to answer here, or in a comment at Wordpress.  What is object-oriented programming?    No one has given me a satisfactory   answer.  I feel like you will not get   a good definition from someone who   goes around saying “object” and   “object-oriented” with his nose in the   air. Nor will you get a good   definition from someone who has done   nothing but object-oriented   programming. No one who understands   both procedural and object-oriented   programming has ever given me a   consistent idea of what an   object-oriented program actually does.  Can someone please give me their ideas of the advantages of object-oriented programming? ",
        "Best answer": "From your blog, it seems that you're familiar with both imperative and functional programming, and that you're familiar with the basic concepts involved in object-oriented programming, but you've just never really had it \"click\" as to what makes it useful.  I'll try to explain in terms of that knowledge, and hope that it's helpful to you. At its core, OOP is a way to use the imperative paradigm to better manage high degrees of complexity by creating \"smart\" data structures that model the problem domain.  In a (standard procedural non-object-oriented) program, you've got two basic things: variables, and code that knows what to do with them.  The code takes input from the user and various other sources, stores it in variables, operates on it, and produces output data which goes to the user or various other locations. Object-oriented programming is a way to simplify your program by taking that basic pattern and repeating it on a smaller scale.  Just like a program is a large collection of data with code that knows what to do with it, each object is a small piece of data bound to code that knows what to do with it. By breaking down the problem domain into smaller pieces and making sure as much data as possible is bound directly to code that knows what to do with it, you make it a lot easier to reason about the process as a whole and also about the sub-issues that make up the process. By grouping data into object classes, you can centralize code related to that data, making relevant code easier both to find and to debug.  And by encapsulating the data behind access specifiers and only accessing it through methods, (or properties, if your language supports them,) you greatly reduce the potential for data corruption or the violation of invariants. And by using inheritance and polymorphism, you can reuse preexisting classes, customizing them to fit your specific needs, without having to either modify the originals or rewrite everything from the ground up.  (Which is a thing you should never do, if you can avoid it.)  Just be careful you understand your base object, or you could end up with killer kangaroos. To me, these are the fundamental principles of object-oriented programming: complexity management, code centralization and improved problem-domain modeling through the creation of object classes, inheritance and polymorphism, and increased safety without sacrificing power or control through the use of encapsulation and properties.  I hope this helps you understand why so many programmers find it useful. EDIT: In response to Joel's question in the comments,  Can you explain what an \"object-oriented program\" contains   (other than these fancy defintions you've outlined) that is fundamentally   different from an imperative program? How do you \"get the ball rolling?\"  A little disclaimer here.  My model of \"an object-oriented program\" is basically the Delphi model, which is very similar to the C#/.NET model since they were created by former Delphi team members.  What I'm saying here may not apply, or not apply as much, in other OO languages. An object-oriented program is one in which all the logic is structured around objects.  Of course this has to be bootstrapped somewhere.  Your typical Delphi program contains initialization code that creates a singleton object called Application.  At the start of the program, it calls Application.Initialize, then a call to Application.CreateForm for every form you want to load into memory from the beginning, and then Application.Run, which displays the main form on screen and starts up the input/event loop that forms the core of any interactive computer programs. Application and your forms poll for incoming events from the OS and translate them into method calls on your object.  One thing that's very common is the use of event handlers, or \"delegates\" in .NET-speak.  An object has a method that says, \"do X and Y, but also check to see if this particular event handler is assigned, and call it if it is.\"  An event handler is a method pointer--a very simple closure that contains a reference to the method and a reference to the object instance--that's used to extend the behavior of objects.  For example, if I have a button object on my form, I customize its behavior by attaching an OnClick event handler, which causes some other object to execute a method when the button is clicked. So in an object-oriented program, most of the work gets done by defining objects with certain responsibilities and linking them together, either through method pointers or by one object directly calling a method defined in another object's public interface.  (And now we're back to encapsulation.)  This is an idea that I had no concept of back before I took OOP classes in college. "
    },
    {
        "ID": "7181",
        "Question": "I have been using mysql for the same amount of time I have been using PHP. However I feel about 10x more confident in PHP; or any other programming language. But I just can't seem to really really grasp SQL. I mean, I can select, update, insert, delete, join, etc. but when it comes to more complex stuff I am lost. How to write a more complicated select statement, how to use indexes properly, which engine to use, etc. For some reason I really find the mysql docs hard to follow, but the php ones easy to follow. I am starting to think that maybe my brain is just wired in a way that makes programming make since but sql confusing. I don't know. Most people think Regex is so confusing but to me it makes more scientific sense than how to construct an sql statement. Please don't get me wrong, I am not hating on SQL, I really like it, I just want to understand it better. Whats the best/fastest way to really master MySQL?  When I say \"master\" I don't mean the guru among gurus, I mean smart enough to figure out most of the questions I ask on SO myself without needing help.  ",
        "Best answer": "Fastest way I found to \"master\" anything was to challenge myself with it. Take a set of data and write down eight to ten different things you would like to query for. A group of students you might want to find the average age of everyone grouped by the grade point average rounded to the nearest whole number. Then write the queries for them. Start with one table, then add more. Practice data joins and built in functions.  "
    },
    {
        "ID": "7364",
        "Question": "This question is something that's been bugging me for the past 3 months since I switched from being a freelancer to working at a Web Design firm. Our sales people often ask us something similar to the following series of questions:  How much does it cost to program a widget How many hours will it take to convert this website to this software. (Without knowing what the website currently runs) etc    How can we give a quote without any information? (No, I can't ask for more info!)  I have another question if a project goes over budget it's bad. Recently, I missed an entire menu when calculating the cost of transferring a website over to a new platform so the project went over budget. My boss was not happy at all, and it's my opinion that some things like this can't be avoided.    2.  What is the general practice for dealing with going over budget            and do projects like web development often go over budget? If you work at a web development/design/similar company:    3.  How does your billable hour system work? For me, we have a time tracking application that we record how many hours we spend on which project and if they are billable or internal (AKA non-billable). If don't meet xx billable hours a week we can get in trouble/fired eventually. Work you do for the company or for clients that isn't billable isn't part of this system, and we often have to do internal work, so I'm wondering if any alternative systems exist. EDIT: Ok I am a developer at this firm not a designer :) Second, I am paid salary, but here is how management looks at it. You have 35 hours a week that you must work. You could be doing work that they bill to clients in that 35 hours and you should. If they figure out a project will take 50 hours and I take 55 hours, that 5 hours could have been spent on another project that wasn't over budget so we just \"lost\" money. Another example is that if I only have 1 project, that is due in two weeks and I spend a day  doing internal work, some how we lost money because I wasn't working. If I worked that day, I would finish a day early and still have no work. Either way, the work is contract so we will get paid the same amount regardless of which days I work! ",
        "Best answer": " Our sales people often ask us   something similar to the following   series of questions: How much does it cost to program a   widget How many hours will it take to   convert this website to this software.  Why are your sales people asking the designers? Sales should have a prestocked list of quotes and system for estimation that has little, if any, correlation to your actual costs. I'm assuming you are salaried.   How can we give a quote without any information? (No, I can't ask for more info!)  Short answer? You can't, don't try. The long answer is still short. If I call you up and say I have a website were people can login, post messages to other users, upload pictures, and make friends, what would it cost to build, what would you say? I could have described the worst social network imaginable, or Facebook. You don't have enough information so you can't give an accurate assessment.   I have another question if a project   goes over budget it's bad. Recently, I   missed an entire menu when calculating   the cost of transferring a website   over to a new platform so the project   went over budget. My boss was not   happy at all, and it's my opinion that   some things like this can't be   avoided.  Define \"over budget.\" Again, I'm assuming salary not hourly. If you went over your time budget, pull some long nights and don't make the same mistake (of missing something) again.  For me, we have a time tracking   application that we record how many   hours we spend on which project and if   they are billable or internal (AKA   non-billable). If don't meet xx   billable hours a week we can get in   trouble/fired eventually. Work you do   for the company or for clients that   isn't billable isn't part of this   system, and we often have to do   internal work, so I'm wondering if any   alternative systems exist.  I'm not sure how I would set that up if I had to create a record of \"billable\" hours. You would probably wind up with a hundred hours +/- a few ever week. I don't stop thinking about code, should that count?  "
    },
    {
        "ID": "7439",
        "Question": "If you were hiring programmers, and could choose between one of (say) the top 100 coders on topcoder.com, or one of the top 100 on stackoverflow.com, which would you choose? At least to me, it would appear that topcoder.com gives a more objective evaluation of pure ability to solve problems and write code. At the same time, despite obvious technical capabilities, this person may lack any hint of social skills -- he may be purely a \"lone coder\", with little or no ability to help/work with others, may lack mentoring ability to help transfer his technical skills to others, etc. On the other hand, stackoverflow.com would at least appear to give a much better indication of peers' opinion of the coder in question, and the degree to which his presence and useful and helpful to others on the \"team\". At the same time, the scoring system is such that somebody who just throws up a lot of mediocre (or even poor answers) will almost inevitably accumulate a positive total of \"reputation\" points -- a single up-vote (perhaps just out of courtesy) will counteract the effects of no fewer than 5 down-votes, and others are discouraged (to some degree) from down-voting because they have to sacrifice their own reputation points to do so. At the same time, somebody who makes little or no technical contribution seems unlikely to accumulate a reputation that lands them (even close to) the top of the heap, so to speak. So, which provides a more useful indication of the degree to which this particular coder is likely to be useful to your organization? If you could choose between them, which set of coders would you rather have working on your team? ",
        "Best answer": "Why choose? When you are hiring, you want to post your offer everywhere. Hiring based on reputation points or any other online results is a terrible idea and I'm pretty sure no one is doing it. Sure that having 30K on StackOverflow will helps getting the attention of the hiring guy, but you will get hired for many others facts.  The experience in your domain or industry of the candidate His physical location related to your office The amount of $$$ he is asking for the job His personnal interests (yes it has an influence, at least on me) His recommandations What people said about him during references checks And more importantely, his seduction abilities! (during interview, you have to seduce within the first 5 minutes)  They are so many factors you can't summarize the hiring process to programmer's (supposed) capabilities. "
    },
    {
        "ID": "7456",
        "Question": "Linguistic relativity is the idea that language shapes the way we think.  My question is, how much, and to what extent, does this apply to programming?  Are some native, natural languages better-suited for thinking about programming than others?  For instance, can the following be stated more concisely in a non-English language?  Select a pivot.  Move all the items less than the pivot to one side of the list, and all the items greater than the pivot to the other side. Does a Chinese-speaking programmer view programming in a radically different lens than an English-speaking programmer, or do the differences fade away when both are immersed in the subject? Are some programming languages and domains easier to think about in one language or another.  For instance, is it any easier to grok Ruby if you are Japanese because the creator of Ruby is Japanese?  Note that this question is not focused on \"how do programming languages affect the way people think about programming\", but rather \"how do natural languages affect the way people think about programming\". To get it out of the way, one language that clearly has a pragmatic advantage is English.  I think the advantage has little to do with programming languages choosing English keywords like if, for, while, and do, just as musicians who don't speak Italian aren't tripped up by words like forte.  It has more to do with communication of ideas with other programmers, as English is the lingua franca these days, at least in the programming world.  For instance, to ask a question in StackOverflow, you really need to know English and know it pretty well if you want good answers.  Although this sounds like an imperialist attitude, it really is true in practice. That aside, how do the intrinsic properties of languages affect how programmers who speak them think about data structures, algorithms, etc.?  Are any languages particularly concise when it comes to talking about logic and programming, allowing native speakers of those languages to think faster? ",
        "Best answer": "I don't know that any particular natural language lends itself to better programming (except maybe Latin?). I do know that knowing more than one language is pretty powerful. Dijkstra said in one of his last interviews (as reprinted in CACM Vol. 53 No. 8, p. 44):  There is an enormous difference   between one who is monolingual and   someone who at least knows a second   language well, because it makes you   much more conscious about language   structure in general. You will   discover that certain constructions in   one language you just can't translate.  "
    },
    {
        "ID": "7472",
        "Question": "Let's say a large corporation is planning to replace it's existing version control system. Let's say it is only considering systems from major vendors that cost hundreds of thousands of dollars because they have \"support.\"  Does version control in an enterprisey environment have to be expensive?  Does your medium/large corporation use a FOSS VCS such as SVN/Git/Mercurial?  What has the experience been?   I have to think it doesn't need to be expensive since there are so many free options, and there are probably companies that provide paid support for FOSS VCS if that is the main concern.   I don't intend this question to compare VCS or decide which is best, rather just understand experiences with VCS in a corporate IT environment. ",
        "Best answer": "Yes.    In my (admittedly limited) experience, the non-FOSS solutions tend to be more \"enterprise-y\".  That is,  They integrate with everything under the sun. They have more built-in controls for complex business logic (permissions, access control, approval, etc). They come with support contracts and reasonably responsive tech support lines. They're well advertised to the non-technical people making VCS decisions at a high level in big companies.  These attributes make them attractive to large companies, especially to people who don't have to use them.  The FOSS alternatives, as counters to the above:  Have plenty of third-party tools to integrate them with everything under the sun (by virtue of being more popular than proprietary alternatives), and tend to be easier to develop third-party tools for, being OS. See previous- easier to to get external tools around a clean, simple, basic tool. By virtue of being more popular, they have a wider community-based support. They don't need said advertising.  Aside from that, my experience with common free VCS (mercurial/svn/etc) has them being faster, more reliable, and easier to use. "
    },
    {
        "ID": "7482",
        "Question": "As per this question: I decided to implement the BitTorrent spec to make my own client/tracker. Now, I was going through the spec, I was about 70% done implementing the BEncoding when I found a link to an implementation of BEncoding in C# written by someone else. Normally, if I were working on production code, I'd use it as a reference to check my own work against, and a baseline to write some tests to run my code against, but I found myself thinking \"I'm making this, it's a for-fun project with no deadlines; I should really implement it myself - I could learn a lot\" but some voice in my head was saying \"Why bother re-inventing the wheel? Take the code, work it so that it's you're style/naming convention and you're done.\" So I'm a bit conflicted. I ended up doing the latter, and some parts of it I found better than what I had written, but I almost feel like I 'cheated'. What's your take? Is it cheating myself? Perfectly normal? A missed opportunity to learn on my own? A good opportunity to have learned from someone else's example? ",
        "Best answer": " If I have seen further it is by standing on the shoulders of giants. Isaac Newton  It is not cheating if the code is open source and you've taken the time to understand it. Now obviously this isn't always possible due to time constraints but try and always have high level overview of the code you are using. Always remember that C was derived from B. "
    },
    {
        "ID": "7530",
        "Question": "There are many stories about intentionally bad code, not only on TheDailyWTF but also on SO. Typical cases include:  Having a useless time-wasting construct (e.g. an empty loop counting to some huge value) so programmers can easily \"speed up\" the application by removing it when they are tasked to. Providing intentionally misleading, wrong or no documentation to generate expensive support requests. Readily generating errors, or worse, generating even though everything worked fine, locking up the application so an expensive support call is required to unlock.  These points display a more or less malicious attitude (even though sometimes by accident), especially the first point occurs rather often. How should one deal with such constructs? Ignore the issue, or just remove the offending code? Notify their manager, or speak to the person who introduced the \"feature\"? ",
        "Best answer": "I've never (in 20-odd years) come across intentionally bad code, but the examples you cite seem (to me at least, but IANAL) to be attempts to defraud either an employer or a customer, so you probably have a legal obligation to point it out to your manager. "
    },
    {
        "ID": "7536",
        "Question": " Possible Duplicate: int* i; or int *i; or int * i;   Thinking about where we place our asterisks; how do those that prefer to keep the \"pointerness\" away from the type and with the identifier (int *i) write code when the identifier is missing? void f(int*); // 1 void f(int *); // 2  The former seems much more common, no matter what your preference when with the identifier.  Is this a special case?  What makes it an exception? However, the first still isn't universal, because I have seen the latter style.  Besides consistency along the lines of \"there's always a space with the identifier, so we have one without\", are there any other reasons to prefer it? What about casts or array and function types?  How would you re-write these: (void*)var /*or*/ (void *)var  int[3] /*or*/ int [3] // more relevant in C++ than C: Example<int[3]>  void(int) /*or*/ void (int) // more relevant in C++ than C: std::function<void(int)>  The latter two would rarely, if ever, be used in C, but are seen with C++ templates. ",
        "Best answer": "I always separate the star from the type, so I use: void f(int *);  void f(int *x) {     char *y = (char *) x;     char x[4]; }  Obviously, I don't separate the brackets from the identifier. That's how I learned to do it from K&R, and who am I to argue? (Also, it seems to be the most readable way.) "
    },
    {
        "ID": "7559",
        "Question": "This happened at least once to me. I'm working on some part of the code base and find a small bug in a different part, and the bug stops me from completing what I'm currently trying to do. Fixing the bug could be as simple as changing a single statement. What do you do in that situation?  Fix the bug and commit it together with your current work Save your current work elsewhere, fix the bug in a separate commit, then continue your work [1] Continue what you're supposed to do, commit the code (even if it breaks the build fails some tests), then fix the bug (and the build make tests pass) in a separate commit  [1] In practice, this would mean: clone the original repository elsewhere, fix the bug, commit/push the changes, pull the commit to the repository you're working on, merge the changes, and continue your work. Edit: I changed number three to reflect what I really meant. ",
        "Best answer": "I have done 1 and 2 and in the end, I think I prefer #2. It allows for more visibility for the bug fix, which may be important for QA/release notes/other developers.  I've also come across a situation where what I thought was a bug actually wasn't (not saying that this is the case here), and \"fixing\" it in a separate commit allowed another dev to contact me and explain what was what instead of the \"fix\" just getting lost in my normal check-in. "
    },
    {
        "ID": "7566",
        "Question": "Developer interviews are different than most other places, because not only do you worry about the people you work with, benefits, job description, etc., you also have EXTRA to worry about after getting the job.  Writing two application with exact requirements can be vastly different if you're working in a loud vs quiet environment, using VS2003/.NET 2.0 vs VS210/.NET 4.0., using SVN vs VSS. Is it ok to give the potential employer the straight-up Joel Test?  I try to ask as many questions to get the type of environment I will be working in, which is extremely important from my perspective, but what's the best way to cut to the chase and just ask the tough questions (like they ask you during the same interview). NOTE: By the \"Joel Test\" I mean a specific list of things that are deal breakers that are important to you (not necessarily Joel), but you may not have time to get out using the traditional casual \"conversational\" way of asking them, so you decide to either email or schedule another meeting, or ask other people, etc. ",
        "Best answer": "A job interview goes both ways -- a company is interviewing you and you are interviewing the company. I wouldn't come out literally with a \"what's your Joel Test score?\", but I would ask the individual questions that were particular deal-breakers for me in a work environment. It doesn't need a huge build-up. A good time to ask these questions as at the technical part of the interview process, when they say \"do you have any questions for us?\". You can lead in with something along the lines of \"can you describe a typical day on the job here?\" and go from there. "
    },
    {
        "ID": "7588",
        "Question": "We currently use once cc.net server for our build process, which builds both .net (using msbuild & nant) and java (using maven and ant).    CC.net monitors source control, and triggers a remote build running on a separate server.  CC.net then collates the results. When we run the remote build, it typically:  runs nunit or junit or similar using mocked data optionally runs a DB script to create a fresh database instance or restore a database from a known position. runs selenium or similar to test UI runs emma or ncover for code coverage builds the system for various deployment environments (test, acceptance, production)  We may have several builds running at a time, some .net and some java (from different project teams). It's quite time consuming to get the remote builds working when we set up a new project and we feel that there must be something more suited to remote builds than cc.net.   Does anyone have any experience with remote builds with continuous integration systems? I don't really want feature lists of CI servers, I'd more appreciate hearing about how you've used them in a multi language, multi server environment. ",
        "Best answer": "Hudson (Update: in today's world, I would use Jenkins, a fork of Hudson.) I've used hudson in both Java and .NET enterprise environments for high visibility projects (you've probably been to some of the sites).  Hudson is solid from the start, but the best part is that there are lots of plugins to do just about anything you want.  Hudson is highly configurable, has a great community, and is really easy to set up in a cluster environment if you need multiple builds going at once.  It's my favorite CI server of all the ones I've used (CC.NET, Hudson, and TFS). Plus, you can use the ChuckNorris plugin to have him give you the thumbs up or down. "
    },
    {
        "ID": "7705",
        "Question": "When dealing with complicated algorithms in languages with support for nested functions (such as Python and D) I often write huge functions (because the algorithm is complicated) but mitigate this by using nested functions to structure the complicated code.  Are huge (100+ line) functions still considered evil even if they're well-structured internally via the use of nested functions? Edit:  For those of you not familiar with Python or D, nested functions in these languages also allow access to the outer function scope.  In D this access allows mutation of variables in the outer scope.  In Python it only allows reading.  In D you can explicitly disable access to the outer scope in a nested function by declaring it static. ",
        "Best answer": "Always remember the rule, a functions does one thing and does it well! If you can do so, avoid nested functions. It hinders readability and testing. "
    },
    {
        "ID": "7747",
        "Question": " It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration  -- Edsger W. Dijkstra I have deep respect to Dijkstra but I don't agree with everything he said/wrote. I disagree specially with this quote on linked paper wrote 35 years ago about the Dartmouth BASIC implementation. Many of my coworkers or friends programmers started with BASIC, questions below have answers that indicate many programmers had their first experience on programming at BASIC. AFAIK many good programmers started at BASIC programming. I'm not talking about Visual Basic or other \"modern\" dialects of BASIC running on machines full of resources. I'm talking about old times BASIC running on \"toy\" computer, that the programmer had to worry about saving small numbers that need not be calculated as a string to save a measly byte because the computer had only a few hundreds of them, or have to use computed goto for lack of a more powerful feature, and many other things which require the programmer to think much before doing something and forcing the programmer to be creative. If you had experience with old time BASIC on a machine with limited resources (have in mind that a simple micro-controller today has much more resources than a computer in 1975, do you think that BASIC help your mind to find better solutions, to think like an engineer or BASIC drag you to dark side of programming and mutilated you mentally? Is good to learn a programming language running on a computer full of resources where the novice programmer can do all wrong and the program runs without big problems? Or is it better to learn where the programmer can't go wrong? What can you say about the BASIC have helped you to be a better/worse programmer? Would you teach old BASIC running on a 2KB (virtual) machine to a coming programmer? Sure, only exposure to BASIC is bad. Maybe you share my opinion that modern BASIC doesn't help too much because modern BASIC, as long other programming languages, gives facilities which allow the programmer doesn't think deeper. Additional information: Why BASIC? ",
        "Best answer": "The Basics popular at the time of the quote were very different from what we had even 20 years ago.  (Are you counting those among your \"modern\" dialects? ;) Forget loops, subroutines, local variables, and everything that Structured Programming (of which Dijkstra and Knuth were big proponents) emphasized.  You had GOTO, and you liked it. In this context, programmers who only knew global variables, invented their own subroutines (using more global variables for parameters and return values!), and wrote spaghetti GOTOs really were mutilated. If you're 30-something or younger today and Basic was your first language, it wasn't the same language Dijkstra was talking about.  Even if you're older and your first Basic had some of these features, as Murph comments below, it still may not have been the same language Dijkstra was talking about.  You've updated the question with context I never knew before:  Dijkstra's quote is from 1975. It wasn't until version 6 that you got separately compilable procedures — which, I believe, shows the beginning of the shift in focus away from GOTO. \"In 1976, Steve Garland added structured programming features to create Dartmouth SBASIC, a precompiler which produced version 6 output...\" [Wikipedia] The context of the quote is clearly before what we now know as control structures, and many users of the time could've been more familiar with the second-to-latest version — i.e. two versions before Garland's, which is v5 and before separately compilable procedures. GOSUB/RETURN still only handle \"simple subroutines\". \"Variable names were limited to A to Z, A0 to A9, B0 to B9, ..., Z0 to Z9, giving a maximum of 286 possible distinct variables.\" ...and they're all global.  "
    },
    {
        "ID": "7823",
        "Question": "In the comment to this great post, Roy Osherove mentioned the OAPT project that is designed to run each assert in a single test. The following is written on the project's home page:  Proper unit tests should fail for exactly one reason, that’s why you should be using one assert per unit test.  And, also, Roy wrote in comments:  My guideline is usually that you test one logical CONCEPT per test. you can have multiple asserts on the same object. they will usually be the same concept being tested.  I think that, there are some cases where multiple assertions are needed (e.g. Guard Assertion), but in general I try to avoid this. What is your opinion? Please provide a real world example where multiple asserts are really needed. ",
        "Best answer": "I don't think it's necessarily a bad thing, but I do think we should strive towards only having single asserts in our tests.  This means you write a lot more tests and our tests would end up testing only one thing at a time. Having said that, I would say maybe half of my tests actually only have one assert.  I think it only becomes a code (test?) smell when you have about five or more asserts in your test. How do you solve multiple asserts? "
    },
    {
        "ID": "7826",
        "Question": "I like the GPL license but the project I'm working on is a more general purpose one that will be used inside other programs. It doesn't run independently like a CMS or application would. So I'm looking around at other projects to see what they've done.  JQuery has an interesting MIT/GPL license Zend framework has New BSD license Symfony framework has MIT license   As developers, is there an established preference that we expect from libraries. I rarely cared because I never build applications that were sold or distributed, which eliminated the possibility that I would be in conflict with the license of a library I'm using, but I want to know what others are thinking. Will you avoid using a library or a framework if it's GPL?  I'm a bit torn. I'd like to use GPL and as the only license (unlike JQuery for example) but I also worry that this will scare developers away who want to use the library to build distributable code. The other thing is I'm seeing that many libraries are frameworks are released as MIT, but I find the MIT license, well, a bit too \"loose\" for my taste. ",
        "Best answer": " Will you avoid using a library or a framework if it's GPL?  Yes. Using a GPL'd library would essentially require me to publish the source of my software that uses the library, or even GPL it (altough this is somewhat unclear - but better not take legal risks). Publishing sources (let alone GPL'ing a software product) is typically impossible with commercial software (depending on corporate policies). And even if I could publish the source, I dislike the idea that some 3rd party library's license defines how I should license my software. Consider using LGPL, or even better, another licence widely used in libraries the Apache License. Regardless of the details - all GPL licenses are quite incomprehensible and subject to continuous re-interpretation by lawyers and by the FSF - it's clear that the spirit of GPL is to make all software free via viral licenses. In practice they're better to avoid, unless, of course, you agree with their goals and understand them. "
    },
    {
        "ID": "7915",
        "Question": "I have been offered an interesting job, but there's a big caveat for me: they use pair programming. I hate the idea of pair programming, and I'm probably not suited for it: I like to do frequent pauses, I hate to see someone programming (I would constantly poke the pair away to code myself), I have to be in full control of the machine I'm working on, I like to work listening music, and basically I don't like to being tied to someone else. I'm not even a social person. I have however never actually worked with true pair programming (besides few times for a short time to help someone else or to solve a complex task together)... so it is pair programming really that bad? And given my attitude, should I refuse the job or should I leave my current one and give a try?  For people that asked about it: I'm looking for a job where formal design and development are used, as I hate my current job where we are \"coding in the wild\". The company is very interested in my technical profile so they insisted even when I specified I never worked with pair programming and that I probably wouldn't like it (besides being an unsociable loner programmer, I don't like and believe the very concept of pair programming). ",
        "Best answer": "Guess what - nobody likes pair programming at first. When I first tried pair programming I was against the idea and I had tons of objections similar to yours.  I don't like to watch someone else coding, I like to listen to music, I feel I'll be quicker working on my own, I don't want to stop and explain myself constantly, etc etc etc. Then I tried it.  And guess what?  It still sucked.  We were having all kinds of problems - I wanted to work different hours to the other guy, we had different keyboard settings and shortcuts, our desks didn't work very well for pair programming etc etc. This went on for about a week.  During that week we were rewriting the entire legacy login system for a distributed application.  We had to learn how some seriously difficult threading issues work, figure out how remoting sinks worked, change tons and tons of legacy code to work with our new login module and pretty much do some of the most hectic coding I've had to do in my career.  After a week of this we deployed it.. and everything just worked.  Not a single bug.  Not one. That's when I figured there might be something to this pair programming nonsense.  We started to do more pair programming.  We even started to force everyone to work in pairs.  It rocked.  I probably learnt more in a month of doing that than I did in the previous 2 years.  The other issues didn't go away.  Sometimes you'll be stuck with a pair that you dislike.  It's gonna happen.  Sometimes you'll struggle to find overlapping working hours.  Sometimes you'll want to just work on your own.  But the advantages of pair programming are just incredible. Since then I've always tried to do as much pair programming as possible.  It's simply the best way of learning.  This company forces everyone to do pair programming? Where do I sign up?  It seems to me they are really enlightened and you will be in a really intense working environment.  Awesome. Having said that, make sure that the pairs are rotated often.  You don't want to be stuck developing with one other developer for months on end - you'll go out of your mind.  We're all human, after all.  Also, check that this has been an ongoing practice for a while.  If it's not, it's not the end of the world, but if it's an accepted practice it's a massive green light. Trust me, you want to work for a company like that. "
    },
    {
        "ID": "7951",
        "Question": "I'm a student of computer science but I am only taking entry level web development classes. I'm looking for some out-of-class reading, tutorials, and other ways of learning but I'm not sure what I should start in.  What is a good set of programming languages (different paradigms?), frameworks, suggested projects, and maybe even some open-source communities that I should start to look at and start learning? Also, maybe even some books or blogs on development processes in the professional world. I'm looking to start getting in to professional development around the end of college. I understand practicing it will be the best way to learn anything but if I don't know what I should practice, I'm lost at that. :) ",
        "Best answer": "These would be my baseline recommendations for topics to cover, not necessarily in-depth, but at least a general understanding: (in no particular order)  A compiled language - C#, Java, C, or if you're brave, C++. Understand about source code gets compiled into something else and then run by the runtime. A scripted language - JavaScript, Python. Know the differences to (1) and the strengths in terms of dynamic typing and rapid development. HTML + CSS. Whether for documentation or test harnesses, you'll use it somewhere. SQL. Data lives in databases. They all have their own flavours, but a basic understanding of SQL helps a lot. Version Control. Pick any system - Subversion, Git, Mercurial, CVS - it doesn't matter which, just understand about the check out, modify, build, merge, review, build, commit workflow. Testing - whether unit testing, automated or manual.  Security. Software systems get attacked - even the un-sexy ones - and users' information is becoming worth more than their bank details. Algorithms - understand Big O notation and that choice of good algorithm matters much more than micro-optimisation. Design Patterns - no point in re-inventing the wheel. The Software Development Lifecycle. Doesn't matter which methodology you prefer, but go find out what they are.  and when you've got the first job: 11.. How your employer measures success. All of the above are moot if your employer has their own unique systems which you have to use. Find out how to be successful in your employers' eyes first and then introduce the items you've learned along the way. "
    },
    {
        "ID": "7966",
        "Question": "I have been listening to Scott Hanselman and Rob Conery's podcast, This Developer's Life. In the latest episode, they discuss personality traits:  1.0.4 - Being Mean. What makes people mean in our   industry? What about aggressive?   Confident? What's the difference?   Would you rather have a drill sergeant   for a boss, or a zen master? We talk   to Cyra Richardson and Giles Bowkett.  It got me thinking, what traits did the best managers you've worked for have in common? EDIT: Just to clarify, as there have been a few close votes, I'm interested in whether there are traits common to managers of developers that are not necessarily those traits that a manager of some other profession requires.  As for whether this is programming related or not, well I don't want to ask this question on a site that isn't about programming because, frankly, I'm not as interested in what people who make cans of soup for a living want from their managers as I am interested in what developers want from their managers. ",
        "Best answer": "Joel Spolsky calls it the \"Abstraction Layer\". Do what it takes to keep me programming. Let me know what's going on in the company, but keep me out of the politics. Eventhough I still have to do it, at least acknowledge that the request is bull sh!t.  "
    },
    {
        "ID": "8090",
        "Question": "Question first: What are some feasible alternatives to time tracking for employees in a web/software development company, and why are they better options Explanation: I work at a company where we work like this. Everybody is paid salary. We have 3 types of work, Contract, Adhoc and Internal (Non billable). Adhoc is just small changes that take a few hours and we just bill the client at the end of the month. Contracts are signed and we have this big long process, the usual. We figure out how much to charge by getting an estimation of the time involved (From the design and the developers), multiplying it by our hourly rate and that's it. So say we estimate 50 hours for a website. We have time tracking software and have to record the time in 15 we spend on it (7:00 to 7:15 for example), the project name, and give it some comments. Now if we go over the 50 hours, we are both losing money and are inefficient.  Now that I've explained how the system works, my question is how else can it be done if a better method exists (Which I'm sure one must). Nobody here likes the current system, we just can't find an alternative. I'd be more than willing to work after hours longer hours on a project to get it done in time, but I'm much inclined to do so with the current system. I'd love to be able to sum up (Or link) to this post for my manager to show them why we should use abc system instead of this system. ",
        "Best answer": "The problem with this method is that it takes no account of the inherent risk in estimates. A best practice for any estimates is to express it as a range of times, e.g. 50 hours ± 15 hours, or something similar. The error term is tough to come up with, but nobody believes it will take exactly 50 hours anyway. There are other approaches aside from the fixed price model; you could use a lower rate, and bill straight hours, but these days, your clients will probably want to transfer the risk to you. That's fine, but it means you need to charge a reasonable risk premium based on the range of time estimates you come up with. "
    },
    {
        "ID": "8111",
        "Question": "I'm a self-taught programmer and have just started a computer science degree to supplement my knowledge and fill in the gaps. However, I'm already debating the direction of my education. I want a 4 year bachelor's degree for sure, but after that, I'm not sure: Is it worth it to get an M.Sc? What about a Ph.D.? What opportunities do these degrees open up? Currently my goal is to be a software developer, but there are a number of fascinating fields in the software industry and I'm certainly interested in investigating many of them. It is in this respect that I think a higher degree may be worth it -- even if it wouldn't necessarily help a career of software development. So will it be worth it? Will grad school open doors? ",
        "Best answer": "It might open up more technical positions and jobs at larger Oracle-type companies. The important thing it does though is gives you a very focused specialty. To employers, you are the best they can find in that specialty (on paper at least). "
    },
    {
        "ID": "8145",
        "Question": "Looking at common Agile practices it seems to me that they (intentionally or unintentionally?)  force developers to spend more time actually working as opposed to reading blogs/articles, chatting, coffee breaks and just plain procrastinating. In particular: 1) Pair programming - the biggest work-forcer, just because it is inconvenient to do all that procrastination when there are two of you sitting together. 2) Short stories - when you have a HUGE chunk of work that must be done in e.g. a month, it is pretty common to slack off in the first three weeks and switch to OMG DEADLINE mode for the last one.  And with the little chunks (that must be done in a day or less) it is exact opposite - you feel that time is tight, there is no space for maneuvering, and you will be held accountable for the task pretty soon, so you start working immediately. 3) Team communication and cohesion - when you underperform in a slow, distanced and silent environment it may feel ok, but when at the end of the day at Scrum meeting everyone boasts what they have accomplished and you have nothing to say you may actually feel ashamed. 4) Testing and feedback - again, it prevents you from keeping tasks \"99% ready\" (when it's actually around 20%) until the deadline suddenly happens. Do you feel that under Agile you work more than under \"conventional\" methodologies? Is this pressure compensated by the more comfortable environment and by the feeling of actually getting right things done quickly? ",
        "Best answer": "The main idea behind the agile methods is to help you be productive - in a positive sense. No one cares if you spend an hour surfing every day if you meet the deadline. Everyone gets mad if you surf half an hour every day but miss your deadline. The solution: Make it easier for you to meet the deadline. As you noticed, pair programming makes sure you stay focused (among all the other advantages like improving skill/knowledge spreading, better code, less bugs, uniform design, etc.). I found that discipline is always a struggle for me. If I pair with someone, chances are that one of us wants some work done today and pulls the other along. So the \"work for a month\" often becomes turns into \"work together for one week\", being surprised how that huge amount of work resolved in the end, spend a day or so recovering (refactoring, fixing TODOs in the code, adding a couple of tests, surfing with a clear conscience) and then grabbing the next month of work. Net result: I'm much more relaxed (more because than despite the constant supervision), team cohesion is much better, work gets done more quickly, people don't hang around some minor issue for hours or even days (because someone else can spot the problem much faster). When you say \"you may actually feel ashamed\", isn't that a good thing? It means you feel that you did wrong and you should. You're not getting paid to get nothing done. Not getting anything done makes you feel helpless, unhappy, unworthy, miserable. Instead of feeling ashamed, stand back and think \"Why didn't I accomplish anything today?\" Do you need help? Is there something you don't understand? Is the current task too hard? You don't like it? Maybe you can switch the task with someone else. Maybe someone else can help you get through. Agile means: Assume responsibility instead of being micro-managed like a puppet on strings. You need a tool? Go to your boss and ask for it. Learn to argue. Learn to stand up and shout when you have to. As for tests, there is a sweet spot when your code suddenly collapses from \"nice\" to \"perfect\". That's the moment when you notice that you need to implement feature X and you thought that will be a nightmare and suddenly realize that the code is almost there. Just a small refactoring here and there. A new class and done. Four weeks of work suddenly became a day. Victory! Triumph! "
    },
    {
        "ID": "8228",
        "Question": "I am rereading Refactoring by Martin Fowler. In Chapter 4, Building Tests, I came across the following passage. In fact, one of the most usefull times to write tests is before you start programming. When you need to add a feature, begin by writing the test. This isn't as backward as it sounds. By writing the test you are asking yourself what needs to be done to add the function. Writing the test also concentrates on the interface rather than the implementation (always a good thing). It also means you have a clear point at which you are done coding--when the test works. While I am an advocate of test-driven development now, I did not remember having been introduced to the concept when I originally read this book nearly 5 years ago. According to Amazon.com, this book was originally published on July 8, 1999. Is this the first published reference to test-first programming or is there something even earlier? ",
        "Best answer": "Jon Bently in Programming Pearls (originaly published 1986) does not specifically mention Test-First programming.   But in the chapter \"Writing Correct Programs\", he describes writing an algorithm by first defining the preconditions, invariants and postconditions, and in the next chapter describes an automated testing framework. It's not quite test-first, but he was definitely laying some of the groundwork. Also, CIO Magazine, March 1993, Bug Busters, by Lucie Juneau,  pg 84:  Test cases ... can be developed even   before any code has been written.    Ideally these cases are based on an   application's requirements ... If developers are given requirements-based tests before they begin to write code, they will design a product that can pass those tests ... \"  "
    },
    {
        "ID": "8236",
        "Question": "I just sent out emails to five local web design companies to my area asking to take drawings to HTML/CSS/jQuery. None of the ones who accepted the deal seem suitable to myself. Others rejected the offer because they wanted to 'provide an end-to-end solution' or are 'booked till June'. The local companies did not seem suitable to myself because my review process is this: goto their website, do a view-source. I'll see really weird things (contact us forms that go nowhere), really old things (mm_menu.js), and portfolios that are non-existent (aren't on the site, don't link anywhere, or otherwise). The company would like to hire as locally as they can rather than out-source to another country. Answers I'm looking for  Processes you use when searching for someone How you qualify their aptitude for the project Anything that you think I'm doing wrong, or should be doing also.  Answers I'm not looking for:  \"Hello sir please contact me we do everything for 10 dolla.\" My bud's great at this stuff, call him. example.com is the best for this.  ",
        "Best answer": "I might start out by searching for people on google, because if their own site isn't findable, I won't trust that mine would be.  Then I would want to see some portfolio sites, ideally ones where they use the technology I'm wanting to use. If they don't have any, they don't get to do the job unless they can show they clearly understand what I'm talking about and how to implement it.  Be ready to widen the net- there are a lot of decent designers and developers around so no point settling for someone weak just because they are very local rather than somewhat local. If I can't find anyone convincing that way, I might look the other way- find some sites that do something like what I am looking for and then find out who developed them.  "
    },
    {
        "ID": "8254",
        "Question": "I'm developing a statically- and strongly-typed, compiled language, and I'm revisiting the idea of whether to include function overloading as a language feature. I realized that I'm a little bit biased, coming mainly from a C[++|#] background. What are the most convincing arguments for and against including function overloading in a language?  EDIT: Is there nobody who has an opposing opinion? Bertrand Meyer (creator of Eiffel back in 1985/1986) calls method overloading this: (source)  a vanity mechanism that brings nothing to the semantic power of an O-O language, but hampers readability and complicates everyone's task  Now those are some sweeping generalizations, but he's a smart guy, so I think it's safe to say he could back them up if he needed to. In fact, he almost had Brad Abrams (one of the CLSv1 developers) convinced that .NET shouldn't support method overloading. (source) That's some powerful stuff. Can anyone shed some light on his thoughts, and whether his viewpoint is still justified 25 years later? ",
        "Best answer": "Function overloading is absolutely critical for C++-style template code.  If I have to use different function names for different types, I can't write generic code.  That would eliminate a large and heavily used part of the C++ library, and much of C++'s functionality. It's usually present in member function names.  A.foo() can call an entirely different function from B.foo(), but both functions are named foo.  It's present in operators, as + does different things when applied to integers and floating-point numbers, and it's often used as a string concatenation operator.  It seems odd not to allow it in regular functions as well. It enables the use of Common Lisp-style \"multimethods\", in which the exact function called depends on two data types.  If you haven't programmed in the Common Lisp Object System, try it before you call this useless.  It's vital for C++ streams. I/O without function overloading (or variadic functions, which are worse) would require a number of different functions, either to print values of different types or to convert values of different types to a common type (like String). Without function overloading, if I change the type of some variable or value I need to change every function that uses it.  It makes it much harder to refactor code. It makes it easier to use APIs when the user doesn't have to remember which type naming convention is in use, and the user can just remember standard function names. Without operator overloading, we'd have to label each function with the types it uses, if that base operation can be used on more than one type.  This is essentially Hungarian notation, the bad way of doing it. Overall, it makes a language far more usable. "
    },
    {
        "ID": "8257",
        "Question": "I'm currently writing a load test for a cache which should test how a cache will react to persistent requests. A colleague and I had differing opinions on how this load testing should be performed. I believe that a load test should be as random as possible. It should model real-world load as much as possible, and the way towards that is randomality. So I have created this random test as follows:  Test data is held in spreadsheets and is loaded into TestRunner objects at startup  The test data is not random  The load test will run 10 random TestRunners in individual Threads at the same time The object returned by the cache will be tested to make sure it is sensible, it is not thoroughly tested  Any tests that fail will be output at the end and each test has a unique ID to easily find failed tests  At random intervals, the cache will be cleared to model the real-world requirement of the cache being cleared at any time. The load test will run for a configurable amount of time  My colleague's idea of what a load test should do is:  Test data is held in spreadsheets and is loaded into TestRunner objects at startup All TestRunner objects are run in a sequential manner Each time the load test is run, it will run the TestRunner objects in the same order  Which methodology do you feel would produce the most reliable load test?  I personally think the random test will produce a more reliable load test as it will model real-world usage. It is not known what order requests will come in when this is released to production, so it should be tested with that unknown element taken into account. However, running all tests in the same sequence each time will make any failures reproducable, which is important during testing. ",
        "Best answer": "Do you have a way to reset the data once the test is run (if this is even necessary)? If so, what about running the non-random test first - to look for failures when run that way (and possible anomalies caused by the data itself)?  Then, after resetting (if necessary), run the random tests to reflect the real world load. "
    },
    {
        "ID": "8297",
        "Question": "My management just asked an unprecedented question in my (admittedly brief) history with the organization: \"What can we do to help you?\" Simultaneously, we're working several big projects for a fairly new client whose ability to push requirements around mid-project is legend. Developing for these guys is like tap dancing on quicksand. Seems like a prime opportunity to propose a shift to a more agile approach. The thing I know I'm going to get asked, and that I don't have any idea about, is how to quote/bid/bill for that sort of project. Do you go hourly? Do you bid a range of prices? Do you charge by the sprint? More generally, the aspect of the Agile Manifesto that reads \"We value customer collaboration over contract negotiation\" is GOING to scare my management. How do you value that in the real world of customers who want a lot for a little? ",
        "Best answer": "We have the same problem in my company.  There's a history of fixed-price, fixed-timeline projects, and our clients aren't generally very progressive. Regarding development with no up-front commitments, I've heard so many fundamentalist agilists say, \"I know it's hard, but you just need to push the benefits\", or, \"They might be skeptical but they'll see how well it went and come back to you next time\".  In some industries, maybe. In ours, that's a load of crap. I can't see any of our customers agreeing to just let us do our thing with no commitment on scope or price. What we've found is that it's not always necessary to change the way you quote/bid/bill customers for an agile project.  You can keep the agile process while sticking to your quote if you manage it properly. Quote the way you normally would (with padding), and set some boundaries around the scope of the project.  From that point on, follow your agile methodology:  Prioritise the work with the customer - develop the important stuff first Develop in small iterations, showing your progress Collaborate with the customer to make sure you're actually developing what they want Grow the spec as you write the software  But more importantly:  If a function turns out to be more complicated than what was originally requested, tell the customer immediately and make sure they're aware it will affect the timeline and/or price. Treat major (or even minor) changes as chargeable change requests.  You're still using Agile internally and getting the benefits, but the customer is seeing a more familiar fixed-price, fixed-timeline, fixed-scope project. Any changes cost money and blow out the time. The hardest part about this is setting the boundaries up front.  It's definitely not something that can be done by just your sales guy, BA, or project manager. You need an experienced developer in those meetings.  You need to nail down the areas that could cause problems and decide on expectations. "
    },
    {
        "ID": "8311",
        "Question": "This might be slightly off topic, but I'll risk it, as the site is about Programmers ! Programmers are good at constantly learning new programming languages, but how good are they at learning a new spoken language ? Have you taken up a foreign language (French/Spanish/etc) as an adult and mastered it? Was it easy? I ask because I have been trying to learn French for quite some time now, and I'm still at the annoying \"Je parle un peu de Française\" stage. I've attended two French courses, one where the majority of the class were programmers, and one where they weren't and the difference in ability was quite apparent. Does a mathematical / logical inclination hinder learning a spoken language where grammar is not in ones and zeros? Or am I just transferring blame instead of simply accepting that I am not good with languages. [It is important that you have not been taught the language in school, as early exposure really gives you the upper hand. I've picked up and got quite good at languages I've been exposed to under the age of 10.] ",
        "Best answer": "I find it easy and fun to learn new languages! The only reason I'm any good at programming is that I've got a strong inclination toward language. All human languages are fundamentally the same, and not even vast differences in phonology, grammar, and vocabulary can get in the way of the fact that all people work in basically the same way. I find it immensely rewarding to draw parallels between etymologies, to discover the underlying nature of what makes a language tick, and to learn how native speakers understand their own language. Not to mention that studying a wide variety of orthographies has given me great clerical accuracy, which is a big help in programming. However, your mileage may vary—I'm a programmer because I'm a linguist, not the other way round, and you can become proficient at programming in many different ways. Edit: Here are a few tips that I think can help programmers with language learning: Natural languages are not programming languages. Natural languages do not have rules, but they do have patterns. If you notice a pattern, don't claim it's a rule and then complain about all of the exceptions. Linguistics is not a black-and-white field. I've noticed that people of a technical mindset get caught up in whether they're \"correct\" and lose sight of the fact that it's more important to be understood. Natural speech has inherent meaning that transcends literalism. Learning a language is not about rote memorisation. No native speaker of Spanish says to himself \"voy, vas, va, vamos, vais, van\" to remember how to conjugate \"to go\". He just does it in running speech because he has developed a sense over time of what sounds right. Do not take a \"phrasebook\" approach to language learning: you will find yourself lost for the appropriate phrase because you won't be able to produce your own. Learning vocabulary is not the same as learning an API. Natural languages are redundant and compressible, and you can use this to your advantage as a student. If you pronounce or spell something wrong, chances are you will still be understood. Look up the etymologies of words to get a sense of their deeper meaning. Having a sense of the language as it was is just as important as knowing the language as it is. It's okay to make some mistakes. Step outside your comfort zone and experiment. Try to talk the way native speakers do. If you notice that you pronounce or articulate something differently, try to discern exactly how. If you don't understand everything someone says, it's okay to ask them to repeat themselves or explain. If you make a mistake, the worst that can happen is a misunderstanding, and if you're confident and outgoing then it turns into a funny situation rather than an awkward, embarrassing one. Have fun. "
    },
    {
        "ID": "8355",
        "Question": "The article \"Promiscuous Pairing and Beginner’s Mind\" (PDF) suggests that you put someone in the pair that knows the least about the particular area of the code base. It also suggests that you swap out the senior member of the pair every 90 minutes or so.  Not only will the newbies learn about that area of the code but they will also think differently compared to someone who already knows the area. Does anybody have experience with this strategy? Does it have any connection with reality? I found other questions about when to use pair programming and whether to accept a job where pair programming is required, but I didn't find any that are specifically about promiscuous pairing and this \"beginner's mind\" strategy. If you are unfamiliar with pair programming, there are interesting articles on Wikipedia and c2.com. ",
        "Best answer": "I think your question understates (perhaps, confuses) Pair Programming and Promiscuous Pairing. When you do pair programming and one of the programmers knows way more about the task at hand, the other programmer learns very quickly (the languages, the tools, the design or requirements of the product they're working on.  I do have experience with that and highly recommend it for bringing your co-workers or yourself up to speed. The idea of Promiscuous Pairing is when you have N programmers on the team and make all possible pairs out of them and rotate those pairs frequently, then such knowledge spreads throughout the team very quickly. "
    },
    {
        "ID": "8402",
        "Question": "I've had an argument recently with a fellow programmer. He was interviewing for a new position and was asked this question:  Give a sequence of numbers starting at   X and ending in Y but with one element   missing so N is Y-X-1, find the   missing element in O(N) or better.  Now, the answer is irrelevant here (but interesting). This started a discussion on whether this was even a good question to ask during an interview. One side: Algorithms are an inherit part of programming and a candidates ability to answer this question supports that this candidate will be a good programmer and be able to solve larger problems and can handle most programming tasks which are ultimately easy to understand and answer. Other side: Writing algorithms from scratch is rarely used in modern programming and therefore is irrelevant in the bigger question of whether the person will be a good programmer. A person could successfully answer this question yet still not be able to do more common programmings tasks. Your thoughts? Good interview question or not? ",
        "Best answer": "I agree with asking a algorithm question, but I disagree with insisting on a specific big-O quality level. Asking this sort of question is interesting to see how the person approaches the problem and what pitfalls they consider in their attempt, but unless they are writing something insanely incorrect or inefficient the actual detail of what they write is not as telling as the fact that they get through the problem solving / design steps in a coherent manner. I ask a similar question, but the people that I have had the best luck with after hire are the folks that gave flawed answers but had the correct idea in their approach. "
    },
    {
        "ID": "8415",
        "Question": "I'm looking for something that allows me to work out an algorithm on a computer similar to how I would do it on a whiteboard. At work, I have a huge whiteboard that I can use for this, but at home, I don't (and can't). This makes it difficult to design algorithms when I'm working on hobby projects. I think better when I use drawings as opposed to text, so pseudocode is no good. I guess what I'm looking for is something like a flowchart program, but that allows a less rigid approach to designing algorithms.  I'd like the program to be cross-platform (Mac, Linux), but I'll be happy even if it just runs on Macs. Free is preferred, but reasonably priced programs are good too. I would prefer programs that people have used and had experiences with, rather than whatever turned up on Google, as I know of some flowchart and mind-mapping software, but haven't really been satisfied with either. I realise that a computer isn't the best platform for doing this kind of stuff, but assume for a moment that it's the only medium I possess. ",
        "Best answer": "If I have to brainstorm some design stuff that takes more complexity than Notepad can cope with, I usually just use a pencil/pen and paper. "
    },
    {
        "ID": "8454",
        "Question": "I was having a chat with a coworker who is working on a polling app and framework. He was asking technical questions and I suggested he open source the application to get more quality opinions from developers who are interested in this problem and are willing to give it heavy though.  He has a different point of view which I think is still valid so I want to open this question for discussion here. He says he believes something like a polling framework should not be open sourced because it will reduce its security and validity as people reveal loopholes through which they can cheat. Can't say I completely disagree. I see a somewhat valid point there, but I always believed that solutions by a group of people are almost always better than a solution thought by a single person asking a small number of coworkers, no matter how smart that person is. Again I'm willing to accept that maybe some types of applications are different.  Does anyone have an argument in his favor? I'd really like to present your responses to him.  ",
        "Best answer": "In fact, being open source helps you to be more secure.  I personally believe that when a program began as closed source and is then first made open source, it often starts less secure for any users (through exposure of vulnerabilities), and over time (say a few years) it has the potential to be much more secure than a closed program. If the program began as open source software, the public scrutiny is more likely to improve its security before it's ready for use by significant numbers of users, but there are several caveats to this statement (it's not an ironclad rule). Just making a program open source doesn't suddenly make a program secure, and just because a program is open source does not guarantee security:  First, people have to actually review the code. This is one of the key points of debate - will people really review code in an open source project? All sorts of factors can reduce the amount of review: being a niche or rarely-used product (where there are few potential reviewers), having few developers, and use of a rarely-used computer language. Clearly, a program that has a single developer and no other contributors of any kind doesn't have this kind of review. On the other hand, a program that has a primary author and many other people who occasionally examine the code and contribute suggests that there are others reviewing the code (at least to create contributions). In general, if there are more reviewers, there's generally a higher likelihood that someone will identify a flaw - this is the basis of the \"many eyeballs\" theory. Note that, for example, the OpenBSD project continuously examines programs for security flaws, so the components in its innermost parts have certainly undergone a lengthy review. Since OSS/FS discussions are often held publicly, this level of review is something that potential users can judge for themselves.       One factor that can particularly reduce review likelihood is not actually being open source. Some vendors like to posture their \"disclosed source\" (also called \"source available\") programs as being open source, but since the program owner has extensive exclusive rights, others will have far less incentive to work \"for free\" for the owner on the code. Even open source licenses which have unusually asymmetric rights (such as the MPL) have this problem. After all, people are less likely to voluntarily participate if someone else will have rights to their results that they don't have (as Bruce Perens says, \"who wants to be someone else's unpaid employee?\"). In particular, since the reviewers with the most incentive tend to be people trying to modify the program, this disincentive to participate reduces the number of \"eyeballs\". Elias Levy made this mistake in his article about open source security; his examples of software that had been broken into (e.g., TIS's Gauntlet) were not, at the time, open source. Second, at least some of the people developing and reviewing the code must know how to write secure programs. Hopefully the existence of this book will help. Clearly, it doesn't matter if there are \"many eyeballs\" if none of the eyeballs know what to look for. Note that it's not necessary for everyone to know how to write secure programs, as long as those who do know how are examining the code changes. Third, once found, these problems need to be fixed quickly and their fixes distributed. Open source systems tend to fix the problems quickly, but the distribution is not always smooth. For example, the OpenBSD developers do an excellent job of reviewing code for security flaws - but they don't always report the identified problems back to the original developer. Thus, it's quite possible for there to be a fixed version in one system, but for the flaw to remain in another. I believe this problem is lessening over time, since no one \"downstream\" likes to repeatedly fix the same problem. Of course, ensuring that security patches are actually installed on end-user systems is a problem for both open source and closed source software.  Another advantage of open source is that, if you find a problem, you can fix it immediately. This really doesn't have any counterpart in closed source. In short, the effect on security of open source software is still a major debate in the security community, though a large number of prominent experts believe that it has great potential to be more secure.  Look at Linux... "
    },
    {
        "ID": "8560",
        "Question": "I'm currently working on throwing together some basic prototypes, partly to gather requirements and partly to design the final UI.  At the moment I'm trying building up the screen using Post-it notes, with yellow notes for information and pink for actions (buttons or menus). The idea being that you can easily move, remove and add information. But I'm sure there are more efficient methods out there.  What is the recommended way for developers to efficiently create non-interactive UI prototypes? And why?  I tried some pen, paper and Post-it note versions and they went down like a lead balloon (likely my drawing skills). In the end I used Balsamiq, which thus far is liked by most users and they get it is prototype. Alas, some people are still having problems with the idea that first one should get an idea of what the application should do via some lo-fi prototypes and really want to \"see something on the screen\" before committing to anything. ",
        "Best answer": "I prefer a whiteboard. It makes it easy to change as you make decisions without redrawing the whole thing.  It's easy to share with other (nearby) developers.  It's easy to annotate using sticky-notes or other colors. "
    },
    {
        "ID": "8565",
        "Question": "Loose coupling is, to some developers, the holy grail of well-engineered software.  It's certainly a good thing when it makes code more flexible in the face of changes that are likely to occur in the foreseeable future, or avoids code duplication.   On the other hand, efforts to loosely couple components increase the amount of indirection in a program, thus increasing its complexity, often making it more difficult to understand and often making it less efficient.   Do you consider a focus on loose coupling without any use cases for the loose coupling (such as avoiding code duplication or planning for changes that are likely to occur in the foreseeable future) to be an anti-pattern?  Can loose coupling fall under the umbrella of YAGNI? ",
        "Best answer": "Is programming practice X good or bad?  Clearly, the answer is always \"it depends.\" If you're looking at your code, wondering what \"patterns\" you can inject, then you're doing it wrong. If you are building your software so that unrelated objects don't fiddle around with each other, then you're doing it right. If you're \"engineering\" your solution so that it can be infinitely extended and changed, then you're actually making it more complicated. I think at the end of the day, you're left with the single truth: is it more or less complicated to have the objects decoupled?  If it is less complicated to couple them, then that is the correct solution.  If it is less complicated to decouple them, then that is the right solution. (I am presently working in a fairly small codebase that does a simple job in a very complicated way, and part of what makes it so complicated is the lack of understanding of the terms \"coupling\" and \"cohesion\" on the part of the original developers.) "
    },
    {
        "ID": "8660",
        "Question": "We're integrating Mercurial slowly in our office and doing web-development we started using named branches.   We haven't quite found a good convention as far as naming our branches though.   We tried:   FeatureName (Can see this causing problem down the line) DEVInitial_FeatureName (Could get confusing when developer come and go down the line) {uniqueID (int)}_Feature  So far the uniqueID_featureName is winning, we are thinking of maintaining it in a small DB just for reference. It would have: branchID(int), featureName(varchar), featureDescription(varchar), date, who etc... This would give us branches like: 1_NewWhizBangFeature, 2_NowWithMoreFoo, ... and we would have an easy reference as to what that branch does without having to check the log. Any better solution out there? ",
        "Best answer": "If you don't have an issue tracker, I recommend setting one up and then using {issue tracker name}_{ticket number}. When someone years from now files a bug and you don't know exactly how the feature was supposed to work, it'll be easy to annotate the file and get back to where the user may have requested that exact functionality. "
    },
    {
        "ID": "8721",
        "Question": "Here's a bit information about me, before starting with the question. I am a Computer Science Undergraduate, Java being my primary coding language. The basic problem in my University are the teaching standards. No one is concerned about teaching coding knowledge to students, rather than just theoretical knowledge. The effect being, most of my fellow college mates don't understand programming at all. Even I haven't been able to come out of the traditional programming environment, which limits my coding to an extent. What are the possible ways by which I can develop and expand my programming/coding skills. Also, can you suggest the sources for the same? Edited: Sources suggesting development of coding skills. ",
        "Best answer": "My favorite quote is from Confucius:  I hear, I know. I see, I remember. I   do, I understand.  All knowledge I got, was from applying one and single strategy:  Take the most challenging path, always.  You want to learn C#? Get a job as a C# developer. You want to learn Italian? Go there with a dictionnary english/italian, and talk Italian You want to learn coding ? Code! "
    },
    {
        "ID": "8748",
        "Question": "Quote from Wikipedia of the article \"High-level programming language\":  A high-level programming language is a programming language with strong abstraction from the details of the computer. In comparison to low-level programming languages, it may use natural language elements, be easier to use, or be more portable across platforms. Such languages hide the details of CPU operations such as memory access models and management of scope.  I.e., as the level of programming language increases, the further away the programmer goes from the hardware on which the program runs. Now I do not know the statistics of language usages across levels, but I would suspect that the higher level languages more and more are replacing the lower level languages. If so, can this lead to a deficit of programmers with computer architecture knowledge? Would this be a problem for the industry? ",
        "Best answer": "It can, but likely won't lead to a problem. It's just economics.  If the vast majority of people lose the ability to understand the underlying architecture, and there is still a huge NEED to understand the underlying architecture, then the ones who do will have jobs and get paid more, while those who don't will only have jobs where that is not needed (and may still get paid more...who knows?). Is it helpful to know? Absolutely. You'll likely be better.  Is it necessary in most cases?  No.  That's why abstraction is so great, we stand on the shoulders of giants without having to be giants ourselves (but there will always be giants around). "
    },
    {
        "ID": "8890",
        "Question": "So the other day my boss (let's call him Colfax) asked me to work on a project, but that I should not discuss it with anyone (including the other programmers on my team) and that it would have to be done outside of normal work hours. In exchange, Colfax offered me \"off-the-book\" vacation days equal to the hours spent on the project. When I asked Colfax if his boss (let's call him Schuyler) knew what he was offering, Colfax said that Schuyler does not know and implied that he (Colfax) would get into trouble if Schuyler found out. My boss also said that if I were to go along with this, my efforts would be remembered for \"future consideration\".  The work is for our employer so everything there is on the up-and-up. However, I have an uneasy feeling about the whole thing. Part of me wants to work on the project -- as it's you know -- coding and coding something cool and fairly simple. On the other hand, the whole thing seems seedy and underhanded.  Would I be a \"bad employee\" for refusing extra work? Or am I morally justified to not do the work?  UPDATE I know it's been a while since I posted this question, but I thought the folks who participated in the discussion might be interested to know that Colfax quit a couple of months after this conversation. So, if I had followed along, it would have probably been for nothing. Regardless, thanks for the comments everyone. ",
        "Best answer": "If you have serious doubts about whether something is ethical, it's probably not.  That inner voice is there for a reason; listen to it. A real red flag should be the boss offering you vacation days \"off the book.\"  That could easily be interpreted as a confession of intent to commit accounting fraud, which is a pretty serious no-no.  Plus, if the boss is willing to hide things from his boss, how do you know he's not willing to hide things from you?  You could be getting set up for something here. Time to take this to someone with authority.  If what he's asking you to do is flat-out illegal, go to the police or the FBI.  Otherwise, go to Schuyler and explain what's going on.  You won't regret it. "
    },
    {
        "ID": "8917",
        "Question": "I've been using SVN for sometime and have been making an awkward, but soon to be rewarding transition over to git. Theres quite a few side / small projects that I'm working on which 90% will never see the light of day. As well - I also have my weekly school assignments / projects, and finally client projects that I have on the go. I've been rolling the idea or question of how or what the best way would be to back up my projects. The solutions I've sought out so far: github which offers an outstanding service - I'm ok with my work being open source, however It's the school work and client work I might not want the public to be open to. github, of course has a payment plan - but let's face it, im a poor colleage student doing what I can to at least put KD on the table! Theres the USB thumbstick that I can take around with me, work off of or back up, but that requires it to be around constently. Theres Dropbox, but thats a little overkill since it already is a form of a version control system, as well - I have quite a few things on the go - putting everything within dropbox would eat a lot of space Finally, theres a private server. I have a mac mini setup as a media server and I was thinking that I could backup to that. Anyways - what are your guys thoughts, how do you handle backing up projects, not only for version control but for the everyday backups. ",
        "Best answer": "If you have a private server with ssh access all you need to do is this: ssh my.server mkdir ~/repo.git && cd ~/repo.git git init --bare # bare repository that will serve as backup location exit # back on your box cd $my_local_git_repo git remote add --mirror backup ssh://my_server:repo.git git config remote.backup.fetch \":\" # don't replace our data when                                    # accidentally fetching from backup git push backup # copy everything new to backup repo  "
    }
]